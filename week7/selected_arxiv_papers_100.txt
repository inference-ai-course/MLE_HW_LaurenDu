================================================================================
ARXIV PAPER SELECTION - DIVERSE ACADEMIC PAPERS
================================================================================
Total Papers Selected: 100
Categories Covered: 19
================================================================================

PAPER #1
----------------------------------------
Title: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning
  Dataset and Comprehensive Benchmark
Link: http://arxiv.org/abs/2509.09680v1
Authors: Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li
Published: 2025-09-11T17:59:59Z
Categories: cs.CV, cs.CL

Abstract:
The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

================================================================================

PAPER #2
----------------------------------------
Title: ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable
  Orthogonal Butterfly Transforms
Link: http://arxiv.org/abs/2509.09679v1
Authors: Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang
Published: 2025-09-11T17:59:51Z
Categories: cs.LG, cs.AI, cs.CL

Abstract:
Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

================================================================================

PAPER #3
----------------------------------------
Title: The Illusion of Diminishing Returns: Measuring Long Horizon Execution in
  LLMs
Link: http://arxiv.org/abs/2509.09677v1
Authors: Akshit Sinha, Arvindh Arun, Shashwat Goel, Steffen Staab, Jonas Geiping
Published: 2025-09-11T17:59:34Z
Categories: cs.AI

Abstract:
Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

================================================================================

PAPER #4
----------------------------------------
Title: SpatialVID: A Large-Scale Video Dataset with Spatial Annotations
Link: http://arxiv.org/abs/2509.09676v1
Authors: Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, Xiaoxiao Long, Hao Zhu, Zhaoxiang Zhang, Xun Cao, Yao Yao
Published: 2025-09-11T17:59:31Z
Categories: cs.CV

Abstract:
Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.

================================================================================

PAPER #5
----------------------------------------
Title: SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning
Link: http://arxiv.org/abs/2509.09674v1
Authors: Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding
Published: 2025-09-11T17:59:17Z
Categories: cs.RO, cs.AI, cs.CL, cs.LG

Abstract:
Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

================================================================================

PAPER #6
----------------------------------------
Title: Locality in Image Diffusion Models Emerges from Data Statistics
Link: http://arxiv.org/abs/2509.09672v1
Authors: Artem Lukoianov, Chenyang Yuan, Justin Solomon, Vincent Sitzmann
Published: 2025-09-11T17:59:08Z
Categories: cs.CV

Abstract:
Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.

================================================================================

PAPER #7
----------------------------------------
Title: Dexplore: Scalable Neural Control for Dexterous Manipulation from
  Reference-Scoped Exploration
Link: http://arxiv.org/abs/2509.09671v1
Authors: Sirui Xu, Yu-Wei Chao, Liuyu Bian, Arsalan Mousavian, Yu-Xiong Wang, Liang-Yan Gui, Wei Yang
Published: 2025-09-11T17:59:07Z
Categories: cs.RO, cs.CV

Abstract:
Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

================================================================================

PAPER #8
----------------------------------------
Title: Can Understanding and Generation Truly Benefit Together -- or Just
  Coexist?
Link: http://arxiv.org/abs/2509.09666v1
Authors: Zhiyuan Yan, Kaiqing Lin, Zongjian Li, Junyan Ye, Hui Han, Zhendong Wang, Hao Liu, Bin Lin, Hao Li, Xue Xu, Xinyan Xiao, Jingdong Wang, Haifeng Wang, Li Yuan
Published: 2025-09-11T17:57:59Z
Categories: cs.CV

Abstract:
In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising "aha moment" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.

================================================================================

PAPER #9
----------------------------------------
Title: Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for
  Medicaid Care Management
Link: http://arxiv.org/abs/2509.09655v1
Authors: Sanjay Basu, Sadiq Y. Patel, Parth Sheth, Bhairavi Muralidharan, Namrata Elamaran, Aakriti Kinra, Rajaie Batniji
Published: 2025-09-11T17:50:06Z
Categories: cs.LG, cs.AI, cs.LO, stat.AP

Abstract:
We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

================================================================================

PAPER #10
----------------------------------------
Title: Retrieval-Augmented Generation for Reliable Interpretation of Radio
  Regulations
Link: http://arxiv.org/abs/2509.09651v1
Authors: Zakaria El Kassimi, Fares Fourati, Mohamed-Slim Alouini
Published: 2025-09-11T17:43:42Z
Categories: cs.IR, cs.AI, cs.CL, cs.LG, eess.SP

Abstract:
We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.

================================================================================

PAPER #11
----------------------------------------
Title: All for One: LLMs Solve Mental Math at the Last Token With Information
  Transferred From Other Tokens
Link: http://arxiv.org/abs/2509.09650v1
Authors: Siddarth Mamidanna, Daking Rai, Ziyu Yao, Yilun Zhou
Published: 2025-09-11T17:41:29Z
Categories: cs.CL, I.2.7

Abstract:
Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

================================================================================

PAPER #12
----------------------------------------
Title: CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype
Link: http://arxiv.org/abs/2509.09638v1
Authors: Amitabh Chakravorty, Jess Kropczynski, Nelly Elsayed
Published: 2025-09-11T17:25:06Z
Categories: cs.CR, cs.HC

Abstract:
With the widespread adoption of cryptocurrencies, cryptojacking has become a
significant security threat to crypto wallet users. This paper presents a
front-end prototype of an AI-powered security dashboard, namely, CryptoGuard.
Developed through a user-centered design process, the prototype was constructed
as a high-fidelity, click-through model from Figma mockups to simulate key user
interactions. It is designed to assist users in monitoring their login and
transaction activity, identifying any suspicious behavior, and enabling them to
take action directly within the wallet interface. The dashboard is designed for
a general audience, prioritizing an intuitive user experience for non-technical
individuals. Although its AI functionality is conceptual, the prototype
demonstrates features like visual alerts and reporting. This work is positioned
explicitly as a design concept, bridging cryptojacking detection research with
human-centered interface design. This paper also demonstrates how usability
heuristics can directly inform a tool's ability to support rapid and confident
decision-making under real-world threats. This paper argues that practical
security tools require not only robust backend functionality but also a
user-centric design that communicates risk and empowers users to take
meaningful action.

================================================================================

PAPER #13
----------------------------------------
Title: DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for
  Low-Latency Zero-Shot Text-To-Speech
Link: http://arxiv.org/abs/2509.09631v1
Authors: Ngoc-Son Nguyen, Hieu-Nghia Huynh-Nguyen, Thanh V. T. Tran, Truong-Son Hy, Van Nguyen
Published: 2025-09-11T17:16:52Z
Categories: cs.SD, cs.CL, cs.CV

Abstract:
Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that
mimics the voice of an unseen speaker using only a short reference sample,
requiring not only speaker adaptation but also accurate modeling of prosodic
attributes. Recent approaches based on language models, diffusion, and flow
matching have shown promising results in zero-shot TTS, but still suffer from
slow inference and repetition artifacts. Discrete codec representations have
been widely adopted for speech synthesis, and recent works have begun to
explore diffusion models in purely discrete settings, suggesting the potential
of discrete generative modeling for speech synthesis. However, existing
flow-matching methods typically embed these discrete tokens into a continuous
space and apply continuous flow matching, which may not fully leverage the
advantages of discrete representations. To address these challenges, we
introduce DiFlow-TTS, which, to the best of our knowledge, is the first model
to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS
explicitly models factorized speech attributes within a compact and unified
architecture. It leverages in-context learning by conditioning on textual
content, along with prosodic and acoustic attributes extracted from a reference
speech, enabling effective attribute cloning in a zero-shot setting. In
addition, the model employs a factorized flow prediction mechanism with
distinct heads for prosody and acoustic details, allowing it to learn
aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS
achieves promising performance in several key metrics, including naturalness,
prosody, preservation of speaker style, and energy control. It also maintains a
compact model size and achieves low-latency inference, generating speech up to
25.8 times faster than the latest existing baselines.

================================================================================

PAPER #14
----------------------------------------
Title: I Know Who Clones Your Code: Interpretable Smart Contract Similarity
  Detection
Link: http://arxiv.org/abs/2509.09630v1
Authors: Zhenguang Liu, Lixun Ma, Zhongzheng Mu, Chengkun Wei, Xiaojun Xu, Yingying Jiao, Kui Ren
Published: 2025-09-11T17:15:51Z
Categories: cs.SE, cs.CR

Abstract:
Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

================================================================================

PAPER #15
----------------------------------------
Title: LoCoBench: A Benchmark for Long-Context Large Language Models in Complex
  Software Engineering
Link: http://arxiv.org/abs/2509.09614v1
Authors: Jielin Qiu, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Jianguo Zhang, Haolin Chen, Shiyu Wang, Ming Zhu, Liangwei Yang, Juntao Tan, Zhepeng Cen, Cheng Qian, Shelby Heinecke, Weiran Yao, Silvio Savarese, Caiming Xiong, Huan Wang
Published: 2025-09-11T16:55:04Z
Categories: cs.SE, cs.AI

Abstract:
The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

================================================================================

PAPER #16
----------------------------------------
Title: Unsteady gas dynamics modeling for leakage detection in parallel
  pipelines
Link: http://arxiv.org/abs/2509.09612v1
Authors: Ilgar G. Aliyev, Konul Gafarbayli, Ahad Mammadov, Firangiz Mammadrazayeva
Published: 2025-09-11T16:53:52Z
Categories: math.OC, 76N25, 76M60, 93C20, 90B06, G.1.6; I.2.8; C.3

Abstract:
This study presents a novel analytical framework for modeling unsteady gas
dynamics in parallel pipeline systems under leakage conditions. The proposed
method introduces a time-dependent leakage mass flow rate function, which
dynamically captures the temporal decay of leakage based on real-time inlet
pressure measurements. This functional form allows for a more physically
consistent and mathematically tractable representation of gas loss compared to
conventional constant-rate or stepwise models. The pipeline system is
partitioned into three regions relative to the leakage point, and closed-form
pressure solutions are derived using Laplace transform techniques. These
expressions enable direct estimation of the leakage location through inverse
pressure profiles, eliminating the need for computationally intensive iterative
schemes. The analytical model is further validated against representative
benchmark scenarios, demonstrating good agreement with literature-based
results. A comparative analysis underscores the model's ability to localize
leakage using minimal sensor data while preserving interpretability - an
essential feature for deployment in industrial environments. The approach
provides a lightweight yet robust alternative to purely numerical or machine
learning-based solutions and offers potential integration into real-time
monitoring systems. This work contributes to the field by unifying gas dynamic
principles, sensor-assisted modeling, and analytical solution strategies to
enhance the reliability and speed of leak detection in modern gas transport
infrastructures.

================================================================================

PAPER #17
----------------------------------------
Title: LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death
  Determination
Link: http://arxiv.org/abs/2509.09602v1
Authors: Yiqun T. Chen, Tyler H. McCormick, Li Liu, Abhirup Datta
Published: 2025-09-11T16:42:22Z
Categories: cs.CL, stat.AP

Abstract:
Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

================================================================================

PAPER #18
----------------------------------------
Title: Iterative energy reduction Galerkin methods and variational adaptivity
Link: http://arxiv.org/abs/2509.09600v1
Authors: Pascal Heid, Thomas P. Wihler
Published: 2025-09-11T16:40:41Z
Categories: math.NA, cs.NA, 35A15, 35B38, 65J15, 65M50, 65N30

Abstract:
Critical points of energy functionals, which are of broad interest, for
instance, in physics and chemistry, in solid and quantum mechanics, in material
science, or in general diffusion-reaction models arise as solutions to the
associated Euler-Lagrange equations. While classical computational solution
methods for such models typically focus solely on the underlying partial
differential equations, we propose an approach that also incorporates the
energy structure itself. Specifically, we examine (linearized) iterative
Galerkin discretization schemes that ensure energy reduction at each step.
Additionally, we provide necessary conditions, which are applicable to a wide
class of problems, that guarantee convergence to critical points of the PDE.
Moreover, in the specific context of finite element discretizations, we present
a very generally applicable adaptive mesh refinement strategy - the so-called
variational adaptivity approach - which, rather than using classical a
posteriori estimates, is based on exploiting local energy reductions. The
theoretical results are validated for several computational experiments in the
context of nonlinear diffusion-reaction models, thereby demonstrating the
effectiveness of the proposed scheme.

================================================================================

PAPER #19
----------------------------------------
Title: Conditioning on PDE Parameters to Generalise Deep Learning Emulation of
  Stochastic and Chaotic Dynamics
Link: http://arxiv.org/abs/2509.09599v1
Authors: Ira J. S. Shokar, Rich R. Kerswell, Peter H. Haynes
Published: 2025-09-11T16:37:45Z
Categories: cs.LG, math.DS, nlin.CD, physics.ao-ph

Abstract:
We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

================================================================================

PAPER #20
----------------------------------------
Title: Graph Alignment via Dual-Pass Spectral Encoding and Latent Space
  Communication
Link: http://arxiv.org/abs/2509.09597v1
Authors: Maysam Behmanesh, Erkan Turan, Maks Ovsjanikov
Published: 2025-09-11T16:36:16Z
Categories: cs.LG, cs.AI, cs.CV

Abstract:
Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

================================================================================

PAPER #21
----------------------------------------
Title: ObjectReact: Learning Object-Relative Control for Visual Navigation
Link: http://arxiv.org/abs/2509.09594v1
Authors: Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid
Published: 2025-09-11T16:34:17Z
Categories: cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY

Abstract:
Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

================================================================================

PAPER #22
----------------------------------------
Title: Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset
  Collector
Link: http://arxiv.org/abs/2509.09592v1
Authors: Aditya Kulkarni, Shahil Manishbhai Patel, Shivam Pradip Tirmare, Vivek Balachandran, Tamal Das
Published: 2025-09-11T16:30:12Z
Categories: cs.CR

Abstract:
To combat phishing attacks -- aimed at luring web users to divulge their
sensitive information -- various phishing detection approaches have been
proposed. As attackers focus on devising new tactics to bypass existing
detection solutions, researchers have adapted by integrating machine learning
and deep learning into phishing detection. Phishing dataset collection is vital
to developing effective phishing detection approaches, which highly depend on
the diversity of the gathered datasets. The lack of diversity in the dataset
results in a biased model. Since phishing websites are often short-lived,
collecting them is also a challenge. Consequently, very few phishing webpage
dataset repositories exist to date. No single repository comprehensively
consolidates all phishing elements corresponding to a phishing webpage, namely,
URL, webpage source code, screenshot, and related webpage resources. This paper
introduces a resource collection tool designed to gather various resources
associated with a URL, such as CSS, Javascript, favicons, webpage images, and
screenshots. Our tool leverages PhishTank as the primary source for obtaining
active phishing URLs. Our tool fetches several additional webpage resources
compared to PyWebCopy Python library, which provides webpage content for a
given URL. Additionally, we share a sample dataset generated using our tool
comprising 4,056 legitimate and 5,666 phishing URLs along with their associated
resources. We also remark on the top correlated phishing features with their
associated class label found in our dataset. Our tool offers a comprehensive
resource set that can aid researchers in developing effective phishing
detection approaches.

================================================================================

PAPER #23
----------------------------------------
Title: Measuring football fever through wearable technology: A case study on
  the German cup final
Link: http://arxiv.org/abs/2509.09569v1
Authors: Timo Adam, Jonas Bauer, Christian Deutscher, Christiane Fuchs, Tamara Schamberger, David Winkelmann
Published: 2025-09-11T16:03:11Z
Categories: stat.AP

Abstract:
Football is the world's most popular sport, evoking strong physiological and
emotional responses among its fans. Yet, the specific dynamics of fan
attachment to matches have received little attention in the literature. In this
paper, we quantify these dynamics through a unique case study from professional
football: the 2025 cup final of the German Football Association (DFB) between
first-division club VfB Stuttgart and third-division club Arminia Bielefeld. We
collected high-resolution smartwatch data, including heart rate and stress
level, from 229 Arminia Bielefeld fans over approximately 12 weeks,
complemented by survey responses on club attachment, match attendance, and
personal characteristics from a subset of 37 participants. By combining
physiological data with survey information, we analyse variations in emotional
engagement across individuals and contexts, as well as physiological reactions
to key match events. This approach provides rare, data-driven insights into the
football fever that captivates fans during high-stakes competitions.
Furthermore, we compare the vital parameters recorded on the day of the match
with baseline levels on non-matchdays throughout the entire observation period.
Our findings reveal pronounced physiological responses among fans, beginning
hours before the match and peaking at kick-off.

================================================================================

PAPER #24
----------------------------------------
Title: What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion
  Detection Datasets
Link: http://arxiv.org/abs/2509.09564v1
Authors: Meghan Wilkinson, Robert H Thomson
Published: 2025-09-11T15:55:21Z
Categories: cs.CR, cs.LG

Abstract:
Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

================================================================================

PAPER #25
----------------------------------------
Title: An improved educational competition optimizer with multi-covariance
  learning operators for global optimization problems
Link: http://arxiv.org/abs/2509.09552v1
Authors: Baoqi Zhao, Xiong Yang, Hoileong Lee, Bowen Dong
Published: 2025-09-11T15:41:14Z
Categories: cs.NE, cs.AI, cs.CE

Abstract:
The educational competition optimizer is a recently introduced metaheuristic
algorithm inspired by human behavior, originating from the dynamics of
educational competition within society. Nonetheless, ECO faces constraints due
to an imbalance between exploitation and exploration, rendering it susceptible
to local optima and demonstrating restricted effectiveness in addressing
complex optimization problems. To address these limitations, this study
presents an enhanced educational competition optimizer (IECO-MCO) utilizing
multi-covariance learning operators. In IECO, three distinct covariance
learning operators are introduced to improve the performance of ECO. Each
operator effectively balances exploitation and exploration while preventing
premature convergence of the population. The effectiveness of IECO is assessed
through benchmark functions derived from the CEC 2017 and CEC 2022 test suites,
and its performance is compared with various basic and improved algorithms
across different categories. The results demonstrate that IECO-MCO surpasses
the basic ECO and other competing algorithms in convergence speed, stability,
and the capability to avoid local optima. Furthermore, statistical analyses,
including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test,
are conducted to validate the superiority of IECO-MCO over the compared
algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO
achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test
suites. Additionally, the practical applicability of the proposed IECO-MCO
algorithm is verified by solving constrained optimization problems. The
experimental outcomes demonstrate the superior performance of IECO-MCO in
tackling intricate optimization problems, underscoring its robustness and
practical effectiveness in real-world scenarios.

================================================================================

PAPER #26
----------------------------------------
Title: A Neuromorphic Incipient Slip Detection System using Papillae Morphology
Link: http://arxiv.org/abs/2509.09546v1
Authors: Yanhui Lu, Zeyu Deng, Stephen J. Redmond, Efi Psomopoulou, Benjamin Ward-Cherrier
Published: 2025-09-11T15:38:31Z
Categories: cs.RO

Abstract:
Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

================================================================================

PAPER #27
----------------------------------------
Title: Bioluminescence tomography: A new regularized shape optimization method
Link: http://arxiv.org/abs/2509.09533v1
Authors: Qianqian Wu, Rongfang Gong, Wei Gong, Ziyi Zhang, Shengfeng Zhu
Published: 2025-09-11T15:21:57Z
Categories: math.NA, cs.NA, math.OC

Abstract:
In this paper, we investigate an inverse source problem arising in
bioluminescence tomography (BLT), where the objective is to recover both the
support and intensity of the light source from boundary measurements. A shape
optimization framework is developed, in which the source strength and its
support are decoupled through first-order optimality conditions. To enhance the
stability of the reconstruction, we incorporate a parameter-dependent coupled
complex boundary method(CCBM) scheme together with perimeter and volume
regularizations. The level-set representation naturally accommodates
topological changes, enabling the reconstruction of multiple, closely located,
or nested sources. Theoretical justifications are provided, and a series of
numerical experiments are conducted to validate the proposed method. The
results demonstrate the robustness, accuracy, and noise-resistance of the
algorithm, as well as its advantages over existing approaches.

================================================================================

PAPER #28
----------------------------------------
Title: Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts
Link: http://arxiv.org/abs/2509.09488v1
Authors: Felix Mächtle, Ashwath Shetty, Jonas Sander, Nils Loose, Sören Pirk, Thomas Eisenbarth
Published: 2025-09-11T14:21:59Z
Categories: cs.CR, cs.AI

Abstract:
Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

================================================================================

PAPER #29
----------------------------------------
Title: Minimality of Tree Tensor Network Ranks
Link: http://arxiv.org/abs/2509.09463v1
Authors: Jana Jovcheva, Tim Seynnaeve, Nick Vannieuwenhoven
Published: 2025-09-11T13:44:49Z
Categories: math.NA, cs.NA, 15A69 (Primary) 65F99 (Secondary)

Abstract:
For a given tree tensor network $G$, we call a tuple of bond dimensions
minimal if there exists a tensor $T$ that can be represented by this network
but not on the same tree topology with strictly smaller bond dimensions. We
establish necessary and sufficient conditions on the bond dimensions of a tree
tensor network to be minimal, generalizing a characterization of Carlini and
Kleppe about existence of tensors with a given multilinear rank. We also show
that in a minimal tree tensor network, the non-minimal tensors form a Zariski
closed subset, so minimality is a generic property in this sense.

================================================================================

PAPER #30
----------------------------------------
Title: Second-order Optimally Stable IMEX (pseudo-)staggered Galerkin
  discretization: application to lava flow modeling
Link: http://arxiv.org/abs/2509.09460v1
Authors: Federico Gatti, Giuseppe Orlando
Published: 2025-09-11T13:43:14Z
Categories: math.NA, cs.NA

Abstract:
We present second-order optimally stable Implicit-Explicit (IMEX) Runge-Kutta
(RK) schemes with application to a modified set of shallow water equations that
can be used to model the dynamics of lava flows. The schemes are optimally
stable in the sense that they satisfy, at the space-time discretization level,
a condition analogous to the \texttt{L}-stability of Runge-Kutta methods for
ordinary differential equations. A novel (pseudo-)staggered Galerkin scheme is
introduced, which can be interpreted as an extension of the classical two-step
Taylor-Galerkin (TG2) scheme. The method is derived by combining a von Neumann
stability analysis with a Lax-Wendroff procedure. For the discretization of the
non-conservative terms that characterize the lava flow model, we employ the
Path-Conservative (PC) method. The proposed scheme is evaluated on a number of
relevant test cases, demonstrating accuracy, robustness, and well-balancing
properties for the lava flow model.

================================================================================

PAPER #31
----------------------------------------
Title: Multipole and Berezinskii-Kosterlitz-Thouless Transitions in the
  Two-component Plasma
Link: http://arxiv.org/abs/2509.09449v1
Authors: Jeanne Boursier, Sylvia Serfaty
Published: 2025-09-11T13:31:40Z
Categories: math-ph, math.MP, math.PR, 82B26, 82B05

Abstract:
We study the two-dimensional two-component Coulomb gas in the canonical
ensemble and at inverse temperature $\beta>2$. In that regime, the partition
function diverges and the interaction needs to be cut off at a length scale
$\lambda>0$. Particles of opposite charges tend to pair into dipoles of length
scale comparable to $\lambda$, which themselves can aggregate into multipoles.
Despite the slow decay of dipole--dipole interactions, we construct a
convergent cluster expansion around a hierarchical reference model that retains
only intra-multipole interactions. This yields a large deviation result for the
number of $2p$-poles as well as a sharp free energy expansion as $N\to\infty$
and $\lambda\to0$ with three contributions: (i) the free energy of $N$
independent dipoles, (ii) a perturbative correction, and (iii) the contribution
of a non-dilute subsystem.
  The perturbative term has two equivalent characterizations: (a) a convergent
Mayer series obtained by expanding around an i.i.d.\ dipole model; and (b) a
variational formula as the minimum of a large-deviation rate function for the
empirical counts of $2p$-poles. The Mayer coefficients exhibit transitions at
$\beta_p=4-\tfrac{2}{p}$, that accumulate at $\beta=4$, which corresponds to
the Berezinskii-Kosterlitz-Thouless transition in the low-dipole-density limit.
At $\beta=\beta_p$ the $p$-dipole cluster integrals switch from non-integrable
to integrable tails.
  The non-dilute system corresponds to the contribution of large dipoles: we
exhibit a new critical length scale $R_{\beta, \lambda}$ which transitions from
$\lambda^{-(\beta-2)/(4-\beta)}$ to $+\infty$ as $\beta$ crosses the critical
inverse temperature $\beta=4$, and which can be interpreted as the maximal
scale such that the dipoles of that scale form a dilute set.

================================================================================

PAPER #32
----------------------------------------
Title: ENSI: Efficient Non-Interactive Secure Inference for Large Language
  Models
Link: http://arxiv.org/abs/2509.09424v1
Authors: Zhiyu He, Maojiang Wang, Xinwen Gao, Yuchuan Luo, Lin Liu, Shaojing Fu
Published: 2025-09-11T13:04:22Z
Categories: cs.CR, cs.AI

Abstract:
Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

================================================================================

PAPER #33
----------------------------------------
Title: A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for
  Real-Time Fatigue Awareness
Link: http://arxiv.org/abs/2509.09404v1
Authors: Tongshun Chen, Zezhou Sun, Yanhan Sun, Yuhao Wang, Dezhen Song, Ke Wu
Published: 2025-09-11T12:38:02Z
Categories: cs.RO

Abstract:
Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

================================================================================

PAPER #34
----------------------------------------
Title: Incorporating Fixed Pole Information in the Data-Driven Least Squares
  Realization Problem
Link: http://arxiv.org/abs/2509.09394v1
Authors: Christof Vermeersch, Sibren Lagauw, Bart De Moor
Published: 2025-09-11T12:22:30Z
Categories: math.OC

Abstract:
In practical least squares realization problems, partial information about
the poles of the dynamical model may be known a priori. Existing techniques for
incorporating prior knowledge, such as prefiltering the given data, are
typically heuristic and lack theoretical guarantees. We extend our previously
developed globally optimal approach for the least squares realization problem
to accommodate fixed poles. In particular, we reformulate the problem as a
(rectangular) multiparameter eigenvalue problem, the eigenvalues of which
characterize all local and global minimizers of the constrained estimation
problem. We present numerical examples to demonstrate the effectiveness of the
proposed method and experimentally validate this letter's central hypothesis:
incorporating a priori information on the poles enhances the estimation
results.

================================================================================

PAPER #35
----------------------------------------
Title: A preconditioned third-order implicit-explicit algorithm with a
  difference of varying convex functions and extrapolation
Link: http://arxiv.org/abs/2509.09391v1
Authors: Kelin Wu, Hongpeng Sun
Published: 2025-09-11T12:17:01Z
Categories: math.OC, cs.NA, math.NA

Abstract:
This paper proposes a novel preconditioned implicit-explicit algorithm
enhanced with the extrapolation technique for non-convex optimization problems.
The algorithm employs a third-order Adams-Bashforth scheme for the nonlinear
and explicit parts and a third-order backward differentiation formula for the
implicit part of the gradient flow in variational functions. The proposed
algorithm, akin to a generalized difference-of-convex (DC) approach, employs a
changing set of convex functions in each iteration. Under the Kurdyka-\L
ojasiewicz (KL) properties, the global convergence of the algorithm is
guaranteed, ensuring that it converges within a finite number of preconditioned
iterations. Our numerical experiments, including least squares problems with
SCAD regularization and the graphical Ginzburg-Landau model, demonstrate the
proposed algorithm's highly efficient performance compared to conventional DC
algorithms.

================================================================================

PAPER #36
----------------------------------------
Title: Thermodynamic coprocessor for linear operations with
  input-size-independent calculation time based on open quantum system
Link: http://arxiv.org/abs/2509.09382v1
Authors: I. V. Vovchenko, A. A. Zyablovsky, A. A. Pukhov, E. S. Andrianov
Published: 2025-09-11T11:58:47Z
Categories: quant-ph, cond-mat.dis-nn, physics.optics

Abstract:
Linear operations, e.g., vector-matrix or vector-vector multiplications, are
core operations of modern neural networks. To diminish computational time,
these operations are implemented by parallel computations using different
coprocessors. In this work we show that open quantum system consisting of
bosonic modes and interacting with bosonic reservoirs can be used as analog
coprocessor implementing multiple vector-matrix multiplications with stochastic
matrices in parallel. Input vectors are encoded in occupancies of reservoirs,
and output result is presented by stationary energy flows. The operation takes
time needed for the system's transition to non-equilibrium stationary state
independently on number of the reservoirs, i.e., on the input vector dimension.
The computations are accompanied by entropy growth. We construct a direct
mapping between open quantum systems and electrical crossbar structures,
showing that dissipation rates multiplied by OQS's modes frequencies can be
seen as conductivities, reservoirs' occupancies can be seen as potentials, and
stationary energy flows can be seen as electric currents.

================================================================================

PAPER #37
----------------------------------------
Title: Robust Non-Linear Correlations via Polynomial Regression
Link: http://arxiv.org/abs/2509.09380v1
Authors: Luca Giuliani, Michele Lombardi
Published: 2025-09-11T11:55:48Z
Categories: cs.LG, cs.AI, cs.NA, math.NA

Abstract:
The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

================================================================================

PAPER #38
----------------------------------------
Title: VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action
  Model
Link: http://arxiv.org/abs/2509.09372v1
Authors: Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, Donglin Wang
Published: 2025-09-11T11:42:21Z
Categories: cs.RO

Abstract:
Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

================================================================================

PAPER #39
----------------------------------------
Title: Expressive Power of Deep Networks on Manifolds: Simultaneous
  Approximation
Link: http://arxiv.org/abs/2509.09362v1
Authors: Hanfei Zhou, Lei Shi
Published: 2025-09-11T11:28:20Z
Categories: math.NA, cs.LG, cs.NA, stat.ML

Abstract:
A key challenge in scientific machine learning is solving partial
differential equations (PDEs) on complex domains, where the curved geometry
complicates the approximation of functions and their derivatives required by
differential operators. This paper establishes the first simultaneous
approximation theory for deep neural networks on manifolds. We prove that a
constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property
that plays a crucial role in controlling generalization error--can approximate
any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an
error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for
$k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero
parameters, a rate that overcomes the curse of dimensionality by depending only
on the intrinsic dimension $d$. These results readily extend to functions in
H\"older-Zygmund spaces. We complement this result with a matching lower bound,
proving our construction is nearly optimal by showing the required number of
parameters matches up to a logarithmic factor. Our proof of the lower bound
introduces novel estimates for the Vapnik-Chervonenkis dimension and
pseudo-dimension of the network's high-order derivative classes. These
complexity bounds provide a theoretical cornerstone for learning PDEs on
manifolds involving derivatives. Our analysis reveals that the network
architecture leverages a sparse structure to efficiently exploit the manifold's
low-dimensional geometry.

================================================================================

PAPER #40
----------------------------------------
Title: Turnpike properties for zero-sum stochastic linear quadratic
  differential games of Markovian regime switching system
Link: http://arxiv.org/abs/2509.09358v1
Authors: Xun Li, Fan Wu, Xin Zhang
Published: 2025-09-11T11:12:57Z
Categories: math.OC

Abstract:
This paper investigates the long-time behavior of zero-sum stochastic
linear-quadratic (SLQ) differential games within Markov regime-switching
diffusion systems and establishes the turnpike property of the optimal triple.
By verifying the convergence of the associated coupled differential Riccati
equations (CDREs) along with their convergence rate, we show that, for a
sufficiently large time horizon, the equilibrium strategy in the finite-horizon
problem can be closely approximated by that of the infinite-horizon problem.
Furthermore, this study enhances and extends existing results concerning
zero-sum SLQ differential games over both finite and infinite horizons.

================================================================================

PAPER #41
----------------------------------------
Title: Low-degree lower bounds via almost orthonormal bases
Link: http://arxiv.org/abs/2509.09353v1
Authors: Alexandra Carpentier, Simone Maria Giancola, Christophe Giraud, Nicolas Verzelen
Published: 2025-09-11T11:07:36Z
Categories: stat.ML, cs.LG

Abstract:
Low-degree polynomials have emerged as a powerful paradigm for providing
evidence of statistical-computational gaps across a variety of high-dimensional
statistical models [Wein25]. For detection problems -- where the goal is to
test a planted distribution $\mathbb{P}'$ against a null distribution
$\mathbb{P}$ with independent components -- the standard approach is to bound
the advantage using an $\mathbb{L}^2(\mathbb{P})$-orthonormal family of
polynomials. However, this method breaks down for estimation tasks or more
complex testing problems where $\mathbb{P}$ has some planted structures, so
that no simple $\mathbb{L}^2(\mathbb{P})$-orthogonal polynomial family is
available. To address this challenge, several technical workarounds have been
proposed [SW22,SW25], though their implementation can be delicate. In this
work, we propose a more direct proof strategy. Focusing on random graph models,
we construct a basis of polynomials that is almost orthonormal under
$\mathbb{P}$, in precisely those regimes where statistical-computational gaps
arise. This almost orthonormal basis not only yields a direct route to
establishing low-degree lower bounds, but also allows us to explicitly identify
the polynomials that optimize the low-degree criterion. This, in turn, provides
insights into the design of optimal polynomial-time algorithms. We illustrate
the effectiveness of our approach by recovering known low-degree lower bounds,
and establishing new ones for problems such as hidden subcliques, stochastic
block models, and seriation models.

================================================================================

PAPER #42
----------------------------------------
Title: [Extended] Ethics in Computer Security Research: A Data-Driven
  Assessment of the Past, the Present, and the Possible Future
Link: http://arxiv.org/abs/2509.09351v1
Authors: Harshini Sri Ramulu, Helen Schmitt, Bogdan Rerich, Rachel Gonzalez Rodriguez, Tadayoshi Kohno, Yasemin Acar
Published: 2025-09-11T11:06:56Z
Categories: cs.CR

Abstract:
Ethical questions are discussed regularly in computer security. Still,
researchers in computer security lack clear guidance on how to make, document,
and assess ethical decisions in research when what is morally right or
acceptable is not clear-cut. In this work, we give an overview of the
discussion of ethical implications in current published work in computer
security by reviewing all 1154 top-tier security papers published in 2024,
finding inconsistent levels of ethics reporting with a strong focus of
reporting institutional or ethics board approval, human subjects protection,
and responsible disclosure, and a lack of discussion of balancing harms and
benefits. We further report on the results of a semi-structured interview study
with 24 computer security and privacy researchers (among whom were also:
reviewers, ethics committee members, and/or program chairs) and their ethical
decision-making both as authors and during peer review, finding a strong desire
for ethical research, but a lack of consistency in considered values, ethical
frameworks (if articulated), decision-making, and outcomes. We present an
overview of the current state of the discussion of ethics and current de-facto
standards in computer security research, and contribute suggestions to improve
the state of ethics in computer security research.

================================================================================

PAPER #43
----------------------------------------
Title: A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent
  and Independent Data under Preferential Sampling
Link: http://arxiv.org/abs/2509.09336v1
Authors: Daniela Silva, Raquel Menezes, Gonçalo Araújo, Ana Machado, Renato Rosa, Ana Moreno, Alexandra Silva, Susana Garrido
Published: 2025-09-11T10:40:36Z
Categories: stat.AP, stat.ME

Abstract:
Sustainable management of marine ecosystems is vital for maintaining healthy
fishery resources, and benefits from advanced scientific tools to accurately
assess species distribution patterns. In fisheries science, two primary data
sources are used: fishery-independent data (FID), collected through systematic
surveys, and fishery-dependent data (FDD), obtained from commercial fishing
activities. While these sources provide complementary information, their
distinct sampling schemes - systematic for FID and preferential for FDD - pose
significant integration challenges. This study introduces a novel
spatio-temporal model that integrates FID and FDD, addressing challenges
associated with zero-inflation and preferential sampling (PS) common in
ecological data. The model employs a six-layer structure to differentiate
between presence-absence and biomass observations, offering a robust framework
for ecological studies affected by PS biases. Simulation results demonstrate
the model's accuracy in parameter estimation across diverse PS scenarios and
its ability to detect preferential signals. Application to the study of the
distribution patterns of the European sardine populations along the southern
Portuguese continental shelf illustrates the model's effectiveness in
integrating diverse data sources and incorporating environmental and
vessel-specific covariates. The model reveals spatio-temporal variability in
sardine presence and biomass, providing actionable insights for fisheries
management. Beyond ecology, this framework offers broad applicability to data
integration challenges in other disciplines.

================================================================================

PAPER #44
----------------------------------------
Title: On the Security of SSH Client Signatures
Link: http://arxiv.org/abs/2509.09331v1
Authors: Fabian Bäumer, Marcus Brinkmann, Maximilian Radoy, Jörg Schwenk, Juraj Somorovsky
Published: 2025-09-11T10:32:13Z
Categories: cs.CR

Abstract:
Administrators and developers use SSH client keys and signatures for
authentication, for example, to access internet backbone servers or to commit
new code on platforms like GitHub. However, unlike servers, SSH clients cannot
be measured through internet scans. We close this gap in two steps. First, we
collect SSH client public keys. Such keys are regularly published by their
owners on open development platforms like GitHub and GitLab. We systematize
previous non-academic work by subjecting these keys to various security tests
in a longitudinal study. Second, in a series of black-box lab experiments, we
analyze the implementations of algorithms for SSH client signatures in 24
popular SSH clients for Linux, Windows, and macOS.
  We extracted 31,622,338 keys from three public sources in two scans. Compared
to previous work, we see a clear tendency to abandon RSA signatures in favor of
EdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139
keys generated from weak randomness, and 149 keys with common or small
factors-the large majority of the retrieved keys exposed no weakness.
  Weak randomness can not only compromise a secret key through its public key,
but also through signatures. It is well-known that a bias in random nonces in
ECDSA can reveal the secret key through public signatures. For the first time,
we show that the use of deterministic nonces in ECDSA can also be dangerous:
The private signing key of a PuTTY client can be recovered from just 58 valid
signatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our
finding in CVE-2024-31497, and they subsequently replaced the nonce generation
algorithm.

================================================================================

PAPER #45
----------------------------------------
Title: ORCA: Unveiling Obscure Containers In The Wild
Link: http://arxiv.org/abs/2509.09322v1
Authors: Jacopo Bufalino, Agathe Blaise, Stefano Secci
Published: 2025-09-11T10:12:56Z
Categories: cs.SE, cs.CR

Abstract:
Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

================================================================================

PAPER #46
----------------------------------------
Title: Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on
  Open & Industry Data
Link: http://arxiv.org/abs/2509.09313v1
Authors: Moritz Mock, Thomas Forrer, Barbara Russo
Published: 2025-09-11T09:58:43Z
Categories: cs.SE

Abstract:
Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

================================================================================

PAPER #47
----------------------------------------
Title: Euler-type methods for Levy-driven McKean-Vlasov SDEs with super-linear
  coefficients: mean-square error analysis
Link: http://arxiv.org/abs/2509.09302v1
Authors: Jingtao Zhu, Yuying Zhao, Siqing Gan
Published: 2025-09-11T09:43:59Z
Categories: math.NA, cs.NA

Abstract:
We develop and analyze a general class of Euler-type numerical schemes for
Levy-driven McKean-Vlasov stochastic differential equations (SDEs), where the
drift, diffusion and jump coefficients grow super-linearly in the state
variable. These numerical schemes are derived by incorporating projections or
nonlinear transformations into the classical Euler method, with the primary
objective of establishing moment bounds for the numerical solutions. This class
of schemes includes the tanh-Euler, tamed-Euler and sine-Euler schemes as
special cases. In contrast to existing approaches that rely on a coercivity
condition (e.g., Assumption B-1 in Kumar et al., arXiv:2010.08585), the
proposed schemes remove such a restrictive assumption. We provide a rigorous
mean-square convergence analysis and establish that the proposed schemes
achieve convergence rates arbitrarily close to 1/2 for the interacting particle
systems associated with Levy-driven McKean-Vlasov SDEs. Several numerical
examples are presented to illustrate the convergence behavior and validate the
theoretical results.

================================================================================

PAPER #48
----------------------------------------
Title: Altered Histories in Version Control System Repositories: Evidence from
  the Trenches
Link: http://arxiv.org/abs/2509.09294v1
Authors: Solal Rapaport, Laurent Pautet, Samuel Tardieu, Stefano Zacchiroli
Published: 2025-09-11T09:34:06Z
Categories: cs.SE

Abstract:
Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

================================================================================

PAPER #49
----------------------------------------
Title: What You Code Is What We Prove: Translating BLE App Logic into Formal
  Models with LLMs for Vulnerability Detection
Link: http://arxiv.org/abs/2509.09291v1
Authors: Biwei Yan, Yue Zhang, Minghui Xu, Runyu Pan, Jinku Li, Xiuzhen Cheng
Published: 2025-09-11T09:27:37Z
Categories: cs.CR, cs.NI

Abstract:
The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

================================================================================

PAPER #50
----------------------------------------
Title: Partial Eigenvalue Assignment for Nonlinear Systems
Link: http://arxiv.org/abs/2509.09256v1
Authors: Shang Wang, Xiaodong Cheng, Yu Kawano, Peter van Heijster
Published: 2025-09-11T08:41:03Z
Categories: math.OC

Abstract:
In this paper, we study control design methods for assigning a subset of
nonlinear right or left eigenvalues to a specified set of scalar-valued
functions via nonlinear Sylvester equations. This framework can be viewed as a
generalization of partial linear eigenvalue assignment (also referred to as
partial pole placement) for linear systems. First, we propose a method for
partial nonlinear right eigenvalue assignment via state feedback using a
nonlinear Sylvester equation and a condition for preserving an open-loop
nonlinear right eigenvalue. This method can be applied to partial stabilization
of nonlinear systems. Then, as the dual problem, we present a method for
partial nonlinear left eigenvalue assignment via the dual nonlinear Sylvester
equation and a condition for preserving an open-loop nonlinear left eigenvalue,
which can be applied to partial observer design for nonlinear system.

================================================================================

PAPER #51
----------------------------------------
Title: Global Optimization of Stochastic Black-Box Functions with Arbitrary
  Noise Distributions using Wilson Score Kernel Density Estimation
Link: http://arxiv.org/abs/2509.09238v1
Authors: Thorbjørn Mosekjær Iversen, Lars Carøe Sørensen, Simon Faarvang Mathiesen, Henrik Gordon Petersen
Published: 2025-09-11T08:20:30Z
Categories: stat.ML, cs.LG, cs.RO

Abstract:
Many optimization problems in robotics involve the optimization of
time-expensive black-box functions, such as those involving complex simulations
or evaluation of real-world experiments. Furthermore, these functions are often
stochastic as repeated experiments are subject to unmeasurable disturbances.
Bayesian optimization can be used to optimize such methods in an efficient
manner by deploying a probabilistic function estimator to estimate with a given
confidence so that regions of the search space can be pruned away.
Consequently, the success of the Bayesian optimization depends on the function
estimator's ability to provide informative confidence bounds. Existing function
estimators require many function evaluations to infer the underlying confidence
or depend on modeling of the disturbances. In this paper, it is shown that the
confidence bounds provided by the Wilson Score Kernel Density Estimator
(WS-KDE) are applicable as excellent bounds to any stochastic function with an
output confined to the closed interval [0;1] regardless of the distribution of
the output. This finding opens up the use of WS-KDE for stable global
optimization on a wider range of cost functions. The properties of WS-KDE in
the context of Bayesian optimization are demonstrated in simulation and applied
to the problem of automated trap design for vibrational part feeders.

================================================================================

PAPER #52
----------------------------------------
Title: On Integrating Large Language Models and Scenario-Based Programming for
  Improving Software Reliability
Link: http://arxiv.org/abs/2509.09194v1
Authors: Ayelet Berzack, Guy Katz
Published: 2025-09-11T07:10:25Z
Categories: cs.SE, cs.AI, 68N19

Abstract:
Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

================================================================================

PAPER #53
----------------------------------------
Title: Moments of additive martingales of branching Lévy processes and
  applications
Link: http://arxiv.org/abs/2509.09188v1
Authors: Yan-Xia Ren, Renming Song, Rui Zhang
Published: 2025-09-11T06:58:58Z
Categories: math.PR

Abstract:
Let $W_t(\theta)$ be the Biggins martingale of a supercritical branching
L\'evy process with non-local branching mechanism, and denote by
$W_\infty(\theta)$ its limit. In this paper, we first study moment properties
of $W_t(\theta)$ and $W_\infty(\theta)$, and the tail behavior of
$W_\infty(\theta)$. We then apply these results to establish central limit
theorems for $W_t(\theta)-W_\infty(\theta)$.

================================================================================

PAPER #54
----------------------------------------
Title: Quantile-based Fractional Generalized Cumulative Past Entropy
Link: http://arxiv.org/abs/2509.09182v1
Authors: Poulami Paul, Chanchal Kundu
Published: 2025-09-11T06:44:32Z
Categories: math.ST, stat.TH, 94A17

Abstract:
Uncertainty in past lifetime distributions and the timing of inactivity in
systems and their components can be effectively measured using the fractional
generalized cumulative past entropy (FGCPE) and its dynamic extension (DFGCPE),
introduced by Di Crescenzo et al. (2021). Building on this framework, we
propose a quantile-based variant, the quantile fractional generalized
cumulative past entropy (QFGCPE), along with its dynamic time-dependent
counterpart (DQFGCPE). Closed-form expressions of these measures are derived
analytically for a variety of lifetime distributions, including those with and
without explicit distribution functions. Fundamental properties such as bounds,
monotonicity, and stochastic orderings are investigated to assess robustness
and interpretability. Furthermore, we construct a nonparametric estimator of
QFGCPE and establish its asymptotic validity through extensive simulation
studies involving bias, mean squared error (MSE), and root mean squared error
(RMSE). Finally, the sensitivity of the proposed QFGCPE measure is examined by
comparing its behavior with the logistic map, demonstrating its ability to
capture transitions from order to chaos.

================================================================================

PAPER #55
----------------------------------------
Title: Structural Complexity and Correlated Disorder in Materials Chemistry
Link: http://arxiv.org/abs/2509.09171v1
Authors: Andrew L. Goodwin
Published: 2025-09-11T06:12:06Z
Categories: cond-mat.dis-nn

Abstract:
Complexity is a measure of information content. Crystalline materials are not
complex systems because their structures can be represented tersely using the
language of crystallography. Disordered materials are also structurally simple
if the disorder present is random: such systems can be described efficiently
through statistical mechanics. True complexity emerges when structures are
neither perfectly crystalline nor randomly disordered -- a middle ground once
named ``organised complexity''. In current parlance, in our field, we use the
term ``correlated disorder'' for this same regime, emphasising the presence and
importance of non-random patterns.

================================================================================

PAPER #56
----------------------------------------
Title: CLARA: A Developer's Companion for Code Comprehension and Analysis
Link: http://arxiv.org/abs/2509.09072v1
Authors: Ahmed Adnan, Mushfiqur Rahman, Saad Sakib Noor, Kazi Sakib
Published: 2025-09-11T00:30:43Z
Categories: cs.SE

Abstract:
Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

================================================================================

PAPER #57
----------------------------------------
Title: STRIDE: Scalable and Interpretable XAI via Subset-Free Functional
  Decomposition
Link: http://arxiv.org/abs/2509.09070v1
Authors: Chaeyun Ko
Published: 2025-09-11T00:19:53Z
Categories: cs.LG, cs.AI, stat.ML

Abstract:
Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

================================================================================

PAPER #58
----------------------------------------
Title: Generalized Blaschke--Santaló-type inequalities, without symmetry
  restrictions
Link: http://arxiv.org/abs/2509.08998v1
Authors: Thomas A. Courtade, Edric Wang
Published: 2025-09-10T20:58:14Z
Categories: math.FA, cs.IT, math.IT, math.PR

Abstract:
Nakamura and Tsuji (2024) recently investigated a many-function
generalization of the functional Blaschke--Santal\'o inequality, which they
refer to as a generalized Legendre duality relation. They showed that, among
the class of all even test functions, centered Gaussian functions saturate this
general family of functional inequalities. Leveraging a certain entropic
duality, we give a short alternate proof of Nakamura and Tsuji's result, and,
in the process, eliminate all symmetry assumptions. As an application, we
establish a Talagrand-type inequality for the Wasserstein barycenter problem
(without symmetry restrictions) originally conjectured by Kolesnikov and Werner
(\textit{Adv.~Math.}, 2022). An analogous geometric Blaschke--Santal\'o-type
inequality is established for many convex bodies, again without symmetry
assumptions.

================================================================================

PAPER #59
----------------------------------------
Title: Time-Fair Benchmarking for Metaheuristics: A Restart-Fair Protocol for
  Fixed-Time Comparisons
Link: http://arxiv.org/abs/2509.08986v1
Authors: Junbo Jacob Lian
Published: 2025-09-10T20:33:54Z
Categories: cs.NE, cs.PF, stat.CO

Abstract:
Numerous purportedly improved metaheuristics claim superior performance based
on equivalent function evaluations (FEs), yet often conceal additional
computational burdens in more intensive iterations, preprocessing stages, or
hyperparameter tuning. This paper posits that wall-clock time, rather than
solely FEs, should serve as the principal budgetary constraint for equitable
comparisons. We formalize a fixed-time, restart-fair benchmarking protocol
wherein each algorithm is allotted an identical wall-clock time budget per
problem instance, permitting unrestricted utilization of restarts, early
termination criteria, and internal adaptive mechanisms. We advocate for the
adoption of anytime performance curves, expected running time (ERT) metrics,
and performance profiles that employ time as the cost measure, all aimed at
predefined targets. Furthermore, we introduce a concise, reproducible checklist
to standardize reporting practices and mitigate undisclosed computational
overheads. This approach fosters more credible and practically relevant
evaluations of metaheuristic algorithms.

================================================================================

PAPER #60
----------------------------------------
Title: Time-dependent correlations of the Edwards-Anderson order parameter
  above the spin-glass transition
Link: http://arxiv.org/abs/2509.08955v1
Authors: Jingjin Song, Sheena K. K. Patel, Rupak Bhattacharya, Yi Yang, Sudip Pandey, Xiao M. Chen, Eric Lee-Wong, Kalyan Sasmal, M. Brian Maple, Eric E. Fullerton, Sujoy Roy, Claudio Mazzoli, Chandra M. Varma, Sunil K. Sinha
Published: 2025-09-10T19:28:55Z
Categories: cond-mat.dis-nn

Abstract:
In 1975 Edwards and Anderson introduced a new paradigm that interacting
quenched systems, such as a spin-glass, have a phase transition in which long
time memory of spatial patterns is realized without spatial correlations. We
show here that the information about the time-dependent correlations above the
spin-glass transition are embedded in the four spin correlations of the
intensity of speckle pattern. This encodes the spin-orientation memory and can
be measured by the technique of resonant magnetic x-ray photon correlation
spectroscopy (RM- XPCS). We have implemented this method to observe and
accurately characterize the critical slowing down of the spin orientation
fluctuations in the classic metallic spin glass alloy $Cu_{1-x}{Mn}_x$ over
time scales of ${2}$ sec. to $2 \times 10^{\mathbf{4}}$ secs. Remarkably the
divergence of the correlation time as a function of temperature is consistent
with the Vogel-Vulcher law, universally used to characterize the viscous
relaxation time in structural glasses. Our method also opens the way for
studying phase transitions in systems such as spin ices, quantum spin liquids,
the structural glass transition, as well as possibly provide new perspectives
on the multifarious problems in which spin-glass concepts have found
applications.

================================================================================

PAPER #61
----------------------------------------
Title: Feature Representation and Clustering of Airport Congestion with Hurst
  Exponent and High Order Statistics
Link: http://arxiv.org/abs/2509.08952v1
Authors: Wei Sun, Zi-Feng Yi, Zhi-Qiang Feng, Ji Ma, Ruo-shi Yang
Published: 2025-09-10T19:22:05Z
Categories: stat.AP

Abstract:
Air traffic controllers benefit from referencing historical dates with
similar complex air traffic conditions to identify potential management
measures and their effects, which is critical for understanding air
transportation system laws and optimizing decisions. This study conducted data
mining using flight timetables. It first explored airport congestion mechanisms
and quantified congestion as time series, then proposed a higher-order
cumulants based time series feature extraction method. This method was fused
with other features to build a high-dimensional airport congestion feature
vector, and finally K-means clustering was applied to extract and analyze
congestion patterns. The clustering method was empirically validated with 2023
flight data from Guangzhou Baiyun International Airport and it accurately
classified airport operational states. To verify universality, the same
framework was applied to 6 airports under the "one-city, two-airports" layout
in Beijing, Shanghai and Chengdu. Results showed significant congestion pattern
differences between existing and newly constructed airports. Conclusions
confirm the proposed feature extraction and clustering framework is effective
and universal, and it can accurately capture airport congestion dynamics. Under
the "one-city, two-airports" layout, existing and newly constructed airports
differ significantly in operational modes, and most single-airport city
airports have operational modes highly consistent with existing airports. This
study provides valuable decision-making references for airport managers and air
traffic controllers. It helps them deepen understanding of air traffic dynamics
and airport congestion patterns, thereby optimizing traffic management
strategies and improving airport operational efficiency.

================================================================================

PAPER #62
----------------------------------------
Title: Activity-driven clustering of jamming run-and-tumble particles: Exact
  three-body steady state by dynamical symmetry
Link: http://arxiv.org/abs/2509.08945v1
Authors: Leo Hahn, Arnaud Guillin, Manon Michel
Published: 2025-09-10T19:11:03Z
Categories: cond-mat.stat-mech, math.PR

Abstract:
We exactly resolve the three-particle steady state of run-and-tumble
particles with jamming interactions, providing the first microscopic
description beyond two bodies. The invariant measure, derived via a
piecewise-deterministic Markov process description and symmetry principles,
reveals persistent, separated, and diffusive regimes. A cascade of scales in
the activity parameter organizes the structural weights, showing the separated
phase dominates at finite activity, while non-uniformity plays only a minor
role. This approach lays the groundwork for tackling the $N$-body problem.

================================================================================

PAPER #63
----------------------------------------
Title: The near-critical random bond FK-percolation model
Link: http://arxiv.org/abs/2509.08938v1
Authors: Emile Avérous, Rémy Mahfouf
Published: 2025-09-10T19:06:26Z
Categories: math.PR, math-ph, math.MP, 82B20

Abstract:
We study FK-percolation where the edge parameters are chosen as independent
random variables in the near-critical regime. We show that if these parameters
satisfy a natural centering condition around the critical point, then the
quenched model typically exhibits critical behaviour at scales much larger than
the deterministic characteristic length. More precisely, in a box of size $N$,
if the homogeneous model with deterministic edge parameter $p$ looks critical
in the regime $|p-p_c|\le \textrm W$, then the quenched model with random edge
parameters $\mathbf p$ that typically satisfy $|\mathbf p-p_c|\le \textrm
W^{1/3}$ looks critical, assuming some conjectured inequality on critical
exponents, and up to logarithmic corrections. We also treat the special case of
Bernoulli percolation, where we show that if one first samples non-degenerate
independent random edge parameters centered around $\frac12$, and then a
percolation configuration on these edges, the quenched model almost surely
looks critical at large scales.

================================================================================

PAPER #64
----------------------------------------
Title: Similarity-based Outlier Detection for Noisy Object Re-Identification
  Using Beta Mixtures
Link: http://arxiv.org/abs/2509.08926v1
Authors: Waqar Ahmad, Evan Murphy, Vladimir A. Krylov
Published: 2025-09-10T18:42:19Z
Categories: cs.CV, cs.AI, cs.LG, math.ST, stat.ML, stat.TH

Abstract:
Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

================================================================================

PAPER #65
----------------------------------------
Title: Documents Are People and Words Are Items: A Psychometric Approach to
  Textual Data with Contextual Embeddings
Link: http://arxiv.org/abs/2509.08920v1
Authors: Jinsong Chen
Published: 2025-09-10T18:31:37Z
Categories: cs.CL, stat.AP, stat.ME

Abstract:
This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

================================================================================

PAPER #66
----------------------------------------
Title: Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum
  Applications
Link: http://arxiv.org/abs/2509.08911v1
Authors: Weiyuan Gong, Tongyang Li, Xinzhao Wang, Zhiyu Zhang
Published: 2025-09-10T18:15:41Z
Categories: cs.LG, cs.AI, cs.DS, quant-ph, stat.ML

Abstract:
The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

================================================================================

PAPER #67
----------------------------------------
Title: Anomalously fast transport in non-integrable lattice gauge theories
Link: http://arxiv.org/abs/2509.08889v1
Authors: Devendra Singh Bhakuni, Roberto Verdel, Jean-Yves Desaules, Maksym Serbyn, Marko Ljubotina, Marcello Dalmonte
Published: 2025-09-10T18:00:05Z
Categories: cond-mat.quant-gas, cond-mat.dis-nn, cond-mat.mes-hall, cond-mat.stat-mech, cond-mat.str-el

Abstract:
Kinetic constraints are generally expected to slow down dynamics in many-body
systems, obstructing or even completely suppressing transport of conserved
charges. Here, we show how gauge theories can defy this wisdom by yielding
constrained models with faster-than-diffusive dynamics. We first show how, upon
integrating out the gauge fields, one-dimensional U(1) lattice gauge theories
are exactly mapped onto XX models with non-local constraints. This new class of
kinetically constrained models interpolates between free theories and highly
constrained local fermionic models. We find that energy transport is
superdiffusive over a broad parameter regime. Even more drastically, spin
transport exhibits ballistic behavior, albeit with anomalous finite-volume
properties as a consequence of gauge invariance. Our findings are relevant to
current efforts in quantum simulations of gauge-theory dynamics and anomalous
hydrodynamics in closed quantum many-body systems.

================================================================================

PAPER #68
----------------------------------------
Title: Narrative-Guided Reinforcement Learning: A Platform for Studying
  Language Model Influence on Decision Making
Link: http://arxiv.org/abs/2509.08785v1
Authors: Anup Tuladhar, Araz Minhas, Adam Kirton, Eli Kinney-Lang
Published: 2025-09-10T17:14:12Z
Categories: cs.AI, cs.MA, stat.ML

Abstract:
We present a preliminary experimental platform that explores how narrative
elements might shape AI decision-making by combining reinforcement learning
(RL) with language model reasoning. While AI systems can now both make
decisions and engage in narrative reasoning, these capabilities have mostly
been studied separately. Our platform attempts to bridge this gap using a
dual-system architecture to examine how narrative frameworks could influence
reward-based learning. The system comprises a reinforcement learning policy
that suggests actions based on past experience, and a language model that
processes these suggestions through different narrative frameworks to guide
decisions. This setup enables initial experimentation with narrative elements
while maintaining consistent environment and reward structures. We implement
this architecture in a configurable gridworld environment, where agents receive
both policy suggestions and information about their surroundings. The
platform's modular design facilitates controlled testing of environmental
complexity, narrative parameters, and the interaction between reinforcement
learning and narrative-based decisions. Our logging system captures basic
decision metrics, from RL policy values to language model reasoning to action
selection patterns. While preliminary, this implementation provides a
foundation for studying how different narrative frameworks might affect
reward-based decisions and exploring potential interactions between
optimization-based learning and symbolic reasoning in AI systems.

================================================================================

PAPER #69
----------------------------------------
Title: PCGBandit: One-shot acceleration of transient PDE solvers via
  online-learned preconditioners
Link: http://arxiv.org/abs/2509.08765v1
Authors: Mikhail Khodak, Min Ki Jung, Brian Wynne, Edmond chow, Egemen Kolemen
Published: 2025-09-10T16:56:53Z
Categories: physics.comp-ph, cs.LG, cs.NA, math.NA, stat.ML

Abstract:
Data-driven acceleration of scientific computing workflows has been a
high-profile aim of machine learning (ML) for science, with numerical
simulation of transient partial differential equations (PDEs) being one of the
main applications. The focus thus far has been on methods that require
classical simulations to train, which when combined with the data-hungriness
and optimization challenges of neural networks has caused difficulties in
demonstrating a convincing advantage against strong classical baselines. We
consider an alternative paradigm in which the learner uses a classical solver's
own data to accelerate it, enabling a one-shot speedup of the simulation.
Concretely, since transient PDEs often require solving a sequence of related
linear systems, the feedback from repeated calls to a linear solver such as
preconditioned conjugate gradient (PCG) can be used by a bandit algorithm to
online-learn an adaptive sequence of solver configurations (e.g.
preconditioners). The method we develop, PCGBandit, is implemented directly on
top of the popular open source software OpenFOAM, which we use to show its
effectiveness on a set of fluid and magnetohydrodynamics (MHD) problems.

================================================================================

PAPER #70
----------------------------------------
Title: Bregman Douglas-Rachford Splitting Method
Link: http://arxiv.org/abs/2509.08739v1
Authors: Shiqian Ma, Lin Xiao, Renbo Zhao
Published: 2025-09-10T16:27:02Z
Categories: math.OC, cs.LG, stat.ML

Abstract:
In this paper, we propose the Bregman Douglas-Rachford splitting (BDRS)
method and its variant Bregman Peaceman-Rachford splitting method for solving
maximal monotone inclusion problem. We show that BDRS is equivalent to a
Bregman alternating direction method of multipliers (ADMM) when applied to the
dual of the problem. A special case of the Bregman ADMM is an alternating
direction version of the exponential multiplier method. To the best of our
knowledge, algorithms proposed in this paper are new to the literature. We also
discuss how to use our algorithms to solve the discrete optimal transport (OT)
problem. We prove the convergence of the algorithms under certain assumptions,
though we point out that one assumption does not apply to the OT problem.

================================================================================

PAPER #71
----------------------------------------
Title: Generative AI as a Safety Net for Survey Question Refinement
Link: http://arxiv.org/abs/2509.08702v1
Authors: Erica Ann Metheney, Lauren Yehle
Published: 2025-09-10T15:52:10Z
Categories: stat.ME, cs.CY, stat.AP

Abstract:
Writing survey questions that easily and accurately convey their intent to a
variety of respondents is a demanding and high-stakes task. Despite the
extensive literature on best practices, the number of considerations to keep in
mind is vast and even small errors can render collected data unusable for its
intended purpose. The process of drafting initial questions, checking for known
sources of error, and developing solutions to those problems requires
considerable time, expertise, and financial resources. Given the rising costs
of survey implementation and the critical role that polls play in media,
policymaking, and research, it is vital that we utilize all available tools to
protect the integrity of survey data and the financial investments made to
obtain it. Since its launch in 2022, ChatGPT and other generative AI model
platforms have been integrated into everyday life processes and workflows,
particularly pertaining to text revision. While many researchers have begun
exploring how generative AI may assist with questionnaire design, we have
implemented a prompt experiment to systematically test what kind of feedback on
survey questions an average ChatGPT user can expect. Results from our
zero--shot prompt experiment, which randomized the version of ChatGPT and the
persona given to the model, shows that generative AI is a valuable tool today,
even for an average AI user, and suggests that AI will play an increasingly
prominent role in the evolution of survey development best practices as precise
tools are developed.

================================================================================

PAPER #72
----------------------------------------
Title: In situ estimation of the acoustic surface impedance using
  simulation-based inference
Link: http://arxiv.org/abs/2509.08873v1
Authors: Jonas M. Schmid, Johannes D. Schmid, Martin Eser, Steffen Marburg
Published: 2025-09-10T15:33:21Z
Categories: cs.SD, physics.data-an

Abstract:
Accurate acoustic simulations of enclosed spaces require precise boundary
conditions, typically expressed through surface impedances for wave-based
methods. Conventional measurement techniques often rely on simplifying
assumptions about the sound field and mounting conditions, limiting their
validity for real-world scenarios. To overcome these limitations, this study
introduces a Bayesian framework for the in situ estimation of
frequency-dependent acoustic surface impedances from sparse interior sound
pressure measurements. The approach employs simulation-based inference, which
leverages the expressiveness of modern neural network architectures to directly
map simulated data to posterior distributions of model parameters, bypassing
conventional sampling-based Bayesian approaches and offering advantages for
high-dimensional inference problems. Impedance behavior is modeled using a
damped oscillator model extended with a fractional calculus term. The framework
is verified on a finite element model of a cuboid room and further tested with
impedance tube measurements used as reference, achieving robust and accurate
estimation of all six individual impedances. Application to a numerical car
cabin model further demonstrates reliable uncertainty quantification and high
predictive accuracy even for complex-shaped geometries. Posterior predictive
checks and coverage diagnostics confirm well-calibrated inference, highlighting
the method's potential for generalizable, efficient, and physically consistent
characterization of acoustic boundary conditions in real-world interior
environments.

================================================================================

PAPER #73
----------------------------------------
Title: MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust
  Optimization
Link: http://arxiv.org/abs/2509.08578v1
Authors: Hong Liu
Published: 2025-09-10T13:27:40Z
Categories: cs.LG, q-bio.PE, q-bio.QM

Abstract:
Timely and robust influenza incidence forecasting is critical for public
health decision-making. To address this, we present MAESTRO, a Multi-modal
Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves
robustness by adaptively fusing multi-modal inputs-including surveillance, web
search trends, and meteorological data-and leveraging a comprehensive
spectro-temporal architecture. The model first decomposes time series into
seasonal and trend components. These are then processed through a hybrid
feature enhancement pipeline combining Transformer-based encoders, a Mamba
state-space model for long-range dependencies, multi-scale temporal
convolutions, and a frequency-domain analysis module. A cross-channel attention
mechanism further integrates information across the different data modalities.
Finally, a temporal projection head performs sequence-to-sequence forecasting,
with an optional estimator to quantify prediction uncertainty. Evaluated on
over 11 years of Hong Kong influenza data (excluding the COVID-19 period),
MAESTRO shows strong competitive performance, demonstrating a superior model
fit and relative accuracy, achieving a state-of-the-art R-square of 0.956.
Extensive ablations confirm the significant contributions of both multi-modal
fusion and the spectro-temporal components. Our modular and reproducible
pipeline is made publicly available to facilitate deployment and extension to
other regions and pathogens.Our publicly available pipeline presents a
powerful, unified framework, demonstrating the critical synergy of advanced
spectro-temporal modeling and multi-modal data fusion for robust
epidemiological forecasting.

================================================================================

PAPER #74
----------------------------------------
Title: Agents of Discovery
Link: http://arxiv.org/abs/2509.08535v1
Authors: Sascha Diefenbacher, Anna Hallin, Gregor Kasieczka, Michael Krämer, Anne Lauscher, Tim Lukas
Published: 2025-09-10T12:25:13Z
Categories: hep-ph, cs.AI, cs.LG, hep-ex, physics.data-an

Abstract:
The substantial data volumes encountered in modern particle physics and other
domains of fundamental physics research allow (and require) the use of
increasingly complex data analysis tools and workflows. While the use of
machine learning (ML) tools for data analysis has recently proliferated, these
tools are typically special-purpose algorithms that rely, for example, on
encoded physics knowledge to reach optimal performance. In this work, we
investigate a new and orthogonal direction: Using recent progress in large
language models (LLMs) to create a team of agents -- instances of LLMs with
specific subtasks -- that jointly solve data analysis-based research problems
in a way similar to how a human researcher might: by creating code to operate
standard tools and libraries (including ML systems) and by building on results
of previous iterations. If successful, such agent-based systems could be
deployed to automate routine analysis components to counteract the increasing
complexity of modern tool chains. To investigate the capabilities of
current-generation commercial LLMs, we consider the task of anomaly detection
via the publicly available and highly-studied LHC Olympics dataset. Several
current models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated
and their stability tested. Overall, we observe the capacity of the agent-based
system to solve this data analysis problem. The best agent-created solutions
mirror the performance of human state-of-the-art results.

================================================================================

PAPER #75
----------------------------------------
Title: Line defects in infinite networks of resistors
Link: http://arxiv.org/abs/2509.08445v1
Authors: Róbert Németh, József Cserti, Gábor Széchenyi
Published: 2025-09-10T09:41:35Z
Categories: cond-mat.dis-nn

Abstract:
We study infinite resistor networks perturbed by line defects, in which the
resistances are periodically modified along a single line. Using the
Sherman-Morrison identity applied to the reciprocal-space representation of the
lattice Green's function, we develop a general analytical framework for
computing the equivalent resistance between arbitrary nodes. The resulting
expression is a one-dimensional integral that is evaluated exactly in special
cases. While our analysis is carried out for the square lattice, the method
readily extends to other lattice geometries and networks with general
impedances. Therefore, this framework is useful for studying the boundary
behavior of topolectrical circuits, which serve as classical analogs of
topological insulators.

================================================================================

PAPER #76
----------------------------------------
Title: kNNSampler: Stochastic Imputations for Recovering Missing Value
  Distributions
Link: http://arxiv.org/abs/2509.08366v1
Authors: Parastoo Pashmchi, Jerome Benoit, Motonobu Kanagawa
Published: 2025-09-10T08:04:04Z
Categories: stat.ML, cs.LG, math.ST, stat.ME, stat.TH

Abstract:
We study a missing-value imputation method, termed kNNSampler, that imputes a
given unit's missing response by randomly sampling from the observed responses
of the $k$ most similar units to the given unit in terms of the observed
covariates. This method can sample unknown missing values from their
distributions, quantify the uncertainties of missing values, and be readily
used for multiple imputation. Unlike popular kNNImputer, which estimates the
conditional mean of a missing response given an observed covariate, kNNSampler
is theoretically shown to estimate the conditional distribution of a missing
response given an observed covariate. Experiments demonstrate its effectiveness
in recovering the distribution of missing values. The code for kNNSampler is
made publicly available (https://github.com/SAP/knn-sampler).

================================================================================

PAPER #77
----------------------------------------
Title: EvolKV: Evolutionary KV Cache Compression for LLM Inference
Link: http://arxiv.org/abs/2509.08315v1
Authors: Bohan Yu, Yekun Chai
Published: 2025-09-10T06:32:49Z
Categories: cs.LG, cs.CL, cs.NE

Abstract:
Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

================================================================================

PAPER #78
----------------------------------------
Title: A Systematic Survey on Large Language Models for Evolutionary
  Optimization: From Modeling to Solving
Link: http://arxiv.org/abs/2509.08269v1
Authors: Yisong Zhang, Ran Cheng, Guoxing Yi, Kay Chen Tan
Published: 2025-09-10T04:05:54Z
Categories: cs.NE, cs.AI

Abstract:
Large Language Models (LLMs), with their strong understanding and reasoning
capabilities, are increasingly being explored for tackling optimization
problems, especially in synergy with evolutionary computation. Despite rapid
progress, however, the field still lacks a unified synthesis and a systematic
taxonomy. This survey addresses this gap by providing a comprehensive review of
recent developments and organizing them within a structured framework. We
classify existing research into two main stages: LLMs for optimization modeling
and LLMs for optimization solving. The latter is further divided into three
paradigms according to the role of LLMs in the optimization workflow: LLMs as
stand-alone optimizers, low-level LLMs embedded within optimization algorithms,
and high-level LLMs for algorithm selection and generation. For each category,
we analyze representative methods, distill technical challenges, and examine
their interplay with traditional approaches. We also review interdisciplinary
applications spanning the natural sciences, engineering, and machine learning.
By contrasting LLM-driven and conventional methods, we highlight key
limitations and research gaps, and point toward future directions for
developing self-evolving agentic ecosystems for optimization. An up-to-date
collection of related literature is maintained at
https://github.com/ishmael233/LLM4OPT.

================================================================================

PAPER #79
----------------------------------------
Title: Convergence and Optimality of the EM Algorithm Under Multi-Component
  Gaussian Mixture Models
Link: http://arxiv.org/abs/2509.08237v1
Authors: Xin Bing, Dehan Kong, Bingqing Li
Published: 2025-09-10T02:40:08Z
Categories: math.ST, stat.TH

Abstract:
Gaussian mixture models (GMMs) are fundamental statistical tools for modeling
heterogeneous data. Due to the nonconcavity of the likelihood function, the
Expectation-Maximization (EM) algorithm is widely used for parameter estimation
of each Gaussian component. Existing analyses of the EM algorithm's convergence
to the true parameter focus primarily on either the two-component case or
multi-component settings with both known mixing probabilities and known,
isotropic covariance matrices. In this work, we establish the minimax optimal
rate of convergence of the EM algorithm for multi-component GMMs in full
generality. The required separation condition between Gaussian components for
EM to converge is the weakest known to date. We develop two distinct analytical
approaches, each tailored to a different regime of separation, reflecting two
complementary perspectives on the use of EM: parameter estimation and
clustering. As a byproduct of our analysis, we show that the EM algorithm, when
used for community detection, also achieves the minimax optimal rate of
misclustering error under milder separation conditions than spectral clustering
and Lloyd's algorithm, an interesting result in its own right. Our analysis
allows the number of components, the minimal mixing probabilities, the
separation between Gaussian components as well as the dimension to grow with
the sample size. Simulation studies corroborate the theoretical findings.

================================================================================

PAPER #80
----------------------------------------
Title: Deploying Robust Decision Support Systems for Transit Headway Control:
  Rider Impacts, Human Factors and Recommendations for Scalability
Link: http://arxiv.org/abs/2509.08231v1
Authors: Joseph Rodriguez, Haris N. Koutsopoulos, Jinhua Zhao
Published: 2025-09-10T02:10:42Z
Categories: stat.AP, cs.CY

Abstract:
Service reliability is critical to transit service delivery. This paper
describes headway control pilots conducted in two high-ridership Chicago bus
routes between 2022 and 2023. A decision support system was developed for a bus
holding strategy based on a reinforcement learning approach. For the pilots, a
user interface enabled supervisors to monitor service and record applied
actions. The first pilot tested terminal-based holding on a route affected by
missed trips from absenteeism. The analysis found improvements in reliability,
and the application of control was shown to outperform days with more service.
The second pilot applied en-route holding in a high-ridership bus route in
Chicago. The evaluation showed wait time improvements with rippled benefits to
stops downstream, and a reduction in transfer times from connecting bus and
rail lines. Compliance analysis based on the supervisor logs on the app
revealed mixed compliance levels from drivers, which were related to the
mentality of schedule adherence and seniority. Recommendations are provided for
practitioners to scale similar efforts.

================================================================================

PAPER #81
----------------------------------------
Title: Contributions to Robust and Efficient Methods for Analysis of High
  Dimensional Data
Link: http://arxiv.org/abs/2509.08155v1
Authors: Kai Yang
Published: 2025-09-09T21:30:01Z
Categories: math.ST, cs.LG, cs.NA, math.NA, math.OC, physics.data-an, stat.TH

Abstract:
A ubiquitous feature of data of our era is their extra-large sizes and
dimensions. Analyzing such high-dimensional data poses significant challenges,
since the feature dimension is often much larger than the sample size. This
thesis introduces robust and computationally efficient methods to address
several common challenges associated with high-dimensional data. In my first
manuscript, I propose a coherent approach to variable screening that
accommodates nonlinear associations. I develop a novel variable screening
method that transcends traditional linear assumptions by leveraging mutual
information, with an intended application in neuroimaging data. This approach
allows for accurate identification of important variables by capturing
nonlinear as well as linear relationships between the outcome and covariates.
Building on this foundation, I develop new optimization methods for sparse
estimation using nonconvex penalties in my second manuscript. These methods
address notable challenges in current statistical computing practices,
facilitating computationally efficient and robust analyses of complex datasets.
The proposed method can be applied to a general class of optimization problems.
In my third manuscript, I contribute to robust modeling of high-dimensional
correlated observations by developing a mixed-effects model based on Tsallis
power-law entropy maximization and discussed the theoretical properties of such
distribution. This model surpasses the constraints of conventional Gaussian
models by accommodating a broader class of distributions with enhanced
robustness to outliers. Additionally, I develop a proximal nonlinear conjugate
gradient algorithm that accelerates convergence while maintaining numerical
stability, along with rigorous statistical properties for the proposed
framework.

================================================================================

PAPER #82
----------------------------------------
Title: DDNet: A Unified Physics-Informed Deep Learning Framework for
  Semiconductor Device Modeling
Link: http://arxiv.org/abs/2509.08073v1
Authors: Roberto Riganti, Matteo G. C. Alasio, Enrico Bellotti, Luca Dal Negro
Published: 2025-09-09T18:22:13Z
Categories: physics.comp-ph, cond-mat.dis-nn

Abstract:
The accurate modeling of semiconductor devices plays a critical role in the
development of new technology nodes and next-generation devices. Semiconductor
device designers largely rely on advanced simulation software to solve the
drift-diffusion equations, a coupled system of nonlinear partial differential
equations that describe carrier transport in semiconductor devices. While these
tools perform well for forward modeling, they are not suitable to address
inverse problems, for example, determining doping profiles, material, and
geometrical parameters given a desired device performance. Meanwhile,
physics-informed neural networks (PINNs) have grown in popularity in recent
years thanks to their ability to efficiently and accurately solve inverse
problems at minimal computational cost compared to forward problems. In this
study, we introduce the Drift-Diffusion Network (DDNet), a unified
physics-informed deep learning solver for the forward and inverse mesh-free
solutions of the drift-diffusion equations of semiconductor device modeling.
Using prototypical device configurations in one- and two spatial dimensions, we
show that DDNet achieves low absolute and relative error compared to
traditional simulation software while additionally solving user-defined inverse
problems with minimal computational overhead. We expect that DDNet will benefit
semiconductor device modeling by facilitating exploration and discovery of
novel device structures across comprehensive parameter sets in a fully
automated way.

================================================================================

PAPER #83
----------------------------------------
Title: Revisiting the Question of Information Content of EXAFS Spectra through
  a Bayesian Approach
Link: http://arxiv.org/abs/2509.07950v1
Authors: Lucy Haddad, Diego Gianolio, Andrei Sapelkin
Published: 2025-09-09T17:39:13Z
Categories: physics.data-an, cond-mat.mtrl-sci

Abstract:
Over the last several decades the Shannon-Nyquist criterion has been widely
used as a measure of the maximum information content in EXAFS spectra and
provided an upper limit on the number of parameters used in fitting data.
However, the criterion implicitly assumes independent parameters which is never
the case in EXAFS analysis. Here we introduce a new criterion to measure the
information content in EXAFS based on Bayesian approach that lifts the above
condition. We test the new criterion by fitting the EXAFS spectrum of liquid
gallium and demonstrate that not only it does constitute a superior measure of
the data information content, but can also provide guidance in data analysis to
differentiate between various fitting strategies.

================================================================================

PAPER #84
----------------------------------------
Title: BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease
  Classification via Gut Microbiota Analysis
Link: http://arxiv.org/abs/2509.07723v1
Authors: Bo Yu, Zhixiu Hua, Bo Zhao
Published: 2025-09-09T13:24:25Z
Categories: cs.AI, cs.LG, q-bio.QM

Abstract:
Background: Parkinson's disease remains a major neurodegenerative disorder
with high misdiagnosis rates, primarily due to reliance on clinical rating
scales. Recent studies have demonstrated a strong association between gut
microbiota and Parkinson's disease, suggesting that microbial composition may
serve as a promising biomarker. Although deep learning models based ongut
microbiota show potential for early prediction, most approaches rely on single
classifiers and often overlook inter-strain correlations or temporal dynamics.
Therefore, there is an urgent need for more robust feature extraction methods
tailored to microbiome data. Methods: We proposed BDPM (A Machine
Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut
Microbiota Analysis). First, we collected gut microbiota profiles from 39
Parkinson's patients and their healthy spouses to identify differentially
abundant taxa. Second, we developed an innovative feature selection framework
named RFRE (Random Forest combined with Recursive Feature Elimination),
integrating ecological knowledge to enhance biological interpretability.
Finally, we designed a hybrid classification model to capture temporal and
spatial patterns in microbiome data.

================================================================================

PAPER #85
----------------------------------------
Title: Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor
Link: http://arxiv.org/abs/2509.08020v1
Authors: Matthew Carter, Lee Devlin, Alexander Philips, Edward Pyzer-Knapp, Paul Spirakis, Simon Maskell
Published: 2025-09-09T08:54:13Z
Categories: q-bio.QM, cs.DC, stat.CO

Abstract:
Quantitative proteomics plays a central role in uncovering regulatory
mechanisms, identifying disease biomarkers, and guiding the development of
precision therapies. These insights are often obtained through complex Bayesian
models, whose inference procedures are computationally intensive, especially
when applied at scale to biological datasets. This limits the accessibility of
advanced modelling techniques needed to fully exploit proteomics data. Although
Sequential Monte Carlo (SMC) methods offer a parallelisable alternative to
traditional Markov Chain Monte Carlo, their high-performance implementations
often rely on specialised hardware, increasing both financial and energy costs.
We address these challenges by introducing an opportunistic computing framework
for SMC samplers, tailored to the demands of large-scale proteomics inference.
Our approach leverages idle compute resources at the University of Liverpool
via HTCondor, enabling scalable Bayesian inference without dedicated
high-performance computing infrastructure. Central to this framework is a novel
Coordinator-Manager-Follower architecture that reduces synchronisation overhead
and supports robust operation in heterogeneous, unreliable environments. We
evaluate the framework on a realistic proteomics model and show that
opportunistic SMC delivers accurate inference with weak scaling, increasing
samples generated under a fixed time budget as more resources join. To support
adoption, we release CondorSMC, an open-source package for deploying SMC
samplers in opportunistic computing environments.

================================================================================

PAPER #86
----------------------------------------
Title: Bayesian Pliable Lasso with Horseshoe Prior for Interaction Effects in
  GLMs with Missing Responses
Link: http://arxiv.org/abs/2509.07501v1
Authors: The Tien Mai
Published: 2025-09-09T08:28:21Z
Categories: stat.ME, stat.AP, stat.CO, stat.ML

Abstract:
Sparse regression problems, where the goal is to identify a small set of
relevant predictors, often require modeling not only main effects but also
meaningful interactions through other variables. While the pliable lasso has
emerged as a powerful frequentist tool for modeling such interactions under
strong heredity constraints, it lacks a natural framework for uncertainty
quantification and incorporation of prior knowledge. In this paper, we propose
a Bayesian pliable lasso that extends this approach by placing
sparsity-inducing priors, such as the horseshoe, on both main and interaction
effects. The hierarchical prior structure enforces heredity constraints while
adaptively shrinking irrelevant coefficients and allowing important effects to
persist. We extend this framework to Generalized Linear Models (GLMs) and
develop a tailored approach to handle missing responses. To facilitate
posterior inference, we develop an efficient Gibbs sampling algorithm based on
a reparameterization of the horseshoe prior. Our Bayesian framework yields
sparse, interpretable interaction structures, and principled measures of
uncertainty. Through simulations and real-data studies, we demonstrate its
advantages over existing methods in recovering complex interaction patterns
under both complete and incomplete data.
  Our method is implemented in the package \texttt{hspliable} available on
Github.

================================================================================

PAPER #87
----------------------------------------
Title: Word2Spike: Poisson Rate Coding for Associative Memories and
  Neuromorphic Algorithms
Link: http://arxiv.org/abs/2509.07361v1
Authors: Archit Kalra, Midhun Sadanand
Published: 2025-09-09T03:15:22Z
Categories: cs.NE, cs.AI

Abstract:
Spiking neural networks offer a promising path toward energy-efficient,
brain-like associative memory. This paper introduces Word2Spike, a novel rate
coding mechanism that combines continuous word embeddings and neuromorphic
architectures. We develop a one-to-one mapping that converts multi-dimensional
word vectors into spike-based attractor states using Poisson processes. Using
BitNet b1.58 quantization, we maintain 97% semantic similarity of continuous
embeddings on SimLex-999 while achieving 100% reconstruction accuracy on 10,000
words from OpenAI's text-embedding-3-large. We preserve analogy performance
(100% of original embedding performance) even under intentionally introduced
noise, indicating a resilient mechanism for semantic encoding in neuromorphic
systems. Next steps include integrating the mapping with spiking transformers
and liquid state machines (resembling Hopfield Networks) for further
evaluation.

================================================================================

PAPER #88
----------------------------------------
Title: Setting Limits on Blazar-Boosted Dark Matter with Xenon-based Detectors
Link: http://arxiv.org/abs/2509.07265v1
Authors: Erin Barillier, Laura Manenti, Knut Mora, Paolo Padovani, Isaac Sarnoff, Yongheng Xu, Bjorn Penning, Francesco Arneodo
Published: 2025-09-08T22:51:39Z
Categories: astro-ph.HE, hep-ex, physics.data-an

Abstract:
Dual-phase xenon time projection chambers achieve optimal sensitivity for
dark matter in the $10$--$1000~\mathrm{GeV}/c^2$ mass range, but sub-GeV dark
matter particles lack sufficient energy to produce nuclear recoils above
detection thresholds in these detectors. Blazar-boosted dark matter offers a
way to overcome this limitation: relativistic jets in active galactic nuclei
can accelerate light dark matter in their host-galaxy halos to energies capable
of producing detectable nuclear recoil signals in xenon-based detectors on
Earth. We present the first blazar-boosted dark matter search incorporating
detector response modeling, using public data from XENON1T and LZ for the
blazar TXS~0506+056. We model dark matter--proton scattering in the jet
environment, covering the full process from jet acceleration to detector
response. We explore how the host-galaxy dark matter density profile impacts
our analysis. Using XENON1T data, we exclude dark matter--nucleon cross
sections for $m_\chi \sim 1~\mathrm{MeV}$ in the range $5.8\times
10^{-31}~\mathrm{cm^2} \lesssim \sigma_{\chi n} \lesssim 6.3\times
10^{-29}~\mathrm{cm^2}$, and using LZ EFT searches we constrain $9.9\times
10^{-32}~\mathrm{cm^2} \lesssim \sigma_{\chi n} \lesssim 2.5\times
10^{-28}~\mathrm{cm^2}$. Our results show that astrophysical uncertainties,
especially the dark matter density profile near the supermassive black hole,
set the primary limitation on the sensitivity rather than detector systematics.
These findings highlight blazar-boosted dark matter as a promising probe of
light dark matter and motivate further astrophysical modeling.

================================================================================

PAPER #89
----------------------------------------
Title: Breaking the Conventional Forward-Backward Tie in Neural Networks:
  Activation Functions
Link: http://arxiv.org/abs/2509.07236v1
Authors: Luigi Troiano, Francesco Gissi, Vincenzo Benedetto, Genny Tortora
Published: 2025-09-08T21:30:00Z
Categories: cs.NE, cs.AI, cs.LG

Abstract:
Gradient-based neural network training traditionally enforces symmetry
between forward and backward propagation, requiring activation functions to be
differentiable (or sub-differentiable) and strictly monotonic in certain
regions to prevent flat gradient areas. This symmetry, linking forward
activations closely to backward gradients, significantly restricts the
selection of activation functions, particularly excluding those with
substantial flat or non-differentiable regions. In this paper, we challenge
this assumption through mathematical analysis, demonstrating that precise
gradient magnitudes derived from activation functions are largely redundant,
provided the gradient direction is preserved. Empirical experiments conducted
on foundational architectures - such as Multi-Layer Perceptrons (MLPs),
Convolutional Neural Networks (CNNs), and Binary Neural Networks (BNNs) -
confirm that relaxing forward-backward symmetry and substituting traditional
gradients with simpler or stochastic alternatives does not impair learning and
may even enhance training stability and efficiency. We explicitly demonstrate
that neural networks with flat or non-differentiable activation functions, such
as the Heaviside step function, can be effectively trained, thereby expanding
design flexibility and computational efficiency. Further empirical validation
with more complex architectures remains a valuable direction for future
research.

================================================================================

PAPER #90
----------------------------------------
Title: Mathematical Discovery of Potential Therapeutic Targets: Application to
  Rare Melanomas
Link: http://arxiv.org/abs/2509.08013v1
Authors: Mahya Aghaee, Victoria Cicchirillo+, Kyle Adams, Alberto Riva, Rebecca Nance-Richey, William Hager, Ashley N. Brown, Elias Sayour, Domenico Santoro, Rowan Milner, Bently Doonan, Helen Moore
Published: 2025-09-08T20:55:31Z
Categories: q-bio.QM

Abstract:
Patients with rare types of melanoma such as acral, mucosal, or uveal
melanoma, have lower survival rates than patients with cutaneous melanoma;
these lower survival rates reflect the lower objective response rates to
immunotherapy compared to cutaneous melanoma. Understanding tumor-immune
dynamics in rare melanomas is critical for the development of new therapies and
for improving response rates to current cancer therapies. Progress has been
hindered by the lack of clinical data and the need for better preclinical
models of rare melanomas. Canine melanoma provides a valuable comparative
oncology model for rare types of human melanomas. We analyzed RNA sequencing
data from canine melanoma patients and combined this with literature
information to create a novel mechanistic mathematical model of melanoma-immune
dynamics. Sensitivity analysis of the mathematical model indicated influential
pathways in the dynamics, providing support for potential new therapeutic
targets and future combinations of therapies. We share our learnings from this
work, to help enable the application of this proof-of-concept workflow to other
rare disease settings with sparse available data.

================================================================================

PAPER #91
----------------------------------------
Title: Predicting effect of novel treatments using molecular pathways and
  real-world data
Link: http://arxiv.org/abs/2509.07204v1
Authors: Adrien Couetoux, Thomas Devenyns, Lise Diagne, David Champagne, Pierre-Yves Mousset, Chris Anagnostopoulos
Published: 2025-09-08T20:35:15Z
Categories: cs.LG, q-bio.QM, I.2.6; I.2.4; J.3

Abstract:
In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in
treating a particular disease prior to clinical testing or any real-world use
has been challenging. In this paper, we propose a flexible and modular machine
learning-based approach for predicting the efficacy of an untested
pharmaceutical for treating a disease. We train a machine learning model using
sets of pharmaceutical-pathway weight impact scores and patient data, which can
include patient characteristics and observed clinical outcomes. The resulting
model then analyses weighted impact scores of an untested pharmaceutical across
human biological molecule-protein pathways to generate a predicted efficacy
value. We demonstrate how the method works on a real-world dataset with patient
treatments and outcomes, with two different weight impact score algorithms We
include methods for evaluating the generalisation performance on unseen
treatments, and to characterise conditions under which the approach can be
expected to be most predictive. We discuss specific ways in which our approach
can be iterated on, making it an initial framework to support future work on
predicting the effect of untested drugs, leveraging RWD clinical data and drug
embeddings.

================================================================================

PAPER #92
----------------------------------------
Title: Comparing unsupervised learning methods for local structural
  identification in colloidal systems
Link: http://arxiv.org/abs/2509.07186v1
Authors: Alptuğ Ulugöl, Jessi Bückmann, Ruizhi Yang, Roy Hoitink, Alfons van Blaaderen, Frank Smallenburg, Laura Filion
Published: 2025-09-08T20:04:17Z
Categories: cond-mat.soft, physics.data-an

Abstract:
Quantifying local structures in self-assembled systems is a central challenge
in soft matter and materials science. When no a priori knowledge of the
relevant structures is available, traditional order parameters often fall
short. Unsupervised machine learning provides a convenient route to
autonomously uncover structural motifs directly from particle configurations.
In this work, we systematically compare three popular dimensionality reduction
techniques; Principal Component Analysis (PCA), Autoencoders (AE), and Uniform
Manifold Approximation and Projection (UMAP), for classifying local
environments in self-assembled systems. We first apply these methods to fluid
and crystal configurations of hard and charged spheres. Thereafter, we apply it
to an icosahedral arrangement of spheres that self-assembled in spherical
confinement, both from simulations as well as from experiments. We demonstrate
that UMAP consistently outperforms the other methods in capturing complex
structural features, offering a robust tool for structural classification
without supervision.

================================================================================

PAPER #93
----------------------------------------
Title: Safe cross-entropy-based importance sampling for rare event simulations
Link: http://arxiv.org/abs/2509.07160v1
Authors: Zhiwei Gao, George Karniadakis
Published: 2025-09-08T19:16:13Z
Categories: math.NA, cs.NA, stat.CO

Abstract:
The Improved Cross-Entropy (ICE) method is a powerful tool for estimating
failure probabilities in reliability analysis. Its core idea is to approximate
the optimal importance-sampling density by minimizing the forward
Kullback-Leibler divergence within a chosen parametric family-typically a
mixture model. However, conventional mixtures are often light-tailed, which
leads to slow convergence and instability when targeting very small failure
probabilities. Moreover, selecting the number of mixture components in advance
can be difficult and may undermine stability. To overcome these challenges, we
adopt a weighted cross-entropy-penalized expectation-maximization (EM)
algorithm that automatically prunes redundant components during the iterative
process, making the approach more stable. Furthermore, we introduce a novel
two-component mixture that pairs a light-tailed distribution with a
heavy-tailed one, enabling more effective exploration of the tail region and
thus accelerating convergence for extremely small failure probabilities. We
call the resulting method Safe-ICE and assess it on a variety of test problems.
Numerical results show that Safe-ICE not only converges more rapidly and yields
more accurate failure-probability estimates than standard ICE, but also
identifies the appropriate number of mixture components without manual tuning.

================================================================================

PAPER #94
----------------------------------------
Title: Nanobot Algorithms for Treatment of Diffuse Cancer
Link: http://arxiv.org/abs/2509.06893v1
Authors: Noble Harasha, Nancy Lynch
Published: 2025-09-08T17:11:59Z
Categories: cs.MA, cs.RO, q-bio.QM

Abstract:
Motile nanosized particles, or "nanobots", promise more effective and less
toxic targeted drug delivery because of their unique scale and precision. We
consider the case in which the cancer is "diffuse", dispersed such that there
are multiple distinct cancer sites. We investigate the problem of a swarm of
nanobots locating these sites and treating them by dropping drug payloads at
the sites. To improve the success of the treatment, the drug payloads must be
allocated between sites according to their "demands"; this requires extra
nanobot coordination. We present a mathematical model of the behavior of the
nanobot agents and of their colloidal environment. This includes a movement
model for agents based upon experimental findings from actual nanoparticles in
which bots noisily ascend and descend chemical gradients. We present three
algorithms: The first algorithm, called KM, is the most representative of
reality, with agents simply following naturally existing chemical signals that
surround each cancer site. The second algorithm, KMA, includes an additional
chemical payload which amplifies the existing natural signals. The third
algorithm, KMAR, includes another additional chemical payload which counteracts
the other signals, instead inducing negative chemotaxis in agents such that
they are repelled from sites that are already sufficiently treated. We present
simulation results for all algorithms across different types of cancer
arrangements. For KM, we show that the treatment is generally successful unless
the natural chemical signals are weak, in which case the treatment progresses
too slowly. For KMA, we demonstrate a significant improvement in treatment
speed but a drop in eventual success, except for concentrated cancer patterns.
For KMAR, our results show great performance across all types of cancer
patterns, demonstrating robustness and adaptability.

================================================================================

PAPER #95
----------------------------------------
Title: Canonicalization of the E value from BLAST similarity search --
  dissimilarity measure and distance function for a metric space of protein
  sequences
Link: http://arxiv.org/abs/2509.06849v1
Authors: Boryeu Mao
Published: 2025-09-08T16:18:13Z
Categories: q-bio.BM, q-bio.QM

Abstract:
Sequence matching algorithms such as BLAST and FASTA have been widely used in
searching for evolutionary origin and biological functions of newly discovered
nucleic acid and protein sequences. As parts of these search tools, alignment
scores and E values are useful indicators of the quality of search results from
querying a database of annotated sequences, whereby a high alignment score (and
inversely a low E value) reflects significant similarity between the query and
the subject (target) sequences. For cross-comparison of results from
sufficiently different queries however, the interpretation of alignment score
as a similarity measure and E value a dissimilarity measure becomes somewhat
nuanced, and prompts herein a judicious distinction of different types of
similarity. We show that an adjustment of E value to account for self-matching
of query and subject sequences corrects for certain ostensibly anomalous
similarity comparisons, resulting in canonical dissimilarity and similarity
measures that would be more appropriate for database applications, such as
all-on-all sequence alignment or selection of diverse subsets. In actual
practice, the canonicalization of E value dissimilarity improves clustering and
the diversity of subset selection. While both E value and the canonical E value
share positivity and symmetry, two of the four axiomatic properties of a metric
space, the canonical E value itself is also reflexive and meets the condition
of triangle inequality, thus an appropriate distance function for a metric
space of protein sequences.

================================================================================

PAPER #96
----------------------------------------
Title: Data-driven discovery of dynamical models in biology
Link: http://arxiv.org/abs/2509.06735v1
Authors: Bartosz Prokop, Lendert Gelens
Published: 2025-09-08T14:28:23Z
Categories: q-bio.QM

Abstract:
Dynamical systems theory describes how interacting quantities change over
time and space, from molecular oscillators to large-scale biological patterns.
Such systems often involve nonlinear feedbacks, delays, and interactions across
scales. Classical modeling derives explicit governing equations, often systems
of differential equations, by combining mechanistic assumptions, experimental
observations, and known physical laws. The growing complexity of biological
processes has, however, motivated complementary data-driven methods that aim to
infer model structure directly from measurements, often without specifying
equations a priori. In this review, we survey approaches for model discovery in
biological dynamical systems, focusing on three methodological families:
regression-based methods, network-based architectures, and decomposition
techniques. We compare their ability to address three core goals: forecasting
future states, identifying interactions, and characterizing system states.
Representative methods are applied to a common benchmark, the Oregonator model,
a minimal nonlinear oscillator that captures shared design principles of
chemical and biological systems. By highlighting strengths, limitations, and
interpretability, we aim to guide researchers in selecting tools for analyzing
complex, nonlinear, and high-dimensional dynamics in the life sciences.

================================================================================

PAPER #97
----------------------------------------
Title: Full Integer Arithmetic Online Training for Spiking Neural Networks
Link: http://arxiv.org/abs/2509.06636v1
Authors: Ismael Gomez, Guangzhi Tang
Published: 2025-09-08T12:54:30Z
Categories: cs.NE

Abstract:
Spiking Neural Networks (SNNs) are promising for neuromorphic computing due
to their biological plausibility and energy efficiency. However, training
methods like Backpropagation Through Time (BPTT) and Real Time Recurrent
Learning (RTRL) remain computationally intensive. This work introduces an
integer-only, online training algorithm using a mixed-precision approach to
improve efficiency and reduce memory usage by over 60%. The method replaces
floating-point operations with integer arithmetic to enable hardware-friendly
implementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs,
RSNNs), showing versatility across architectures. Evaluations on MNIST and the
Spiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models
achieve accuracy comparable to or better than full-precision baselines using
16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in
low-precision and deeper models, performance remains robust. In conclusion, the
proposed integer-only online learning algorithm presents an effective solution
for efficiently training SNNs, enabling deployment on resource-constrained
neuromorphic hardware without sacrificing accuracy.

================================================================================

PAPER #98
----------------------------------------
Title: An AI system to help scientists write expert-level empirical software
Link: http://arxiv.org/abs/2509.06503v1
Authors: Eser Aygün, Anastasiya Belyaeva, Gheorghe Comanici, Marc Coram, Hao Cui, Jake Garrison, Renee Johnston Anton Kast, Cory Y. McLean, Peter Norgaard, Zahra Shamsi, David Smalling, James Thompson, Subhashini Venugopalan, Brian P. Williams, Chujun He, Sarah Martinson, Martyna Plomecka, Lai Wei, Yuchen Zhou, Qian-Ze Zhu, Matthew Abraham, Erica Brand, Anna Bulanova, Jeffrey A. Cardille, Chris Co, Scott Ellsworth, Grace Joseph, Malcolm Kane, Ryan Krueger, Johan Kartiwa, Dan Liebling, Jan-Matthis Lueckmann, Paul Raccuglia, Xuefei, Wang, Katherine Chou, James Manyika, Yossi Matias, John C. Platt, Lizzie Dorfman, Shibl Mourad, Michael P. Brenner
Published: 2025-09-08T10:08:36Z
Categories: cs.AI, q-bio.QM

Abstract:
The cycle of scientific discovery is frequently bottlenecked by the slow,
manual creation of software to support computational experiments. To address
this, we present an AI system that creates expert-level scientific software
whose goal is to maximize a quality metric. The system uses a Large Language
Model (LLM) and Tree Search (TS) to systematically improve the quality metric
and intelligently navigate the large space of possible solutions. The system
achieves expert-level results when it explores and integrates complex research
ideas from external sources. The effectiveness of tree search is demonstrated
across a wide range of benchmarks. In bioinformatics, it discovered 40 novel
methods for single-cell data analysis that outperformed the top human-developed
methods on a public leaderboard. In epidemiology, it generated 14 models that
outperformed the CDC ensemble and all other individual models for forecasting
COVID-19 hospitalizations. Our method also produced state-of-the-art software
for geospatial analysis, neural activity prediction in zebrafish, time series
forecasting and numerical solution of integrals. By devising and implementing
novel solutions to diverse tasks, the system represents a significant step
towards accelerating scientific progress.

================================================================================

PAPER #99
----------------------------------------
Title: Largevars: An R Package for Testing Large VARs for the Presence of
  Cointegration
Link: http://arxiv.org/abs/2509.06295v1
Authors: Anna Bykhovskaya, Vadim Gorin, Eszter Kiss
Published: 2025-09-08T02:40:35Z
Categories: econ.EM, stat.CO, stat.ME

Abstract:
Cointegration is a property of multivariate time series that determines
whether its non-stationary, growing components have a stationary linear
combination. Largevars R package conducts a cointegration test for
high-dimensional vector autoregressions of order k based on the large N, T
asymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a
modification of the Johansen likelihood ratio test. In the absence of
cointegration the test converges to the partial sum of the Airy_1 point
process, an object arising in random matrix theory.
  The package and this article contain simulated quantiles of the first ten
partial sums of the Airy_1 point process that are precise up to the first 3
digits. We also include two examples using Largevars: an empirical example on
S&P100 stocks and a simulated VAR(2) example.

================================================================================

PAPER #100
----------------------------------------
Title: TPCpp-10M: Simulated proton-proton collisions in a Time Projection
  Chamber for AI Foundation Models
Link: http://arxiv.org/abs/2509.05792v1
Authors: Shuhang Li, Yi Huang, David Park, Xihaier Luo, Haiwang Yu, Yeonju Go, Christopher Pinkenburg, Yuewei Lin, Shinjae Yoo, Joseph Osborn, Christof Roland, Jin Huang, Yihui Ren
Published: 2025-09-06T17:59:38Z
Categories: physics.data-an

Abstract:
Scientific foundation models hold great promise for advancing nuclear and
particle physics by improving analysis precision and accelerating discovery.
Yet, progress in this field is often limited by the lack of openly available
large scale datasets, as well as standardized evaluation tasks and metrics.
Furthermore, the specialized knowledge and software typically required to
process particle physics data pose significant barriers to interdisciplinary
collaboration with the broader machine learning community.
  This work introduces a large, openly accessible dataset of 10 million
simulated proton-proton collisions, designed to support self-supervised
training of foundation models. To facilitate ease of use, the dataset is
provided in a common NumPy format. In addition, it includes 70,000 labeled
examples spanning three well defined downstream tasks: track finding, particle
identification, and noise tagging, to enable systematic evaluation of the
foundation model's adaptability.
  The simulated data are generated using the Pythia Monte Carlo event generator
at a center of mass energy of sqrt(s) = 200 GeV and processed with Geant4 to
include realistic detector conditions and signal emulation in the sPHENIX Time
Projection Chamber at the Relativistic Heavy Ion Collider, located at
Brookhaven National Laboratory.
  This dataset resource establishes a common ground for interdisciplinary
research, enabling machine learning scientists and physicists alike to explore
scaling behaviors, assess transferability, and accelerate progress toward
foundation models in nuclear and high energy physics. The complete simulation
and reconstruction chain is reproducible with the sPHENIX software stack. All
data and code locations are provided under Data Accessibility.

================================================================================
