Title: Test Set Quality in Multilingual LLM Evaluation
arXiv URL: http://arxiv.org/abs/2508.02635v1
Authors: Kranti Chalamalasetti, Gabriel Bernier-Colborne, Yvan Gauthier, Sowmya Vajjala
Date: 2025-08-04
Processed: 2025-08-04 23:55:58
================================================================================

=== PAGE 1 === Test Set Quality in Multilingual LLM Evaluation Kranti Chalamalasetti!, Gabriel Bernier-Colborne”, Yvan Gauthier”, Sowmya Vajjala’, "University of Potsdam, Germany, *National Research Council, Canada kranti.chalamalasetti@uni-potsdam.de, gabriel.bernier-colborne@nrc-cnrc.gc.ca {yvan.gauthier, sowmya.vajjala}@nrc-cnrc.gc.ca Abstract Several multilingual benchmark datasets have beendeveloped in a semi-automatic manner in the recent past to measure progress and under- stand the state-of-the-art in the multilingual ca- pabilities of Large Language Models. How- ever, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilin- gual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance differ- ence across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on ad- dressing the dataset quality issues. 1 Introduction Achieving multilingual fairness in AI systems that incorporate large language models (LLMs) re- quires not only careful curation of pre-training data and post-training data, but also (and perhaps more importantly) evaluation data, as only the latter can enable us to accurately track progress of these sys- tems on the various tasks they perform. There has been a lot of recent work on the devel- opment of evaluation datasets across several lan- guages (Huang et al., 2023; Yiiksel et al., 2024; Son et al., 2025; Hupkes and Bogoychev, 2025; Tran et al., 2025; Sibaee et al., 2025). In most cases, these evaluation sets are automatically ex- tracted from web sources followed by varying de- grees of manual oversight. They are then used as benchmarks to compare performances of LLMs. From past NLP research, we know that even high quality task-specific data sources created with ex- pert human annotations are prone to errors (Boyd et al., 2008; Bernier-Colborne and Vajjala, 2024; Reiss et al., 2020). More recently, Gema et al. (2025) discuss errors in MMLU (Hendrycks et al., 2021), a popular LLM evaluation dataset that has since been translated into multiple languages (from English) and is being used as a multilingual LLM performance benchmark (Singh et al., 2024). This kind of scrutiny is mostly restricted to English test sets, though. In this background, we took a closer look at two recent multilingual datasets and performed a man- ual analysis for one French and two Telugu test subsets. A comparison of various LLMs between the original and cleaned versions of the test sets re- veal large variations (up to 10%) in both languages, raising questions about the quality of the resources. Based on these results, we provide some recom- mendations on how to address test set quality. We hope this discussion will serve as a starting point leading to a broader discussion around multilin- gual evaluation and test set creation. 2 Related Work Datasets for various tasks have been the subject of denoising or re-annotation studies in NLP research of the past, including part-of-speech tagging (Sil- berztein, 2018), dependency parsing (Alzetta et al., 2017; Wisniewski, 2018), entity linking (Jha et al., 2017) and named entity recognition (Wang et al., 2019; Reiss et al., 2020; Muthuraman et al., 2021; Stanislawek et al., 2019; Bernier-Colborne and Va- jjala, 2024) Most of this work focused on English, but other languages have been studied, such as Hindi (Saha et al., 2009), Japanese (Ichihara et al., 2015), and Uyghur (Abudukelimu et al., 2018) in the case of NER. Some past work looked at Swedish, Czech and German datasets in the con- text of parsing (Boyd et al., 2008). === PAGE 2 === In the context of LLM evaluation, recent work by Gema et al. (2025) looked at the well-known MMLU dataset for English, finding that over 6% of its questions contain errors such as ambiguous phrasing, incorrect ground truths, or unclear op- tions. Plaza et al. (2024) examine MMLU’s Span- ish version and reveal that many test item failures are due to automated translation errors (including mistranslated names, technical terms, cultural mis- matches, and grammatical issues). Cengiz et al. (2025) evaluate 17 Turkish benchmarks across six quality dimensions (including answer, grammar correctness, cohesion and coherence), finding that about 70% fail to meet their proposed quality stan- dards. We follow this lead, but look into other mul- tilingual datasets and languages in this paper. 3 Our Approach Our approach can be summarized as comprising the follow steps: a) we manually analyzed French and Telugu versions of a test set, b) compared the performance of 10 LLMs in terms of the difference in accuracy between the two versions of the test set for each language, and c) replicated this setup with another dataset, for Telugu. Details of the process are described below: Dataset: We used INcLupE44 from Romanou et al. (2024), a multilingual LLM evaluation dataset comprised of multiple-choice questions au- tomatically extracted academic and professional exam questions compiled from the web as our test dataset, as it is a recent multilingual test set and is not a translated version of English. We chose French and Telugu, the native languages spoken by the authors, to ensure two annotators per language. Annotation Process: | Based on some prelimi- nary analysis, for both the language subsets, we identified three primary issues in the test sets: unanswerable questions, incorrect question/answer pairs; question or answer being in English in- stead of the target language. Two annotators (na- tive speakers) per language manually analyzed the French and Telugu language test sets to mark each sample with any of these three concerns or as “no concerns”; Only samples unanimously marked as “no concerns” were included in the final cleaned dataset. Table 1 shows a summary of the datasets before and after cleanup. A qualitative analysis of this dataset is presented in Section 4.1. Almost half of the Telugu samples, and about Test Subset | # Orig. | # Clean French 419 327 Telugu 548 285 Table 1: # samples in original and cleaned test sets 25% of the French samples were removed in the cleaned version. Note that our approach to discard is aggressive, as we remove all samples where at least one annotator expressed a concern.! LLM Evaluation: We evaluated 10 LLMs in total, considering both open weight and propri- etary LLMs as well as small and large LLMs. All the larger LLMs (>15B - GPT40, Claude-3.7, Gemini-2.0-Flash, LLama3.3-70B, Gemma3-27B) that cannot be hosted on a laptop are accessed via OpenRouter’ and the smaller (<15B - Gemma3- 12B, Aya-Expanse:8B, Qwen2.5-7B, LLama3.2- 7B, Gemma2-9B) models are downloaded and run locally on a laptop, via Ollama’ in their 4-bit quan- tized versions. Table 4 in the appendix gives more details about the LLMs we used. All the evaluations were conducted through the Inspect LLM evaluation framework* with its de- fault prompts. Most evaluated models list French among supported languages (e.g., Claude, LLaMA 3, Qwen, Aya). Gemini, Gemma3, and GPT-40 do not have published language lists. In contrast, none of the models explicitly list Telugu as sup- ported. Some (e.g., Gemma3) claim broad multi- lingual coverage, but do not provide language lists. Since the dataset is in the multiple-choice format, we considered accuracy as the evaluation measure and used it to compare the difference between orig- inal and cleaned test sets. Section 4.2 discusses the results of this evaluation. Additionally, we did a replication experiment using another Telugu test set, from the MILU dataset (Verma et al., 2024), which is comparable in size to IncLuDE44’s Telugu test set. This exper- iment was designed to compare trends in dataset quality and LLM evaluation performance. More details are provided in Section 4.3. ‘Our revised versions of the datasets with comments will be shared publicly. “https: //openrouter.ai/ 3https: //ollama.com/ “https: //inspect.aisi.org-uk/ === PAGE 3 === 4 Results We first present a qualitative analysis of the IN- CLUDE44 test sets, followed by quantitative per- formance comparisons between the original and cleaned versions, and conclude with a replication study. 4.1 Qualitative Analysis of the Datasets In both languages, we noticed several cases of “unanswerable questions”, questions that miss in- formation such as the year, country, etc. For exam- ple, the Telugu dataset has a question: “Who won the recent Asia Under-14 Tennis Championship?”, giving four female names as the possible options. The right answer as per the dataset is true in 2018, and we annotated such questions as “unanswer- able” as we would need that context to answer cor- rectly. There were questions with missing context, for instance questions that are geography-specific but had no region or location specified in the ques- tion. There were also examples of incomplete ques- tions, undefined symbols in choices, or incorrect answers in both languages. There were several question and/or answers in English, in the Telugu subset. We present examples of the identified is- sues in French and Telugu in Tables 5 and 6, re- spectively, in the appendix, along with further dis- cussion of these issues (Section B). Many quality issues that we observe can be ex- plained by how the dataset was compiled. The au- thors of the dataset (Romanou et al., 2024) describe a process of automatic extraction, followed by man- ual check by native speakers to check if the extrac- tion is correct, filtering out questions with images or tables, and adding some meta-data. This au- tomated process can inadvertently generate ques- tions for which there are multiple valid answers, where the context of the question is insufficient (e.g., if an image was removed), or if the question is not time-specific enough to be correctly answered years later. The type of concerns with the questions varies by language. In French, the concerns are more evenly distributed between incorrect questions, in- correct answers, and unanswerable questions — with only a single question in the wrong language. In Telugu, however, the large majority of concerns were around english-text questions, and there were very few incorrect questions or answers (see Fig- ure | in the Appendix). In French, the annotators were provided with an initial set of four categories, e.g. incorrect language, incorrect question or choices or answer, unanswerable question, and no concerns. They were asked to adjudicate cases where they dis- agreed on the category. The discussion led them to define several specific error types. Some of these are illustrated in Table 5. Besides these, the an- notators found one duplicate question (with same choices and answer). Among the most frequent problems identified were questions that were about one specific country or jurisdiction (i.e., France) without mentioning that country or jurisdiction. If such questions were used outside of that country or jurisdiction to assess LLM safety, they could lead to incorrect conclusions. There were also sev- eral questions where more than one choice was as- sessed to be valid, or the correct answer was in- correct or at least debatable. In some cases, prob- lems seem to have arisen due to the way in which questions and choices were extracted from their sources — this includes questions where the choices assume a different number of blanks that the ques- tion presents, and questions that refer to some fig- ure or additional context that is not included in this dataset. 4.2 Performance variation across LLMs between the dataset versions Table 2 shows the performance of the various LLMs on the modified version of the dataset, with change from the original dataset indicated in the parentheses, for the two languages we considered. Detailed accuracies and standard errors per model, per dataset can be seen in Table 8 in the appendix. Model French Telugu GPT-4o0 Claude3.7-Sonnet Gemini2.0-Flash Llama-3.3-70B-it Gemma3-27B-it Gemmas3-12B Aya-Expanse:8b Qwen2.5-7B LLama3.2-7B Gemma2-9B 0.88 (79.2%) 0.89(47.4%) 0.83(46.5%) 0.77(45%) 0.74($5.4%) 0.71(47.1%) 0.66(74.4%) 0.66(45.8%) 0.52(43%) 0.68(46%) 0.66 (3.2%) 0.71 (45.7%) 0.76(+4.7%) 0.59($ 9.5%) 0.57(43.7%) 0.34(40.8%) 0.27(+0.9%) 0.32(40.5%) 0.29(+0.9%) 0.47(46.9%) Table 2: Performance with the cleaned versions of IN- CLUDE44 for French and Telugu test sets (and the change from original test set) Not surprisingly, accuracy tends to be higher with larger models, in both languages. We no- === PAGE 4 === tice that even the large and very large LLMs see large increases in the performance with the cleaned dataset compared to the original dataset for both French and Telugu. Interestingly, three of the five small, local language models too had an over 5% increase with the cleaned version of the French test set, but the increases were modest (under 1%) in the case of Telugu, where the original perfor- mance was already quite poor. The increases are also not uniform between the two languages even with the larger models. For example, GPT-40 sees a 9% increase for French, but only a 3% increase for Telugu. Yet, these fluctuations are large enough to warrant probing further into a central question: what are we evaluating against? They also serve as a reminder to report differences across languages more specifically. 4.3 Replication The evaluation so far dealt with two languages and different web sources, but the test sets were both constructed in a similar manner. To under- stand if the quality issues are due to the method of data collection, we replicated the analysis using a dataset from a different source, for one language, Telugu. MILU (Verma et al., 2024) is a multi-task Indian language understanding benchmark cover- ing 11 languages and is intended to be used as an evaluation dataset with LLMs. The dataset spans a range of domains and subjects and is collected by scraping websites that publish questions and an- swers from various past competitive exams, simi- larly to IncLuDE44. The cleaning process is auto- matic and a sample from the dataset was manually evaluated for quality in the original paper. We took a sample of 500 test items (out of the total 7.3K) for our manual analysis. While we notice similar issues to INcCLUDE44 (incomplete questions, unanswerable questions, in- correct questions, questions in English, etc), there is less disagreement between the two annotators on “No Concerns” and 383/500 (77%) are retained in the cleaned version. Examples of the removed samples can be found in the Appendix (Table 7). Table 3 shows the performance difference of the LLMs on the cleaned version along with the dif- ference from the original version. The variations seem to be lesser for this dataset compared to IN- CLUDE44, and there are also cases where the per- formance with the cleaned dataset is slightly lower than the original dataset. Model GPT-4o Claude3.7-Sonnet Gemin2.0-Flas Llama-3.3-70B-it Gemma3-27B-it Accuracy (% Diff) 0.74(+4.4%) 0.74 (¢3.1%) 0.84(F2.3%) 0.64(72.4%) 0.66(73.6%) Gemma3-12B 0.33( 0.2%) Aya-Expanse:8b 0.29 (| 0.1%) Qwen2.5-7B 0.33(, 1.7%) LLama3.2-7B 0.26(, 1.7%) Gemma2-9B 0.45(71.2%) Table 3: Performance with the cleaned version of MILU-Te subset compared to original subset 5 Conclusions and Discussion Our analysis revealed quality issues in the datasets we analyzed. LLM evaluations on the original and cleaned versions of the datasets revealed large dif- ferences in performance between the two versions, sometimes amounting to almost 10%, in both lan- guages. A replication experiment with a dataset from another source had similar issues, but to a lesser degree. Moreover, the type of concerns we identified in the datasets varied widely depending on the language. This limits how much one can infer from the performance of LLMs across lan- guages when using unverified, uncleaned datasets. Based on these experiments, we recommend the following as a call for further research on dataset quality: 1. Test sets should not be considered immutable and should be subject to further quality assur- ance, either by the creators or by others using them for conducting LLM evaluations. 2. Test set developers should have a provision to version them and evaluation studies should consider reporting results with cleaner, modi- fied versions where possible. 3. Model developers can consider adding small scale qualitative analyses for languages they can read, to identify potential limitations of their models as well as the test datasets used. 4. More research should go into automatic or semi-automatic identification of dataset qual- ity, potentially utilizing the recent develop- ments in LLM-as-a-judge approaches. === PAGE 5 === Limitations This study suffers from at least two specific limi- tations. Firstly, we chose only two languages, and small test sets as we opted for manual annotations - but we don’t see this exercise as an end in itself and hope that this will lead into more discussion and more effort in this direction. Secondly, our annotation guidelines too were somewhat loosely defined and we just took “no concerns” samples without attempting to fix the source for the other samples. The results of this study should be consid- ered along with these limitations of the annotation approach. Acknowledgments This research was conducted at the NRC on behalf of the Canadian AI Safety Institute. References Halidanmu Abudukelimu, Abudoukelimu Abulizi, Bo- liang Zhang, Xiaoman Pan, Di Lu, Heng Ji, and Yang Liu. 2018. Error analysis of Uyghur name tagging: Language-specific techniques and remaining chal- lenges. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Chiara Alzetta, Felice Dell’ Orletta, Simonetta Monte- magni, and Giulia Venturi. 2017. Dangerous rela- tions in dependency treebanks. In Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories, pages 201-210, Prague, Czech Republic. Gabriel Bernier-Colborne and Sowmya Vajjala. 2024. Annotation errors and ner: A study with ontonotes 5.0. Preprint, arXiv:2406.19172. Adriane Boyd, Markus Dickinson, and W Detmar Meurers. 2008. On detecting errors in dependency treebanks. Research on Language and Computation, 6:113-137. Ayse Aysu Cengiz, Ahmet Kaan Sever, Elif Ecem Umiitlii, Naime Seyma Erdem, Burak Aytan, Biisra Tufan, Abdullah Topraksoy, Esra Darici, and Cagri Toraman. 2025. Evaluating the quality of benchmark datasets for low-resource languages: A case study on turkish. CoRR, abs/2504.09714. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Man- cino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Bar- ale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile Van Krieken, and Pasquale Minervini. 2025. Are we done with MMLU? In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Lin- guistics: Human Language Technologies (Volume 1: Long Papers), pages 5069-5096, Albuquerque, New Mexico. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein- hardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Ly, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Infor- mation Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Dieuwke Hupkes and Nikolay Bogoychev. 2025. Mul- tiloko: a multilingual local knowledge bench- mark for llms spanning 31 languages. CoRR, abs/2504. 10356. Masaaki Ichihara, Kanako Komiya, Tomoya Iwakura, and Maiko Yamazaki. 2015. Error analysis of named entity recognition in becwj. Recall, 61:2641. Kunal Jha, Michael Réder, and Axel-Cyrille Ngonga Ngomo. 2017. All that glitters is not gold—rule-based curation of reference datasets for named entity recog- nition and entity linking. In European Semantic Web Conference, pages 305-320. Springer. Karthik Muthuraman, Frederick Reiss, Hong Xu, Bryan Cutler, and Zachary Eichenberger. 2021. Data clean- ing tools for token classification tasks. In Proceed- ings of the Second Workshop on Data Science with Human in the Loop: Language Advances, pages 59— 61, Online. Association for Computational Linguis- tics. Irene Plaza, Nina Melero, Cristina del Pozo, Javier Conde, Pedro Reviriego, Marina Mayor-Rocher, and Maria Grandury. 2024. Spanish and LLM bench- marks: is MMLU lost in translation? CoRR, abs/2406.17789. Frederick Reiss, Hong Xu, Bryan Cutler, Karthik Muthuraman, and Zachary Eichenberger. 2020. Identifying incorrect labels in the conll-2003 corpus. In Proceedings of the 24th conference on computa- tional natural language learning, pages 215-226. Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mo- hamed A Haggag, Alfonso Amayuelas, and 1 others. 2024. Include: Evaluating multilingual language un- derstanding with regional knowledge. arXiv preprint arXiv:2411.19799, === PAGE 6 === Sujan Kumar Saha, Sudeshna Sarkar, and Pabitra Mitra. 2009. Hindi named entity annotation error detection and correction. Language forum, 35(2):73-93. Serry Sibaee, Omer Nacar, Adel Ammar, Yasser Al- Habashi, Abdulrahman A1-Batati, and Wadii Boulila. 2025. From guidelines to practice: A new paradigm for arabic language model evaluation. arXiv preprint arXiv:2506.01920. Max Silberztein. 2018. Using linguistic resources to evaluate the quality of annotated corpora. In Pro- ceedings of the First Workshop on Linguistic Re- sources for Natural Language Processing, pages 2— 11, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Shivalika Singh, Angelika Romanou, Clémentine Four- rier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchi- sio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, and 4 others. 2024. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evalua- tion. Preprint, arXiv:2412.03304. Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheon- bok Park, Kang Min Yoo, and Stella Biderman. 2025. KMMLWU: measuring massive multitask language un- derstanding in korean. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 4076-4104. Associ- ation for Computational Linguistics. Tomasz Stanislawek, Anna Wrdéblewska, Alicja W6j- cicka, Daniel Ziembicki, and Przemyslaw Biecek. 2019. Named entity recognition - is there a glass ceil- ing? In Proceedings of the 23rd Conference on Com- putational Natural Language Learning (CoNLL), pages 624-633, Hong Kong, China. Association for Computational Linguistics. Khanh-Tung Tran, Barry O’Sullivan, and Hoang D Nguyen. 2025. Irlbench: A multi-modal, cultur- ally grounded, parallel irish-english benchmark for open-ended Ilm reasoning evaluation. arXiv preprint arXiv: 2505. 13498. Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, and Jaydeep Sen. 2024. Milu: A multi-task indic language understand- ing benchmark. arXiv preprint arXiv:2411.02538. Zihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Ji- acheng Liu, and Jiawei Han. 2019. CrossWeigh: Training named entity tagger from imperfect anno- tations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)), pages 5154-5163, Hong Kong, China. Association for Computational Linguistics. Guillaume Wisniewski. 2018. Errator: a tool to help de- tect annotation errors in the Universal Dependencies project. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Arda Yiiksel, Abdullatif Koksal, Liitfi Kerem Senel, Anna Korhonen, and Hinrich Schiitze. 2024. Turk- ishmmlu: Measuring massive multitask language un- derstanding in turkish. In Findings of the Associa- tion for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 7035-7055. Association for Computational Linguis- tics. A Details about LLMs LLM Open? Provider GPT-4o (gpt-40-2024-08-06) | No OpenAI?® Claude-3.7-Sonnet No Anthropic?® Gemini-2.0-Flash No Google?” LLama3.3-70B-Instruct Yes Meta?*® Gemma3-27B-Instruct Yes Google?” Gemma3-12B Yes Google?” Aya-Expanse-8B Yes Cohere?” Qwen-2.5-7B Yes Alibaba?” LLama-3.2-7B Yes Meta?” Gemma2-9B Yes Google?” Table 4: Details about the LLMs compared. Super- scripts indicates how we accessed the models. OR indi- cates OpenRouter, and OL indicates Ollama. B_ Inctupe44 Examples Distribution of Dataset Concerns by Language 205 mm French (#419 questions) 200 mm Telugu (#548 questions) Incorrect Questions/Answers Unanswerable Questions Figure 1: Distribution of concerns across French and Telugu datasets in INCLUDE44 test set. Tables 5 and 6 and show examples of problem- atic questions and choices from the test set, an- notated with explanations. Note that, in the case === PAGE 7 === of French, the annotators also spotted one dupli- cate questions (with identical choices and answer). Figure 1 showcases the distribution of problematic questions. Out of the total, French had 42 Incorrect Q/A cases and 50 Unanswerable questions, while Telugu had 205 and 235 respectively. Unanswer- able questions are further categorized as: 1. Timeline Sensitivity: Questions whose an- swers change depending on the timeline.(e.g., aS BBRS sedate wocs5-14 BA wrod Coase OB 25%? — EN: Who won the re- cent Asian Under-14 Tennis Championship ?) 2. Geographic Dependency: Questions whose answers vary across countries. (e.g., Les doc- uments obligatoires a présenter en cas de con- trdle de police sont: — EN: The mandatory documents to be presented in the event of a police check are:) 3. Missing Context: Questions that require addi- tional information to answer correctly. (e.g., J’arrive en premier sur le lieu de cet accident, en attendant les secours je peux : — EN: I ar- rive first at the scene of this accident, while waiting for help I can:) or (¢.g., §08 » 22) Kaodod BOS HHosXo Sargrardaigoc — EN: Observe the ’Pi’ picture below and answer the questions below. — no image is provided in the question.) Incorrect Q/A questions can be further catego- rized as follows: 1. Incorrect questions: These are questions where the phrasing, logic, or structure leads to misleading or mismatched answer options. Questions like, Soepiom d°0SB8 DBE, Fee) SGBOSd arson DQyodacs Sacco? — EN: A ‘building built using the technology and methods of the Sultans ?), where the majority of the given answer choices describe build- ings that were constructed using such meth- ods. To answer correctly, the question should have included a negation, such as “was not built using...’ to match the intent of the an- swers.). Similarly in questions such as Dans une économie a deux acteurs, il y a une of- fre excessive sur le marché des produits si: — EN: In an economic system with two agents, there is oversupply in the commodities market if, but the answer options use undefined vari- ables like C, S, I, Y. 2. Multiple acceptable answers: Questions where more than one choice is valid (e.g., Ma consommation de carburant augmente si : — EN: My fuel consumption increases if: with options like [“J’adopte une conduite nerveuse.”, “Il pleut.”, “Mes pneus sont sous gonflés.”, “J’utilise la climatisation.”], all of which are potentially correct) 3. Code-mixed or English-only questions: These include questions or answer choices that are wholly or partially in English, despite being in a regional language context. For example: To which category does a TV belong as a teaching aid? — a question intended for a Telugu context but presented entirely in English. Choices like |’between 30°C to 50°C’, ’between 21°C to 27°C’, Less than 25°C’, "More than 25°C’] or [’b, ¢’, ’a, c’, ’a, b, c’, "b Seo’), where mixing languages disrupts consistency. C MILU-Te Examples Table 7 shows examples from the MILU-Te dataset and the associated errors/concerns. D_ Detailed Performance Table Table 8 shows the detailed accuracy and standard error statistics for all the LLMs, across the orig- inal and cleaned versions of the three datasets (INCLUDE44-Te, INCLUDE44-Fr, MILU-Te). === PAGE 8 === Question Choices Concern Membre de I’ Union Européenne: (Mem- ber of the European Union:) [Italie, Allemagne, Finlande, Norvége] [Italy, Germany, Finland, Norway] Multiple valid answers. Territoire densément peuplé de la Terre... (Densely populated area on Earth) [les territoires entre les 20 O et 23 0 de latitude nord., les régions situées sur V’équateur., les régions de plaines de la zone tempérée., les versants sud des hautes montagnes.] [areas between 20 and 23 latitude North, areas along the equator, temperate plains, the southern slope of mountains] Wrong/debatable answer (i.e. the pro- vided answer here conflicts with sources we consulted). J’ai mon permis depuis 8 mois. Je peux circuler 4 (I got my license 8 months ago. I can drive at) (130km/h, 110km/h, 100km/h, 90km/h] Country-specific, but country is not mentioned. Combien de pays compte 1’Afrique 2? (How many countries are there in Africa?) (40, 60, 57, 75] Time-specific, but time is not men- tioned. Classez ces planétes de la plus éloignée du soleil 4 la plus proche : (Sort these planets from furthest to closest to the sun: ) [1-3-2-4, 2-4-1-3, 3-4-1-2, 41-23] Insufficient context (e.g. missing fig- ure). Remplissez les blancs avec la bonne suite de mots : Distribue ces flyers dans les magasins de la ville (Fill in the blanks: Distribute these flyers in the stores in town) [‘‘diférents-différants-différant- différend”’, “différents-différents- différant-différend”’, “diférents- différents-différand-différent’’, “différents-différants-différand- différent’’] Incorrect number of blanks. Dans une économie 4 deux acteurs, il y a une offre excessive sur le marché des produits si (In an economic system with two agents, there is oversupply in the commodities market if) [“C+I<Y”, “S+CsI’, “S+bY”, “S<P’] Undefined choices. variables/symbols in Une carte routiére est a 1léchelle 1/250 000 (A road map has a scale of 1/250,000) {1 km, 25 km, 100 km, 10 km] Incomplete or unclear question. La sclérose est : (Sclerosis is:) [Une induration anormale d’un tissu ou d’un organe’”, “1+2+3”, “1+3”, “2+4”] [ “Abnormal hardening of body tissue”, “74243”, “143”, “24+4”] Choices make no sense. Parmi les recettes suivantes, laquelle est affectée aux collectivités locales ? (Among these revenues, which goes to local governments ?) [“Le droit de consommation sur les tabacs manufacturés’, “Les droits de douanes”, “Réponse : A : Aucune des réponses n’est correcte”, “Limpdt sur le revenu’”’] [ “Excise duties on tobacco products”, “Customs duties”, “Answer: A: None of these answers are correct”, “Income tax” ] Biased choices (e.g. only one choice is prefixed by “Réponse”. L’un des facteurs de la demande en de- vises est (One factor of currency de- mand is) [aucune des deux, toutes les deux, l’exportation de marchandises, V’importation de capital] [neither, both, commodity exports, capital imports ] Awkward order of choices. Mediterran éghajlat uralkodik ezen a tajon: [Chypre, La Sicile, Créte, Dalmatie] Question not in French. Table 5: Examples of some issues in a sample from the French test set. === PAGE 9 === Question Choices Concern To which category does a TV belong as a teaching aid? [Audio aid, *Audio-visual aid*, Authen- tic aid, Visual aid] Question/Answer in English. BESO BOAN GAT wocs5-14 GAO onc Sarw 0265 569? (Who won the recent Asia under-14 Tennis Champi- onship?) [Soa Sono Ser, *Sozsis VBsoo*, SOS, Qc%°08 S085] [Kumkum Neela, San- jana Sirimalla, Mallika, Priyanshi San- ket] Unanswerable Question- Year needs to be specified. This seems to be from 2018. Dd}, &dgso0 ) TQS S SeGodapo 8? (In which state did the Chipko move- ment start?) [aeg5Ss, Desde, *aesormoe*, 8,0] [Uttar Pradesh, Madhya Pradesh, Ut- tarkhand, Sikkm] Ambiguous Answer - A is right when the incident happened, but it falls into the state in answer C according to to- day’s division which came into being in 2000s. NoePxoe Boss SHIT, TS Shee Xo a0 DOoserces Sencs0.? (Which structure followed the technology and conventions of the Sultans?) [Son serd, wf Dirg, sosrcwrs S57°B, *Séy soxrS*] [Panch Mahal, Hasth Bihisht, Humayun Tomb, Padma Mahal] Incorrect Question - Missing negation in the question results in all other an- swers except the gold standard one be- ing correct. Given thatFind the value of (36.164, 36.304, 37.164, *37.304*] Incomplete question, and in English. Table 6: Examples of quality issues in the Telugu subset of the Include44 test set. Each row shows a question, its answer options, and the annotation team’s concern. Question Choices Concern BSE SAA go erqnBocsb SgXow S'So ar aH S06OD Doryew BoB. S068 0020808 808 S8toHeo S8Na°? (The Government of India has set up a Na- tional Council for Transgender Persons. Are the following statements correct re- garding the Council?) [85@0 2 <B8ctoo 3; 85@0 1 SB8atoo 2; €Se0 1 S98o%» 3; 1, 2 S86» 3] [Only 2 and 3; Only 1 and 2; Only 1 and 3; 1,2, and 3] Incomplete Question. Options are not provided in the question. QQ) GD wscvo5o de, 5 808 8%) S DSPErdo Qaigoe. 4 Sosesyoro Swe$0 esarosoo or. 75,00,000. soseéy5o 2 W0G 3 NosseySo SSS Sooe§o esaratoo oes? (Answer the following question after studying the given pie-chart. If the income for four years in total is Rs. 75,00,000, what is the income from year 2 to year 3?) | 7°. 43, 00,000; d5*. 42,00,000; >. 45,00, 000; as». 42,50, 000] Incomplete question. No pie-chart pro- vided. TDBSET NBS wS%™H) KBowsoe. (Identify the grammatical sentence.) (“No other boy is as taller as Sub- hash” “Gold is one of the more precious metal.” “Mohan is the young boy in the class.” “The metrological department says ’this year, Hyderabad will face the hottest summer in the decade’.’”] Question tests English knowledge. DHPSo Srger es ¥ eso 580? (Who is the current president of Cuba?) lord seas; SH sras: BYE 2rGor, 5 ® ® @ we So Dgrar deer] [[Ralph Castro; Fidel Castro; Alberto Herrera; Tomas Estrada Unanswerable Question - Year needs to be specified. None of the answers are correct in 2025. Palma]] as eed aye aarar 3,00,000. aor | [5,70,000; 5,50,000; 5,30,000; | Incorrect Question. There seems to be 5,10,000] some formatting issue, perhaps missing a Sy8dev PALA SOSeHYOS’ SdowP 9 HS anrar esqrdorr 6%, 71 2%, 9%, 10 1 2 %, ... M BOGSYVSD aPDApAr, G. 8 HoddySino Borges esr eos a (The current population of a city is 3,00,000. The population growth rate expected in the coming years is 6%, 7 1 2%, 9% and 10 1 2% respectively. What is the estimated population after 8 years?) decimal points. Table 7: Examples of some issues in a sample from the Telugu test set of MILU (Verma et al., 2024) === PAGE 10 === INcLUDE44-Te ORIG-548 Samples CLEAN-285Samples Model acc stderr acc stderr Gpt-4o 0.631 0.0206 0.663 0.028 Claude3.7-Sonnet 0.655 0.0203 0.712 0.0269 Gemin2.0-Flash 0.714 0.0173 0.761 0.0253 Llama 3.3 70B Instruct | 0.498 0.0214 0.593 0.0292 Gemma3-27B-it 0.538 0.0213 0.575 0.0293 Gemma3-12B 0.336 0.0202 0.344 0.0282 Aya-Expanse:8b 0.265 0.0189 0.274 0.0265 Qwen?2.5-7B 0.318 0.0199 0.323 0.028 LLama3.2-3B 0.286 0.0193 0.295 0.0271 Gemma2-9B 0.398 0.0209 0.467 0.0296 INcLupDE44-Fr ORIG-419 Samples | CLEAN-327 Samples Model acc stderr acc stderr Gpt-4o 0.792 0.198 0.884 0.0177 Claude3.7-Sonnet 0.816 0.0189 0.89 0.0173 Gemin2.0-Flash 0.77 0.0206 0.835 0.0206 Llama 3.3 70B Instruct | 0.721 0.0219 0.771 0.0233 Gemma3-27B-it 0.683 0.0228 0.737 0.0244 Gemma3-12B 0.642 0.0234 0.713 0.0251 Aya-Expanse:8b 0.613 0.0238 0.657 0.0263 Qwen?2.5-7B 0.606 0.0239 0.664 0.0262 LLama3.2-7B 0.487 0.0244 0.517 0.0277 Gemma2-9B 0.616 0.0238 0.676 0.0259 MILU-Te ORIG-500 Samples | CLEAN-383 Samples Model acc stderr acc stderr Gpt-4o 0.7 0.0205 0.744 0.0223 Claude3.7-Sonnet 0.708 0.0204 0.739 0.0225 Gemin2.0-Flash 0.82 0.0172 0.843 0.0186 Llama 3.3 70B Instruct | 0.618 0.0218 0.642 0.0245 Gemma3-27B-it 0.622 0.0217 0.658 0.0243 Gemma3-12B 0.328 0.021 0.326 0.024 Aya-Expanse:8b 0.296 0.0204 0.295 0.0233 Qwen?2.5-7B 0.346 0.0213 0.329 0.024 LLama3.2-3B 0.278 0.0201 0.261 0.0225 Gemma2-9B 0.442 0.022 0.454 0.0255 Table 8: Detailed Performance For All The Models/Datasets 10