{
  "1.pdf": "cs.CL9809020 15 Sep 1998 Linear Segmentation and Segment Significance Min-Yen Kan, Judith L. Klavans and Kathleen R. McKeown Department of Computer Science and Center for Research on Information Access Columbia University New York, NY 10027, USA {min,klavans,kathy}cs.columbia.edu Abstract We present a new method for discovering a segmental discourse structure of a document while categorizing each segments function and importance. Segments are determined by a zero-sum weighting scheme, used on occurrences of noun phrases and pronominal forms retrieved from the document. Segment roles are then calculated from the distribution of the terms in the segment. Finally, we present results of evaluation in terms of precision and recall which surpass earlier approaches 1. Introduction Identification of discourse structure can be extremely useful to natural language processing applications such as automatic text summarization or information retrieval (IR). For example, a summarization agent might chose to summarize each discourse segment separately. Also, segmentation of a document into blocks of topically similar text can assist a search engine in choosing to retrieve or highlight a segment in which a query term occurs. In this paper, we present a topical segmentation program that achieves a 10 increase in both precision and recall over comparable previous work. In addition to segmenting, the system also labels the function of discovered discourse 1 This material is based upon work supported by the National Science Foundation under Grant No. (NSF IRI-9618797) and by the Columbia University Center for Research on Information Access. segments as to their relevance towards the whole. It identifies 1) segments that contribute some detail towards the main topic of the input, 2) segments that summarize the key points, and 3) segments that contain less important information. We evaluated our segment classification as part of a summarization system that utilizes highly pertinent segments to extract key sentences. We investigated the applicability of this system on general domain news articles. Generally, we found that longer articles, usually beyond a three-page limit, tended to have their own prior segmentation markings consisting of headers or bullets, so these were excluded. We thus concentrated our work on a corpus of shorter articles, averaging roughly 800-1500 words in length: 15 from the Wall Street Journal in the Linguistic Data Consortiums 1988 collection, and 5 from the on-line The Economist from 1997. We constructed an evaluation standard from human segmentation judgments to test our output. 1 SEGMENTER: Linear Segmentation For the purposes of discourse structure identification, we follow a formulation of the problem similar to Hearst (1994), in which zero or more segment boundaries are found at various paragraph separations, which identify one or more topical text segments. Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Our segmentation methodology has three distinct phases (Figure 1), which are executed sequentially. We will describe each of these phases in detail. Figure 1. SEGMENTER Architecture 1.1 Extracting Useful Tokens The task of determining segmentation breaks depends fundamentally on extracting useful topic information from the text. We extract three categories of information, which reflect the topical content of a text, to be referred to as terms for the remainder of the paper: In order to find these three types of terms, we first tag the text with part of speech (POS) information. Two methods were investigated for assigning POS tags to the text: 1) running a specialized tagging program or 2) using a simple POS table lookup. We chose to use the latter to assign tags for time efficiency reasons (since the segmentation task is often only a preprocessing stage), but optimized the POS table to favor high recall of the 3 term types, whenever possible 2. The resulting system was faster than the initial prototype that used the former approach by more than a magnitude, with a slight decline in precision that was not statistically significant. However, if a large system requires accurate tags after segmentation and the cost of tagging is not an issue, then tagging should be used instead of lookup. 2 We based our POS table lookup on NYUs COMLEX (Grishman et al. 1994). After simplifying COMLEXs categories to only reflect information important to to our three term types, we flattened all multi-category words (i.e. jump as V or N) to a single category by a strategy motivated to give high term recall (i.e. jump maps to N, because NP is a term type.) Once POS tags have been assigned, we can retrieve occurrences of noun phrases by searching the document for this simple regular expression: (Adj  Noun) Noun This expression captures a simple noun phrase without any complements. More complex noun phrases such as proprietor of Stags Leap Wine Cellars in Napa Valley are captured as three different phrases: proprietor, Stags Leap Wine Cellars and Napa Valley. We deliberately made the regular expression less powerful to capture as many noun phrases as possible, since the emphasis is on high NP recall. After retrieving the terms, a post- processing phase combines related tokens together. For possessive pronouns, we merge each possessive with its appropriate personal pronoun (my or mine with I, etc.) For noun phrases, we canonicalize noun phrases according to their heads. For example, if the noun phrases red wine and wine are found in a text, we subsume the occurrences of red wine into the occurrences of wine, under the condition that there are no other wine headed phrases, such as white wine. Finally, we perform thresholding to filter irrelevant words, following the guidelines set out by Justeson and Katz (1995). We use a frequency threshold of two occurrences to determine topicality, and discard any pronouns or noun phrases that occur only once. 1.2 Weighting Term Occurrences Once extracted, terms are then evaluated to arrive at segmentation. 1.2.1 Link Length Given a single term (noun phrase or pronominal form) and the distribution of its occurrences, we link related occurrences together. We use proximity as our metric for relatedness. If two occurrences of a term occur within n sentences, we link them together as a single unit, and repeat until no larger units can be built. This idea is a simpler interpretation of the notion of lexical chains. Morris and Hirst (1991) first proposed this notion to chain semantically related words together via a 1. proper noun phrases; 2. common noun phrases; 3. personal and possessive pronouns. Find Terms Weigh Term Links Score Segment Boundarie s in- put seg- ments thesaurus, while we chose only repetition of the same stem word 3. However, for these three categories of terms we noticed that the linking distance differs depending on the type of term in question, with proper nouns having the maximum allowable distance and the pronominal forms having the least. Proper nouns generally refer to the same entity, almost regardless of the number of intervening sentences. Common nouns often have a much shorter scope of reference, since a single token can be used to repeatedly refer to different instances of its class. Personal pronouns scope even more closely, as is expected of an anaphoric or referring expression where the referent can be, by definition, different over an active discourse. Any term occurrences that were not linked were then dropped from further consideration. Thus, link length or linking distance refers to the number of sentences allowed to intervene between two occurrences of a term. 1.2.2 Assigning Weights After links are established, weighting is assigned. Since paragraph level boundaries are not considered in the previous step, we now label each paragraph with its positional relationship to each terms link(s). We describe these four categories for paragraph labeling and illustrate them in the figure below. Front: a paragraph in which a link begins. During: a paragraph in which a link occurs, but is not a front paragraph. Rear: a paragraph in which a link just stopped occurring the paragraph before. No link: any remaining paragraphs. paras 1 2 3 4 5 7 8 sents 12345678901234567890123456789012345 wine : 1xx1 1x21 type :n f d r n f d Figure 2a. A term wine, and its occurrences and type. 3 We also tried to semantically cluster terms by using Miller et al. (1990)s WordNet 1.5 with edge counting to determine relatedness, as suggested by Hearst (1997). However, results showed only minor improvement in precision and over a tenfold increase in execution time. Figure 2a shows the algorithm as developed thus far in the paper, operating on the term wine. The term appears a total of six times, as shown by the numbers in the central row. These occurrences have been grouped together into two term links, as joined by the xs. The bottom type line labels each paragraph with one of the four paragraph relations. We see that it is possible for a term to have multiple front or rear paragraphs, as illustrated, since a terms occurrences might be separated between disparate links. Then, for each of the four categories of paragraph labeling mentioned before, and for each of the three term types, we assign a different segmentation score, listed in Table 1, whose values were derived by training, to be discussed in section 1.2.4. Table 1 - Overview of weighting and linking scheme used in SEGMENTER; starred scores to be calculated later. For noun phrases, we assume that the introduction of the term is a point at which a new topic may start; this is Youmanss (1991) Vocabulary Management Profile. Similarly, when a term is no longer being used, as in rear paragraphs, the topic may be closed. This observation may not be as direct as vocabulary introduction, and thus presumably not as strong a marker of topic change as the former. Moreover, paragraphs in which the link persists throughout indicate that a topic continues; thus we see a negative score assigned to during paragraphs. When we apply the same paragraph labeling to pronoun forms, the same rationale applies with some modifications. Since the majority of pronoun referents occur before the pronoun (i.e. anaphoric as opposed to cataphoric), we do not weigh the front boundary heavily, but instead place the emphasis on the rear. Term Paragraph Type with respect to term Link Type front rear duringNo link Length Proper NP 10 8 -3  8 Common NP 10 8 -3  4 Pronouns  Possessives 1 13 -1  0 1.2.3. Zero Sum Normalization When we iterate the weighting process described above over each term, and total the scores assigned, we come up with a numerical score for an indication of which paragraphs are more likely to beh a topical boundary. The higher the numerical score, the higher the likelihood that the paragraph is a beginning of a new topical segment. The question then is what should the threshold be? paras 1 2 3 4 5 7 8 sents 12345678901234567890123456789012345 wine : 1xx1 1x21 type :n f d r n f d score: 10 -3 8  10 -3 sum to balance in zero-sum weighting: 12 zero :-6 10 -3 8 -6 10 -3 Figure 2b. A term wine, its links and score assignment to paragraphs. To solve this problem, we zero-sum the weights for each individual term. To do this, we first sum the total of all scores assigned to any front, rear and during paragraphs that we have previously assigned a score to and then evenly distribute to the remaining no link paragraphs the negative of this sum. This ensures that the net sum of the weight assigned by the weighting of each term sums to zero, and thus the weighting of the entire article, also sums to zero. In cases where no link paragraphs do not exist for a term, we cannot perform zero-summing, and take the scores assigned as is, but this is in small minority of cases. This process of weighting followed by zero-summing is shown by the extending the wine example, in Figure 2b, as indicated by the score and zero lines. With respect to individual paragraphs, the summed score results in a positive or negative total. A positive score indicates a boundary, i.e. the beginning of a new topical segment, whereas a negative score indicates the continuation of a segment. This use of zero sum weighting makes the problem of finding a threshold trivial, since the data is normalized around the value zero. 1.2.4 Finding Local Maxima Examination of the output indicated that for long and medium length documents, zero-sum weighting would yield good results. However, for the documents we investigated, namely documents of short length (800-1500 words), we have observed that multiple consecutive paragraphs, all with a positive summed score, actually only have a single, true boundary. In these cases, we take the maximal valued paragraph for each of these clusters of positive valued paragraphs as the only segment boundary. Again, this only makes sense for paragraphs of short length, where the distribution of words would smear the segmentation values across paragraphs. In longer length documents, we do not expect this phenomenon to occur, and thus this process can be skipped. After finding local maxima, we arrive at the finalized segment boundaries. 1.3 Algorithm Training To come up with the weights used in the segmentation algorithm and to establish the position criteria used later in the segment relevance calculations, we split our corpus of articles in four sets and performed 4-fold cross validation training, intentionally keeping the five Economist articles together in one set to check for domain specificity. Our training phase consisted of running the algorithm with a range of different parameter settings to determine the optimal settings. We tried a total of 5 x 5 x 3 x 3  225 group settings for the four variables (front, rear, during weights and linking length settings) for each of the three (common nouns, proper nouns and pronoun forms) term types. The results of each run were compared against a standard of user segmentation judgments, further discussed in Section 3. The results noted that a sizable group of settings (approximately 10) seemed to produce very close to optimal results. This group of settings was identical across all four cross validation training runs, so we believe the algorithm is fairly robust, but we cannot safely conclude this without constructing a more extensive trainingtesting corpus. 2 SEGNIFIER: Segment Significance Once segments have been determined, how can we go about using them? As illustrated in the introduction, segments can be utilized as-is by information retrieval and automatic summarization applications by treating segments as individual documents. However, this approach loses information about the cohesiveness of the text as a whole unit. What we are searching for is a framework for processing segments both as 1) sub-documents of a whole, and as 2) independent entities. This enables us to ask a parallel set of general questions concerning 1) how segments differ from each other, and 2) how a segment contributes to the document as a whole. In this portion of the paper, we deal with instances of the two questions: 1) Can we decide whether a text segment is important? 2) How do we decide what type of function a segment serves? These two questions are related; together, they might be said to define the task of finding segment significance. We will show a two-stage, sequential approach that attempts this task in the context of the article itself. Assessing segment significance with respect to a specific query could be quite different. Figure 3 - SEGNIFIER Architecture 2.1 Segment Importance Informally, segment importance is defined as the degree to which a given segment presents key information about the article as a whole. Our method for calculating this metric is given in the section below. We apply a variant of Saltons (1989) information retrieval metric, Term Frequency  Inverse Document Frequency (TFIDF) to noun phrases (no pronominial tokens are used in this algorithm). Intuitively, a segment containing noun phrases which are then also used in other segments of the document will be more central to the text than a segment that contains noun phrases that are used only within that one segment. We call this metric TFSF4, since we base the importance of a 4 SF  Segment frequency (How many segments does the term occur in) segment on the distribution of noun phrases within the document. Note that this is not exactly analogous to IDF; we do not compute inverse segment frequency (ISF); this is because we are looking for segments with noun phrases that occur throughout a text rather that segments which are characterized by local noun phrases. Higher scores along the TFSF metric indicate a more central segment, which we equate with segment importance. SEGNIFIER first calculates the TFSF score for each noun phrase term using the term occurrence information and segment boundaries provided by the segmentation program. However, segment importance cannot be derived from merely summing together each terms TFSF score; we must also track in which segments the noun phrase occurs. This is needed to decide the coverage of the noun phrase in the segment. We illustrate segment coverage by the example of two hypothetical segments A-2 and B-2 in Figure 4. If we assert that the terms in each segment are equivalent, we can show that segment B-2 has better coverage because two noun phrases in B-2 taken together appear across all three segments, whereas in A-2 the noun phrase cover only two segments. Figure 4 - Segment NP Coverage To calculate coverage, SEGNIFIER first iterates over all the occurrences of all terms within a segment, and then increments the score. The increment depends on the number of terms previously seen that also fall in the same segment. We use a harmonic series to determine the score: for the first occurrence of a term in some segment, 1 is added to the segments coverage score; a second occurrence adds 12; a third, 13, and so forth. Calculate Segment Importance Determine Segment Function segmented text (from Figure 1) Segment Significance labeled segments xxxxxxxxxxxxxx yyyyyyyyyyyyyy seg A-1 seg A-2 seg A-3 A B xxxxxxxxxxxxxx yyyyyyyyyyyyyy seg B-1 seg B-2 seg B-3 seg 1a seg 2a seg 3a We normalize both the sum of the TFSF scores over its terms and its coverage score to calculate the segment importance of a segment. Segment importance in our current system is given by a sum of these two numbers; thus the range is from 0.0 (not important) to 2.0 (maximally important). We summarize the algorithm for calculating segment importance in the psuedocode in Figure 5 below. for each segment { {  TFSF calculation TF_SF  sum of TF_SF per NP term; TF_SF  TF_SF  (max TF_SF over all segments); } {  coverage calculations coverage  sum of coverage per NP term; coverage  coverage  (max coverage over all segments); } seg_importance  TF_SF  coverage; } Figure 5 - Segment importance psuedocode 2.2 Segment Functions Contrasting with segment importance, which examines the prominence of a segment versus every other segment, we now turn to examine segment function, which looks at the role of the segment in discourse structure. We currently classify segments into three types: a. Summary Segments  A summary segment contains a summary of the article. We assume either the segment functions as an overview (towards the beginning of an article) or as a conclusion (near the end of an article), so the position of the segment within the document is one of the determining factors. According to our empirical study, summary segments are segments with the highest segment importance out of segments that occur within the first and last 20 of an article. In addition, the importance rating must be among the highest 10 of all segments. b. Anecdotal Segments  Material that draws a reader into the main body of the article itself are known in the field of journalism as anecdotal leads. Similarly, closing remarks are often clever comments for effect, but do not convey much content. In our attempts to try to detect these segments, we have restricted our scope to the first and last segments of an article. Empirical evidence suggests that in the domain of journalistic text, at least a single person is introduced during an anecdotal segment, to relate the interesting fact or narrative. This person is often not mentioned outside the segment; since the purpose of relating the anecdote is limited in scope to that segment. Accordingly, SEGNIFIER looks for a proper noun phrase that occurs only within the candidate segment, and not in other segments. This first or last segment is then labeled as anecdotal, if it has not been already selected as the summary segment. This method worked remarkably well on our data although we need to address cases where the anecdotal material has a more complex nature. For example, anecdotal material is also sometimes woven throughout the texts of some documents. c. Support Segments  These segments are the default segment type. Currently, if we cannot assign a segment as either a summary or an anecdotal segment, it is deemed to be a support segment. 2.3 Related work on Segment Significance There has been a large body of work done of assessing the importance of passages and the assignment of discourse functions to them. Chen and Withgott (1992) examine the problem of audio summarization in domain of speech, using instances emphasized speech to determine and demarcate important phrases. Although their work is similar to the use of terms to demarcate segments, the nature of the problem is different. The frequency of terms in text versus emphasized speech in audio forces different approaches to be taken. Singhal and Salton (1996) examined determining paragraph connectedness via vector space model similarity metrics, and this approach may extend well to the segment level. Considering the problem from another angle, discourse approaches have focused on shorter units than multi-paragraph segments, but Rhetorical Structure Theory (Marcu 1997 and others) may be able to scale up to associate rhetorical functions with segments. Our work is a first attempt to bring these fields together to solve the problem of segment importance and function. 3 Evaluation 3.1 Segmentation Evaluation For the segmentation algorithm we used a web- based segmentation evaluation facility to gather segmentation judgments. Each of the 20 articles in the corpus was segmented by at least four human judges, and the majority opinion of segment boundaries was computed as the evaluation standard (Klavans et al. 1998). Human judges achieved on average only 62.4 agreement with the majority opinion, as seen in Table 2. Passonneau and Litman (1993) show that this surprisingly low agreement is often the result of evaluators being divided between those who regard segments as more localized and those who prefer to split only on large boundaries. We then verified that the task was well defined by testing for a strong correlation between the markings of the human judges. We test for inter-judge reliability using Cochran (1950)s Q- test, also discussed in Passonneau and Litman (1993). We found a very high correlation between judges indicating that modeling the task was indeed feasible; the results showed that there was less than a 0.15 chance on average that the judges segment marks agreed by chance. We also calculated Kappa (K), another correlation statistic that corrects for random chance agreement. Kappa values range from -1.0, showing complete negative correlation to 1.0, indicating complete positive correlation. Surprisingly, the calculations of K showed only a weak level of agreement between judges (K avg  .331, S.D. .153). Calculations of the significance of K showed that results were generally significant to the 5 level, indicating that although the interjudge agreement is weak, it is statistically significant and observable. We computed SEGMENTERs performance by completing the 4-fold cross validation on the test cases. Examining SEGMENTERs results show a significant improvement over the initial algorithm of Hearst 1994 (called TEXTTILING), both in precision and recall. A future step could be to compare our segmenting algorithm against other more recent systems (such as Yaari 1997, Okumura and Honda 1994). We present two different baselines to compare the work against. First, we applied a Monte Carlo simulation that segments at paragraph breaks with a 33 probability. We executed this baseline 10,000 times on each article and averaged the scores. A more informed baseline is produced by applying a hypergeometric distribution, which calculates the probability of some number of successes by sampling without replacement. For example, this distribution gives the expected number of red balls drawn from a sample of n balls from an urn containing N total balls, where only r are red. If we allow the number of segments, r, to be given, we can apply this to segmentation to pick r segments from N paragraphs. By comparing the results in Table 3, we can see that the correct number of segments (r) is difficult to determine. TEXTTILINGs performance falls below the hypergeomtric baseline, but on the average, SEGMENTER outperforms it. However, notice that the performance of the algorithm and TEXTTILING quoted in this paper are low in comparison to reports by others. We believe this is due to the weak level of agreement between judges in our trainingtesting evaluation corpus. The wide range of performance hints at the 15 WSJ 5 Economist Total Precision Recall Precision Recall Precision Recall avg S.D. avg S.D. avg S.D. avg S.D. avg S.D. avg S.D. Monte Carlo 33 29.0 9.2 33.3 .02 32.8 12.6 33.3 .02 29.8 9.9 33.3 .02 Hypergeometric 30.6 NA 30.6 NA 32.9 NA 32.9 NA 32.0 NA 32.0 NA TEXTTILING 28.2 18.1 33.4 25.9 18.3 20.7 18.7 18.5 25.8 18.7 29.8 27.8 SEGMENTER 47.0 21.4 45.1 24.4 28.6 26.2 22.67 25.2 42.6 23.5 39.6 25.9 Human Judges 67.0 11.4 80.4 8.9 55.8 17.2 71.9 4.6 62.4 13.5 78.2 87.6 Table 2 - Evaluation Results on Precision and Recall Scales variation which segmentation algorithms may experience when faced with different kinds of input. 3.2. Segment Significance Evaluation As mentioned previously, segments and segment type assessments have been integrated into a key sentence extraction program (Klavans et al. 1998). This summary-directed sentence extraction differs from similar systems in its focus on high recall; further processing of the retrieved sentences would discard unimportant sentences and clauses. This system used the location of the first sentence of the summary segment as one input feature for deciding key sentences, along with standard features such as title words, TFIDF weights for the words of a sentence, and the occurrences of communication verbs. This task-based evaluation of both modules together showed that combining segmentation information yielded markedly better results. In some instances only segmentation was able to identify certain key sentences; all other features failed to find these sentences. Overall, a 3.1 improvement in recall was directly achieved by adding segment significance output, increasing the systems recall from 39 to 42. Since the system was not built with precision as a priority, so although precision of the system dropped 3, we believe the overall effects of adding the segmentation information was valuable. 4 Future Work Improvements to the current system can be categorized along the lines of the two modules. For segmentation, applying machine learning techniques (Beeferman et al. 1997) to learn weights is a high priority. Moreover we feel shared resources for segmentation evaluation should be established 5, to aid in a comprehensive cross-method study and to help alleviate the problems of significance of small-scale evaluations as discussed in Klavans et al (1998). 5 For the purposes of our own evaluation, we constructed web-based software tool that allows users to annotate a document with segmentation markings. We propose initiating a distributed cross evaluation of text segmentation work, using our system as a component to store and share user-given and automatic markings. For judging segment function, we plan to perform a direct assessment of the accuracy of segment classification. We want to expand and refine our definition of the types of segment function to include more distinctions, such as the difference between documentsegment borders (Reynar 1994). This would help in situations where input consists of multiple articles or a continuous stream, as in Kanade et al. (1997). 5 Conclusion In this paper we have shown how multi-paragraph text segmentation can model discourse structure by addressing the dual problems of computing topical text segments and subsequently assessing their significance. We have demonstrated a new algorithm that performs linear topical segmentation in an efficient manner that is based on linguistic principles. We achieve a 10 increase in accuracy and recall levels over prior work (Hearst 1994, 1997). Our evaluation corpus exhibited a weak level of agreement among judges, which we believe correlates with the low level of performance of automatic segmentation programs as compared to earlier published works (Hearst 1997). Additionally, we describe an original method to evaluate a segments significance: a two part metric that combines a measure of a segments generality based on statistical approaches, and a classification of a segments function based on empirical observations. An evaluation of this metric established its utility as a means of extracting key sentences for summarization. Acknowledgements The authors would like to thank Slava M. Katz, whose insights and insistence on quality work helped push the first part of the research forward. We are also indebted to Susan Lee of the University of California, Berkeley 6 for providing empirical validation of the segment significance through her key sentence extraction system. Thanks are also due to Yaakov Yaari of Bar-Ilan University, for helping us hunt down additional segmentation corpora. Finally, we thank the anonymous reviewers and the Columbia natural language group members, whose careful critiques led to a more careful evaluation of the papers techniques. 6 Who was supported by the Computing Research Association. References Barzilay R. and Elhadad M. (1997) Using Lexical Chains for Text Summarization. Proceedings of the Intelligent Scaleable Text Summarization Workshop, ACL, Madrid. Beeferman D., Berger A. and Lafferty J. (1997) Text Segmentation Using Exponential Models. Proceedings of the Second Conference on Empirical Methods in Natural Language Processing. Carletta J. (1996) Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, vol. 22(2), pp. 249-254 Chen F. R. and Withgott M. (1992) The Use of Emphasis to Automatically Summarize A Spoken Discourse. Proceedings of 1992 IEEE Intl Conference on Acoustics, Speech and Signal Processing, vol. 1, pp. 229-232. Cochran W. G. (1950) The comparison of percentages in matched samples. Biometrika vol. 37, pp. 256- 266. Grishman R., Macleod C. and Meyers A. (1994). COMLEX Syntax: Building a Computational Lexicon, Procedings of the 15th Intl Conference on Computational Linguistics (COLING-94). Hearst M. A. (1994) Multi-Paragraph Segmentation of Expository Text, Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics. Hearst M. A. (1997) TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages, Computational Linguistics, vol 23(1), pp. 33-64. Justeson J. and Katz S. (1995) Technical Terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering, vol. 1(1), pp. 9-29. Klavans J., McKeown K., Kan M. and Lee S. (1998) Resources for the Evaluation of Summarization Techniques. Proceedings of the 1st Intl Conference on Language Resources and Evaluation, Grenada, Spain: May. 1998. Kanade T, et al. (1997) Spot-It:Segmenting News Videos into Topics. Proceedings of the Digital Library Initiatives Project-Wide Workshop, Carnegie Mellon University, pp. 41-55. Kozima H. (1993) Text segmentation based on similarity between words. Proceedings of the 31th Annual Meeting of the Association of Computational Linguistics, pp. 286-288. Marcu D. (1997) The Rhetorical Parsing of Natural Language Texts. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pp 96-103. Miller G. A., Beckwith R., Fellbaum C., Gross D. and Miller K. J. (1990) WordNet: An on-line lexical database. Journal of Lexicography, vol. 3, pp. 235- 244. Morris J. and Hirst G. (1991) Lexical Coherence Computed by Thesaural Relations as an Indicator of the Structure of Text. Computational Linguistics, vol. 17(1), pp 21-42. Okumura M. and Honda T. (1994) Word sense disambiguation and text segmentation based on lexical cohesion. Procedings of the 15th Intl Conference on Computational Linguistics (COLING-94), pp. 755-761. Passonneau R. J. and Litman D. J. (1993) Intention- based segmentation: human reliability and correlation with linguistic cues. Proceeding of the 31st Annual Meeting of the Association of Computation Linguistics, pp. 148-155. Reynar J. (1994) An Automatic Method of Finding Topic Boundaries\" Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (student session), Las Cruces, New Mexico. Salton G. (1989) Automatic text processing: the transformation, analysis, and retrieval of information by computer. Addison-Wesley, Reading, Massachusetts. Singhal A. and Salton G. (1995) Automatic Text Browsing Using Vector Space Model. Proceedings of the Dual-Use Technologies and Applications Conference, pp. 318-324. Yaari Y. (1997) Segmentation of Expository Text by Hierarchical Agglomerative Clustering. Recent Advances in NLP 1997, Bulgaria. Youmans C. G. (1991) A new tool for discourse analysis: The vocabulary-management profile. Language, vol. 67, pp. 763-789.",
  "2.pdf": "arXiv:cs9809022v1 [cs.CL] 17 Sep 1998 Modelling Users, Intentions, and Structure in Spoken Dialog Bernd Ludwig, Gunther Gorz, and Heinrich Niemann March 31, 2018 Abstract We outline how utterances in dialogs can be interpreted using a partial ﬁrst order logic. We exploit the capability of this logic to talk about the truth status of formulae to deﬁne a notion of coherence between utterances and explain how this coherence relation can serve for the construction of ANDOR trees that represent the segmentation of the dialog. In a BDI model we formalize basic assumptions about dialog and cooperative behaviour of participants. These assumptions provide a basis for inferring speech acts from coherence relations between utterances and attitudes of dialog participants. Speech acts prove to be useful for determin- ing dialog segments deﬁned on the notion of completing expectations of dialog participants. Finally, we sketch how explicit segmentation signalled by cue phrases and performatives is covered by our dialog model. 1 Introduction During the last years, a large number of spoken language dialog systems have been developed whose functionality normally is restricted to a certain application domain. [SadMor97] give an quite extensive overview of existing implementations. Only few systems for generating dialog managers exist or are under development currently. These tools identify task and discourse structure and describe it by means of ﬁnite state au- tomata. Using these tools one can easily and quickly implement spoken language human-machine communication for simple tasks. Nevertheless, this approach lacks theoretical suﬃciency for a large number of phenomena occurring frequently in natural language dialogs. [SadMor97] state that these limitations rule out these approaches as a basis for computational models of intelligent interaction. Recently, there has been some research on extracting dialog structure out of annotated corpora ([Moe97]); algorithms for learning probability distributions of speech acts are used in this case. The estimated distributions serve as a basis for generating stochastic models for sequences of speech acts. But in this case, exploring common elements of dialogs in diﬀerent domains is substituted by an abstract optimization process, although knowledge of these elements could be useful for improving parameter estimation. On the other hand, many approaches to dialog processing consider diﬀerent structural elements important to gain a deeper understanding of the eﬀects that utterances have on the dialog itself and its participants. But these approaches do not take spoken language and the problems related to speech recognition into account. Following the opinon of [Poe94] we consider the separation of describing and described situation to be a crucial point for dialog processing. This reﬂection is backed up by philological and linguistic research on discourse ([Die87], [Mar92], [BriGoe82], [Bun97]). On this basis we present fundamental elements of dialog structure to handle even spoken language. The main aim of this paper is to build a bridge between research on spoken language processing and study of discourse structure and cognitive approaches to communication between individuals in order to conceptualize dialog systems that integrate experience from all areas of research just mentioned. 1 2 Diﬀerent Previous Approaches As there exists a vast amount of literature on discourse and user models, we ﬁrst give an account of some of the important directions of research performed up to now. The main interest of all this work is to make precise the notion of context which is said to be of great importance for natural language understanding. Inspired by the work of Grosz and Sidner ([GroSid86]) on the interrelations of task and discourse structure, a diversiﬁcation to structural, semantical and plan-oriented studies has taken place. 2.1 Discourse Structure The fundamental consideration for work on the structure of discourse is that there is a correspon- dence between the ordering of utterances in a discourse and how they are related among each other on the semantic level ([Gar94], [Gar97]). Tree structures are used to describe the semantic coherence of discourse. By using these structures, one can deﬁne constraints on possible places for attaching a new utterance to a existing discourse ([Web91]) and on accessibility relations for potential referents for deictic expressions ([Pol95]). In approaches based on Discourse Represen- tation Theory ([KamRey93]) this correspondence is captured by construction rules (which can be deﬁned in terms of an extended λ-calculussee [Kus96]) building up Discourse Representation Structures that describe the coherence relation of all the included contributions1. 2.2 Speech Act based Theories Many semantic theories work only locally, i.e. they describe the meaning of one single utterance, ignoring its context and the discourse situation in general. Insofar, they are unable to account for the functionality (i.e. intention) of a perceived utterance. Basing on earlier work by Austin, [Sea69] proposed a theory of speech acts that has been fundamental for research on this area. Speech acts are implemented in dialog managers to derive hypotheses of how the current utterance contributes to the dialog so far. But considering the last utterance only is insuﬃcient for a cooperative dialog participant as this local view does not pay attention to expectations of other parties involved in the dialog (e.g. when somebody asks a question, she expects the following utterance to be an answer to it). Consequently, to describe coherence in dialog steps, the eﬀects of previous utterances have to be recorded somehow. So, the structural approach of conversational games mentioned brieﬂy above provides an explanation for the speaker uttering something in the course of dialog. Another point of view to the coherence problem is taken by [TraAll94]: they propose that the speech acts associated to each utterance impose social and conventional obligations on the hearer and therefore constrain the set of possible legal responses. Equivalently, one can state that after uttering something the speaker has certain expectations that she wants to be fulﬁlled by any response that will be given in the next dialog step. 2.3 Intentions, Plans, and Coherence of Utterances By now, we are able to describe how linearly (by time of being uttered) ordered contributions to a dialog can be integrated into a (partially ordered) discourse structure, but it is still impossible to explain the motivations of the speaker to use a certain speech act, especially in the case when the expectations introduced by the previous are violated. To answer this question, one has to study the mental attitudes of dialog participants. Motivations for engaging in a dialog can be taken into account by studying planning of utter- ances. Discourse planning is discussed extensively e.g. by Lambert and Lochbaum ([Lam93] and 1DRT has been extended by many researchers in diﬀerent wayse.g. by Asher for capturing discourse segments ([Ash93]) or in the VERBMOBIL project for handling complex phenomena of spontaneous speech for machine translation ([vm135], [vm83]). But all these theories initially describe monologs and therefore do not consider multi-party communication which is characteristic for dialogs. 2 [Loc94]). Both authors concentrate on the integration of domain dependent planning steps into the interpretation of utterances. Approaches such as those by [MooYou94] or [CarCar94] devise a model for collaborative plans for response generation. Another line of research focusses on the BDI model which is a more domain independent ap- proach. This accentuates an agent-based view of dialog as the participants in dialog and their personal attitudes are considered to be of main interest. Beliefs, desires, and intentions are as- sumed to drive utterances and speech acts ([AshLas97], [AshSin93]). So the key problem for the interpretation by the dialog manager is the reconstruction of the speakers attitudes from what she uttered. Coherence of utterances is obtained by fundamental assumptions on cooperative behaviour ([Grice]) and by analyzing how the content of utterances coheres on the basis of the (mutually believed) domain knowledge ([AshLas91], [AshLas94], [AshLasObe92]). Along this di- rection of research there is also some work on coherence relations between utterances ([Kno96], for an overview see [BatRon94]). These relations characterize the logical connection between ut- terances and thereby serve as a basic instrument for an analysis of the argumentative structure described by a given dialog. 2.4 Spoken Language Phenomena In spoken language hesitations, repairs, etc. are very common, because in oral communication concentration on the topic of the dialog limits mental resources available for speech production. These phenomena cannot be captured by semantic formalisms as sketched above. To overcome this problem, multi-level processing of spoken language utterances has been proposed in the literature (e.g. see [TraHin91]). 3 Interpretation of Utterances This section explains our approach how utterances can be interpreted using First Order Partial Information Ionic Logic (FIL, [Abd95]) as a language for describing the semantics of utterances. A central issue of dialog management is that dialogs are motivated by the speakers desire to add information to the knowledge of the dialog participants. On the other hand, it occurs frequently that the shared knowledge of the participants does not contain enough information to meet the expectations that are pending between speaker and hearer. For that reason, our semantic language must be able to handle situations of partial knowledge. FIL provides formulae (so called ionic formulae) like ({φ1, ..., φk}, ξ) meaning intuitively that ξ is true when it is plausible that Φ  {φ1, ..., φk} (called justiﬁcation set or justiﬁcation context) is true, too (see [Abd95], Sect. 5). Φ is the set of missing information to infer ξ. FIL can be used to compute such justiﬁcation contexts. We incorporate FIL for the description of conditions in a DRT-based framework to represent dialog structure. An example of such a discourse representation structure (DRS) would be: Does a plane depart from Athens to Rome? has the semantic representation   t Rome Athens Plane(t) Depart(t) Airport(Athens) Airport(Rome) From(t, Athens) To(t, Rome)   In DRT, there exists a number of construction rules for incremental composition of several individual utterances. One can even infer whether DRS KN is a consequence of the DRS K1, ..., KN1. But whilst in standard DRT conditions are described by classical ﬁrst order formulae, in 3 our approach FIL is used for that purpose. As FIL is a partial logic, we can compute whether KN is undeﬁned given K1, ..., KN1. This is true, as FIL allows to talk about the truth value of a formula: undeﬁned(φ)  φ  φ So, we are able to assign one of the following three consequence states to KN:   KN. KN follows from the discourse so far.   KN  KN: KN follows from previous utterances.   KN  KN: KN is still undeﬁned. Using the deduction theorem ({φ1, ..., φK}  ψ    φ1...φK  ψ) we have established a coherence relation between utterances via implication in FIL. When φ1  ...  φK  ψ is true, ({φ1, ..., φK}, ψ) is true, too. Interpreting an utterance requires a knowledge base  for the representation of the domain relevant knowledge. As described in [LudGoNie98], we use description logics to deﬁne the notions to be understood by the dialog manager for a given application. More precisely, description logics serve for constructing a terminology of the domain, thereby representing domain dependent, but situation independent knowledge. In order to interpret a speciﬁc utterance in a given situation, the given terminology is instantiated by concrete facts that are entailed by the semantic representation of the current utterance. To give a simple example of this idea, we could state in the knowledge base of a ﬂight information system that a ﬂight from a departure location to an arrival location is a ﬂight characterized by the existence of an airport at the departure and the arrival location, respectively. In description logics, we could say: FlightFromTo  From.airport  To.airport  Flight So, this deﬁnition characterizes knowledge that holds in every situation in the given application domain. On the other hand, the DRS above describes a concrete situation. We conclude that the consequence states mentioned above have to be understood as conse- quence on the basis of a situation independent knowledge base that characterizes the application domain. To interpret utterances in a given situation, the dialog manager tries to infer the con- sequence state of the current utterance relying on his domain knowledge. In general, this state depends on a certain justiﬁcation context as outlined above. As will be shown below, we can verify the truth value of all elements in a justiﬁcation context Φ if we interpret each φi as a question to the hearer and view the subsequent response as an answer to this question. This means that the dialog managers planning steps are strongly aﬀected by the results of inference in its knowledge base. From that view and the semantics of ({φ1, ..., φK}, ψ) we can derive an n-ary ANDOR tree that reﬂects the discourse structure of the discussed dialog segment. In the tradition of [GroSto84] and [GabRey92] we consider (free) discourse referents of inter- rogative pronouns as λ-bound variables. If the problem solver ﬁnds a solution for the posed query, then it binds these variables to discourse referents that have been introduced earlier (or during the process of problem solving). Of course, there can be more than one possible substitution of the λ-variables. For given variables x1, ..., xM, we denote a substitution of all variables by discourse referents t1, ..., tM as Σ  {[x1t1], ..., [xMtM]}. {Σ1, ..., ΣK} is a set of K pairwise distinct substitutions. In the general case where the result of the inference process consists of a set {Σ1, ..., ΣK} of answer substitutions and a set {φ1, ..., φN1} of justiﬁcation substitutions, each Σi induces an edge in a OR-subtree of the overall discourse structure. This tree structure is part of the describing situation for the current dialog. Below we will introduce operations on the dialog tree that characterize how the structure is expanded in the course of dialog. In this sense, such a tree constitutes the syntax of the current dialog. But there is a strong connection to what could be called dialog semantics. It is grounded basically on the meaning of the edges in the tree: they express the fact that parent and child nodes are coherent in the sense of FIL consequence explained above. Furthermore, by the notion of satisﬁability of FIL 4 ionic formulae we exploit the tree structure to reformulate Grosz and Sidners relations dominance and satisfaction precedence2: because justiﬁcation context Φ  {φ1, ..., φk}, when (Φ, ψ) is given, is true if and only if all φi are true on the justiﬁcation level3, we have ψ  φi for all 1  i  k. And as  φi for one i  {1, ..., k} implies  (Φ, ψ), we obtain φi  φj for 1  i  j  k. 4 Basic Elements of Dialogs 4.1 Empirical Evidence for the Need of User Models Dialog managers for real world applications have to be robust in the sense that they always terminate an (user-)initiated dialog in a controlled way. So, the study of how to build robust generic dialog managers implies to reason about what structures exist in a dialog and how they get modiﬁed by utterances. On the other hand, it is also important to understand how dialogs aﬀect the participants and their future utterances. A ﬁrst approach to this problem is to see the function of utterances as that of updating the shared knowledgea data structure maintained and used by all dialog participants. From this point of view, each dialog participant infers the same consequences from every new utterance. But as pointed out in the AI literature, actually people hold personal assumptions about the meaning of an utterance. These assumptions can diﬀer among dialog participants. We illustrate this by an example taken from the TRAINS corpus (see [TrainsCorpus]): 1.1 M: okay 1.2 : we have to get .. a 1.3 : tanker car of orange juice to uh Avon 1.4 : and a 1.5 : boxcar of bananas to Corning 1.6 : and we have to do that by 3 PM today 2.1 S: okay 3.1 M: okay 3.2 : so lets see umm 3.3 : ... we probably have to take the tanker car 3.4 : from Corning to Elmira 3.5 : to get uh 3.6 : orange juice in it 3.7 : um [2sec] 3.8 : [click] and uh 3.9 : how far is it from Corning to Elmira 3.10 : how long would it take 4.1 S: 2 hours 5.1 M: m hm 5.2 : okay so [2sec] 5.3 : why dont we uh 5.4 : lets see 5.5 : [sniﬀ] 5.6 : okay why dont we 5.7 : would w 5.8 : uh 5.9 : why dont we consider sending uh 5.10 : engine E2 5.11 : to Corning 5.12 : to get the tanker car 5.13 : and uh 5.14 : bring it back to Elmira [2sec] 5.15 : uh 5.16 : and uh have them [2sec] 5.17 : have them ﬁll it 5.18 : with OJ 5.19 : so how long would it take 6.1 S: well y  you need to get oranges 6.2 : to the OJ factory 7.1 M: oh  okay 7.2 : theres no oranges there yet 7.3 : okay so In (1.1) to (1.6) M describes the goal of this dialog and some constraints. Doing that he makes some of his mental attitudes public, thereby assuming that S will be able to interpret them appropriately. So (1.1) to (1.6) do not transport content about the domain, but about M exclusively. The impact of the observation that utterances can contain domain relevant knowledge 2α dominates β (α  β) if and only if β is part of α, while α satisfaction-precedes β (α  β) if and only if α is neccessary for β 3i.e. (stated in model-theoretic semantics) there exists an interpretation that expands any interpretation that makes ψ true in such a way that it assigns true to all φi, too. 5 as well as knowledge about other dialog participants is enormous: In (6.1) and (6.2) S tries to explain why M will not be able to reach his goals by explaining why Ms information about the domain and the current domain scenario is incomplete or false. This is possible only because S can diﬀerentiate between his own domain knowledge and that transported by the previous utterances. As a consequence, S cooperative behaviour is made possible by his ability to reason about the domain and about his assumptions of Ms view of the domain. This example shows that cooperative dialog managers must maintain some sort of user model. Our approach will be discussed in the remainder of this section. 4.2 Rational Behaviour of Dialog Participants Discussing dialog management, one normally assumes that people initiate communication with others in order to get help for achieving a certain goal. From these observations, we can derive that questions are asked to get an answer that completes the speakers knowledge in some way. For an answer to be helpful, it has to meet certain constraints (expressed by [Grice] in his maxims of cooperation): ﬁrst, it has to be coherent with the question so that it can deliver valuable information. And, of course, it should be true. These requirements pose constraints on the behaviour of the person that is giving the reponse, too. This person has to be cooperative, i.e. she should adopt the speakers goals, as far as she can realize them. Honesty is another crucial point. For a person not feeling obliged to telling the truth is not a reliable source of information. Our dicussion is restricted to dialogs that fulﬁll the requirements above. I.e., we assume some amount of rational behaviour for all dialog participants. To reason about goals and intentions, one has to study the cognitive structures that underly rational behaviour. For dialogs, these structures are described at length in [TR663]. One major challenge for designing robust dialog systems seems to be how to reconstruct the contents of the mental states of the user out of the utterancesthe only observable facts. So the study of how language can transfer attitudes and reﬂect planning steps of dialog participants becomes very important. [Eng88] notes that in German basically one can communicate three diﬀerent types of utterances:  constative: state facts. Our requirement of honesty admits the conclusion that nothing wrong is intentionally stated to be true.  interrogative: ask something.  imperative: request something. 4.3 Discourse Domain and Application Domain To formalize the intuitions described up to now, we need a framework that enables us to talk about utterances or the corresponding DRS, respectively. We can achieve this by introducing a discourse domain4 that is the domain of describing situations. Atomic elements are DRS whereas in the application domain (consequently to be deﬁned as the domain of described situations) atomic elements are objects of the application. This reﬂects the ability of natural language to climb up to a meta-level, e.g. simply by saying What you have told me up to now, has been clear to me. But I cant understand what I should do now. If one thinks about a situation when a teacher instructs a pupil how to achieve some goal, then it becomes obvious that responses as above refer only to the instructions, but not to what is instructed currently. We use this framework to reason about the relation of utterances and speakers attitudes. Following the notation in [AshLas97], we express the connection between formulating attitudes and the attitudes 4The discourse domain is not the domain of discourse as we try to make clear in the remainder of this section. We call the domain of discourse the application domain to express the fact that discourse structure and application structure (sometimes called task structure) are diﬀerent and not isomorphic. 6 themselves in the following way5: constative(K)  WI(BR(K)) (1) interrogative(K)  WI(KI(K)) (2) imperative(K)  WI(doR(K)) (3) To describe defeasible rules, we exploit FIL ionic formulae: defaults eventually to be defeated by overriding exceptions are expressed as ionic formulae. By that the consequence of an implication is assumed to be valid as long as there is no evidence to the contrary. If we have φ  (ψ, ψ) and φ, then ψ is considered true, unless there is evidence for ψ to be false. So we can formulate  a principle of sincerity: II(BR(K))  (BI(K), BI(K)) (4)  principles of cooperation: WI(K)  BI(K)  (WR(K)  BR(K), WR(K)  BR(K)) (5) WI(doI(K))  (WR(KI(K)), WR(KI(K))) (6) After having deﬁned how linguistically motivated types of utterances and mental states are interrelated among each other and how fundamental principles of collaboration in dialogs are expressed formally, we show (by means of two examples) how mental states and speech acts are connected via defeasible rules in FIL: WI(KI(K))  (queryI(K), queryI(K)) (7) BR(( (ξ  K)  (ξ  K)))  WR(KI(K))  WR(BI(ξ))  (informR(ξ), informR(ξ)) (8) Informally, the ﬁrst rule states that anything one wants to know is normally asked for, while the second rule claims that if one wants another dialog participant to know something and believes it could be a consequence of something else and wants the other to know that, too, then one informs about that new fact. We can draw the following conclusion: After the speaker (I) has asked K and gets ξ as response, she can infer defeasibly informR(ξ) assuming that ξ is constative and relying on cooperation if there is no evidence for R (the hearer of Is utterance, but now speaker of ξ) against the belief ξ  K. Furthermore, by sincerity I can also infer that R intends I to believe ξ  K. This example outlines our approach of to how reconstruct speech acts from observable utterance types via reasoning about the speakers intentions. Recognizing speech acts is important for a cooperative dialog manager as it helps to explore what state the dialog currently is in (e.g. a question has been asked and an appropriate answer is now expected). It seems to be just this notion of dialog state that captures the interactive nature of dialogs in contrast to monological discourse like a newspaper article. Describing how the transition of the dialog state looks like in the course of utterances therefore extends the notion of coherence in discourse sketched above. But, as can be seen easily from the inform rule (8) above, coherence is the link betwen mental states and speech acts: no speech act can be recognized without reasoning about coherence of utterances. In the rule above, informR(ξ) can be inferred only if  ξ  K or  ξ  K. The case when ξ  K is undeﬁned, will be discussed later. 5We denote the speaker I (initiator) and the hearer R (responder). B expresses beliefs, W desires, I intentions, and K knowledge 7 4.4 Dialog States and Dialog Structure First we turn to the discussion of dialog states and their transition relation: As noted in the literature (see e.g. [TraAll94, CriWeb97]), utterances in dialogs induce expectations of what a plausible response should look like. Stated diﬀerently, by her speech act, the speaker poses some obligation on the hearer that constrains the preferred responses. Nevertheless, a model of dialog that does not restrict the range of valid utterances too much, should give an account for a cooperative reaction even if expectations have been violated in a response. Meeting an expectation and thereby completing the conversational game opened by the initator, the responder implicitly performs a segmentation of the discourse. In what follows, we describe the syntax of discourse segments using a context free grammar whose rules are attributed by operations on the dialog structure depending on the coherence between the current utterance and the knowledge shared between the dialog participants. In that way, we deﬁne the semantics of discourse segmentation in terms of coherence of utterances using the notion of the ANDOR tree introduced above. 4.4.1 Simple Segments In a simple segment the speaker poses no obligations on the hearer. When no obligations are pending, an inform act would create a simple segment according to the following rule: SEG  informI(K) (9) SEG  newAND(SEG, K) This rule states K can be added as a new fact to the shared knowledge, and therefore be inserted as an AND branch into the currently centered segment. In the absence of pending expectations, the root segment (i.e. the root of the dialog tree) is centered by default6. 4.4.2 Complex Segments Dialog segments are called complex when the speaker assigns expectations to her utterance. For example, when asking a question she expects an answer to be given by the hearer that helps perform her intentions. But it is obvious that the hearer could have reason not to fulﬁll the expectations (at least temporarily): she could need additional information ﬁrst in order to answer the question and therefore respond with a new one. After this question will have been answered the hearer will respond according to the still pending expectation. We capture this situation with the following rules: SEG  queryI(K) QUERYEXP (10) SEG1  newAND(SEG1, newSubTree(K, QUERYEXP)) SEG  SEG SEG (11) SEG1  newAND(SEG2, SEG3) Here, the ﬁrst rule says that a segment can be initiated by a query imposing an obligation to answer it. On the other hand, as it is expressed in the second rule, a segment can consist of two subsegments forming an AND subtree. QUERYEXP follows the rule: QUERYEXP  ANSWER (12) QUERYEXP  ANSWER QUERYEXP  ANSWER QUERYEXP (13) QUERYEXP1  newOR(ANSWER2, QUERYEXP3) 6Below, after having introduced the operations on the dialog tree, we will give an example illustrating the application of the segmentation rules. 8 ANSWER is deﬁned as follows: ANSWER  informI(K) (14) ANSWER  newNode(K) ANSWER  SEG informI(K) (15) ANSWER  changeRoot(SEG, K) 4.5 Example In this section, we give an example of how our dialog model behaves at work. For the purpose of illustration we use the following short ﬂight information dialog: α User: I want a ﬂight from Athens. β1 System: Where do you want to go to? γ1 User: To Rome. β2 System: On which day? γ2 User: On Monday. β3 System: When do you want to depart? γ3 User: At 12 oclock. ρ System: There are several connections: AZ717 at 06:55, OA233 at 09:05, AZ725 at 09:20, OA235 at 12:55, AZ721 at 16:00, OA239 at 17:10, AZ723 at 20:20. As will be discussed in Sect. 7.2, I want expresses a desire explicitly and is therefore repre- sented as WI(α). From that we conclude (queryI(α), queryI(α)). Besides of that, we have the implications WR(KI(α)) by cooperation and informI(α). On the other hand, grammatically α is constative. Therefore constative(α)  WI(KR(α)) and, consequently, (informI(α), informI(α)). But justiﬁcation context informI(α) is in conﬂict with informI(α). Therefore, the only derivable hypothesis for αs speech act is queryI(α). According to the rules for discourse segmentation above, this induces a QUERYEXP for R (i.e. the dialog manager). This obligation is expressed in the user model by WR(KI(α)). In order to fulﬁll this desire, the dialog manager tries to evalute the consequence status of α (see Sect. 3). This evaluation is performed on the basis of the domain model (for a more elaborate example see [LudGoNie98]). Its result is that the status of α depends on the justiﬁcation context {At, To, On}. I.e. departure day and time as well as the destination are still unknown. So, mental attitudes of the dialog manager change: a new desire WR(KR(β1)) is added that implies (queryR(β1), queryR(β1)). I.e. a new discourse obligation is being introduced leaving the old one pending. For the users response constative(γ1) holds, implying WI(KR(γ1)). From γ1 the dialog man- ager infers γ1  β1. Consequently, it assumes BI(γ1  β1) and (informI(γ1), informI(γ1)). As there is no evidence against informI(γ1) and no other hypothesis for a possible speech act, informI(γ1) is assumed. This completes queryR(β1). With this completion, the dialog manager obtains information about the destination. Processing of β2, γ2 and β3, γ3 is performed analogously. After that, the justiﬁcation context inferred during the interpretation α has been answered completely. Having got all this information the dialog manager can compute a solution for α that is uttered in ρ. Fig. 1 shows the dialog segmentation for this example, while Fig. 2 sketches the coherence structure of all utterances, represented in a DRS in Fig. 3. A point worth mentioning is that the structure of the justiﬁcation context as an AND tree is embedded in the coherence structure of the dialog. 9 SEG    queryI(α) QUERYEXP ANSWER    SEG    SEG    QUERYEXP ANSWER informI(γ1) queryR(β1) SEG    SEG    QUERYEXP ANSWER informI(γ2) queryR(β2) SEG    QUERYEXP ANSWER informI(γ3) queryR(β3) informR(ρ) Figure 1: Segmentation of the Example Dialog queryI(α) informR(ρ)     queryR(β1) informI(γ1) queryR(β2) informI(γ2) queryR(β3) informI(γ3) Figure 2: Coherence structure of the Example Dialog 5 Incoherence and its Eﬀects on Dialog Structure This section explains in more detail how our dialog manager handles situations when two utter- ances are incoherent in the sense mentioned earlier although they otherwise would not violate any expectations. Consider the dialog is Fig. 4 for a motivation of the problem: This dialog is quite regular according to the rules on dialog state described in the previous section until U utters θ. Shared domain knowledge  and the facts collected during the dialog do not allow to conclude that θ and η are coherent. One can derive that   {θ}  η  η. So the question arises which speech act to assign to θ. To answer that question we have to go back some steps in the dialog: in occasion of uttering γ, S made U assume WS(KS(γ)) implying WU(KS(γ)). From this observation and from   {ζ}  γ, S can infer that U wants to give an answer to γ thereby ignoring Ss expectation that ǫ will be answered by U. How can such a situation be described in our model of dialog states? Following [Tra94], we assume that several speech acts can be assigned to one utterance allowing the speaker to express multiple intentions at one time. Except of the inform act derived be inferring coherence between θ and γ, we assign a cancel act to Us last utterance. cancel has the meaning that a pending dialog obligation is violated intentionally as it is the case when U referred to γ when responding to η. 10   α β1 β2 β3 γ1 γ2 γ3 ρ α    u α α    f Athens Flight(f) From(f, Athens)   want(u, α)   β1  λl.   u Go(u) Location(l) To(u, l)   γ1  β1(Rome) β2  λd.   u Go(u) Day(d) On(u, d)   γ2  β2(Monday) β3  λt.   u Go(u) Time(t) At(u, t)   γ3  β3(12 oclock) ρ    c AZ717 OA233 AZ725 OA235 AZ721 OA239 AZ723 Several(c) Connection(c) Flight(AZ717) At(AZ717, 06 : 55) Flight(OA233) At(OA233, 09 : 05) Flight(AZ725) At(AZ725, 09 : 20) Flight(OA235) At(OA235, 12 : 55) Flight(AZ721) At(AZ721, 16 : 00) Flight(OA239) At(OA239, 17 : 10) Flight(AZ723) At(AZ723, 20 : 20)     Figure 3: Discourse Representation Structure for the Example Dialog cancel is deﬁned by ( K  K)  informR(K)  cancelR(K) (16) To integrate cancel into speech act processing, we add a new rule for ANSWER: ANSWER  cancelI(K) (17) ANSWER   (18) ?α U: Is there a ﬂight to Rome on Saturday? .β S: Yes. LH745 at 10:38, AZ304 at 15:03, or 2G261 at 16:25. ?γ S: Which airline do you prefer? .δ U: The Alitalia ﬂight would be quite convenient. ?ǫ U: Do they oﬀer business class? .ζ S: Yes, they do. ?η S: Have you got a MilleMiglia card? .θ U: [Hmm, actually] I rather prefer Lufthansa due to their superb service. Figure 4: Example Dialog 11 6 Conﬁguration of Dialog Managers Our approach to dialog understanding as it has been characterized up to now is dominated by the idea to separate domain independent algorithms and data structures from domain dependent data for speciﬁc applications and to separate the discourse domain proper from the application domain. By the isolation of domain independent dialog elements we try to explore the minimal amount of dialog structures to be conﬁgured for a speciﬁc application. In particular, we have distinguished three models contributing to the setup of a dialog system for a given application:  Domain model It deﬁnes the notions that exist in the application domain and how these notions are being interpreted. Additionally, it describes how the vocabulary of the domain is connected with notions deﬁned in the domain model (see Sect. 3).  Dialog model The conversational games valid for the application (see Sect. 4.3) and the rules how moves of diﬀerent games can be interleaved among each other are deﬁned in the dialog model. Rules for the games specify how the dialog structure (represented by the dialog manager as an ANDOR tree) is aﬀected by a certain game. In addition, it is possible to restrict dialog participants to diﬀerent sets of conversational games that they are allowed to begin. E.g. in a dialog model without mixed initiative the user would not be permitted to begin a query game, but only be allowed to react with inform. It is an open question whether a restriction of the kind just described could serve as a suﬃcient characterization of the complexity of dialogs.  Model of dialog participants (user model) In order to reason about motivations for conversational games one has to connect game moves (i.e. speech acts) with mental attitudes of dialog participants. For this purpose, the user model deﬁnes neccessary conditions of mental attitudes for each speech act. On the other hand, it also contains the general principles of rational behaviour that hold between the attitudes of dialog participants (see 4.2). During the conﬁguration for a speciﬁc application the models sketched above have to be deﬁned. We argue that at least for the dialog model and the user model there exists a large application independent subset of deﬁnitions that holds for any application and has only to be completed for a concrete domain. In many cases, such a subset would reduce conﬁguration to the deﬁnition of an appropriate domain model. From this point of view, it would be worth analyzing which classes of dialogs could be covered by proposals for domain independent sets of speech acts (such as the one described by the Discourse Resource Initiativesee [DRI97]). 7 Explicit Modiﬁcation and Segmentation of Dialog Struc- tures Any model for describing discourse as the one sketched in this paper should give an account not only for an implicit construction of dialog structures, but also for its explicit modiﬁcation by the dialog participants (see e.g. [Coh90]). Such an account would reﬂect the capability of dialog to talk about what [Bun97] has called dialog control, or about attitudes and mental states of dialog participants. Switching to the meta-level is normally signalled by cue phrases or performatives. In the remainder of this section we will discuss how these special types of utterances that have a well-deﬁned meaning only in the describing situation of the dialog aﬀect our dialog model. 12 7.1 Cue Phrases In our opinion, many natural language expressions are like polymorphous operators in object- oriented programming languages: they take arguments of diﬀerent type and have diﬀerent seman- tics each time. This view is shared by other researchers, tooe.g. [Ben88]. E.g. in the utterance Do you want to depart from Munich or from Frankfurt? or expresses a choice between two locationsi.e. two objects of the described situation. On the other hand, in Will you go there by bus or rather take the car? or again states two possible alternatives, but, in this case, they are utterancesi.e. objects of the describing situation. To incorporate interpretation of cue phrases of into the dialog model we rely on Knotts work ([Kno96]) on coherence relations. Knott discusses extensively how cue phrases contribute to the understanding of discourse coherence: He assumes that any cue phrase has the function of an operator between previous utterances α1, ..., αN and an utterance β following the cue phrase. These utterances are connected by a defeasible rule P1 ...PN  C which we can express in FIL as (P1  ...  PN  C, P1  ...  PN  C). Each cue phrase has an associated set of features like polarity of the consequent etc. that deﬁne its semantics and how the utterances in the scope of the cue phrase are linked to Pi and C. E.g. for but, we have Pi : αi and C : β. Consequently for α, but β coherence between α and β is expressed by α  β. For the general case, coherence between utterances in the scope of the cue phrase can be established if {P1, ..., PN}  C or {P1, ..., PN} C. This result can be exploited to update the dialog structure appropriately. For a deeper investigation of this topic lets have a look at the following dialog: ?α U: Is there a ﬂight to Rome on Saturday? .β S: Yes. AZ631 at 15:03 ?γ U: How much is it? .δ S: DM 528 plus tax. ?ǫ U: or on Monday, what about that? .ζ S: On Monday you can ﬂy with Debonair. ?η U: How much is a ticket? .θ S: DM 199 plus tax. How is or on Monday? processed and integrated into the dialog structure? Firstly, it has to be noticed that ellipsis resolution on the PP on Monday yields as syntactic referent Is there a ﬂight to Rome?. Secondly, the DRS for the describing situation after δ has been processed, is as shown in Fig. 5.   α β γ δ α  λ f.   s r f flight(f) Rome(r) to(f, r) Saturday(s) on(f, s)   β  α(az631) γ  λ p.  az631p cost(az631, p)  δ  γ(528Tax)   Figure 5: DRS after δ has been processed. Combining the remark on the ellipsis on Monday and the DRS for the dialog so far, we ﬁnd that the right-hand argument of the operator or is α[SaturdayMonday]7. So, we can denote a 7i.e. in α all appearences of Saturday are substituted by Monday 13 DRS (see Fig. 6) that describes the semantics of ǫ. ǫ    X α[SaturdayMonday] α  λ f.   s r f flight(f) Rome(r) to(f, r) Saturday(s) on(f, s)   X ? X or α[SaturdayMonday]   Figure 6: DRS for ǫ Obviously, as can be seen from the DRS for ǫ, the problem of ﬁnding an appropriate discourse referent for X can be solved by anaphora resolution in the describing situation. The only an- tecedent in the describing situation compatible with α[SaturdayMonday] is α. So, α has been centered by ǫ. The impact of ǫ on the dialog structure is determined essentially by how or modiﬁes the dialog segmentation: the segment of α is substituted by an OR subtree representing the two arguments of or (see Fig. 7). For the utterances to follow, ǫ is the center, and dialog processing works as usual. 7.2 Performatives and Modal Verbs Performatives and modal verbs state assertions not about the described, but the describing situ- ation; more precisely, they express assertions about mental states and speech acts as in I must leave you now, On what day do you want to depart?, I suggest not to pay at all for this bad ﬁlm. As such utterances do not talk about the described situation, they cannot be processed as if they did. Consequently, speech act recognition does not apply as normally in this case. For that reason, all rules above for inferring speech acts are defeasible. Therefore we can devise special rules for performatives and modal verbs that override defeasible inferences based solely on syntactic and prosodic criteria if an inspection of the content of the utterance provides evidence against these default conclusions. To do this, we classify performatives and modal verbs according to what mental state they operate on or what speech act they express. In the utterance I want to ﬂy to Rome on Saturday., want expresses implicitly the query Is there a ﬂight to Rome on Saturday.. It is clear immediately that the utterance is actually not an inform act that poses no obligations to respond cooperatively on the hearer. So desires and γ  α β ROOT δ ǫ  α[SaturdayMonday] Figure 7: Dialog Tree after Applying α or α[SaturdayMonday] 14 queries are defeasibly connected by: WI(doI(K))  (queryI(K), queryI(K)) (19) WI(doI(K)) overrides inform in the following way: WI(doI(K))  informI(K) (20) When the dialog manager starts processing the utterance above, it infers the following:  informI(K) with justiﬁcation context informI(K)  queryI(K) with justiﬁcation context queryI(K) As the semantics of I want to belongs to the class WI(doI(K)), we can use this fact (that has been derived from the semantic representation of the utterance) to infer informI(K) by (20). As a result, the only valid inference is queryI(K). By cooperation, we can infer WR(KI(K)). After that, we are in exactly the same situation as if the speaker had asked directly for a ﬂight to Rome. 8 Conclusions We have reported about work in progress on developing a theoretical approach on dialog structure in order to build domain independent, cooperative, and robust dialog managers. The theoretical framework outlined in this paper has already been implemented partially and was tested for a small domain. In an evaluation, the implementation has proven to perform well. On the other hand, we have analyzed a large corpus of train-inquiry dialogs collected with our EVAR ([EckNie94]) system. An important result is that the quality of speech recognition determines how cooperative the EVAR system8 really is. To improve this situation we work towards improving dialog theory as presented in this paper by integrating BDI-oriented, structural, and plan based approaches to dialog understanding. Experience from analyzing dialogs that have not been terminated success- fully has shown that our approach is capable to overcome the failures that caused the inacceptable terminations. 9 Future Research Using the results presented in this paper as a basis, we will continue our work on dialog struc- ture by describing precisely how a processing level for handling typically diﬃcult phenomena of spoken language like repairs etc. can be integrated into our model. We intend to achieve this by incorporating Traums model of grounding (see [Tra94]) into our framework. This mechanism will have to be expanded to work properly on word hypothesis graphs that are the basic data structure of the common ground. This allows for a full exploitation of the results produced by the speech recognizer. Furthermore, we want to integrate our implementation of a chunk parser as a robust algorithmic approach to a theory of incremental discourse processing as described e.g. in [Poe94]. References [Abd95] N. Abdallah, The Logic of Partial Information, Springer, New York 1995 [Ash93] N. Asher, Reference to Abstract Objects in Discourse, Kluwer, Dordrecht 1993 [AshLas91] N. Asher, A. Lascarides, Dicourse Relations and Defeasible Knowledge, in: Proceedings of the 29nd Annual Meeting of the Association of Computational Linguistics (ACL 91), pp. 5563, Berkeley 1994 [AshLas94] N. Asher, A. Lascarides, Intentions and Information in Discourse, in: Proceedings of the 32nd Annual Meeting of the Association of Computational Linguistics (ACL 94), pp. 3541, Las Cruces 1994 [AshLas97] N. Asher, A. Lascarides, Questions in Dialogue, to appear in Linguistics and Philosophy 8EVAR is not based on the dialog model sketched in this paper. 15 [AshLasObe92] N. Asher, A. Lascarides, J. Oberlander, Inferring Discourse Relations in Context, in: Pro- ceedings of the 30th Annual Meeting of the Association of Computational Linguistics (ACL 92), pp. 18, Delaware 1992 [AshSin93] N. Asher, M. Singh, A Logic of Intentions and Beliefs, in: Journal of Philosophical Logic, (22):5, pp. 513544 [BatRon94] J. Bateman, K. Ronhuis, Coherence relations: Analysis and speciﬁcation, Technical Report R 1.1.2: a, b, The DANDELION Consortium, Darmstadt 1994 [Ben88] J. van Benthem, A manual of intensional logic, CSLI lecture notes, 21988 [vm135] J. Bos et al., Compositional Semantics in Verbmobil, Verbmobil-Report 135, 1996 [BriGoe82] A. Brietzmann, G. Gorz, Pragmatics in Speech Understanding Revisited, in: Proceedings of COL- ING 82, pp. 4954, Prag 1982 [Bun97] H. Bunt, Dynamic Interpretation and Dialogue Theory, in: M. Taylor, D. Bouwhuis, F. Neel (Ed.), The structure of multi-modal dialogue, vol. 2, John Benjamins, Amsterdam 1997 [CarCar94] J. Chu-Carroll, S. Carberry, A Plan-Based Model for Response Generation in Collaborative Task- Oriented Dialogues, in: Proceedings of the Twelfth National Conference on Artiﬁcial Intelligence (AAAI 94), Seattle 1994 [Coe82] H. Coelho, A Formalism for the Structured Analysis of Dialogues, in: Proceedings of COLING 82, pp. 6170, Prag 1982 [Coh90] P. Cohen, H. Levesque, Performatives in a Rationally Based Speech Act Theory, in: Proceedings of the 28nd Annual Meeting of the Association of Computational Linguistics (ACL 90), Pittsburgh 1990 [CriWeb97] D. Cristea, B.L. Webber, Expectations in Incremental Discourse Processing, Technical Report, University A.I. Cuza, Iasi (Romania) 1997 [Die87] G. Diewald, Telefondialog: Beschreibungsversuche zwischen Gesprachsanalyse und Textlinguistik, Zulas- sungsarbeit, Universitat Erlangen 1987 (in German) [DRI97] Discourse Ressource Initiative, Standards for Dialogue Coding in Natural Language Processing, Re- port No. 167, Dagstuhl-Seminar [vm83] K. Eberle, Zu einer Semantik fur Dialogverstehen und Ubersetzung, Verbmobil-Report 83, 1995 [EckNie94] W. Eckert, H. Niemann Semantic Analysis in a Robust Spoken Dialog System, in: Proc. Int. Conf. on Spoken Language Processing, Yokohama, 1994, pp. 107110 [Eng88] U. Engel, Deutsche Grammatik, Groos, Heidelberg 1988 (in German) [GabRey92] D. Gabbay, U. Reyle, Direct Deductive Computation on Discourse Representation Structures, Linguistics and Philosophy, August 1992 [Gar94] C. Gardent, Multiple Discourse Dependencies, ILLC report LP9418, Amsterdam 1994 [Gar97] C. Gardent, Discourse TAG, CLAUS-Report 89, Saarbrucken 1997 [Grice] H.P. Grice, Logic and Conversation, in: P. Code, J. Morgan (Ed.), Syntax and Semantics, vol. 3, New York, Academic Press, S. 4158 [GroSto84] J. Groenendijk, M. Stokhof, Studies on the Semantics of Questions and the Pragmatics of Answers, PhD thesis, Centrale Interfaculteit, Amsterdam 1984 [TrainsCorpus] D. Gross, J. F. Allen and D. R. Traum, The TRAINS 91 Dialogues, TRAINS Technical Note 92-1, Computer Science Dept., University of Rochester, June 1993. [GroSid86] B. Grosz, C. Sidner, Attention, Intentions and the Structure of Discourse, Computational Linguis- tics, 12, pp. 175204, 1986 [KamRey93] Kamp, Hans and Reyle, Uwe: From Discourse to Logic, Kluwer, Dordrecht 1993 [Kar77] L. Kartunnen, Syntax and Semantics of Questions, in: Linguistics and Philosophy 1, 1977 [Kno96] A. Knott, A Data-Driven Methodology for Motivating a Set of Coherence Relations, PhD thesis, Uni- versity of Edinburgh 1996 [Kus96] S. Kuschert, Higher Order Dynamics: Relating Operational and Denotational Semantics for λ-DRT, CLAUS-Report 84, Saarbrucken 1996 [Kyo91] J. Kowtko, et al., Conversational games within dialogue, in: Proceedings of the DANDI Workshop on Discourse Coherence, 1991. [Lam93] L. Lambert, Recognizing Complex Discourse Acts: A Trepartite Plan-Based Model of Dialogue, PhD thesis, University of Delaware 1993 [Loc94] K. Lochbaum, Using Collaborative Plans to Model the Intentional Structure of Discourse, Harvard University, PhD Thesis, 1994. [Mar92] J. Martin, English text: systems and structure, Benjamins, Amsterdam 1992 [Moe97] J.-U. Moeller, DIA-MOLE: An Unsupervised Learning Approach to Adaptive Dialogue Models for Spo- ken Dialogue Systems, in: Proceedings of EUROSPEECH 97, Rhodes, pp. 22712274 [MooYou94] R. Young, J. Moore, DPOCL: a principled approach to discourse planning, in: Proceedings of the Seventh International Workshop on Natural Language Generation, Kennebunkport 1994 [LudGoNie98] B. Ludwig, G. Gorz, H. Niemann, Combining Expression And Content in Domains for Dialog Managers, in: Proceedings of DL 98, ITC-irst Technical Report 9805-03, Trento 1998 [Poe94] M. Poesio, Discourse Interpretation and the Scope of Operators, Ph.D. Thesis, Computer Science Dept., University of Rochester 1994 [Pol95] L. Polanyi, The Lingustic Structure of Discourse, CSLI Technical Report 96-200, CSLI Publications, Stanford 1996 16 [Pul97] S. Pulman, Conversational Games, Belief Revision and Bayesian Networks, in: Proceedings of 7th Com- putational Linguistic in the Netherlands meeting, 1996 [ReiMei95] Norbert Reitinger, Elisabeth Maier, Utilizing Statistical Dialogue Act Processing in VERBMO- BIL, Verbmobil-Report 80, 1995 [SadMor97] D. Sadek, R. De Mori, Dialogue Systems, in: R. De Mori (Ed.), Spoken Dialogues with Com- puters, Academic Press 1997 (to appear) [Sea69] J. Searle, Speech Acts: An Essay in the Philosophy of Language, Cambridge University Press 1969 [Web91] B.L. Webber, Structure and Ostension in the Interpretation of Discourse Deixis, in: Language and Cognitive Process, 6:(2), pp. 107135, 1991 [TraHin91] D. Traum, E. Hinkelman, Conversation Acts in Task-Oriented Spoken Dialog, in: Computational Intelligence, 8(3):575599, 1992 [Tra94] D. Traum, A Computational Theory of Grounding in Natural Language Conversation, Ph.D. Thesis, Computer Science Dept., University of Rochester 1994 [TraAll94] D. Traum, J. Allen, Discouse Obligations in Dialogue Processing, in: Preceedings of the 32nd Annual Meeting of the Association of Computational Linguistics (ACL 94), pp. 18, Las Cruces 1994 [TR663] D. Traum et al., Knowledge Representation in the TRAINS-93 Conversation System, Technical Report 663, University of Rochester, 1996 17",
  "3.pdf": "arXiv:cs9809024v2 [cs.CL] 18 Sep 1998 A Lexicalized Tree Adjoining Grammar for English The XTAG Research Group Institute for Research in Cognitive Science University of Pennsylvania 3401 Walnut St., Suite 400A Philadelphia, PA 19104-6228 http:www.cis.upenn.eduxtag 31 August 1998 Contents I General Information 1 1 Getting Around 3 2 Feature-Based, Lexicalized Tree Adjoining Grammars 5 2.1 TAG formalism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Lexicalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Uniﬁcation-based features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3 Overview of the XTAG System 11 3.1 System Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.1.1 Tree Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.1.2 Tree Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.1.3 Tree Grafting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.1.4 The Grammar Development Environment . . . . . . . . . . . . . . . . . . 15 3.2 Computer Platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4 Underview 19 4.1 Subcategorization Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.2 Complements and Adjuncts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 4.3 Non-S constituents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.4 Case Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.4.1 Approaches to Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.4.1.1 Case in GB theory . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.4.1.2 Minimalism and Case . . . . . . . . . . . . . . . . . . . . . . . . 22 4.4.2 Case in XTAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.4.3 Case Assigners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.4.3.1 Prepositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.4.3.2 Verbs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.4.4 PRO in a uniﬁcation based framework . . . . . . . . . . . . . . . . . . . . 25 II Verb Classes 28 5 Where to Find What 30 i 6 Verb Classes 37 6.1 Intransitive: Tnx0V . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.2 Transitive: Tnx0Vnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.3 Ditransitive: Tnx0Vnx1nx2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6.4 Ditransitive with PP: Tnx0Vnx1pnx2 . . . . . . . . . . . . . . . . . . . . . . . . 40 6.5 Ditransitive with PP shift: Tnx0Vnx1tonx2 . . . . . . . . . . . . . . . . . . . . . 41 6.6 Sentential Complement with NP: Tnx0Vnx1s2 . . . . . . . . . . . . . . . . . . . 42 6.7 Intransitive Verb Particle: Tnx0Vpl . . . . . . . . . . . . . . . . . . . . . . . . . 43 6.8 Transitive Verb Particle: Tnx0Vplnx1 . . . . . . . . . . . . . . . . . . . . . . . . 44 6.9 Ditransitive Verb Particle: Tnx0Vplnx1nx2 . . . . . . . . . . . . . . . . . . . . . 45 6.10 Intransitive with PP: Tnx0Vpnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 6.11 Predicative Multi-word with Verb, Prep anchors: Tnx0VPnx1 . . . . . . . . . . . 47 6.12 Sentential Complement: Tnx0Vs1 . . . . . . . . . . . . . . . . . . . . . . . . . . 48 6.13 Intransitive with Adjective: Tnx0Vax1 . . . . . . . . . . . . . . . . . . . . . . . . 49 6.14 Transitive Sentential Subject: Ts0Vnx1 . . . . . . . . . . . . . . . . . . . . . . . 49 6.15 Light Verbs: Tnx0lVN1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 6.16 Ditransitive Light Verbs with PP Shift: Tnx0lVN1Pnx2 . . . . . . . . . . . . . . 51 6.17 NP It-Cleft: TItVnx1s2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.18 PP It-Cleft: TItVpnx1s2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.19 Adverb It-Cleft: TItVad1s2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.20 Adjective Small Clause Tree: Tnx0Ax1 . . . . . . . . . . . . . . . . . . . . . . . . 54 6.21 Adjective Small Clause with Sentential Complement: Tnx0A1s1 . . . . . . . . . 54 6.22 Adjective Small Clause with Sentential Subject: Ts0Ax1 . . . . . . . . . . . . . . 55 6.23 Equative BE: Tnx0BEnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6.24 NP Small Clause: Tnx0N1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6.25 NP Small Clause with Sentential Complement: Tnx0N1s1 . . . . . . . . . . . . . 57 6.26 NP Small Clause with Sentential Subject: Ts0N1 . . . . . . . . . . . . . . . . . . 57 6.27 PP Small Clause: Tnx0Pnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 6.28 Exhaustive PP Small Clause: Tnx0Px1 . . . . . . . . . . . . . . . . . . . . . . . 59 6.29 PP Small Clause with Sentential Subject: Ts0Pnx1 . . . . . . . . . . . . . . . . . 59 6.30 Intransitive Sentential Subject: Ts0V . . . . . . . . . . . . . . . . . . . . . . . . . 60 6.31 Sentential Subject with to complement: Ts0Vtonx1 . . . . . . . . . . . . . . . . 61 6.32 PP Small Clause, with Adv and Prep anchors: Tnx0ARBPnx1 . . . . . . . . . . 61 6.33 PP Small Clause, with Adj and Prep anchors: Tnx0APnx1 . . . . . . . . . . . . 62 6.34 PP Small Clause, with Noun and Prep anchors: Tnx0NPnx1 . . . . . . . . . . . 63 6.35 PP Small Clause, with Prep anchors: Tnx0PPnx1 . . . . . . . . . . . . . . . . . 64 6.36 PP Small Clause, with Prep and Noun anchors: Tnx0PNaPnx1 . . . . . . . . . . 64 6.37 PP Small Clause with Sentential Subject, and Adv and Prep anchors: Ts0ARBPnx1 65 6.38 PP Small Clause with Sentential Subject, and Adj and Prep anchors: Ts0APnx1 66 6.39 PP Small Clause with Sentential Subject, and Noun and Prep anchors: Ts0NPnx1 66 6.40 PP Small Clause with Sentential Subject, and Prep anchors: Ts0PPnx1 . . . . . 68 6.41 PP Small Clause with Sentential Subject, and Prep and Noun anchors: Ts0PNaPnx1 68 6.42 Predicative Adjective with Sentential Subject and Complement: Ts0A1s1 . . . . 69 6.43 Locative Small Clause with Ad anchor: Tnx0nx1ARB . . . . . . . . . . . . . . . 69 6.44 Exceptional Case Marking: TXnx0Vs1 . . . . . . . . . . . . . . . . . . . . . . . . 71 ii 6.45 Idiom with V, D, and N anchors: Tnx0VDN1 . . . . . . . . . . . . . . . . . . . . 72 6.46 Idiom with V, D, A, and N anchors: Tnx0VDAN1 . . . . . . . . . . . . . . . . . 72 6.47 Idiom with V and N anchors: Tnx0VN1 . . . . . . . . . . . . . . . . . . . . . . . 73 6.48 Idiom with V, A, and N anchors: Tnx0VAN1 . . . . . . . . . . . . . . . . . . . . 74 6.49 Idiom with V, D, A, N, and Prep anchors: Tnx0VDAN1Pnx2 . . . . . . . . . . . 75 6.50 Idiom with V, A, N, and Prep anchors: Tnx0VAN1Pnx2 . . . . . . . . . . . . . . 75 6.51 Idiom with V, N, and Prep anchors: Tnx0VN1Pnx2 . . . . . . . . . . . . . . . . 76 6.52 Idiom with V, D, N, and Prep anchors: Tnx0VDN1Pnx2 . . . . . . . . . . . . . . 77 7 Ergatives 79 8 Sentential Subjects and Sentential Complements 81 8.1 S or VP complements? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 8.2 Complementizers and Embedded Clauses in English: The Data . . . . . . . . . . 82 8.3 Features Required . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 8.4 Distribution of Complementizers . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 8.5 Case assignment, for and the two tos . . . . . . . . . . . . . . . . . . . . . . . . 85 8.6 Sentential Complements of Verbs . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 8.6.1 Exceptional Case Marking Verbs . . . . . . . . . . . . . . . . . . . . . . . 89 8.7 Sentential Subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 8.8 Nouns and Prepositions taking Sentential Complements . . . . . . . . . . . . . . 92 8.9 PRO control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 8.9.1 Types of control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 8.9.2 A feature-based analysis of PRO control . . . . . . . . . . . . . . . . . . . 94 8.9.3 The nature of the control feature . . . . . . . . . . . . . . . . . . . . . . . 94 8.9.4 Long-distance transmission of control features . . . . . . . . . . . . . . . . 94 8.9.5 Locality constraints on control . . . . . . . . . . . . . . . . . . . . . . . . 95 8.10 Reported speech . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 9 The English Copula, Raising Verbs, and Small Clauses 98 9.1 Usages of the copula, raising verbs, and small clauses . . . . . . . . . . . . . . . . 98 9.1.1 Copula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 9.1.2 Raising Verbs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 9.1.3 Small Clauses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 9.1.4 Raising Adjectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 9.2 Various Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 9.2.1 Main Verb Raising to INFL  Small Clause . . . . . . . . . . . . . . . . . 101 9.2.2 Auxiliary  Null Copula . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 9.2.3 Auxiliary  Predicative Phrase . . . . . . . . . . . . . . . . . . . . . . . . 101 9.2.4 Auxiliary  Small Clause . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 9.3 XTAG analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 9.4 Non-predicative BE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 10 Ditransitive constructions and dative shift 109 11 It-clefts 112 iii III Sentence Types 114 12 Passives 116 13 Extraction 118 13.1 Topicalization and the value of the inv feature . . . . . . . . . . . . . . . . . 120 13.2 Extracted subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 13.3 Wh-moved NP complement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 13.4 Wh-moved object of a P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 13.5 Wh-moved PP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 13.6 Wh-moved S complement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 13.7 Wh-moved Adjective complement . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 14 Relative Clauses 126 14.1 Complementizers and clauses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 14.1.1 Further constraints on the null Comp ǫC . . . . . . . . . . . . . . . . . . . 131 14.2 Reduced Relatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 14.2.1 Restrictive vs. Non-restrictive relatives . . . . . . . . . . . . . . . . . . . . 133 14.3 External syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 14.4 Other Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 14.4.1 Interaction with adjoined Comps . . . . . . . . . . . . . . . . . . . . . . . 133 14.4.2 Adjunction on PRO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 14.4.3 Adjunct relative clauses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 14.4.4 ECM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 14.5 Cases not handled . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 14.5.1 Partial treatment of free-relatives . . . . . . . . . . . . . . . . . . . . . . . 135 14.5.2 Adjunct P-stranding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 14.5.3 Overgeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 14.5.3.1 how as wh-NP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 14.5.3.2 for-trace eﬀects . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 14.5.3.3 Internal head constraint . . . . . . . . . . . . . . . . . . . . . . . 136 14.5.3.4 Overt Comp constraint on stacked relatives . . . . . . . . . . . . 137 15 Adjunct Clauses 138 15.0.4 Multi-word Subordinating Conjunctions . . . . . . . . . . . . . . . . . . . 139 15.1 Bare Adjunct Clauses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 15.2 Discourse Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 16 Imperatives 143 17 Gerund NPs 145 17.1 Determiner Gerunds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 17.2 NP Gerunds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 17.3 Gerund Passives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 iv IV Other Constructions 151 18 Determiners and Noun Phrases 153 18.1 The Wh-Feature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 18.2 Multi-word Determiners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 18.3 Genitive Constructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 18.4 Partitive Constructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 18.5 Adverbs, Noun Phrases, and Determiners . . . . . . . . . . . . . . . . . . . . . . 163 19 Modiﬁers 167 19.1 Adjectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 19.2 Noun-Noun Modiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 19.3 Time Noun Phrases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 19.4 Prepositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 19.5 Adverbs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175 19.6 Locative Adverbial Phrases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 20 Auxiliaries 185 20.1 Non-inverted sentences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 20.2 Inverted Sentences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 20.3 Do-Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 20.3.1 In negated sentences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 20.3.2 In inverted yesno questions . . . . . . . . . . . . . . . . . . . . . . . . . . 193 20.4 Inﬁnitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 20.5 Semi-Auxiliaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194 20.5.1 Marginal Modal dare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194 20.5.2 Other semi-auxiliaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 20.5.3 Other Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 21 Conjunction 197 21.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 21.2 Adjective, Adverb, Preposition and PP Conjunction . . . . . . . . . . . . . . . . 197 21.3 Noun Phrase and Noun Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . 197 21.4 Determiner Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 21.5 Sentential Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 21.6 Comma as a conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 21.7 But-not, not-but, and-not and ǫ-not . . . . . . . . . . . . . . . . . . . . . . . . . . 202 21.8 To as a Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 21.9 Predicative Coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 21.10Pseudo-coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 22 Comparatives 208 22.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 22.2 Metalinguistic Comparatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 22.3 Propositional Comparatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 22.3.1 Nominal Comparatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 v 22.3.2 Adjectival Comparatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216 22.3.3 Adverbial Comparatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 22.4 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 23 Punctuation Marks 222 23.1 Appositives, parentheticals and vocatives . . . . . . . . . . . . . . . . . . . . . . 223 23.1.1 βnxPUnxPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 23.1.2 βnPUnxPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 23.1.3 βnxPUnx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 23.1.4 βPUpxPUvx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 23.1.5 βpuARBpuvx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 23.1.6 βsPUnx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 23.1.7 βnxPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 23.2 Bracketing punctuation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 23.2.1 Simple bracketing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 23.2.2 βsPUsPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 23.3 Punctuation trees containing no lexical material . . . . . . . . . . . . . . . . . . . 230 23.3.1 αPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 23.3.2 βPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231 23.3.3 βsPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 23.3.4 βsPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 23.3.5 βvPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 23.3.6 βpPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 23.4 Other trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 23.4.1 βspuARB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 23.4.2 βspuPnx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 23.4.3 βnxPUa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 V Appendices 236 A Future Work 238 A.1 Adjective ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 A.2 More work on Determiners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 A.3 -ing adjectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 A.4 Verb selectional restrictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 A.5 Thematic Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 B Metarules 241 B.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 B.2 The deﬁnition of a metarule in XTAG . . . . . . . . . . . . . . . . . . . . . . . . 242 B.2.1 Node names, variable instantiation, and matches . . . . . . . . . . . . . . 242 B.2.2 Structural Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 B.2.3 Output Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 B.2.4 Feature Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 B.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 vi B.4 The Access to the Metarules through the XTAG Interface . . . . . . . . . . . . . 248 C Lexical Organization 253 C.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 C.2 System Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 C.2.1 Subcategorization frames . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 C.2.2 Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 C.2.3 Lexical Redistribution Rules (LRRs) . . . . . . . . . . . . . . . . . . . . . 256 C.2.4 Tree generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256 C.3 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 C.4 Generating grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 C.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260 D Tree Naming conventions 261 D.1 Tree Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 D.2 Trees within tree families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 D.3 Assorted Initial Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262 D.4 Assorted Auxiliary Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262 D.4.1 Relative Clause Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 E Features 264 E.1 Agreement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264 E.1.1 Agreement and Movement . . . . . . . . . . . . . . . . . . . . . . . . . . . 266 E.2 Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266 E.2.1 ECM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 E.2.2 Agreement and Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 E.3 Extraction and Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 E.3.1 Inversion, Part 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 E.3.2 Inversion, Part 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269 E.4 Clause Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269 E.4.1 Auxiliary Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270 E.5 Relative Clauses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270 E.6 Complementizer Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 E.6.1 Verbs with object sentential complements . . . . . . . . . . . . . . . . . . 272 E.6.2 Verbs with sentential subjects . . . . . . . . . . . . . . . . . . . . . . . . . 273 E.6.3 That-trace and for-trace eﬀects . . . . . . . . . . . . . . . . . . . . . . . . 274 E.7 Determiner ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 E.8 Punctuation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 E.9 Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 E.10 Comparatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 E.11 Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 E.12 Other Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 vii F Evaluation and Results 277 F.1 Parsing Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 F.2 TSNLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 F.3 Chunking and Dependencies in XTAG Derivations . . . . . . . . . . . . . . . . . 279 F.4 Comparison with IBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 F.5 Comparison with Alvey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 F.6 Comparison with CLARE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 viii List of Figures 2.1 Elementary trees in TAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Substitution in TAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Adjunction in TAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4 Lexicalized Elementary trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.5 Substitution in FB-LTAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.6 Adjunction in FB-LTAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.7 Lexicalized Elementary Trees with Features . . . . . . . . . . . . . . . . . . . . . 10 3.1 XTAG system diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 Output Structures from the Parser . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.3 Interfaces database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.4 XTAG Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.1 Diﬀerent subcategorization frames for the verb buy . . . . . . . . . . . . . . . . . 20 4.2 Trees illustrating the diﬀerence between Complements and Adjuncts . . . . . . . 21 4.3 Lexicalized NP trees with case markings . . . . . . . . . . . . . . . . . . . . . . . 23 4.4 Assigning case in prepositional phrases . . . . . . . . . . . . . . . . . . . . . . . . 24 4.5 Case assignment to NP arguments . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.6 Assigning case according to verb form . . . . . . . . . . . . . . . . . . . . . . . . 25 4.7 Proper case assignment with auxiliary verbs . . . . . . . . . . . . . . . . . . . . . 26 6.1 Declarative Intransitive Tree: αnx0V . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.2 Declarative Transitive Tree: αnx0Vnx1 . . . . . . . . . . . . . . . . . . . . . . . . 38 6.3 Declarative Ditransitive Tree: αnx0Vnx1nx2 . . . . . . . . . . . . . . . . . . . . 39 6.4 Declarative Ditransitive with PP Tree: αnx0Vnx1pnx2 . . . . . . . . . . . . . . . 40 6.5 Declarative Ditransitive with PP shift Trees: αnx0Vnx1Pnx2 (a) and αnx0Vnx2nx1 (b) 41 6.6 Declarative Sentential Complement with NP Tree: βnx0Vnx1s2 . . . . . . . . . . 43 6.7 Declarative Intransitive Verb Particle Tree: αnx0Vpl . . . . . . . . . . . . . . . . 44 6.8 Declarative Transitive Verb Particle Tree: αnx0Vplnx1 (a) and αnx0Vnx1pl (b) . 44 6.9 Declarative Ditransitive Verb Particle Tree: αnx0Vplnx1nx2 (a), αnx0Vnx1plnx2 (b) and αnx0Vnx1nx2pl (c) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 6.10 Declarative Intransitive with PP Tree: αnx0Vpnx1 . . . . . . . . . . . . . . . . . 46 6.11 Declarative PP Complement Tree: αnx0VPnx1 . . . . . . . . . . . . . . . . . . . 47 6.12 Declarative Sentential Complement Tree: βnx0Vs1 . . . . . . . . . . . . . . . . . 48 6.13 Declarative Intransitive with Adjective Tree: αnx0Vax1 . . . . . . . . . . . . . . 49 6.14 Declarative Sentential Subject Tree: αs0Vnx1 . . . . . . . . . . . . . . . . . . . . 50 ix 6.15 Declarative Light Verb Tree: αnx0lVN1 . . . . . . . . . . . . . . . . . . . . . . . 51 6.16 Declarative Light Verbs with PP Tree: αnx0lVN1Pnx2 (a), αnx0lVnx2N1 (b) . . 52 6.17 Declarative NP It-Cleft Tree: αItVpnx1s2 . . . . . . . . . . . . . . . . . . . . . . 52 6.18 Declarative PP It-Cleft Tree: αItVnx1s2 . . . . . . . . . . . . . . . . . . . . . . . 53 6.19 Declarative Adverb It-Cleft Tree: αItVad1s2 . . . . . . . . . . . . . . . . . . . . . 54 6.20 Declarative Adjective Small Clause Tree: αnx0Ax1 . . . . . . . . . . . . . . . . . 54 6.21 Declarative Adjective Small Clause with Sentential Complement Tree: αnx0A1s1 55 6.22 Declarative Adjective Small Clause with Sentential Subject Tree: αs0Ax1 . . . . 56 6.23 Declarative Equative BE Tree: αnx0BEnx1 . . . . . . . . . . . . . . . . . . . . . 56 6.24 Declarative NP Small Clause Trees: αnx0N1 . . . . . . . . . . . . . . . . . . . . 57 6.25 Declarative NP with Sentential Complement Small Clause Tree: αnx0N1s1 . . . 58 6.26 Declarative NP Small Clause with Sentential Subject Tree: αs0N1 . . . . . . . . 58 6.27 Declarative PP Small Clause Tree: αnx0Pnx1 . . . . . . . . . . . . . . . . . . . . 59 6.28 Declarative Exhaustive PP Small Clause Tree: αnx0Px1 . . . . . . . . . . . . . . 60 6.29 Declarative PP Small Clause with Sentential Subject Tree: αs0Pnx1 . . . . . . . 60 6.30 Declarative Intransitive Sentential Subject Tree: αs0V . . . . . . . . . . . . . . . 61 6.31 Sentential Subject Tree with to complement: αs0Vtonx1 . . . . . . . . . . . . . 61 6.32 Declarative PP Small Clause tree with two-word preposition, where the ﬁrst word is an adverb, and the second word is a preposition: αnx0ARBPnx1 . . . . . . . . 62 6.33 Declarative PP Small Clause tree with two-word preposition, where the ﬁrst word is an adjective, and the second word is a preposition: αnx0APnx1 . . . . . . . . 63 6.34 Declarative PP Small Clause tree with two-word preposition, where the ﬁrst word is a noun, and the second word is a preposition: αnx0NPnx1 . . . . . . . . . . . 63 6.35 Declarative PP Small Clause tree with two-word preposition, where both words are prepositions: αnx0PPnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 6.36 Declarative PP Small Clause tree with three-word preposition, where the middle noun is marked for null adjunction: αnx0PNaPnx1 . . . . . . . . . . . . . . . . . 65 6.37 Declarative PP Small Clause with Sentential Subject Tree, with two-word prepo- sition, where the ﬁrst word is an adverb, and the second word is a preposition: αs0ARBPnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 6.38 Declarative PP Small Clause with Sentential Subject Tree, with two-word prepo- sition, where the ﬁrst word is an adjective, and the second word is a preposition: αs0APnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 6.39 Declarative PP Small Clause with Sentential Subject Tree, with two-word prepo- sition, where the ﬁrst word is a noun, and the second word is a preposition: αs0NPnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 6.40 Declarative PP Small Clause with Sentential Subject Tree, with two-word prepo- sition, where both words are prepositions: αs0PPnx1 . . . . . . . . . . . . . . . . 68 6.41 Declarative PP Small Clause with Sentential Subject Tree, with three-word preposition, where the middle noun is marked for null adjunction: αs0PNaPnx1 . 69 6.42 Predicative Adjective with Sentential Subject and Complement: αs0A1s1 . . . . 70 6.43 Declarative Locative Adverbial Small Clause Tree: αnx0nx1ARB . . . . . . . . . 70 6.44 Wh-moved Locative Small Clause Tree: αW1nx0nx1ARB . . . . . . . . . . . . . 71 6.45 ECM Tree: βXnx0Vs1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.46 Declarative Transitive Idiom Tree: αnx0VDN1 . . . . . . . . . . . . . . . . . . . 72 x 6.47 Declarative Idiom with V, D, A, and N Anchors Tree: αnx0VDAN1 . . . . . . . 73 6.48 Declarative Idiom with V and N Anchors Tree: αnx0VN1 . . . . . . . . . . . . . 74 6.49 Declarative Idiom with V, A, and N Anchors Tree: αnx0VAN1 . . . . . . . . . . 74 6.50 Declarative Idiom with V, D, A, N, and Prep Anchors Tree: αnx0VDAN1Pnx2 . 75 6.51 Declarative Idiom with V, A, N, and Prep Anchors Tree: αnx0VAN1Pnx2 . . . . 76 6.52 Declarative Idiom with V, N, and Prep Anchors Tree: αnx0VN1Pnx2 . . . . . . 77 6.53 Declarative Idiom with V, D, N, and Prep Anchors Tree: αnx0VDN1Pnx2 . . . . 77 7.1 Ergative Tree: αEnx1V . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 8.1 Tree βCOMPs, anchored by that . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 8.2 Sentential complement tree: βnx0Vs1 . . . . . . . . . . . . . . . . . . . . . . . . 87 8.3 Trees for The emu thinks that the aardvark smells terrible. . . . . . . . . . . . . . 87 8.4 Tree for Who smells terrible? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 8.5 ECM tree: βXnx0Vs1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 8.6 Sample ECM parse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 8.7 ECM passive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 8.8 Comparison of assign-comp values for sentential subjects: αs0Vnx1 (a) and sentential complements: βnx0Vs1 (b) . . . . . . . . . . . . . . . . . . . . . . . . . 92 8.9 Sample trees for preposition: βPss (a) and noun: αNXNs (b) taking sentential complements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 8.10 Tree for persuaded . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 8.11 Tree for leave . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 8.12 Derived tree for Srini persuaded Mickey to leave . . . . . . . . . . . . . . . . . . . 96 8.13 Derivation tree for Srini persuaded Mickey to leave . . . . . . . . . . . . . . . . . 96 9.1 Predicative trees: αnx0N1 (a), αnx0Ax1 (b) and αnx0Pnx1 (c) . . . . . . . . . . 102 9.2 Copula auxiliary tree: βVvx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 9.3 Predicative AP tree with features: αnx0Ax1 . . . . . . . . . . . . . . . . . . . . . 105 9.4 Consider tree for embedded small clauses . . . . . . . . . . . . . . . . . . . . . . 106 9.5 Raising verb with experiencer tree: βVpxvx . . . . . . . . . . . . . . . . . . . . . 106 9.6 Raising adjective tree: βVvx-adj . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 9.7 Equative BE trees: αnx0BEnx1 (a) and αInvnx0BEnx1 (b) . . . . . . . . . . . . 108 10.1 Dative shift trees: αnx0Vnx1Pnx2 (a) and αnx0Vnx2nx1 (b) . . . . . . . . . . . 110 11.1 It-cleft with PP clefted element: αItVpnx1s2 (a) and αInvItVpnx1s2 (b) . . . . . 113 12.1 Passive trees in the Sentential Complement with NP tree family: βnx1Vs2 (a), βnx1Vbynx0s2 (b) and βnx1Vs2bynx0 (c) . . . . . . . . . . . . . . . . . . . . . . 116 13.1 Transitive tree with object extraction: αW1nx0Vnx1 . . . . . . . . . . . . . . . . 119 13.2 Intransitive tree with subject extraction: αW0nx0V . . . . . . . . . . . . . . . . 122 13.3 Ditransitive trees with direct object: αW1nx0Vnx1nx2 (a) and indirect object extraction: αW2nx0Vnx1nx2 (b) . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 13.4 Ditransitive with PP tree with the object of the PP extracted: αW2nx0Vnx1pnx2124 13.5 Ditransitive with PP with PP extraction tree: αpW2nx0Vnx1pnx2 . . . . . . . . 124 xi 13.6 Predicative Adjective tree with extracted adjective: αWA1nx0Vax1 . . . . . . . . 125 14.1 Relative clause trees in the transitive tree family: βNc1nx0Vnx1 (a) and βN0nx0Vnx1 (b) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 14.2 Adjunct relative clause tree with PP-pied-piping in the transitive tree family: βNpxnx0Vnx1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 14.3 Determiner tree with rel-clause feature: βDnx . . . . . . . . . . . . . . . . . 134 15.1 Auxiliary Trees for Subordinating Conjunctions . . . . . . . . . . . . . . . . . . . 138 15.2 Trees Anchored by Subordinating Conjunctions: βvxPARBPs and βvxParbPs . . 140 15.3 Sample Participial Adjuncts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 15.4 Example of discourse conjunction, from Seuss The Lorax . . . . . . . . . . . . . 142 16.1 Transitive imperative tree: αInx0Vnx1 . . . . . . . . . . . . . . . . . . . . . . . . 144 17.1 Determiner Gerund tree from the transitive tree family: αDnx0Vnx1 . . . . . . . 147 17.2 NP Gerund tree from the transitive tree family: αGnx0Vnx1 . . . . . . . . . . . 149 17.3 Passive Gerund trees from the transitive tree family: αGnx1Vbynx0 (a) and αGnx1V (b) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 18.1 NP Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 18.2 Determiner Trees with Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 18.3 Multi-word Determiner tree: βDDnx . . . . . . . . . . . . . . . . . . . . . . . . . 160 18.4 Genitive Determiner Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 18.5 Genitive NP tree for substitution: αDnxG . . . . . . . . . . . . . . . . . . . . . . 162 18.6 Partitive Determiner Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 18.7 (a) Adverb modifying a determiner; (b) Adverb modifying a noun phrase . . . . 165 19.1 Standard Tree for Adjective modifying a Noun: βAn . . . . . . . . . . . . . . . . 168 19.2 Multiple adjectives modifying a noun . . . . . . . . . . . . . . . . . . . . . . . . . 169 19.3 Noun-noun compounding tree: βNn (not all features displayed) . . . . . . . . . . 170 19.4 Time Phrase Modiﬁer trees: βNs, βNvx, βvxN, βnxN . . . . . . . . . . . . . . . 172 19.5 Time NPs with and without a determiner . . . . . . . . . . . . . . . . . . . . . . 172 19.6 Time NP trees: Two diﬀerent attachments . . . . . . . . . . . . . . . . . . . . . . 173 19.7 Time NPs in diﬀerent positions (βvxN, βnxN and βNs) . . . . . . . . . . . . . . 173 19.8 Time NPs: Derived tree and Derivation (βNvx position) . . . . . . . . . . . . . . 174 19.9 Selected Prepositional Phrase Modiﬁer trees: βPss, βnxPnx, βvxP and βvxPPnx 175 19.10Adverb Trees for pre-modiﬁcation of S: βARBs (a) and post-modiﬁcation of a VP: βvxARB (b) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177 19.11Derived tree for How did you fall? . . . . . . . . . . . . . . . . . . . . . . . . . . 178 19.12Complex adverb phrase modiﬁer: βARBarbs . . . . . . . . . . . . . . . . . . . . 179 19.13Selected Focus and Multi-word Adverb Modiﬁer trees: βARBnx, βPARBd and βPaPd . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 19.14Selected Multi-word Adverb Modiﬁer trees (for adverbs like sort of, kind of): βNPax, βNPvx, βvxNP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 xii 19.15Selected Multi-word Adverb Modiﬁer trees (for adverbs like a little, a bit): βvxDA, βDAax, βDNpx. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 19.16Locative Modiﬁer Trees: βnxnxARB, βnxARB . . . . . . . . . . . . . . . . . . . 183 19.17Locative Phrases featuring NP and Adverb Degree Speciﬁcations . . . . . . . . . 184 20.1 Auxiliary verb tree for non-inverted sentences: βVvx . . . . . . . . . . . . . . . 187 20.2 Auxiliary trees for The music should have been being played . . . . . . . . . . . . 188 20.3 The music should have been being played . . . . . . . . . . . . . . . . . . . . . . . 189 20.4 Trees for auxiliary verb inversion: βVs (a) and βVvx (b) . . . . . . . . . . . . . . 190 20.5 will John buy a backpack ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 21.1 Tree for adjective conjunction: βa1CONJa2 and a resulting parse tree . . . . . . 198 21.2 Tree for NP conjunction: βCONJnx1CONJnx2 and a resulting parse tree . . . . 199 21.3 Tree for determiner conjunction: βd1CONJd2.ps . . . . . . . . . . . . . . . . . . 200 21.4 Tree for sentential conjunction: βs1CONJs2 . . . . . . . . . . . . . . . . . . . . . 201 21.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 21.6 βa1CONJa2 (a) anchored by comma and (b) anchored by and . . . . . . . . . . . 202 21.7 Tree for conjunction with but-not: βpx1CONJARBpx2 . . . . . . . . . . . . . . . 203 21.8 Tree for conjunction with not-but: βARBnx1CONJnx2 . . . . . . . . . . . . . . 204 21.9 Example of conjunction with to . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 21.10Coordination schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 21.11An example of the conjoin operation. {1} denotes a shared dependency. . . . . . 206 21.12Coordination as adjunction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206 22.1 Tree for Metalinguistic Adjective Comparative: βARBaPa . . . . . . . . . . . . . 210 22.2 Tree for Adjective-Extreme Comparative: βARBPa . . . . . . . . . . . . . . . . . 211 22.3 Nominal comparative trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 22.4 Tree for Lone Comparatives: αCARB . . . . . . . . . . . . . . . . . . . . . . . . 213 22.5 Comparative conjunctions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 22.6 Comparative conjunctions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 22.7 Adjunction of βnxPnx to NP modiﬁed by comparative adjective. . . . . . . . . . 216 22.8 Elliptical adjectival comparative trees . . . . . . . . . . . . . . . . . . . . . . . . 217 22.9 Comparativized adjective triggering βCnxPnx. . . . . . . . . . . . . . . . . . . . 218 22.10Adjunction of βaxPnx to comparative adjective. . . . . . . . . . . . . . . . . . . 219 22.11Adverbial comparative trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 23.1 The βnxPUnxPU tree, anchored by parentheses . . . . . . . . . . . . . . . . . . . 224 23.2 An N-level modiﬁer, using the βnPUnx tree . . . . . . . . . . . . . . . . . . . . . 225 23.3 The derived trees for an NP with (a) a peripheral, dash-separated appositive and (b) an NP colon expansion (uttered by the Mouse in Alices Adventures in Wonderland) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 23.4 The βPUpxPUvx tree, anchored by commas . . . . . . . . . . . . . . . . . . . . . 226 23.5 Tree illustrating the use of βPUpxPUvx . . . . . . . . . . . . . . . . . . . . . . . 227 23.6 A tree illustrating the use of sPUnx for a colon expansion attached at S. . . . . . 228 23.7 βPUsPU anchored by parentheses, and in a derivation, along with βPUnxPU . . 229 23.8 βPUs, with features displayed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231 xiii 23.9 βsPUs, with features displayed . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 B.1 Metarule for wh-movement of subject . . . . . . . . . . . . . . . . . . . . . . . . 248 B.2 Metarule for wh-movement of object . . . . . . . . . . . . . . . . . . . . . . . . . 248 B.3 Metarule for general wh movement of an NP . . . . . . . . . . . . . . . . . . . . 249 B.4 Application of wh-movement rule to Tnx0Vnx1Pnx2 . . . . . . . . . . . . . . . . 249 B.5 Parallel application of metarules . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 B.6 Sequential application of metarules . . . . . . . . . . . . . . . . . . . . . . . . . . 251 B.7 Cumulative application of metarules . . . . . . . . . . . . . . . . . . . . . . . . . 251 C.1 Lexical Organization: System Overview . . . . . . . . . . . . . . . . . . . . . . . 254 C.2 Some subcategorization blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 C.3 Transformation blocks for extraction . . . . . . . . . . . . . . . . . . . . . . . . . 255 C.4 Elementary trees generated from combining blocks . . . . . . . . . . . . . . . . . 256 C.5 Partial inheritance lattice in English . . . . . . . . . . . . . . . . . . . . . . . . . 257 C.6 Implementation of the system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258 C.7 Interface for creating a grammar . . . . . . . . . . . . . . . . . . . . . . . . . . . 258 C.8 Part of the Interface for creating blocks . . . . . . . . . . . . . . . . . . . . . . . 259 xiv Abstract This document describes a sizable grammar of English written in the TAG formalism and implemented for use with the XTAG system. This report and the grammar described herein supersedes the TAG grammar described in [XTAG-Group, 1995]. The English grammar de- scribed in this report is based on the TAG formalism developed in [Joshi et al., 1975], which has been extended to include lexicalization ([Schabes et al., 1988]), and uniﬁcation-based feature structures ([Vijay-Shanker and Joshi, 1991]). The range of syntactic phenomena that can be handled is large and includes auxiliaries (including inversion), copula, raising and small clause constructions, topicalization, relative clauses, inﬁnitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO constructions, noun-noun modiﬁcations, extraposition, determiner sequences, genitives, negation, noun-verb contractions, sentential adjuncts and imperatives. This techni- cal report corresponds to the XTAG Release 83198. The XTAG grammar is continuously updated with the addition of new analyses and modiﬁcation of old ones, and an online version of this report can be found at the XTAG web page: http:www.cis.upenn.eduxtag. Acknowledgements We are immensely grateful to Aravind Joshi for supporting this project. The following people have contributed to the development of grammars in the project: Anne Abeille, Jason Baldridge, Rajesh Bhatt, Kathleen Bishop, Raman Chandrasekar, Sharon Cote, Beatrice Daille, Christine Doran, Dania Egedi, Tim Farrington, Jason Frank, Caroline Heycock, Beth Ann Hockey, Roumyana Izvorski, Karin Kipper, Daniel Karp, Seth Kulick, Young-Suk Lee, Heather Matayek, Patrick Martin, Megan Moser, Sabine Petillon, Rashmi Prasad, Laura Siegel, Yves Schabes, Victoria Tredinnick and Raﬀaella Zanuttini. The XTAG system has been developed by: Tilman Becker, Richard Billington, Andrew Chalnick, Dania Egedi, Devtosh Khare, Albert Lee, David Magerman, Alex Mallet, Patrick Paroubek, Rich Pito, Gilles Prigent, Carlos Prolo, Anoop Sarkar, Yves Schabes, William Schuler, B. Srinivas, Fei Xia, Yuji Yoshiie and Martin Zaidel. We would also like to thank Michael Hegarty, Lauri Karttunen, Anthony Kroch, Mitchell Marcus, Martha Palmer, Owen Rambow, Philip Resnik, Beatrice Santorini and Mark Steedman. In addition, Jeﬀ Aaronson, Douglas DeCarlo, Mark-Jason Dominus, Mark Foster, Gaylord Holder, David Magerman, Ken Noble, Steven Shapiro and Ira Winston have provided technical support. Adminstrative support was provided by Susan Deysher, Carolyn Elken, Jodi Kerper, Christine Sandy and Trisha Yannuzzi. This work was partially supported by NSF Grant SBR8920230 and ARO Grant DAAH0404- 94-G-0426. Part I General Information 1 Chapter 1 Getting Around This technical report presents the English XTAG grammar as implemented by the XTAG Research Group at the University of Pennsylvania. The technical report is organized into four parts, plus a set of appendices. Part 1 contains general information about the XTAG system and some of the underlying mechanisms that help shape the grammar. Chapter 2 contains an introduction to the formalism behind the grammar and parser, while Chapter 3 contains information about the entire XTAG system. Linguists interested solely in the grammar of the XTAG system may safely skip Chapters 2 and 3. Chapter 4 contains information on some of the linguistic principles that underlie the XTAG grammar, including the distinction between complements and adjuncts, and how case is handled. The actual description of the grammar begins with Part 2, and is contained in the following three parts. Parts 2 and 3 contains information on the verb classes and the types of trees allowed within the verb classes, respectively, while Part 4 contains information on trees not included in the verb classes (e.g. NPs, PPs, various modiﬁers, etc). Chapter 5 of Part 2 contains a table that attempts to provide an overview of the verb classes and tree types by providing a graphical indication of which tree types are allowed in which verb classes. This has been cross-indexed to tree ﬁgures shown in the tech report. Chapter 6 contains an overview of all of the verb classes in the XTAG grammar. The rest of Part 2 contains more details on several of the more interesting verb classes, including ergatives, sentential subjects, sentential complements, small classes, ditransitives, and it-clefts. Part 3 contains information on some of the tree types that are available within the verb classes. These tree types correspond to what would be transformations in a movement based approach. Not all of these types of trees are contained in all of the verb classes. The table (previously mentioned) in Part 2 contains a list of the tree types and indicates which verb classes each occurs in. Part 4 focuses on the non-verb class trees in the grammar. NPs and determiners are presented in Chapter 18, while the various modiﬁer trees are presented in Chapter 19. Auxiliary verbs, which are classed separate from the verb classes, are presented in Chapter 20, while certain types of conjunction are shown in Chapter 21. The XTAG treatment of comparatives is presented in Chapter 22, and our treatment of punctuation is discussed in Chapter 23. Throughout the technical report, mention is occasionally made of changes or analyses that we hope to incorporate in the future. Appendix A details a list of these and other future work. The appendices also contain information on some of the nitty gritty details of the 3 4 CHAPTER 1. GETTING AROUND XTAG grammar, including a system of metarules which can be used for grammar development and maintenance in Appendix B, a system for the organization of the grammar in terms of an inheritance hierarchy is in Appendix C, the tree naming conventions used in XTAG are explained in detail in Appendix D, and a comprehensive list of the features used in the grammar is given in Appendix E. Appendix F contains an evaluation of the XTAG grammar, including comparisons with other wide coverage grammars. Chapter 2 Feature-Based, Lexicalized Tree Adjoining Grammars The English grammar described in this report is based on the TAG formalism ([Joshi et al., 1975]), which has been extended to include lexicalization ([Schabes et al., 1988]), and uniﬁcation-based feature structures ([Vijay-Shanker and Joshi, 1991]). Tree Adjoining Lan- guages (TALs) fall into the class of mildly context-sensitive languages, and as such are more powerful than context free languages. The TAG formalism in general, and lexicalized TAGs in particular, are well-suited for linguistic applications. As ﬁrst shown by [Joshi, 1985] and [Kroch and Joshi, 1987], the properties of TAGs permit us to encapsulate diverse syntactic phenomena in a very natural way. For example, TAGs extended domain of locality and its factoring of recursion from local dependencies lead, among other things, to a localization of so-called unbounded dependencies. 2.1 TAG formalism The primitive elements of the standard TAG formalism are known as elementary trees. Ele- mentary trees are of two types: initial trees and auxiliary trees (see Figure 2.1). In describing natural language, initial trees are minimal linguistic structures that contain no recursion, i.e. trees containing the phrasal structure of simple sentences, NPs, PPs, and so forth. Initial trees are characterized by the following: 1) all internal nodes are labeled by non-terminals, 2) all leaf nodes are labeled by terminals, or by non-terminal nodes marked for substitution. An initial tree is called an X-type initial tree if its root is labeled with type X. Recursive structures are represented by auxiliary trees, which represent constituents that are adjuncts to basic structures (e.g. adverbials). Auxiliary trees are characterized as follows: 1) all internal nodes are labeled by non-terminals, 2) all leaf nodes are labeled by terminals, or by non-terminal nodes marked for substitution, except for exactly one non-terminal node, called the foot node, which can only be used to adjoin the tree to another node1, 3) the foot node has the same label as the root node of the tree. 1A null adjunction constraint (NA) is systematically put on the foot node of an auxiliary tree. This disallows adjunction of a tree onto the foot node itself. 5 6 CHAPTER 2. FEATURE-BASED, LEXICALIZED TREE ADJOINING GRAMMARS X X X Initial Tree: Auxiliary Tree: Figure 2.1: Elementary trees in TAG There are two operations deﬁned in the TAG formalism, substitution2 and adjunction. In the substitution operation, the root node on an initial tree is merged into a non-terminal leaf node marked for substitution in another initial tree, producing a new tree. The root node and the substitution node must have the same name. Figure 2.2 shows two initial trees and the tree resulting from the substitution of one tree into the other. 2 2 Y Y X  1 X Y Figure 2.2: Substitution in TAG In an adjunction operation, an auxiliary tree is grafted onto a non-terminal node anywhere in an initial tree. The root and foot nodes of the auxiliary tree must match the node at which the auxiliary tree adjoins. Figure 2.3 shows an auxiliary tree and an initial tree, and the tree resulting from an adjunction operation. A TAG G is a collection of ﬁnite initial trees, I, and auxiliary trees, A. The tree set of a TAG G, T (G) is deﬁned to be the set of all derived trees starting from S-type initial trees in I whose frontier consists of terminal nodes (all substitution nodes having been ﬁlled). The string language generated by a TAG, L(G), is deﬁned to be the set of all terminal strings on the frontier of the trees in T (G). 2Technically, substitution is a specialized version of adjunction, but it is useful to make a distinction between the two. 2.2. LEXICALIZATION 7 3 X 2 2 Y Y Y Y 3 1 X Y  Figure 2.3: Adjunction in TAG 2.2 Lexicalization Lexicalized grammars systematically associate each elementary structure with a lexical anchor. This means that in each structure there is a lexical item that is realized. It does not mean simply adding feature structures (such as head) and uniﬁcation equations to the rules of the formalism. These resultant elementary structures specify extended domains of locality (as compared to CFGs) over which constraints can be stated. Following [Schabes et al., 1988] we say that a grammar is lexicalized if it consists of 1) a ﬁnite set of structures each associated with a lexical item, and 2) an operation or operations for composing the structures. Each lexical item will be called the anchor of the corresponding structure, which deﬁnes the domain of locality over which constraints are speciﬁed. Note then, that constraints are local with respect to their anchor. Not every grammar is in a lexicalized form.3 In the process of lexicalizing a grammar, the lexicalized grammar is required to be strongly equivalent to the original grammar, i.e. it must produce not only the same language, but the same structures or tree set as well. NP N John Sr NP0 VP V walked VPr VP NA PP P to NP NP N Philadelphia (a) (b) (c) (d) Figure 2.4: Lexicalized Elementary trees 3Notice the similarity of the deﬁnition of a lexicalized grammar with the oﬀ line parsability constraint ([Kaplan and Bresnan, 1983]). As consequences of our deﬁnition, each structure has at least one lexical item (its anchor) attached to it and all sentences are ﬁnitely ambiguous. 8 CHAPTER 2. FEATURE-BASED, LEXICALIZED TREE ADJOINING GRAMMARS In Figure 2.4, which shows sample initial and auxiliary trees, substitution sites are marked by a , and foot nodes are marked by an . This notation is standard and is followed in the rest of this report. 2.3 Uniﬁcation-based features In a uniﬁcation framework, a feature structure is associated with each node in an elementary tree. This feature structure contains information about how the node interacts with other nodes in the tree. It consists of a top part, which generally contains information relating to the supernode, and a bottom part, which generally contains information relating to the subnode. Substitution nodes, however, have only the top features, since the tree substituting in logically carries the bottom features. Y tr br X Y t U tr br X Y t  Figure 2.5: Substitution in FB-LTAG The notions of substitution and adjunction must be augmented to ﬁt within this new frame- work. The feature structure of a new node created by substitution inherits the union of the features of the original nodes. The top feature of the new node is the union of the top features of the two original nodes, while the bottom feature of the new node is simply the bottom feature of the top node of the substituting tree (since the substitution node has no bottom feature). Figure 2.54 shows this more clearly. Adjunction is only slightly more complicated. The node being adjoined into splits, and its top feature uniﬁes with the top feature of the root adjoining node, while its bottom feature uni- ﬁes with the bottom feature of the foot adjoining node. Again, this is easier shown graphically, as in Figure 2.65. The embedding of the TAG formalism in a uniﬁcation framework allows us to dynamically specify local constraints that would have otherwise had to have been made statically within the trees. Constraints that verbs make on their complements, for instance, can be implemented through the feature structures. The notions of Obligatory and Selective Adjunction, crucial 4abbreviations in the ﬁgure: ttop feature structure, trtop feature structure of the root, brbottom feature structure of the root, Uuniﬁcation 5abbreviations in the ﬁgure: ttop feature structure, bbottom feature structure, trtop feature structure of the root, brbottom feature structure of the root, tftop feature structure of the foot, bfbottom feature structure of the foot, Uuniﬁcation 2.3. UNIFICATION-BASED FEATURES 9 X  t Y b Y Y br tf bf tr X t U tr b U bf tf br Y Y Figure 2.6: Adjunction in FB-LTAG to the formation of lexicalized grammars, can also be handled through the use of features.6 Perhaps more important to developing a grammar, though, is that the trees can serve as a schemata to be instantiated with lexical-speciﬁc features when an anchor is associated with the tree. To illustrate this, Figure 2.7 shows the same tree lexicalized with two diﬀerent verbs, each of which instantiates the features of the tree according to its lexical selectional restrictions. In Figure 2.7, the lexical item thinks takes an indicative sentential complement, as in the sentence John thinks that Mary loves Sally. Want takes a sentential complement as well, but an inﬁnitive one, as in John wants to love Mary. This distinction is easily captured in the features and passed to other nodes to constrain which trees this tree can adjoin into, both cutting down the number of separate trees needed and enforcing conceptual Selective Adjunctions (SA). 6The remaining constraint, Null Adjunction (NA), must still be speciﬁed directly on a node. 10 CHAPTER 2. FEATURE-BASED, LEXICALIZED TREE ADJOINING GRAMMARS Sr assign-comp : inf_nilind_nil displ-const : set1 : - tense : 1 assign-case : 2 agr : 3 assign-comp : 4 mode : 5 inv : - comp : nil displ-const : set1 : 6 wh : 7 - extracted : - NP0 case : 2 agr : 3 wh : 7 VP assign-case : 2 agr : 3 tense : 1 assign-comp : 4 mode : 5 displ-const : set1 : 6 mainv : 8 tense : 9 mode : 10 assign-comp : 11 assign-case : 12 agr : 13 passive : 14 - displ-const : set1 : - V mainv : 8 tense : 9 mode : 10 assign-comp : 11 assign-case : 12 agr : 13 passive : 14 mode : ind tense : pres mainv : - assign-comp : ind_nilthatrelifwhether assign-case : nom agr : 3rdsing :  num : sing pers : 3 thinks S1 displ-const : set1 : - assign-comp : inf_nilind_nil inv : - comp : thatwhetherifnil mode : indsbjnct Sr assign-comp : inf_nilind_nil displ-const : set1 : - tense : 1 assign-case : 2 agr : 3 assign-comp : 4 mode : 5 inv : - comp : nil displ-const : set1 : 6 wh : 7 - extracted : - NP0 case : 2 agr : 3 wh : 7 VP assign-case : 2 agr : 3 tense : 1 assign-comp : 4 mode : 5 displ-const : set1 : 6 mainv : 8 tense : 9 mode : 10 assign-comp : 11 assign-case : 12 agr : 13 passive : 14 - displ-const : set1 : - V mainv : 8 tense : 9 mode : 10 assign-comp : 11 assign-case : 12 agr : 13 passive : 14 mode : ind tense : pres mainv : - assign-comp : ind_nilthatrelifwhether assign-case : nom agr : 3rdsing :  num : sing pers : 3 wants S1 displ-const : set1 : - assign-comp : inf_nilind_nil inv : - comp : whetherfornil mode : inf think tree want tree Figure 2.7: Lexicalized Elementary Trees with Features Chapter 3 Overview of the XTAG System This section focuses on the various components that comprise the parser and English grammar in the XTAG system. Persons interested only in the linguistic analyses in the grammar may skip this section without loss of continuity, although a quick glance at the tagset used in XTAG and the set of non-terminal labels used will be useful. We may occasionally refer back to the various components mentioned in this section. 3.1 System Description Figure 3.1 shows the overall ﬂow of the system when parsing a sentence; a summary of each component is presented in Table 3.1. At the heart of the system is a parser for lexicalized TAGs ([Schabes and Joshi, 1988; Schabes, 1990]) which produces all legitimate parses for the sentence. The parser has two phases: Tree Selection and Tree Grafting. 3.1.1 Tree Selection Since we are working with lexicalized TAGs, each word in the sentence selects at least one tree. The advantage of a lexicalized formalism like LTAGs is that rather than parsing with all the trees in the grammar, we can parse with only the trees selected by the words in the input sentence. In the XTAG system, the selection of trees by the words is done in several steps. Each step attempts to reduce ambiguity, i.e. reduce the number of trees selected by the words in the sentence. Morphological Analysis and POS Tagging The input sentence is ﬁrst submitted to the Morphological Analyzer and the Tagger. The morphological analyzer ([Karp et al., 1992]) consists of a disk-based database (a compiled version of the derivational rules) which is used to map an inﬂected word into its stem, part of speech and feature equations corresponding to inﬂectional information. These features are inserted at the anchor node of the tree eventually selected by the stem. The POS Tagger can be disabled in which case only information from the morphological analyzer is used. The morphology data was originally extracted from the Collins English Dictionary ([Hanks, 1979]) and Oxford 11 12 CHAPTER 3. OVERVIEW OF THE XTAG SYSTEM Derivation Structure Input Sentence Morph Analyzer Parser Morph DB Tree Grafting Tree Selection Syn DB Trees DB Stat DB P.O.S Blender Tagger Lex Prob DB Figure 3.1: Overview of XTAG system Advanced Learners Dictionary ([Hornby, 1974]) available through ACL-DCI ([Liberman, 1989]), and then cleaned up and augmented by hand ([Karp et al., 1992]). POS Blender The output from the morphological analyzer and the POS tagger go into the POS Blender which uses the output of the POS tagger as a ﬁlter on the output of the morphological analyzer. Any words that are not found in the morphological database are assigned the POS given by the tagger. Syntactic Database The syntactic database contains the mapping between particular stem(s) and the tree templates or tree-families stored in the Tree Database (see Table 3.1). The syntactic database also contains a list of feature equations that capture lexical idiosyn- crasies. The output of the POS Blender is used to search the Syntactic Database to produce a set of lexicalized trees with the feature equations associated with the word(s) in the syntactic database uniﬁed with the feature equations associated with the trees. Note that the features in the syntactic database can be assigned to any node in the tree and not just to the anchor node. The syntactic database entries were originally extracted from the Oxford Advanced Learners Dictionary ([Hornby, 1974]) and Oxford Dictionary for Contemporary Idiomatic English ([Cowie and Mackin, 1975]) available through ACL- DCI ([Liberman, 1989]), and then modiﬁed and augmented by hand ([Egedi and Martin, 1994]). There are more than 31,000 syntactic database entries.1 Selected entries from this database are shown in Table 3.2. Default Assignment For words that are not found in the syntactic database, default trees and tree-families are assigned based on their POS tag. Filters Some of the lexicalized trees chosen in previous stages can be eliminated in order to 1This number does not include trees assigned by default based on the part-of-speech of the word. 3.1. SYSTEM DESCRIPTION 13 Component Details Morphological Consists of approximately 317,000 inﬂected items Analyzer and derived from over 90000 stems. Morph Database Entries are indexed on the inﬂected form and return the root form, POS, and inﬂectional information. POS Tagger Wall Street Journal-trained trigram tagger ([Church, 1988]) and Lex Prob extended to output N-best POS sequences Database ([Soong and Huang, 1990]). Decreases the time to parse a sentence by an average of 93. Syntactic More than 30,000 entries. Database Each entry consists of: the uninﬂected form of the word, its POS, the list of trees or tree-families associated with the word, and a list of feature equations that capture lexical idiosyncrasies. Tree Database 1094 trees, divided into 52 tree families and 218 individual trees. Tree families represent subcategorization frames; the trees in a tree family would be related to each other transformationally in a movement-based approach. X-Interface Menu-based facility for creating and modifying tree ﬁles. User controlled parser parameters: parsers start category, enabledisableretry on failure for POS tagger. Storageretrieval facilities for elementary and parsed trees. Graphical displays of tree and feature data structures. Hand combination of trees by adjunction or substitution for grammar development. Ability to manually assign POS tag andor Supertag before parsing Table 3.1: System Summary reduce ambiguity. Two methods are currently used: structural ﬁlters which eliminate trees which have impossible spans over the input sentence and a statistical ﬁlter based on unigram probabilities of non-lexicalized trees (from a hand corrected set of approximately 6000 parsed sentences). These methods speed the runtime by approximately 87. Supertagging Before parsing, one can avail of an optional step of supertagging the sentence. This step uses statistical disambiguation to assign a unique elementary tree (or supertag) to each word in the sentence. These assignments can then be hand-corrected. These supertags are used as a ﬁlter on the tree assignments made so far. More information on supertagging can be found in ([Srinivas, 1997a; Srinivas, 1997b]). 3.1.2 Tree Database The Tree Database contains the tree templates that are lexicalized by following the various steps given above. The lexical items are inserted into distinguished nodes in the tree template called the anchor nodes. The part of speech of each word in the sentence corresponds to the label of the anchor node of the trees. Hence the tagset used by the POS Tagger corresponds exactly to the labels of the anchor nodes in the trees. The tagset used in the XTAG system is 14 CHAPTER 3. OVERVIEW OF THE XTAG SYSTEM INDEXporousnessENTRYporousnessPOSN TREESBNXN BN CNn FEATURESN_card- N_const- N_decreas- N_definite- N_gen- N_quan- N_refl- INDEXcooENTRYcooPOSVFAMILYTnx0V INDEXengrossENTRYengrossPOSVFAMILYTnx0Vnx1 FEATURESTRANS INDEXforbearENTRYforbearPOSVFAMILYTnx0Vs1 FEATURESS1_WH- S1_inf_for_nil INDEXhaveENTRYhavePOSVENTRYoutPOSPL FAMILYTnx0Vplnx1 Table 3.2: Example Syntactic Database Entries. given in Table 3.3. The tree templates are subdivided into tree families (for verbs and other predicates), and tree ﬁles which are simply collections of trees for lexical items like prepositions, determiners, etc2. 3.1.3 Tree Grafting Once a particular set of lexicalized trees for the sentence have been selected, XTAG uses an Earley-style predictive left-to-right parsing algorithm for LTAGs ([Schabes and Joshi, 1988; Schabes, 1990]) to ﬁnd all derivations for the sentence. The derivation trees and the associated derived trees can be viewed using the X-interface (see Table 3.1). The X-interface can also be used to save particular derivations to disk. The output of the parser for the sentence I had a map yesterday is illustrated in Figure 3.2. The parse tree3 represents the surface constituent structure, while the derivation tree represents the derivation history of the parse. The nodes of the derivation tree are the tree names anchored by the lexical items4. The composition operation is indicated by the nature of the arcs: a dashed line is used for substitution and a bold line for adjunction. The number beside each tree name is the address of the node at which the operation took place. The derivation tree can also be interpreted as a dependency graph with unlabeled arcs between words of the sentence. 2 The nonterminals in the tree database are A, AP, Ad, AdvP, Comp, Conj, D, N, NP, P, PP, Punct, S, V, VP. 3The feature structures associated with each note of the parse tree are not shown here. 4Appendix D explains the conventions used in naming the trees. 3.1. SYSTEM DESCRIPTION 15 Sr NP N I VPr VP NA V had NPr D a NPf NA N map PP P on NPr D my NPf NA N desk αnx0Vnx1[had] αNXN[I] (1) βvxPnx[on] (2) αNXN[desk] (2.2) βDnx[my] (0) αNXN[map] (2.2) βDnx[a] (0) Parse Tree Derivation Tree Figure 3.2: Output Structures from the Parser Part of Speech Description A Adjective Ad Adverb Comp Complementizer D Determiner G Genitive Noun I Interjection N Noun P Preposition PL Particle Punct Punctuation V Verb Table 3.3: XTAG tagset 3.1.4 The Grammar Development Environment Working with and developing a large grammar is a challenging process, and the importance of having good visualization tools cannot be over-emphasized. Currently the XTAG system has X-windows based tools for viewing and updating the morphological and syntactic databases ([Karp et al., 1992; Egedi and Martin, 1994]). These are available in both ASCII and binary- encoded database format. The ASCII format is well-suited for various UNIX utilities (awk, sed, grep) while the database format is used for fast access during program execution. However even the ASCII formatted representation is not well-suited for human readability. An X- windows interface for the databases allows users to easily examine them. Searching for speciﬁc information on certain ﬁelds of the syntactic database is also available. Also, the interface allows a user to insert, delete and update any information in the databases. Figure 3.3(a) shows the 16 CHAPTER 3. OVERVIEW OF THE XTAG SYSTEM interface for the morphology database and Figure 3.3(b) shows the interface for the syntactic database. (a) Morphology database (b) Syntactic database Figure 3.3: Interfaces to the database maintenance tools XTAG also has a parsing and grammar development interface ([Paroubek et al., 1992]). This interface includes a tree editor, the ability to vary parameters in the parser, work with multiple grammars andor parsers, and use metarules for more eﬃcient tree editing and construction ([Becker, 1994]). The interface is shown in Figure 3.4. It has the following features:  Menu-based facility for creating and modifying tree ﬁles and loading grammar ﬁles.  User controlled parser parameters, including the root category (main S, embedded S, NP, etc.), and the use of the tagger (onoﬀretry on failure).  Storageretrieval facilities for elementary and parsed trees.  The production of postscript ﬁles corresponding to elementary and parsed trees.  Graphical displays of tree and feature data structures, including a scroll web for large tree structures.  Mouse-based tree editor for creating and modifying trees and feature structures.  Hand combination of trees by adjunction or substitution for use in diagnosing grammar problems.  Metarule tool for automatic aid to the generation of trees by using tree-based transfor- mation rules 3.2. COMPUTER PLATFORM 17 Figure 3.4: Interface to the XTAG system 3.2 Computer Platform XTAG was developed on the Sun SPARC station series. It has been tested on various Sun platforms including Ultra-1, Ultra-Enterprise. XTAG is freely available from the XTAG web page at http:www.cis.upenn.eduxtag. It requires 75 MB of disk space (once all binaries and databases are created after the install). XTAG requires the following software to run:  A machine running UNIX and X11R4 (or higher). Previous releases of X will not work. X11R4 is free software which usually comes bundled with your OS. It is also freely available for various platforms at http:www.xfree86.org  A Common Lisp compiler which supports the latest deﬁnition of Common Lisp (Steeles Common Lisp, second edition). XTAG has been tested on Lucid Common LispSPARC Solaris, Version: 4.2.1. Allegro CL is no longer directly supported, however there have been third party ports to recent versions of Allegro CL.  CLX version 4 or higher. CLX is the Lisp equivalent to the Xlib package written in C.  Mark Kantrowitzs Lisp Utilities from CMU: logical-pathnames and defsystem. A patched version of CLX (Version 5.02) for SunOS 5.5.1 and the CMU Lisp Utilities are provided in our ftp directory for your convenience. However, we ask that you refer to the appropriate sources for updates. The morphology database component ([Karp et al., 1992]), no longer under licensing re- strictions, is available as a separate download from the XTAG web page (see above for URL). The syntactic database component is also available as part of the XTAG system ([Egedi and Martin, 1994]). 18 CHAPTER 3. OVERVIEW OF THE XTAG SYSTEM More information can be obtained on the XTAG web page at http:www.cis.upenn.eduxtag. Chapter 4 Underview The morphology, syntactic, and tree databases together comprise the English grammar. A lexical item that is not in the databases receives a default tree selection and features for its part of speech and morphology. In designing the grammar, a decision was made early on to err on the side of acceptance whenever there are conﬂicting opinions as to whether or not a construction is grammatical. In this sense, the XTAG English grammar is intended to function primarily as an acceptor rather than a generator of English sentences. The range of syntactic phenomena that can be handled is large and includes auxiliaries (including inversion), cop- ula, raising and small clause constructions, topicalization, relative clauses, inﬁnitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO constructions, noun-noun modiﬁcations, extrapo- sition, determiner sequences, genitives, negation, noun-verb contractions, clausal adjuncts and imperatives. 4.1 Subcategorization Frames Elementary trees for non-auxiliary verbs are used to represent the linguistic notion of subcate- gorization frames. The anchor of the elementary tree subcategorizes for the other elements that appear in the tree, forming a clausal or sentential structure. Tree families group together trees belonging to the same subcategorization frame. Consider the following uses of the verb buy: (1) Srini bought a book. (2) Srini bought Beth a book. In sentence (1), the verb buy subcategorizes for a direct object NP. The elementary tree anchored by buy is shown in Figure 4.1(a) and includes nodes for the NP complement of buy and for the NP subject. In addition to this declarative tree structure, the tree family also contains the trees that would be related to each other transformationally in a movement based approach, i.e passivization, imperatives, wh-questions, relative clauses, and so forth. Sentence (2) shows that buy also subcategorizes for a double NP object. This means that buy also selects the double NP object subcategorization frame, or tree family, with its own set of transformationally related sentence structures. Figure 4.1(b) shows the declarative structure for this set of sentence structures. 19 20 CHAPTER 4. UNDERVIEW Sr NP0 VP V bought NP1 Sr NP0 VP V bought NP1 NP2 (a) (b) Figure 4.1: Diﬀerent subcategorization frames for the verb buy 4.2 Complements and Adjuncts Complements and adjuncts have very diﬀerent structures in the XTAG grammar. Complements are included in the elementary tree anchored by the verb that selects them, while adjuncts do not originate in the same elementary tree as the verb anchoring the sentence, but are instead added to a structure by adjunction. The contrasts between complements and adjuncts have been extensively discussed in the linguistics literature and the classiﬁcation of a given element as one or the other remains a matter of debate (see [Rizzi, 1990], [Larson, 1988], [Jackendoﬀ, 1990], [Larson, 1990], [Cinque, 1990], [Obernauer, 1984], [Lasnik and Saito, 1984], and [Chomsky, 1986]). The guiding rule used in developing the XTAG grammar is whether or not the sentence is ungrammatical without the questioned structure.1 Consider the following sentences: (3) Srini bought a book. (4) Srini bought a book at the bookstore. (5) Srini arranged for a ride. (6) Srini arranged. Prepositional phrases frequently occur as adjuncts, and when they are used as adjuncts they have a tree structure such as that shown in Figure 4.2(a). This adjunction tree would adjoin into the tree shown in Figure 4.1(a) to generate sentence (4). There are verbs, however, such as arrange, hunger and diﬀerentiate, that take prepositional phrases as complements. Sentences (5) and (6) clearly show that the prepositional phrase are not optional for arrange. For these sentences, the prepositional phrase will be an initial tree (as shown in Figure 4.2(b)) that substitutes into an elementary tree, such as the one anchored by the verb arrange in Figure 4.2(c). 1Iteration of a structure can also be used as a diagnostic: Srini bought a book at the bookstore on Walnut Street for a friend. 4.3. NON-S CONSTITUENTS 21 VPr VP PP P at NP PP P for NP Sr NP0 VP V arranged PP  (a) (b) (c) Figure 4.2: Trees illustrating the diﬀerence between Complements and Adjuncts Virtually all parts of speech, except for main verbs, function as both complements and adjuncts in the grammar. More information is available in this report on various parts of speech as complements: adjectives (e.g. section 6.13), nouns (e.g. section 6.2), and prepositions (e.g. section 6.10); and as adjuncts: adjectives (section 19.1), adverbs (section 19.5), nouns (section 19.2), and prepositions (section 19.4). 4.3 Non-S constituents Although sentential trees are generally considered to be special cases in any grammar, insofar as they make up a starting category, it is the case that any initial tree constitutes a phrasal constituent. These initial trees may have substitution nodes that need to be ﬁlled (by other initial trees), and may be modiﬁed by adjunct trees, exactly as the trees rooted in S. Although grouping is possible according to the heads or anchors of these trees, we have not found any classiﬁcation similar to the subcategorization frames for verbs that can be used by a lexical entry to group select a set of trees. These trees are selected one by one by each lexical item, according to each lexical items idiosyncrasies. The grammar described by this technical report places them into several ﬁles for ease of use, but these ﬁles do not constitute tree families in the way that the subcategorization frames do. 4.4 Case Assignment 4.4.1 Approaches to Case 4.4.1.1 Case in GB theory GB (Government and Binding) theory proposes the following case ﬁlter as a requirement on S-structure.2 Case Filter Every overt NP must be assigned abstract case. [Haegeman, 1991] 2There are certain problems with applying the case ﬁlter as a requirement at the level of S-structure. These issues are not crucial to the discussion of the English XTAG implementation of case and so will not be discussed here. Interested readers are referred to [Lasnik and Uriagereka, 1988]. 22 CHAPTER 4. UNDERVIEW Abstract case is taken to be universal. Languages with rich morphological case marking, such as Latin, and languages with very limited morphological case marking, like English, are all presumed to have full systems of abstract case that diﬀer only in the extent of morphological realization. In GB, abstract case is argued to be assigned to NPs by various case assigners, namely verbs, prepositions, and INFL. Verbs and prepositions are said to assign accusative case to NPs that they govern, and INFL assigns nominative case to NPs that it governs. These governing categories are constrained as to where they can assign case by means of barriers based on minimality conditions, although these are relaxed in exceptional case marking situations. The details of the GB analysis are beyond the scope of this technical report, but see [Chomsky, 1986] for the original analysis or [Haegeman, 1991] for an overview. Let it suﬃce for us to say that the notion of abstract case and the case ﬁlter are useful in accounting for a number of phenomena including the distribution of nominative and accusative case, and the distribution of overt NPs and empty categories (such as PRO). 4.4.1.2 Minimalism and Case A major conceptual diﬀerence between GB theories and Minimalism is that in Minimalism, lexical items carry their features with them rather than being assigned their features based on the nodes that they end up at. For nouns, this means that they carry case with them, and that their case is checked when they are in SPEC position of AGRs or AGRo, which subsequently disappears [Chomsky, 1992]. 4.4.2 Case in XTAG The English XTAG grammar adopts the notion of case and the case ﬁlter for many of the same reasons argued in the GB literature. However, in some respects the English XTAG grammars implementation of case more closely resembles the treatment in Chomskys Minimalism frame- work [Chomsky, 1992] than the system outlined in the GB literature [Chomsky, 1986]. As in Minimalism, nouns in the XTAG grammar carry case with them, which is eventually checked. However in the XTAG grammar, noun cases are checked against the case values assigned by the verb during the uniﬁcation of the feature structures. Unlike Chomskys Minimalism, there are no separate AGR nodes; the case checking comes from the verbs directly. Case assignment from the verb is more like the GB approach than the requirement of a SPEC-head relationship in Minimalism. Most nouns in English do not have separate forms for nominative and accusative case, and so they are ambiguous between the two. Pronouns, of course, are morphologically marked for case, and each carries the appropriate case in its feature. Figures 4.3(a) and 4.3(b) show the NP tree anchored by a noun and a pronoun, respectively, along with the feature values associated with each word. Note that books simply gets the default case nomacc, while she restricts the case to be nom. 4.4.3 Case Assigners 4.4. CASE ASSIGNMENT 23 NP pron : 1 wh : 2 case : 3 nomacc agr : 4 N pron : 1 wh : 2 case : 3 agr : 4 agr : 3rdsing : - num : plur pers : 3 wh : - books NP pron : 1 wh : 2 - case : 3 nomacc agr : 4 N agr : 4 case : 3 pron : 1 wh : 2 pron :  refl : - case : nom poss : - agr : gen : fem 3rdsing :  num : sing pers : 3 she (a) (b) Figure 4.3: Lexicalized NP trees with case markings 4.4.3.1 Prepositions Case is assigned in the XTAG English grammar by two lexical categories - verbs and preposi- tions.3 Prepositions assign accusative case (acc) through their assign-case feature, which is linked directly to the case feature of their objects. Figure 4.4(a) shows a lexicalized preposition tree, while Figure 4.4(b) shows the same tree with the NP tree from Figure 4.3(a) substituted into the NP position. Figure 4.4(c) is the tree in Figure 4.4(b) after uniﬁcation has taken place. Note that the case ambiguity of books has been resolved to accusative case. 4.4.3.2 Verbs Verbs are the other part of speech in the XTAG grammar that can assign case. Because XTAG does not distinguish INFL and VP nodes, verbs must provide case assignment on the subject position in addition to the case assigned to their NP complements. Assigning case to NP complements is handled by building the case values of the complements directly into the tree that the case assigner (the verb) anchors. Figures 4.5(a) and 4.5(b) show an S tree4 that would be anchored5 by a transitive and ditransitive verb, respectively. Note that the case assignments for the NP complements are already in the tree, even though there is not yet a lexical item anchoring the tree. Since every verb that selects these trees (and other trees in each respective subcategorization frame) assigns the same case to the complements, 3For also assigns case as a complementizer. See section 8.5 for more details. 4Features not pertaining to this discussion have been taken out to improve readability and to make the trees easier to ﬁt onto the page. 5The diamond marker () indicates the anchor(s) of a structure if the tree has not yet been lexicalized. 24 CHAPTER 4. UNDERVIEW PP assign-case : 1 wh : 2 P assign-case : 1 assign-case : acc of NP case : 1 wh : 2 PP assign-case : 5 wh : 6 P assign-case : 5 assign-case : acc of NP case : 5 wh : 6 pron : 1 wh : 2 case : 3 agr : 4 N pron : 1 wh : 2 case : 3 nomacc agr : 4 agr : 3rdsing : - num : plur pers : 3 wh : - books PP assign-case : 1 acc wh : 2 - P assign-case : 1 of NP agr : 3 3rdsing : - num : plur pers : 3 pron : 4 case : 1 wh : 2 N pron : 4 wh : 2 case : 1 agr : 3 books (a) (b) (c) Figure 4.4: Assigning case in prepositional phrases building case features into the tree has exactly the same result as putting the case feature value in each verbs lexical entry. Sr assign-case : 3 agr : 4 NP0 wh : - case : 3 agr : 4 VP assign-case : 3 agr : 4 assign-case : 1 agr : 2 V assign-case : 1 agr : 2 NP1 case : acc Sr assign-case : 3 agr : 4 NP0 wh : - case : 3 agr : 4 VP assign-case : 3 agr : 4 assign-case : 1 agr : 2 V assign-case : 1 agr : 2 NP1 case : acc NP2 case : acc (a) (b) Figure 4.5: Case assignment to NP arguments The case assigned to the subject position varies with verb form. Since the XTAG grammar treats the inﬂected verb as a single unit rather than dividing it into INFL and V nodes, case, along with tense and agreement, is expressed in the features of verbs, and must be passed in the appropriate manner. The trees in Figure 4.6 show the path of linkages that joins the assign- case feature of the V to the case feature of the subject NP. The morphological form of 4.4. CASE ASSIGNMENT 25 the verb determines the value of the assign-case feature. Figures 4.6(a) and 4.6(b) show the same tree6 anchored by diﬀerent morphological forms of the verb sing, which give diﬀerent values for the assign-case feature. Sr assign-case : 3 agr : 4 NP0 wh : - case : 3 agr : 4 VP assign-case : 3 agr : 4 assign-case : 1 agr : 2 V assign-case : 1 agr : 2 agr : pers : 3 num : sing 3rdsing :  assign-case : nom mode : ind sings NP1 case : acc Sr assign-case : 3 agr : 4 NP0 wh : - case : 3 agr : 4 VP assign-case : 3 agr : 4 assign-case : 1 agr : 2 V assign-case : 1 agr : 2 assign-case : none mode : ger singing NP1 case : acc (a) (b) Figure 4.6: Assigning case according to verb form The adjunction of an auxiliary verb onto the VP node breaks the assign-case link from the main V, replacing it with a link from the auxiliary verb instead.7 The progressive form of the verb in Figure 4.6(b) has the feature-value assign-casenone, but this is overridden by the adjunction of the appropriate form of the auxiliary word be. Figure 4.7(a) shows the lexicalized auxiliary tree, while Figure 4.7(b) shows it adjoined into the transitive tree shown in Figure 4.6(b). The case value passed to the subject NP is now nom (nominative). 4.4.4 PRO in a uniﬁcation based framework Tensed forms of a verb assign nominative case, and untensed forms assign case none, as the progressive form of the verb sing does in Figure 4.6(b). This is diﬀerent than assigning no case at all, as one form of the inﬁnitive marker to does. See Section 8.5 for more discussion of this special case.) The distinction of a case none from no case is indicative of a divergence from the standard GB theory. In GB theory, the absence of case on an NP means that only PRO can ﬁll that NP. With feature uniﬁcation as is used in the FB-LTAG grammar, the absence of case on an NP means that any NP can ﬁll it, regardless of its case. This is due to the mechanism of uniﬁcation, in which if something is unspeciﬁed, it can unify with anything. Thus we have a speciﬁc case none to handle verb forms that in GB theory do not assign case. PRO is the 6Again, the feature structures shown have been restricted to those that pertain to the VNP interaction. 7See section 20.1 for a more complete explanation of how this relinking occurs. 26 CHAPTER 4. UNDERVIEW VPr agr : 1 assign-case : 2 V agr : 1 assign-case : 2 agr : pers : 3 num : sing 3rdsing :  assign-case : nom is VP Sr assign-case : 3 agr : 4 NP0 wh : - case : 3 agr : 4 VPr assign-case : 3 agr : 4 agr : 1 assign-case : 2 V agr : 1 assign-case : 2 agr : pers : 3 num : sing 3rdsing :  assign-case : nom is VP assign-case : 5 agr : 6 V assign-case : 5 agr : 6 assign-case : none mode : ger singing NP1 case : acc (a) (b) Figure 4.7: Proper case assignment with auxiliary verbs 4.4. CASE ASSIGNMENT 27 only NP with case none. Note that although we are drawn to this treatment by our use of uniﬁcation for feature manipulation, our treatment is very similar to the assignment of null case to PRO in [Chomsky and Lasnik, 1993]. [Watanabe, 1993] also proposes a very similar approach within Chomskys Minimalist framework.8 8See Sections 8.1 and 8.9 for additional discussion of PRO. Part II Verb Classes 28 Chapter 5 Where to Find What The two page table that follows gives an overview of what types of trees occur in various tree families with pointers to discussion in this report. An entry in a cell of the table indicates that the tree(s) for the construction named in the row header are included in the tree family named in the column header. Entries are of two types. If the particular tree(s) are displayed andor discussed in this report the entry gives a page number reference to the relevant discussion or ﬁgure.1 Otherwise, a  indicates inclusion in the tree family but no ﬁgure or discussion related speciﬁcally to that tree in this report. Blank cells indicate that there are no trees for the construction named in the row header in the tree family named in the column header. Two tables are given below. The ﬁrst one gives the expansion of abbreviations in the table headers. The second table gives the name given to each tree family in the actual XTAG grammar. This makes it easier to ﬁnd the description of each tree family in Chapter 6 and to compare the description with the online XTAG grammar. 1Since Chapter 6 has a brief discussion and a declarative tree for every tree family, page references are given only for other sections in which discussion or tree diagrams appear. 30 31 Abbreviation Full Name Sent. Subj. w. to Sentential Subject with to PP complement Pred. Mult-wd. ARB, P Predicative Multi-word PP with Adv, Prep anchors Pred. Mult-wd. A, P Predicative Multi-word PP with Adj, Prep anchors Pred. Mult-wd. N, P Predicative Multi-word PP with Noun, Prep anchors Pred. Mult-wd. P, P Predicative Multi-word PP with two Prep anchors Pred. Mult-wd. no int. mod. Predicative Multi-word PP with no internal modiﬁcation Pred. Sent. Subj., ARB, P Predicative PP with Sentential Subject, and Adv, Prep anchors Pred. Sent. Subj., A, P Predicative PP with Sentential Subject, and Adj, Prep anchors Pred. Sent. Subj., Conj, P Predicative PP with Sentential Subject, and Conj, Prep anchors Pred. Sent. Subj., N, P Predicative PP with Sentential Subject, and Noun, Prep anchors Pred. Sent. Subj., P, P Predicative PP with Sentential Subject, and two Prep anchors Pred. Sent. Subj., no int-mod Predicative PP with Sentential Subject, no internal modiﬁcation Pred. Locative Predicative anchored by a Locative Adverb Pred. A Sent. Subj., Comp. Predicative Adjective with Sentential Subject and Complement Sentential Comp. with NP Sentential Complement with NP Pred. Mult wd. V, P Predicative Multi-word with Verb, Prep anchors Adj. Sm. Cl. w. Sentential Subj. Adjective Small Clause with Sentential Subject NP Sm. Clause w. Sentential Subj. NP Small Clause with Sentential Subject PP Sm. Clause w. Sentential Subj. PP Small Clause with Sentential Subject NP Sm. Cl. w. Sent. Comp. NP Small Clause with Sentential Complement Adj. Sm. Cl. w. Sent. Comp. Adjective Small Clause with Sentential Complement Exhaustive PP Sm. Cl. Exhaustive PP Small Clause Ditrans. Light Verbs w. PP Shift Ditransitive Light Verbs with PP Shift Ditrans. Light Verbs wo PP Shift Ditransitive Light Verbs without PP Shift YN question YesNo question Wh-mov. NP complement Wh-moved NP complement Wh-mov. S comp. Wh-moved S complement Wh-mov. Adj comp. Wh-moved Adjective complement Wh-mov. object of a P Wh-moved object of a P Wh-mov. PP Wh-moved PP Topic. NP complement Topicalized NP complement Det. gerund Determiner gerund Rel. cl. on NP comp. Relative clause on NP complement Rel. cl. on PP comp. Relative clause on PP complement Rel. cl. on NP object of P Relative clause on NP object of P Pass. with wh-moved subj. Passive with wh-moved subject (with and without by phrase) Pass. w. wh-mov. ind. obj. Passive with wh-moved indirect object (with and without by phrase) Pass. w. wh-mov. obj. of the by phrase Passive with wh-moved object of the by phrase Pass. w. wh-mov. by phrase Passive with wh-moved by phrase Trans. Idiom with V, D and N Transitive Idiom with Verb, Det and Noun anchors Idiom with V, D, N Idiom with V, D, and N anchors Idiom with V, D, A, N Idiom with V, D, A, and N anchors Idiom with V, N Idiom with V, and N anchor Idiom with V, A, N Idiom with V, A, and N anchors Idiom with V, D, N, P Idiom with V, D, N, and Prep anchors Idiom with V, D, A, N, P Idiom with V, D, A, N, and Prep anchors Idiom with V, N, P Idiom with V, N, and Prep anchors Idiom with V, A, N, P Idiom with V, A, N, and Prep anchors 32 CHAPTER 5. WHERE TO FIND WHAT Full Name XTAG Name Intransitive Sentential Subject Ts0V Sentential Subject with to complement Ts0Vtonx1 PP Small Clause, with Adv and Prep anchors Tnx0ARBPnx1 PP Small Clause, with Adj and Prep anchors Tnx0APnx1 PP Small Clause, with Noun and Prep anchors Tnx0NPnx1 PP Small Clause, with Prep anchors Tnx0PPnx1 PP Small Clause, with Prep and Noun anchors Tnx0PNaPnx1 PP Small Clause with Sentential Subject, and Adv and Prep anchors Ts0ARBPnx1 PP Small Clause with Sentential Subject, and Adj and Prep anchors Ts0APnx1 PP Small Clause with Sentential Subject, and Noun and Prep anchors Ts0NPnx1 PP Small Clause with Sentential Subject, and Prep anchors Ts0PPnx1 PP Small Clause with Sentential Subject, and Prep and Noun anchors Ts0PNaPnx1 Exceptional Case Marking TXnx0Vs1 Locative Small Clause with Ad anchor Tnx0nx1ARB Predicative Adjective with Sentential Subject and Complement Ts0A1s1 Transitive Tnx0Vnx1 Ditransitive with PP shift Tnx0Vnx1tonx2 Ditransitive Tnx0Vnx1nx2 Ditransitive with PP Tnx0Vnx1pnx2 Sentential Complement with NP Tnx0Vnx1s2 Intransitive Verb Particle Tnx0Vpl Transitive Verb Particle Tnx0Vplnx1 Ditransitive Verb Particle Tnx0Vplnx1nx2 Intransitive with PP Tnx0Vpnx1 Sentential Complement Tnx0Vs1 Light Verbs Tnx0lVN1 Ditransitive Light Verbs with PP Shift Tnx0lVN1Pnx2 Adjective Small Clause with Sentential Subject Ts0Ax1 NP Small Clause with Sentential Subject Ts0N1 PP Small Clause with Sentential Subject Ts0Pnx1 Predicative Multi-word with Verb, Prep anchors Tnx0VPnx1 Adverb It-Cleft TItVad1s2 NP It-Cleft TItVnx1s2 PP It-Cleft TItVpnx1s2 Adjective Small Clause Tree Tnx0Ax1 Adjective Small Clause with Sentential Complement Tnx0A1s1 Equative BE Tnx0BEnx1 NP Small Clause Tnx0N1 NP with Sentential Complement Small Clause Tnx0N1s1 PP Small Clause Tnx0Pnx1 Exhaustive PP Small Clause Tnx0Px1 Intransitive Tnx0V Intransitive with Adjective Tnx0Vax1 Transitive Sentential Subject Ts0Vnx1 Idiom with V, D and N Tnx0VDN1 Idiom with V, D, A, and N anchors Tnx0VDAN1 Idiom with V and N anchors Tnx0VN1 Idiom with V, A, and N anchors Tnx0VAN1 Idiom with V, D, N, and Prep anchors Tnx0VDN1Pnx2 Idiom with V, D, A, N, and Prep anchors Tnx0VDAN1Pnx2 Idiom with V, N, and Prep anchors Tnx0VN1Pnx2 Idiom with V, A, N, and Prep anchors Tnx0VAN1Pnx2 33 Tree families Intransitive Sentential Subj Sent. Subj. w. to Pred. Mult-wd. ARB, P Pred. Mult-wd. A, P Pred. Mult-wd. N, P Pred. Mult-wd. P, P Pred. Mult-wd. no int. mod. Pred. Sent. Subj., ARB, P Pred. Sent. Subj., A, P Pred. Sent. Subj., N, P Pred. Sent. Subj., P, P Pred. Sent. Subj., no int-mod ECM Pred. Locative Pred. A Sent. Subj., Comp. Constructions Declarative             89 70 Passive w  wo by phrase 91 YN quest. Wh-moved subject                Wh-mov. NP complement, DO or IO Wh-mov. S comp. Wh-mov. Adj. or Adv. comp. 71 Wh-mov. object of a P      Wh-mov. PP    Topic. NP comp. Imperative      Det. gerund NP gerund     Ergative Rel. cl. on subj. w NP        Rel. cl. on subj. w Comp        Rel. cl. on NP comp., DO, IO w NP Rel. cl. on NP comp., DO, IO w Comp Rel. cl. on PP comp. w pied-piping   Rel. cl. on NP object of P w NP           Rel. cl. on NP object of P w Comp          Rel. cl. on adjunct w PP               Rel. cl. on adjunct w Comp               Pass. w. wh-mov. subj. Pass. w. wh-mov. ind. obj. Pass. w. wh-mov. obj. of by phrase Pass. w. wh-mov. by phrase 34 CHAPTER 5. WHERE TO FIND WHAT Tree families Transitive Ditransitive with PP shift Ditransitive Ditransitive with PP Sentential Comp. with NP Intransitive Verb Particle Transitive Verb Particle Ditransitive Verb Particle Intransitive with PP Sentential Complement Trans. Light Vs Ditrans. Light Vs Adj. Sm. Cl. w. Sentential Subj. NP Sm. Cl. w. Sentential Subj. PP Sm. Cl. w. Sentential Subj. Pred. Mult. wd. V, P Constructions Declarative 24 110 24      21 10,87       Passive w  wo by phrase     116     YN quest. Wh-moved subject                Wh-mov. NP complement, DO or IO 119  123     Wh-mov. S comp.   Wh-mov. Adj. or Adv. comp.  Wh-mov. object of a P  124    Wh-mov. PP  124   Topic. NP comp.        Imperative 144             Det. gerund 147             NP gerund 149             Ergative 80 Rel. cl. on subj. w NP              Rel. cl. on subj. w Comp              Rel. cl. on NP comp., DO, IO w NP          Rel. cl. on NP comp., DO, IO w Comp          Rel. cl. on PP comp. w pied-piping          Rel. cl. on NP object of P w NP           Rel. cl. on NP object of P w Comp           Rel. cl. on adjunct w PP                 Rel. cl. on adjunct w Comp                 Parenthetical quoting clause   Past-participal as arg Adj   Past-participial NP pre-mod   Pass. w. wh-mov. subj.          Pass. w. wh-mov. ind. obj.       Pass. w. wh-mov. obj. of by phrase          Pass. w. wh-mov. by phrase         35 Tree families Adverb It-Cleft NP It-Cleft PP It-Cleft Adj. Small Clause Adj. Sm. Cl. w. Sent. Comp. Equative BE NP Small Clause NP Sm. Cl. w. Sent. Comp. PP Small Clause Exhaustive PP Sm. Cl. Intransitive Intransitive with Adjective Transitive Sentential Subj Constructions Declarative   113 105  108 102  102    92 Passive w  wo by phrase YN quest.   113  Wh-moved subject 87      122 88  Wh-mov. NP complement, DO or IO    Wh-mov. S comp. Wh-mov. Adj. or Adv. comp.   125 Wh-mov. object of a P  Wh-mov. PP   Topic. NP comp.   Imperative         Det. gerund NP gerund         Ergative Rel. cl. on subj. w NP         Rel. cl. on subj. w Comp         Rel. cl. on NP comp., DO, IO w NP Rel. cl. on NP comp., DO, IO w Comp Rel. cl. on PP comp. w pied-piping  Rel. cl. on NP object of P w NP  Rel. cl. on NP object of P w Comp  Rel. cl. on adjunct w PP             Rel. cl. on adjunct w Comp             Participial NP pre-mod  Pass. w. wh-mov. subj. Pass. w. wh-mov. ind. obj. Pass. w. wh-mov. obj. of by phrase Pass. w. wh-mov. by phrase 36 CHAPTER 5. WHERE TO FIND WHAT Tree families Idiom with V, D, N Idiom with V, D, A, N Idiom with V, N Idiom with V, A, N Idiom with V, D, N, P Idiom with V, D, A, N, P Idiom with V, N, P Idiom with V, A, N, P Constructions Declarative         Passive w  wo by phrase         YN quest. Wh-moved subject         Wh-mov. NP complement, DO or IO Wh-mov. S comp. Wh-mov. Adj. or Adv. comp. Wh-mov. object of a P Wh-mov. PP Topic. NP comp. Imperative         Det. gerund NP gerund         Ergative Rel. cl. on subj. w NP         Rel. cl. on subj. w Comp         Rel. cl. on NP comp., DO, IO w NP Rel. cl. on NP comp., DO, IO w Comp Rel. cl. on PP comp. w pied-piping Rel. cl. on NP object of P w NP Rel. cl. on NP object of P w Comp Rel. cl. on adjunct w PP         Rel. cl. on adjunct w Comp         Pass. w. wh-mov. subj. Pass. w. wh-mov. ind. obj. Pass. w. wh-mov. obj. of by phrase         Pass. w. wh-mov. by phrase         Outer Pass. w. and wo. by phrase     Outer Pass. w. Rel. cl. on subj. w. Comp      Outer Pass. w. Rel. cl. on subj. w. NP      Chapter 6 Verb Classes Each main1 verb in the syntactic lexicon selects at least one tree family2 (subcategorization frame). Since the tree database and syntactic lexicon are already separated for space eﬃciency (see Chapter 3), each verb can eﬃciently select a large number of trees by specifying a tree family, as opposed to each of the individual trees. This approach allows for a considerable reduction in the number of trees that must be speciﬁed for any given verb or form of a verb. There are currently 52 tree families in the system.3 This chapter gives a brief description of each tree family and shows the corresponding declarative tree4, along with any peculiar characteristics or trees. It also indicates which transformations are in each tree family, and gives the number of verbs that select that family.5 A few sample verbs are given, along with example sentences. 6.1 Intransitive: Tnx0V Description: This tree family is selected by verbs that do not require an object complement of any type. Adverbs, prepositional phrases and other adjuncts may adjoin on, but are not required for the sentences to be grammatical. 1,878 verbs select this family. Examples: eat, sleep, dance Al ate . Seth slept . Hyun danced . Declarative tree: See Figure 6.1. Other available trees: wh-moved subject, subject relative clause with and without comp, adjunct (gap-less) relative clause with comp, adjunct (gap-less) relative clause with PP pied-piping, imperative, determiner gerund, NP gerund, pre-nominal participal. 1Auxiliary verbs are handled under a diﬀerent mechanism. See Chapter 20 for details. 2See section 3.1.2 for explanation of tree families. 3An explanation of the naming convention used in naming the trees and tree families is available in Ap- pendix D. 4Before lexicalization, the  indicates the anchor of the tree. 5Numbers given are as of August 1998 and are subject to some change with further development of the grammar. 37 38 CHAPTER 6. VERB CLASSES Sr NP0 VP V Figure 6.1: Declarative Intransitive Tree: αnx0V 6.2 Transitive: Tnx0Vnx1 Description: This tree family is selected by verbs that require only an NP object complement. The NPs may be complex structures, including gerund NPs and NPs that take sentential complements. This does not include light verb constructions (see sections 6.15 and 6.16). 4,343 verbs select the transitive tree family. Examples: eat, dance, take, like Al ate an apple . Seth danced the tango . Hyun is taking an algorithms course . Anoop likes the fact that the semester is ﬁnished . Declarative tree: See Figure 6.2. Sr NP0 VP V NP1 Figure 6.2: Declarative Transitive Tree: αnx0Vnx1 Other available trees: wh-moved subject, wh-moved object, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, object relative clause with and without comp, imperative, determiner gerund, NP gerund, passive with by phrase, passive without by phrase, passive with wh-moved subject and by phrase, passive with wh-moved subject and no by phrase, passive with wh-moved object out of the by phrase, passive with wh-moved by phrase, passive with relative clause on subject and by phrase with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive with relative clause on object on the by phrase with and without compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase, ergative, ergative with wh-moved subject, ergative with subject relative clause with and without comp, ergative with adjunct (gap-less) relative clause with compwith 6.3. DITRANSITIVE: TNX0VNX1NX2 39 PP pied-piping. In addition, two other trees that allow transitive verbs to function as adjectives (e.g. the stopped truck) are also in the family. 6.3 Ditransitive: Tnx0Vnx1nx2 Description: This tree family is selected by verbs that take exactly two NP complements. It does not include verbs that undergo the ditransitive verb shift (see section 6.5). The apparent ditransitive alternates involving verbs in this class and benefactive PPs (e.g. John baked a cake for Mary) are analyzed as transitives (see section 6.2) with a PP adjunct. Benefactives are taken to be adjunct PPs because they are optional (e.g. John baked a cake vs. John baked a cake for Mary). 122 verbs select the ditransitive tree family. Examples: ask, cook, win Christy asked Mike a question . Doug cooked his father dinner . Dania won her sister a stuﬀed animal . Declarative tree: See Figure 6.3. Sr NP0 VP V NP1 NP2 Figure 6.3: Declarative Ditransitive Tree: αnx0Vnx1nx2 Other available trees: wh-moved subject, wh-moved direct object, wh-moved indirect ob- ject, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, direct object relative clause with and without comp, in- direct object relative clause with and without comp, imperative, determiner gerund, NP gerund, passive with by phrase, passive without by phrase, passive with wh-moved subject and by phrase, passive with wh-moved subject and no by phrase, passive with wh-moved object out of the by phrase, passive with wh-moved by phrase, passive with wh-moved indirect object and by phrase, passive with wh-moved indirect object and no by phrase, passive with relative clause on subject and by phrase with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive with relative clause on object of the by phrase with and without compwith PP pied-piping, passive with relative clause on the indirect object and by phrase with and without comp, passive with relative clause on the indirect object and no by phrase with and without comp, pas- sive withwithout by-phrase with adjunct (gap-less) relative clause with compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase. 40 CHAPTER 6. VERB CLASSES 6.4 Ditransitive with PP: Tnx0Vnx1pnx2 Description: This tree family is selected by ditransitive verbs that take a noun phrase followed by a prepositional phrase. The preposition is not constrained in the syntactic lexicon. The preposition must be required and not optional - that is, the sentence must be ungram- matical with just the noun phrase (e.g. John put the table). No verbs, therefore, should select both this tree family and the transitive tree family (see section 6.2). This tree family is also distinguished from the ditransitive verbs, such as give, that undergo verb shifting (see section 6.5). There are 62 verbs that select this tree family. Examples: associate, put, refer Rostenkowski associated money with power . He put his reputation on the line . He referred all questions to his attorney . Declarative tree: See Figure 6.4. Sr NP0 VP V NP1 PP2 Figure 6.4: Declarative Ditransitive with PP Tree: αnx0Vnx1pnx2 Other available trees: wh-moved subject, wh-moved direct object, wh-moved object of PP, wh-moved PP, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, direct object relative clause with and without comp, object of PP relative clause with and without compwith PP pied-piping, imper- ative, determiner gerund, NP gerund, passive with by phrase, passive without by phrase, passive with wh-moved subject and by phrase, passive with wh-moved subject and no by phrase, passive with wh-moved object out of the by phrase, passive with wh-moved by phrase, passive with wh-moved object out of the PP and by phrase, passive with wh-moved object out of the PP and no by phrase, passive with wh-moved PP and by phrase, passive with wh-moved PP and no by phrase, passive with relative clause on subject and by phrase with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive with relative clause on object of the by phrase with and without compwith PP pied-piping, passive with relative clause on the object of the PP and by phrase with and without compwith PP pied-piping, passive with relative clause on the object of the PP and no by phrase with and without compwith PP pied-piping, passive with and without by phrase with adjunct (gap-less) relative clause with compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase. 6.5. DITRANSITIVE WITH PP SHIFT: TNX0VNX1TONX2 41 6.5 Ditransitive with PP shift: Tnx0Vnx1tonx2 Description: This tree family is selected by ditransitive verbs that undergo a shift to a to prepositional phrase. These ditransitive verbs are clearly constrained so that when they shift, the prepositional phrase must start with to. This is in contrast to the Ditransitives with PP in section 6.4, in which verbs may appear in [NP V NP PP] constructions with a variety of prepositions. Both the dative shifted and non-shifted PP complement trees are included. 56 verbs select this family. Examples: give, promise, tell Bill gave Hillary ﬂowers . Bill gave ﬂowers to Hillary . Whitman promised the voters a tax cut . Whitman promised a tax cut to the voters . Pinnochino told Gepetto a lie . Pinnochino told a lie to Gepetto . Declarative tree: See Figure 6.5. Sr NP0 VP V NP1 PP2 P2 to NP2 Sr NP0 VP V NP2 NP1 (a) (b) Figure 6.5: Declarative Ditransitive with PP shift Trees: αnx0Vnx1Pnx2 (a) and αnx0Vnx2nx1 (b) Other available trees: Non-shifted: wh-moved subject, wh-moved direct object, wh-moved indirect object, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, direct object relative clause with compwith PP pied-piping, indirect object relative clause with and without compwith PP pied-piping, imperative, NP gerund, passive with by phrase, passive without by phrase, passive with wh-moved subject and by phrase, passive with wh-moved subject and no by phrase, passive with wh-moved object out of the by phrase, passive with wh-moved by phrase, passive with wh-moved indirect object and by phrase, passive with wh-moved indirect object and no by phrase, passive with relative clause on subject and by phrase with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive with relative clause on object of the by phrase with and without compwith PP 42 CHAPTER 6. VERB CLASSES pied-piping, passive with relative clause on the indirect object and by phrase with and without compwith PP pied-piping, passive with relative clause on the indirect object and no by phrase with and without compwith PP pied-piping, passive withwithout by-phrase with adjunct (gap-less) relative clause with compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase; Shifted: wh-moved subject, wh-moved direct object, wh-moved object of PP, wh-moved PP, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, direct object relative clause with compwith PP pied-piping, object of PP relative clause with and without compwith PP pied-piping, imperative, determiner gerund, NP gerund, passive with by phrase, passive without by phrase, passive with wh-moved subject and by phrase, passive with wh-moved subject and no by phrase, passive with wh-moved object out of the by phrase, passive with wh-moved by phrase, passive with wh-moved object out of the PP and by phrase, passive with wh-moved object out of the PP and no by phrase, passive with wh-moved PP and by phrase, passive with wh-moved PP and no by phrase, passive with relative clause on subject and by phrase with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive with relative clause on object of the by phrase with and without compwith PP pied-piping, passive with relative clause on the object of the PP and by phrase with and without compwith PP pied-piping, passive with relative clause on the object of the PP and no by phrase with and without compwith PP pied-piping, passive withwithout by-phrase with adjunct (gap-less) relative clause with compwith PP pied- piping, gerund passive with by phrase, gerund passive without by phrase. 6.6 Sentential Complement with NP: Tnx0Vnx1s2 Description: This tree family is selected by verbs that take both an NP and a sentential complement. The sentential complement may be inﬁnitive or indicative. The type of clause is speciﬁed by each individual verb in its syntactic lexicon entry. A given verb may select more than one type of sentential complement. The declarative tree, and many other trees in this family, are auxiliary trees, as opposed to the more common initial trees. These auxiliary trees adjoin onto an S node in an existing tree of the type speciﬁed by the sentential complement. This is the mechanism by which TAGs are able to maintain long-distance dependencies (see Chapter 13), even over multiple embeddings (e.g. What did Bill tell Mary that John said?). 79 verbs select this tree family. Examples: beg, expect, tell Srini begged Mark to increase his disk quota . Beth told Jim that it was his turn . Declarative tree: See Figure 6.6. Other available trees: wh-moved subject, wh-moved object, wh-moved sentential comple- ment, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, object relative clause with and without comp, imperative, determiner gerund, NP gerund, passive with by phrase before sentential complement, pas- sive with by phrase after sentential complement, passive without by phrase, passive with 6.7. INTRANSITIVE VERB PARTICLE: TNX0VPL 43 Sr NP0 VP V NP1 S2 Figure 6.6: Declarative Sentential Complement with NP Tree: βnx0Vnx1s2 wh-moved subject and by phrase before sentential complement, passive with wh-moved subject and by phrase after sentential complement, passive with wh-moved subject and no by phrase, passive with wh-moved object out of the by phrase, passive with wh-moved by phrase, passive with relative clause on subject and by phrase before sentential comple- ment with and without comp, passive with relative clause on subject and by phrase after sentential complement with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive withwithout by-phrase with adjunct (gap- less) relative clause with compwith PP pied-piping, gerund passive with by phrase befor sentential complement, gerund passive with by phrase after the sentential complement, gerund passive without by phrase, parenthetical reporting clause. 6.7 Intransitive Verb Particle: Tnx0Vpl Description: The trees in this tree family are anchored by both the verb and the verb parti- cle. Both appear in the syntactic lexicon and together select this tree family. Intransitive verb particles can be diﬃcult to distinguish from intransitive verbs with adverbs adjoined on. The main diagnostics for including verbs in this class are whether the meaning is compositional or not, and whether there is a transitive version of the verbverb particle combination with the same or similar meaning. The existence of an alternate composi- tional meaning is a strong indication for a separate verb particle construction. There are 159 verbverb particle combinations. Examples: add up, come out, sign oﬀ The numbers never quite added up . John ﬁnally came out (of the closet) . I think that I will sign oﬀ now . Declarative tree: See Figure 6.7. Other available trees: wh-moved subject, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, imperative, determiner gerund, NP gerund. 44 CHAPTER 6. VERB CLASSES Sr NP0 VP V PL Figure 6.7: Declarative Intransitive Verb Particle Tree: αnx0Vpl 6.8 Transitive Verb Particle: Tnx0Vplnx1 Description: Verbverb particle combinations that take an NP complement select this tree family. Both the verb and the verb particle are anchors of the trees. Particle movement has been taken as the diagnostic to distinguish verb particle constructions from intransitives with adjoined PPs. If the alleged particle is able to undergo particle movement, in other words appear both before and after the direct object, then it is judged to be a particle. Items that do not undergo particle movement are taken to be prepositions. In many, but not all, of the verb particle cases, there is also an alternate prepositional meaning in which the lexical item did not move. (e.g. He looked up the number (in the phonebook). He looked the number up. Srini looked up the road (for Purnimas car). He looked the road up.) There are 489 verbverb particle combinations. Examples: blow oﬀ, make up, pick out He blew oﬀ his linguistics class for the third time . He blew his linguistics class oﬀ for the third time . The dyslexic leprechaun made up the syntactic lexicon . The dyslexic leprechaun made the syntactic lexicon up . I would like to pick out a new computer . I would like to pick a new computer out . Declarative tree: See Figure 6.8. Sr NP0 VP V PL NP1 Sr NP0 VP V NP1 PL (a) (b) Figure 6.8: Declarative Transitive Verb Particle Tree: αnx0Vplnx1 (a) and αnx0Vnx1pl (b) Other available trees: wh-moved subject with particle before the NP, wh-moved subject with particle after the NP, wh-moved object, subject relative clause with particle before 6.9. DITRANSITIVE VERB PARTICLE: TNX0VPLNX1NX2 45 the NP with and without comp, subject relative clause with particle after the NP with and without comp, object relative clause with and without comp, adjunct (gap-less) relative clause with particle before the NP with compwith PP pied-piping, adjunct (gap-less) relative clause with particle after the NP with compwith PP pied-piping, imperative with particle before the NP, imperative with particle after the NP, determiner gerund with particle before the NP, NP gerund with particle before the NP, NP gerund with particle after the NP, passive with by phrase, passive without by phrase, passive with wh- moved subject and by phrase, passive with wh-moved subject and no by phrase, passive with wh-moved object out of the by phrase, passive with wh-moved by phrase, passive with relative clause on subject and by phrase with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive with relative clause on object of the by phrase with and without compwith PP pied-piping, passive withwithout by-phrase with adjunct (gap-less) relative clause with compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase. 6.9 Ditransitive Verb Particle: Tnx0Vplnx1nx2 Description: Verbverb particle combinations that select this tree family take 2 NP comple- ments. Both the verb and the verb particle anchor the trees, and the verb particle can occur before, between, or after the noun phrases. Perhaps because of the complexity of the sentence, these verbs do not seem to have passive alternations (A new bank account was opened up Michelle by me). There are 4 verbverb particle combinations that select this tree family. The exhaustive list is given in the examples. Examples: dish out, open up, pay oﬀ, rustle up I opened up Michelle a new bank account . I opened Michelle up a new bank account . I opened Michelle a new bank account up . Declarative tree: See Figure 6.9. Sr NP0 VP V PL NP1 NP2 Sr NP0 VP V NP1 PL NP2 Sr NP0 VP V NP1 NP2 PL (a) (b) (c) Figure 6.9: Declarative Ditransitive Verb Particle Tree: αnx0Vplnx1nx2 (a), αnx0Vnx1plnx2 (b) and αnx0Vnx1nx2pl (c) Other available trees: wh-moved subject with particle before the NPs, wh-moved subject with particle between the NPs, wh-moved subject with particle after the NPs, wh-moved indirect object with particle before the NPs, wh-moved indirect object with particle after 46 CHAPTER 6. VERB CLASSES the NPs, wh-moved direct object with particle before the NPs, wh-moved direct object with particle between the NPs, subject relative clause with particle before the NPs with and without comp, subject relative clause with particle between the NPs with and without comp, subject relative clause with particle after the NPs with and without comp, indirect object relative clause with particle before the NPs with and without comp, indirect object relative clause with particle after the NPs with and without comp, direct object relative clause with particle before the NPs with and without comp, direct object relative clause with particle between the NPs with and without comp, adjunct (gap- less) relative clause with compwith PP pied-piping, imperative with particle before the NPs, imperative with particle between the NPs, imperative with particle after the NPs, determiner gerund with particle before the NPs, NP gerund with particle before the NPs, NP gerund with particle between the NPs, NP gerund with particle after the NPs. 6.10 Intransitive with PP: Tnx0Vpnx1 Description: The verbs that select this tree family are not strictly intransitive, in that they must be followed by a prepositional phrase. Verbs that are intransitive and simply can be followed by a prepositional phrase do not select this family, but instead have the PP adjoin onto the intransitive sentence. Accordingly, there should be no verbs in both this class and the intransitive tree family (see section 6.1). The prepositional phrase is not restricted to being headed by any particular lexical item. Note that these are not transitive verb particles (see section 6.8), since the head of the PP does not move. 169 verbs select this tree family. Examples: grab, impinge, provide Seth grabbed for the brass ring . The noise gradually impinged on Danias thoughts . A good host provides for everyones needs . Declarative tree: See Figure 6.10. Sr NP0 VP V PP1 Figure 6.10: Declarative Intransitive with PP Tree: αnx0Vpnx1 Other available trees: wh-moved subject, wh-moved object of the PP, wh-moved PP, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith 6.11. PREDICATIVE MULTI-WORD WITH VERB, PREP ANCHORS: TNX0VPNX1 47 PP pied-piping, object of the PP relative clause with and without compwith PP pied- piping, imperative, determiner gerund, NP gerund, passive with by phrase, passive without by phrase, passive with wh-moved subject and by phrase, passive with wh-moved subject and no by phrase, passive with wh-moved by phrase, passive with relative clause on subject and by phrase with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive with relative clause on object of the by phrase with and without compwith PP pied-piping, passive withwithout by-phrase with adjunct (gap-less) relative clause with compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase. 6.11 Predicative Multi-word with Verb, Prep anchors: Tnx0VPnx1 Description: This tree family is selected by multiple anchor verbpreposition pairs which together have a non-compositional interpretation. For example, think of has the non- compositional interpretion involving the inception of a notion or mental entity in addition to the interpretion in which the agent is thinking about someone or something. Anchors for this tree must be able to take both gerunds and regular NPs in the second noun position. To allow adverbs to appear between the verb and the preposition, the trees contain an extra VP level. Several of the verbs which select the Tnx0Vpnx1 family, but which should not have quite the freedom it allows, will be moving to this family for the next release. 28 verbpreposition pairs select this tree family. Examples: think of, believe in, depend on Calvin thought of a new idea . Hobbes believes in sleeping all day . Bill depends on drinking coﬀee for stimulation . Declarative tree: See Figure 6.11. S r NP 0 VP1 VP2 V PP P NP 1 Figure 6.11: Declarative PP Complement Tree: αnx0VPnx1 Other available trees: wh-moved subject, wh-moved object, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, object relative clause with and without comp, imperative, determiner gerund, NP gerund, passive 48 CHAPTER 6. VERB CLASSES with by phrase, passive without by phrase, passive with wh-moved subject and by phrase, passive with wh-moved subject and no by phrase, passive with wh-moved object out of the by phrase, passive with wh-moved by phrase, passive with relative clause on subject and by phrase with and without comp, passive with relative clause on subject and no by phrase with and without comp, passive with relative clause on object on the by phrase with and without compwith PP pied-piping, passive withwithout by-phrase with adjunct (gap-less) relative clause with compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase. In addition, two other trees that allow transitive verbs to function as adjectives (e.g. the thought of idea) are also in the family. 6.12 Sentential Complement: Tnx0Vs1 Description: This tree family is selected by verbs that take just a sentential complement. The sentential complement may be of type inﬁnitive, indicative, or small clause (see Chapter 9). The type of clause is speciﬁed by each individual verb in its syntactic lexicon entry, and a given verb may select more than one type of sentential complement. The declarative tree, and many other trees in this family, are auxiliary trees, as opposed to the more common initial trees. These auxiliary trees adjoin onto an S node in an existing tree of the type speciﬁed by the sentential complement. This is the mechanism by which TAGs are able to maintain long-distance dependencies (see Chapter 13), even over multiple embeddings (e.g. What did Bill think that John said?). 338 verbs select this tree family. Examples: consider, think Dania considered the algorithm unworkable . Srini thought that the program was working . Declarative tree: See Figure 6.12. Sr NP0 VP V S1 Figure 6.12: Declarative Sentential Complement Tree: βnx0Vs1 Other available trees: wh-moved subject, wh-moved sentential complement, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied- piping, imperative, determiner gerund, NP gerund, parenthetical reporting clause. 6.13. INTRANSITIVE WITH ADJECTIVE: TNX0VAX1 49 6.13 Intransitive with Adjective: Tnx0Vax1 Description: The verbs that select this tree family take an adjective as a complement. The adjective may be regular, comparative, or superlative. It may also be formed from the special class of adjectives derived from the transitive verbs (e.g. agitated, broken). See section 6.2). Unlike the Intransitive with PP verbs (see section 6.10), some of these verbs may also occur as bare intransitives as well. This distinction is drawn because adjectives do not normally adjoin onto sentences, as prepositional phrases do. Other intransitive verbs can only occur with the adjective, and these select only this family. The verb class is also distinguished from the adjective small clauses (see section 6.20) because these verbs are not raising verbs. 34 verbs select this tree family. Examples: become, grow, smell The greenhouse became hotter . The plants grew tall and strong . The ﬂowers smelled wonderful . Declarative tree: See Figure 6.13. Sr NP0  VP V AP1  Figure 6.13: Declarative Intransitive with Adjective Tree: αnx0Vax1 Other available trees: wh-moved subject, wh-moved adjective (how), subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, imperative, NP gerund. 6.14 Transitive Sentential Subject: Ts0Vnx1 Description: The verbs that select this tree family all take sentential subjects, and are often referred to as psych verbs, since they all refer to some psychological state of mind. The sentential subject can be indicative (complementizer required) or inﬁnitive (complemen- tizer optional). 100 verbs that select this tree family. Examples: delight, impress, surprise that the tea had rosehips in it delighted Christy . to even attempt a marathon impressed Dania . For Jim to have walked the dogs surprised Beth . Declarative tree: See Figure 6.14. 50 CHAPTER 6. VERB CLASSES Sr S0 VP V NP1 Figure 6.14: Declarative Sentential Subject Tree: αs0Vnx1 Other available trees: wh-moved subject, wh-moved object, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.15 Light Verbs: Tnx0lVN1 Description: The verbnoun pairs that select this tree families are pairs in which the interpre- tation is non-compositional and the noun contributes argument structure to the predicate (e.g. The man took a walk. vs. The man took a radio). The verb and the noun occur together in the syntactic database, and both anchor the trees. The verbs in the light verb constructions are do, give, have, make and take. The noun following the light verb is (usu- ally) in a bare inﬁnitive form (have a good cry) and usually occurs with a(n). However, we include deverbal nominals (take a bath, give a demonstration) as well. Constructions with nouns that do not contribute an argument structure (have a cigarette, give NP a black eye) are excluded. In addition to semantic considerations of light verbs, they diﬀer syntactically from Transitive verbs (section 6.2) as well in that the noun in the light verb construction does not extract. Some of the verb-noun anchors for this family, like take aim and take hold disallow determiners, while others require particular determiners. For example, have think must be indeﬁnite and singular, as attested by the ungrammaticality of John had the thinksome thinks. Another anchor, take leave can occur either bare or with a possesive pronoun (e.g., John took his leave, but not John took the leave). This is accomplished through feature speciﬁcation on the lexical entries. There are 259 verbnoun pairs that select the light verb tree. Examples: give groan, have discussion, make comment The audience gave a collective groan . We had a big discussion about closing the libraries . The professors made comments on the paper . Declarative tree: See Figure 6.15. Other available trees: wh-moved subject, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, imperative, determiner gerund, NP gerund. 6.16. DITRANSITIVE LIGHT VERBS WITH PP SHIFT: TNX0LVN1PNX2 51 Sr NP0 VP V NP1 N1 Figure 6.15: Declarative Light Verb Tree: αnx0lVN1 6.16 Ditransitive Light Verbs with PP Shift: Tnx0lVN1Pnx2 Description: The verbnoun pairs that select this tree family are pairs in which the interpre- tation is non-compositional and the noun contributes argument structure to the predicate (e.g. Dania made Srini a cake. vs. Dania made Srini a loan). The verb and the noun occur together in the syntactic database, and both anchor the trees. The verbs in these light verb constructions are give and make. The noun following the light verb is (usually) a bare inﬁnitive form (e.g. make a promise to Anoop). However, we include deverbal nominals (e.g. make a payment to Anoop) as well. Constructions with nouns that do not contribute an argument structure are excluded. In addition to semantic considerations of light verbs, they diﬀer syntactically from the Ditransitive with PP Shift verbs (see section 6.5) as well in that the noun in the light verb construction does not extract. Also, passivization is severely restricted. Special determiner requirments and restrictions are handled in the same manner as for the Tnx0lVN1 family. There are 18 verbnoun pairs that select this family. Examples: give look, give wave, make promise Dania gave Carl a murderous look . Amanda gave us a little wave as she left . Dania made Doug a promise . Declarative tree: See Figure 6.16. Other available trees: Non-shifted: wh-moved subject, wh-moved indirect object, subject relative clause with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, indirect object relative clause with and without compwith PP pied- piping, imperative, NP gerund, passive with by phrase, passive with by-phrase with ad- junct (gap-less) relative clause with compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase Shifted: wh-moved subject, wh-moved object of PP, wh-moved PP, subject relative clause with and without comp, object of PP relative clause with and without compwith PP pied-piping, imperative, determiner gerund, NP gerund, passive with by phrase with adjunct (gap-less) relative clause with compwith PP pied-piping, gerund passive with by phrase, gerund passive without by phrase. 52 CHAPTER 6. VERB CLASSES Sr NP0 VP V NP1 N1 PP2 P2 to NP2 Sr NP0 VP V NP2 NP1 N1 (a) (b) Figure 6.16: Declarative Light Verbs with PP Tree: αnx0lVN1Pnx2 (a), αnx0lVnx2N1 (b) 6.17 NP It-Cleft: TItVnx1s2 Description: This tree family is selected by be as the main verb and it as the subject. Together these two items serve as a multi-component anchor for the tree family. This tree family is used for it-clefts in which the clefted element is an NP and there are no gaps in the clause which follows the NP. The NP is interpreted as an adjunct of the following clause. See Chapter 11 for additional discussion. Examples: it be it was yesterday that we had the meeting . Declarative tree: See Figure 6.17. Sr NP0 N VP V VP1 V1 ε PP1 P1 NP1 S2 Figure 6.17: Declarative NP It-Cleft Tree: αItVpnx1s2 Other available trees: inverted question, wh-moved object with be inverted, wh-moved ob- ject with be not inverted, adjunct (gap-less) relative clause with compwith PP pied- piping. 6.18. PP IT-CLEFT: TITVPNX1S2 53 6.18 PP It-Cleft: TItVpnx1s2 Description: This tree family is selected by be as the main verb and it as the subject. Together these two items serve as a multi-component anchor for the tree family. This tree family is used for it-clefts in which the clefted element is a PP and there are no gaps in the clause which follows the PP. The PP is interpreted as an adjunct of the following clause. See Chapter 11 for additional discussion. Examples: it be it was at Kent State that the police shot all those students . Declarative tree: See Figure 6.18. Sr NP0 N VP V VP1 V1 ε PP1 P1 NP1 S2 Figure 6.18: Declarative PP It-Cleft Tree: αItVnx1s2 Other available trees: inverted question, wh-moved prepositional phrase with be inverted, wh-moved prepositional phrase with be not inverted, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.19 Adverb It-Cleft: TItVad1s2 Description: This tree family is selected by be as the main verb and it as the subject. Together these two items serve as a multi-component anchor for the tree family. This tree family is used for it-clefts in which the clefted element is an adverb and there are no gaps in the clause which follows the adverb. The adverb is interpreted as an adjunct of the following clause. See Chapter 11 for additional discussion. Examples: it be it was reluctantly that Dania agreed to do the tech report . Declarative tree: See Figure 6.19. Other available trees: inverted question, wh-moved adverb how with be inverted, wh-moved adverb how with be not inverted, adjunct (gap-less) relative clause with compwith PP pied-piping. 54 CHAPTER 6. VERB CLASSES Sr NP0 N VP V VP1 V1 ε Ad1 S2 Figure 6.19: Declarative Adverb It-Cleft Tree: αItVad1s2 6.20 Adjective Small Clause Tree: Tnx0Ax1 Description: These trees are not anchored by verbs, but by adjectives. They are explained in much greater detail in the section on small clauses (see section 9.3). This section is presented here for completeness. 3244 adjectives select this tree family. Examples: addictive, dangerous, wary cigarettes are addictive . smoking cigarettes is dangerous . John seems wary of the Surgeon Generals warnings . Declarative tree: See Figure 6.20. Sr NP0 VP V NA ε AP1 A Figure 6.20: Declarative Adjective Small Clause Tree: αnx0Ax1 Other available trees: wh-moved subject, wh-moved adjective how, relative clause on subject with and without comp, imperative, NP gerund, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.21 Adjective Small Clause with Sentential Complement: Tnx0A1s1 Description: This tree family is selected by adjectives that take sentential complements. The sentential complements can be indicative or inﬁnitive. Note that these trees are anchored 6.22. ADJECTIVE SMALL CLAUSE WITH SENTENTIAL SUBJECT: TS0AX1 55 by adjectives, not verbs. Small clauses are explained in much greater detail in section 9.3. This section is presented here for completeness. 669 adjectives select this tree family. Examples: able, curious, disappointed Christy was able to ﬁnd the problem . Christy was curious whether the new analysis was working . Christy was sad that the old analysis failed . Declarative tree: See Figure 6.21. Sr NP0 VP V NA ε AP1 A S1 Figure 6.21: Declarative Adjective Small Clause with Sentential Complement Tree: αnx0A1s1 Other available trees: wh-moved subject, wh-moved adjective how, relative clause on subject with and without comp, imperative, NP gerund, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.22 Adjective Small Clause with Sentential Subject: Ts0Ax1 Description: This tree family is selected by adjectives that take sentential subjects. The sentential subjects can be indicative or inﬁnitive. Note that these trees are anchored by adjectives, not verbs. Most adjectives that take the Adjective Small Clause tree family (see section 6.20) take this family as well.6 Small clauses are explained in much greater detail in section 9.3. This section is presented here for completeness. 3,185 adjectives select this tree family. Examples: decadent, incredible, uncertain to eat raspberry chocolate truﬄe ice cream is decadent . that Carl could eat a large bowl of it is incredible . whether he will actually survive the experience is uncertain . Declarative tree: See Figure 6.22. Other available trees: wh-moved subject, wh-moved adjective, adjunct (gap-less) relative clause with compwith PP pied-piping. 6No great attempt has been made to go through and decide which adjectives should actually take this family and which should not. 56 CHAPTER 6. VERB CLASSES Sr S0 VP V NA ε AP1 A Figure 6.22: Declarative Adjective Small Clause with Sentential Subject Tree: αs0Ax1 6.23 Equative BE: Tnx0BEnx1 Description: This tree family is selected only by the verb be. It is distinguished from the predicative NPs (see section 6.24) in that two NPs are equated, and hence interchange- able (see Chapter 9 for more discussion on the English copula and predicative sentences). The XTAG analysis for equative be is explained in greater detail in section 9.4. Examples: be That man is my uncle. Declarative tree: See Figure 6.23. Sr NP0 VPr V VP1 V1 ε1 NP1 Figure 6.23: Declarative Equative BE Tree: αnx0BEnx1 Other available trees: inverted-question. 6.24 NP Small Clause: Tnx0N1 Description: The trees in this tree family are not anchored by verbs, but by nouns. Small clauses are explained in much greater detail in section 9.3. This section is presented here for completeness. 5,595 nouns select this tree family. 6.25. NP SMALL CLAUSE WITH SENTENTIAL COMPLEMENT: TNX0N1S1 57 Examples: author, chair, dish Dania is an author . that blue, warped-looking thing is a chair . those broken pieces were dishes . Declarative tree: See Figure 6.24. Sr NP0 VP V ε NP1 N Figure 6.24: Declarative NP Small Clause Trees: αnx0N1 Other available trees: wh-moved subject, wh-moved object, relative clause on subject with and without comp, imperative, NP gerund, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.25 NP Small Clause with Sentential Complement: Tnx0N1s1 Description: This tree family is selected by the small group of nouns that take sentential com- plements by themselves (see section 8.8). The sentential complements can be indicative or inﬁnitive, depending on the noun. Small clauses in general are explained in much greater detail in the section 9.3. This section is presented here for completeness. 141 nouns select this family. Examples: admission, claim, vow The aﬃdavits are admissions that they killed the sheep . there is always the claim that they were insane . this is his vow to ﬁght the charges . Declarative tree: See Figure 6.25. Other available trees: wh-moved subject, wh-moved object, relative clause on subject with and without comp, imperative, NP gerund, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.26 NP Small Clause with Sentential Subject: Ts0N1 Description: This tree family is selected by nouns that take sentential subjects. The sentential subjects can be indicative or inﬁnitive. Note that these trees are anchored by nouns, not 58 CHAPTER 6. VERB CLASSES Sr NP0 VP V NA ε NP1 N S1 Figure 6.25: Declarative NP with Sentential Complement Small Clause Tree: αnx0N1s1 verbs. Most nouns that take the NP Small Clause tree family (see section 6.24) take this family as well.7 Small clauses are explained in much greater detail in section 9.3. This section is presented here for completeness. 5,519 nouns select this tree family. Examples: dilemma, insanity, tragedy whether to keep the job he hates is a dilemma . to invest all of your money in worms is insanity . that the worms died is a tragedy . Declarative tree: See Figure 6.26. Sr S0 VP V NA ε NP1 N Figure 6.26: Declarative NP Small Clause with Sentential Subject Tree: αs0N1 Other available trees: wh-moved subject, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.27 PP Small Clause: Tnx0Pnx1 Description: This family is selected by prepositions that can occur in small clause construc- tions. For more information on small clause constructions, see section 9.3. This section is presented here for completeness. 39 prepositions select this tree family. 7No great attempt has been made to go through and decide which nouns should actually take this family and which should not. 6.28. EXHAUSTIVE PP SMALL CLAUSE: TNX0PX1 59 Examples: around, in, underneath Chris is around the corner . Trisha is in big trouble . The dog is underneath the table . Declarative tree: See Figure 6.27. Sr NP0 VP V NA ε PP1 P NP1 Figure 6.27: Declarative PP Small Clause Tree: αnx0Pnx1 Other available trees: wh-moved subject, wh-moved object of PP, relative clause on subject with and without comp, relative clause on object of PP with and without compwith PP pied-piping, imperative, NP gerund, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.28 Exhaustive PP Small Clause: Tnx0Px1 Description: This family is selected by exhaustive prepositions that can occur in small clauses. Exhaustive prepositions are prepositions that function as prepositional phrases by themselves. For more information on small clause constructions, please see section 9.3. The section is included here for completeness. 33 exhaustive prepositions select this tree family. Examples: abroad, below, outside Dr. Joshi is abroad . The workers are all below . Clove is outside . Declarative tree: See Figure 6.28. Other available trees: wh-moved subject, wh-moved PP, relative clause on subject with and without comp, imperative, NP gerund, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.29 PP Small Clause with Sentential Subject: Ts0Pnx1 Description: This tree family is selected by prepositions that take sentential subjects. The sentential subject can be indicative or inﬁnitive. Small clauses are explained in much 60 CHAPTER 6. VERB CLASSES Sr NP0 VP V NA ε PP1 P Figure 6.28: Declarative Exhaustive PP Small Clause Tree: αnx0Px1 greater detail in section 9.3. This section is presented here for completeness. 39 preposi- tions select this tree family. Examples: beyond, unlike that Ken could forget to pay the taxes is beyond belief . to explain how this happened is outside the scope of this discussion . for Ken to do something right is unlike him . Declarative tree: See Figure 6.29. Sr S0 VP V NA ε PP1 P NP1 Figure 6.29: Declarative PP Small Clause with Sentential Subject Tree: αs0Pnx1 Other available trees: wh-moved subject, relative clause on object of the PP with and with- out compwith PP pied-piping, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.30 Intransitive Sentential Subject: Ts0V Description: Only the verb matter selects this tree family. The sentential subject can be indicative (complementizer required) or inﬁnitive (complementizer optional). Examples: matter to arrive on time matters considerably . that Joshi attends the meetings matters to everyone . Declarative tree: See Figure 6.30. 6.31. SENTENTIAL SUBJECT WITH TO COMPLEMENT: TS0VTONX1 61 Sr S0 VP V Figure 6.30: Declarative Intransitive Sentential Subject Tree: αs0V Other available trees: wh-moved subject, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.31 Sentential Subject with to complement: Ts0Vtonx1 Description: The verbs that select this tree family are fall, occur and leak. The sentential sub- ject can be indicative (complementizer required) or inﬁnitive (complementizer optional). Examples: fall, occur, leak to wash the car fell to the children . that he should leave occurred to the party crasher . whether the princess divorced the prince leaked to the press . Declarative tree: See Figure 6.31. Sr S0 VP V PP1 P1 to NP1 Figure 6.31: Sentential Subject Tree with to complement: αs0Vtonx1 Other available trees: wh-moved subject, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.32 PP Small Clause, with Adv and Prep anchors: Tnx0ARBPnx1 Description: This family is selected by multi-word prepositions that can occur in small clause constructions. In particular, this family is selected by two-word prepositions, where the 62 CHAPTER 6. VERB CLASSES ﬁrst word is an adverb, the second word a preposition. Both components of the multi- word preposition are anchors. For more information on small clause constructions, see section 9.3. 8 multi-word prepositions select this tree family. Examples: ahead of, close to The little girl is ahead of everyone else in the race . The project is close to completion . Declarative tree: See Figure 6.32. Sr NP0 VP V ε PP1 P1 Ad P NP1 Figure 6.32: Declarative PP Small Clause tree with two-word preposition, where the ﬁrst word is an adverb, and the second word is a preposition: αnx0ARBPnx1 Other available trees: wh-moved subject, wh-moved object of PP, relative clause on subject with and without comp, relative clause on object of PP with and without comp, adjunct (gap-less) relative clause with compwith PP pied-piping, imperative, NP Gerund. 6.33 PP Small Clause, with Adj and Prep anchors: Tnx0APnx1 Description: This family is selected by multi-word prepositions that can occur in small clause constructions. In particular, this family is selected by two-word prepositions, where the ﬁrst word is an adjective, the second word a preposition. Both components of the multi- word preposition are anchors. For more information on small clause constructions, see section 9.3. 8 multi-word prepositions select this tree family. Examples: according to, void of The operation we performed was according to standard procedure . He is void of all feeling . Declarative tree: See Figure 6.33. Other available trees: wh-moved subject, relative clause on subject with and without comp, relative clause on object of PP with and without comp, wh-moved object of PP, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.34. PP SMALL CLAUSE, WITH NOUN AND PREP ANCHORS: TNX0NPNX1 63 Sr NP0 VP V NA ε PP1 P1 A P NP1 Figure 6.33: Declarative PP Small Clause tree with two-word preposition, where the ﬁrst word is an adjective, and the second word is a preposition: αnx0APnx1 6.34 PP Small Clause, with Noun and Prep anchors: Tnx0NPnx1 Description: This family is selected by multi-word prepositions that can occur in small clause constructions. In particular, this family is selected by two-word prepositions, where the ﬁrst word is a noun, the second word a preposition. Both components of the multi- word preposition are anchors. For more information on small clause constructions, see section 9.3. 1 multi-word preposition selects this tree family. Examples: thanks to The fact that we are here tonight is thanks to the valiant eﬀorts of our staﬀ . Declarative tree: See Figure 6.34. Sr NP0 VP V NA ε PP1 P1 N P NP1 Figure 6.34: Declarative PP Small Clause tree with two-word preposition, where the ﬁrst word is a noun, and the second word is a preposition: αnx0NPnx1 Other available trees: wh-moved subject, wh-moved object of PP, relative clause on subject with and without comp, relative clause on object with comp, adjunct (gap-less) relative 64 CHAPTER 6. VERB CLASSES clause with compwith PP pied-piping. 6.35 PP Small Clause, with Prep anchors: Tnx0PPnx1 Description: This family is selected by multi-word prepositions that can occur in small clause constructions. In particular, this family is selected by two-word prepositions, where both words are prepositions. Both components of the multi-word preposition are anchors. For more information on small clause constructions, see section 9.3. 9 multi-word prepositions select this tree family. Examples: on to, inside of that detective is on to you . The red box is inside of the blue box . Declarative tree: See Figure 6.35. Sr NP0 VP V NA ε PP1 P P1 P2 NP1 Figure 6.35: Declarative PP Small Clause tree with two-word preposition, where both words are prepositions: αnx0PPnx1 Other available trees: wh-moved subject, wh-moved object of PP, relative clause on subject with and without comp, relative clause on object of PP with and without compwith PP pied-piping, imperative, wh-moved object of PP, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.36 PP Small Clause, with Prep and Noun anchors: Tnx0PNaPnx1 Description: This family is selected by multi-word prepositions that can occur in small clause constructions. In particular, this family is selected by three-word prepositions. The ﬁrst and third words are always prepositions, and the middle word is a noun. The noun is marked for null adjunction since it cannot be modiﬁed by noun modiﬁers. All three components of the multi-word preposition are anchors. For more information on small clause constructions, see section 9.3. 24 multi-word preposition select this tree family. 6.37. PP SMALL CLAUSE WITH SENTENTIAL SUBJECT, AND ADV AND PREP ANCHORS: TS0ARBP Examples: in back of, in line with, on top of The red plaid box should be in back of the plain black box . The evidence is in line with my newly concocted theory . She is on top of the world . She is on direct top of the world . Declarative tree: See Figure 6.36. Sr NP0 VP V NA ε PP1 P P1 N NA P2 NP1 Figure 6.36: Declarative PP Small Clause tree with three-word preposition, where the middle noun is marked for null adjunction: αnx0PNaPnx1 Other available trees: wh-moved subject, wh-moved object of PP, relative clause on subject with and without comp, relative clause on object of PP with and without compwith PP pied-piping, adjunct (gap-less) relative clause with compwith PP pied-piping, imperative, NP Gerund. 6.37 PP Small Clause with Sentential Subject, and Adv and Prep anchors: Ts0ARBPnx1 Description: This tree family is selected by multi-word prepositions that take sentential sub- jects. In particular, this family is selected by two-word prepositions, where the ﬁrst word is an adverb, the second word a preposition. Both components of the multi-word preposi- tion are anchors. The sentential subject can be indicative or inﬁnitive. Small clauses are explained in much greater detail in section 9.3. 2 prepositions select this tree family. Examples: due to, contrary to that David slept until noon is due to the fact that he never sleeps during the week . that Michaels joke was funny is contrary to the usual status of his comic attempts . Declarative tree: See Figure 6.37. 66 CHAPTER 6. VERB CLASSES Sr S0 VP V NA ε PP1 P1 Ad P NP1 Figure 6.37: Declarative PP Small Clause with Sentential Subject Tree, with two-word prepo- sition, where the ﬁrst word is an adverb, and the second word is a preposition: αs0ARBPnx1 Other available trees: wh-moved subject, relative clause on object of the PP with and with- out comp, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.38 PP Small Clause with Sentential Subject, and Adj and Prep anchors: Ts0APnx1 Description: This tree family is selected by multi-word prepositions that take sentential sub- jects. In particular, this family is selected by two-word prepositions, where the ﬁrst word is an adjective, the second word a preposition. Both components of the multi-word prepo- sition are anchors. The sentential subject can be indicative or inﬁnitive. Small clauses are explained in much greater detail in section 9.3. 5 prepositions select this tree family. Examples: devoid of, according to that he could walk out on her is devoid of all reason . that the conversation erupted precisely at that moment was according to my theory . Declarative tree: See Figure 6.38. Other available trees: wh-moved subject, relative clause on object of the PP with and with- out comp, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.39 PP Small Clause with Sentential Subject, and Noun and Prep anchors: Ts0NPnx1 Description: This tree family is selected by multi-word prepositions that take sentential sub- jects. In particular, this family is selected by two-word prepositions, where the ﬁrst word is a noun, the second word a preposition. Both components of the multi-word preposi- 6.39. PP SMALL CLAUSE WITH SENTENTIAL SUBJECT, AND NOUN AND PREP ANCHORS: TS0NPN Sr S0 VP V NA ε PP1 P1 A P NP1 Figure 6.38: Declarative PP Small Clause with Sentential Subject Tree, with two-word prepo- sition, where the ﬁrst word is an adjective, and the second word is a preposition: αs0APnx1 tion are anchors. The sentential subject can be indicative or inﬁnitive. Small clauses are explained in much greater detail in section 9.3. 1 preposition selects this tree family. Examples: thanks to that she is worn out is thanks to a long day in front of the computer terminal . Declarative tree: See Figure 6.39. Sr S0 VP V NA ε PP1 P1 N P NP1 Figure 6.39: Declarative PP Small Clause with Sentential Subject Tree, with two-word prepo- sition, where the ﬁrst word is a noun, and the second word is a preposition: αs0NPnx1 Other available trees: wh-moved subject, relative clause on object of the PP with and with- out comp, adjunct (gap-less) relative clause with compwith PP pied-piping. 68 CHAPTER 6. VERB CLASSES 6.40 PP Small Clause with Sentential Subject, and Prep an- chors: Ts0PPnx1 Description: This tree family is selected by multi-word prepositions that take sentential sub- jects. In particular, this family is selected by two-word prepositions, where both words are prepositions. Both components of the multi-word preposition are anchors. The sentential subject can be indicative or inﬁnitive. Small clauses are explained in much greater detail in section 9.3. 3 prepositions select this tree family. Examples: outside of that Mary did not complete the task on time is outside of the scope of this discussion . Declarative tree: See Figure 6.40. Sr S0 VP V NA ε PP1 P P1 P2 NP1 Figure 6.40: Declarative PP Small Clause with Sentential Subject Tree, with two-word prepo- sition, where both words are prepositions: αs0PPnx1 Other available trees: wh-moved subject, relative clause on object of the PP with and with- out comp, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.41 PP Small Clause with Sentential Subject, and Prep and Noun anchors: Ts0PNaPnx1 Description: This tree family is selected by multi-word prepositions that take sentential sub- jects. In particular, this family is selected by three-word prepositions. The ﬁrst and third words are always prepositions, and the middle word is a noun. The noun is marked for null adjunction since it cannot be modiﬁed by noun modiﬁers. All three components of the multi-word preposition are anchors. Small clauses are explained in much greater detail in section 9.3. 9 prepositions select this tree family. Examples: on account of, in support of that Joe had to leave the beach was on account of the hurricane . 6.42. PREDICATIVE ADJECTIVE WITH SENTENTIAL SUBJECT AND COMPLEMENT: TS0A1S169 that Maria could not come is in support of my theory about her . that Maria could not come is in directstrictdesparate support of my theory about her . Declarative tree: See Figure 6.41. Sr S0 VP V NA ε PP1 P P1 N NA P2 NP1 Figure 6.41: Declarative PP Small Clause with Sentential Subject Tree, with three-word prepo- sition, where the middle noun is marked for null adjunction: αs0PNaPnx1 Other available trees: wh-moved subject, relative clause on object of the PP with and with- out comp, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.42 Predicative Adjective with Sentential Subject and Com- plement: Ts0A1s1 Description: This tree family is selected by predicative adjectives that take sentential subjects and a sentential complement. This tree family is selected by likely and certain. Examples: likely, certain that Max continues to drive a Jaguar is certain to make Bill jealous . for the Jaguar to be towed seems likely to make Max very angry . Declarative tree: See Figure 6.42. Other available trees: wh-moved subject, adjunct (gap-less) relative clause with compwith PP pied-piping. 6.43 Locative Small Clause with Ad anchor: Tnx0nx1ARB Description: These trees are not anchored by verbs, but by adverbs that are part of locative adverbial phrases. Locatives are explained in much greater detail in the section on the locative modiﬁer trees (see section 19.6). The only remarkable aspect of this tree family is 70 CHAPTER 6. VERB CLASSES Sr S0 VP V NA ε AP1 A S1 Figure 6.42: Predicative Adjective with Sentential Subject and Complement: αs0A1s1 the wh-moved locative tree, αW1nx0nx1ARB, shown in Figure 6.44. This is the only tree family with this type of transformation, in which the entire adverbial phrase is wh-moved but not all elements are replaced by wh items (as in how many city blocks away is the record store?). Locatives that consist of just the locative adverb or the locative adverb and a degree adverb (see Section 19.6 for details) are treated as exhaustive PPs and therefore select that tree family (Section 6.28) when used predicatively. For an extensive description of small clauses, see Section 9.3. 26 adverbs select this tree family. Examples: ahead, oﬀshore, behind the crash is three blocks ahead the naval battle was many kilometers oﬀshore how many blocks behind was Max? Declarative tree: See Figure 6.43. Sr NP0 VP V NA ε AdvP NP1 Ad Figure 6.43: Declarative Locative Adverbial Small Clause Tree: αnx0nx1ARB Other available trees: wh-moved subject, relative clause on subject with and without comp, wh-moved locative, imperative, NP gerund. 6.44. EXCEPTIONAL CASE MARKING: TXNX0VS1 71 S q AdvP NP  Ad S r NP 0 VP V ε0 AdvP 1 NA ε Figure 6.44: Wh-moved Locative Small Clause Tree: αW1nx0nx1ARB 6.44 Exceptional Case Marking: TXnx0Vs1 Description: This tree family is selected by verbs that are classiﬁed as exceptional case mark- ing, meaning that the verb asssigns accusative case to the subject of the sentential com- plement. This is in contrast to verbs in the Tnx0Vnx1s2 family (section 6.6), which assign accusative case to a NP which is not part of the sentential complement. ECM verbs take sentential complements which are either an inﬁnitive or a bare inﬁnitive. As with the Tnx0Vs1 family (section 6.12), the declarative and other trees in the Xnx0Vs1 family are auxiliary trees, as opposed to the more common initial trees. These auxiliary trees adjoin onto an S node in an existing tree of the type speciﬁed by the sentential complement. This is the mechanism by which TAGs are able to maintain long-distance dependencies (see Chapter 13), even over multiple embeddings (e.g. Who did Bill expect to eat beans?) or who did Bill expect Mary to like? See section 8.6.1 for details on this family. 20 verbs select this tree family. Examples: expect, see Van expects Bob to talk . Bob sees the harmonica fall . Declarative tree: See Figure 6.45. Sr NP0 VP V S1 Figure 6.45: ECM Tree: βXnx0Vs1 72 CHAPTER 6. VERB CLASSES Other available trees: wh-moved subject, subject relative clause with and without comp, adjunct (gap-less) relative clause with and without compwith PP pied-piping, imperative, NP gerund. 6.45 Idiom with V, D, and N anchors: Tnx0VDN1 Description: This tree family is selected by idiomatic phrases in which the verb, determiner, and NP are all frozen (as in He kicked the bucket.). Only a limited number of transfor- mations are allowed, as compared to the normal transitive tree family (see section 6.2). Other idioms that have the same structure as kick the bucket, and that are limited to the same transformations would select this tree, while diﬀerent tree families are used to handle other idioms. Note that John kicked the bucket is actually ambiguous, and would result in two parses - an idiomatic one (meaning that John died), and a compositional transitive one (meaning that there is an physical bucket that John hit with his foot). 1 idiom selects this family. Examples: kick the bucket Nixon kicked the bucket . Declarative tree: See Figure 6.46. Sr NP0 VP V NP1 DetP1 D1 N1 Figure 6.46: Declarative Transitive Idiom Tree: αnx0VDN1 Other available trees: subject relative clause with and without comp, declarative, wh-moved subject, imperative, NP gerund, adjunct gapless relative with compwith PP pied-piping, passive, wwo by-phrase, wh-moved object of by-phrase, wh-moved by-phrase, relative (with and without comp) on subject of passive, PP relative. 6.46 Idiom with V, D, A, and N anchors: Tnx0VDAN1 Description: This tree family is selected by transitive idioms that are anchored by a verb, determiner, adjective, and noun. 19 idioms select this family. 6.47. IDIOM WITH V AND N ANCHORS: TNX0VN1 73 Examples: have a green thumb, sing a diﬀerent tune Martha might have a green thumb, but its uncertain after the death of all the plants. After his conversion John sang a diﬀerent tune. Declarative tree: See Figure 6.47. Sr NP0  VP V NP1 D1  A N1  Figure 6.47: Declarative Idiom with V, D, A, and N Anchors Tree: αnx0VDAN1 Other available trees: Subject relative clause with and without comp, adjunct relative clause with compwith PP pied-piping, wh-moved subject, imperative, NP gerund, passive with- out by phrase, passive with by phrase, passive with wh-moved object of by phrase, passive with wh-moved by phrase, passive with relative on object of by phrase with and without comp. 6.47 Idiom with V and N anchors: Tnx0VN1 Description: This tree family is selected by transitive idioms that are anchored by a verb and noun. 15 idioms select this family. Examples: draw blood, cry wolf Grahams retort drew blood. The neglected boy cried wolf. Declarative tree: See Figure 6.48. Other available trees: Subject relative clause with and without comp, adjunct relative clause with compwith PP pied-piping, wh-moved subject, imperative, NP gerund, passive with- out by phrase, passive with by phrase, passive with wh-moved object of by phrase, passive with wh-moved by phrase, passive with relative on object of by phrase with and without comp. 74 CHAPTER 6. VERB CLASSES Sr NP0  VP V NP1 N1  Figure 6.48: Declarative Idiom with V and N Anchors Tree: αnx0VN1 6.48 Idiom with V, A, and N anchors: Tnx0VAN1 Description: This tree family is selected by transitive idioms that are anchored by a verb, adjective, and noun. 4 idioms select this family. Examples: break new ground, cry bloody murder The avant-garde ﬁlm breaks new ground. The investors cried bloody murder after the suspicious takeover. Declarative tree: See Figure 6.49. Sr NP0  VP V NP1 A N1  Figure 6.49: Declarative Idiom with V, A, and N Anchors Tree: αnx0VAN1 Other available trees: Subject relative clause with and without comp, adjunct relative clause with compwith PP pied-piping, wh-moved subject, imperative, NP gerund, passive with- out by phrase, passive with by phrase, passive with wh-moved object of by phrase, passive with wh-moved by phrase, passive with relative on object of by phrase with and without comp. 6.49. IDIOM WITH V, D, A, N, AND PREP ANCHORS: TNX0VDAN1PNX2 75 6.49 Idiom with V, D, A, N, and Prep anchors: Tnx0VDAN1Pnx2 Description: This tree family is selected by transitive idioms that are anchored by a verb, determiner, adjective, noun, and preposition. 6 idioms select this family. Examples: make a big deal about, make a great show of John made a big deal about a miniscule dent in his car. The company made a big show of paying generous dividends. Declarative tree: See Figure 6.50. Sr NP0  VP V NP1 D1  A N1  PP2 P2  NP2  Figure 6.50: Declarative Idiom with V, D, A, N, and Prep Anchors Tree: αnx0VDAN1Pnx2 Other available trees: Subject relative clause with and without comp, adjunct relative clause with compwith PP pied-piping, wh-moved subject, imperative, NP gerund, passive with- out by phrase, passive with by phrase, passive with wh-moved object of by phrase, passive with wh-moved by phrase, outer passive without by phrase, outer passive with by phrase, outer passive with wh- moved by phrase, outer passive with wh-moved object of by phrase, outer passive without by phrase with relative on the subject with and without comp, outer passive with by phrase with relative on subject with and without comp. 6.50 Idiom with V, A, N, and Prep anchors: Tnx0VAN1Pnx2 Description: This tree family is selected by transitive idioms that are anchored by a verb, adjective, noun, and preposition. 3 idioms select this family. Examples: make short work of John made short work of the glazed ham. Declarative tree: See Figure 6.51. 76 CHAPTER 6. VERB CLASSES Sr NP0  VP V NP1 A N1  PP2 P2  NP2  Figure 6.51: Declarative Idiom with V, A, N, and Prep Anchors Tree: αnx0VAN1Pnx2 Other available trees: Subject relative clause with and without comp, adjunct relative clause with compwith PP pied-piping, wh-moved subject, imperative, NP gerund, passive with- out by phrase, passive with by phrase, passive with wh-moved object of by phrase, passive with wh-moved by phrase, outer passive without by phrase, outer passive with by phrase, outer passive with wh- moved by phrase, outer passive with wh-moved object of by phrase, outer passive without by phrase with relative on the subject with and without comp, outer passive with by phrase with relative on subject with and without comp. 6.51 Idiom with V, N, and Prep anchors: Tnx0VN1Pnx2 Description: This tree family is selected by transitive idioms that are anchored by a verb, noun, and preposition. 6 idioms select this family. Examples: look daggers at, keep track of Maria looked daggers at her ex-husband across the courtroom. The company kept track of its inventory. Declarative tree: See Figure 6.52. Other available trees: Subject relative clause with and without comp, adjunct relative clause with compwith PP pied-piping, wh-moved subject, imperative, NP gerund, passive with- out by phrase, passive with by phrase, passive with wh-moved object of by phrase, passive with wh-moved by phrase, outer passive without by phrase, outer passive with by phrase, outer passive with wh- moved by phrase, outer passive with wh-moved object of by phrase, outer passive without by phrase with relative on the subject with and without comp, outer passive with by phrase with relative on subject with and without comp. 6.52. IDIOM WITH V, D, N, AND PREP ANCHORS: TNX0VDN1PNX2 77 Sr NP0  VP V NP1 N1  PP2 P2  NP2  Figure 6.52: Declarative Idiom with V, N, and Prep Anchors Tree: αnx0VN1Pnx2 6.52 Idiom with V, D, N, and Prep anchors: Tnx0VDN1Pnx2 Description: This tree family is selected by transitive idioms that are anchored by a verb, determiner, noun, and preposition. 17 idioms select this family. Examples: make a mess of, keep the lid on John made a mess out of his new suit. The tabloid didnt keep a lid on the imminent celebrity nuptials. Declarative tree: See Figure 6.53. Sr NP0  VP V NP1 D1  N1  PP2 P2  NP2  Figure 6.53: Declarative Idiom with V, D, N, and Prep Anchors Tree: αnx0VDN1Pnx2 Other available trees: Subject relative clause with and without comp, adjunct relative clause with compwith PP pied-piping, wh-moved subject, imperative, NP gerund, passive with- out by phrase, passive with by phrase, passive with wh-moved object of by phrase, passive with wh-moved by phrase, outer passive without by phrase, outer passive with by phrase, outer passive with wh- moved by phrase, outer passive with wh-moved object of by phrase, outer passive without 78 CHAPTER 6. VERB CLASSES by phrase with relative on the subject with and without comp, outer passive with by phrase with relative on subject with and without comp. Chapter 7 Ergatives Verbs in English that are termed ergative display the kind of alternation shown in the sentences in (7) below. (7) The sun melted the ice . The ice melted . The pattern of ergative pairs as seen in (7) is for the object of the transitive sentence to be the subject of the intransitive sentence. The literature discussing such pairs is based largely on syntactic models that involve movement, particularly GB. Within that framework two basic approaches are discussed:  Derived Intransitive The intransitive member of the ergative pair is derived through processes of movement and deletion from:  a transitive D-structure [Burzio, 1986]; or  transitive lexical structure [Hale and Keyser, 1986; Hale and Keyser, 1987]  Pure Intransitive The intransitive member is intransitive at all levels of the syntax and the lexicon and is not related to the transitive member syntactically or lexically [Napoli, 1988]. The Derived Intransitive approachs notions of movement in the lexicon or in the grammar are not represented as such in the XTAG grammar. However, distinctions drawn in these ar- guments can be translated to the FB-LTAG framework. In the XTAG grammar the diﬀerence between these two approaches is not a matter of movement but rather a question of tree fam- ily membership. The relation between sentences represented in terms of movement in other frameworks is represented in XTAG by membership in the same tree family. Wh-questions and their indicative counterparts are one example of this. Adopting the Pure Intransitive approach suggested by [Napoli, 1988] would mean placing the intransitive ergatives in a tree family with other intransitive verbs and separate from the transitive variants of the same verbs. This would result in a grammar that represented intransitive ergatives as more closely related to other intransitives than to their transitive counterparts. The only hint of the relation between the 79 80 CHAPTER 7. ERGATIVES intransitive ergatives and the transitive ergatives would be that ergative verbs would select both tree families. While this is a workable solution, it is an unattractive one for the English XTAG grammar because semantic coherence is implicitly associated with tree families in our analysis of other constructions. In particular, constancy in thematic role is represented by constancy in node names across sentence types within a tree family. For example, if the object of a declarative tree is NP1 the subject of the passive tree(s) in that family will also be NP1. The analysis that has been implemented in the English XTAG grammar is an adaptation of the Derived Intransitive approach. The ergative verbs select one family, Tnx0Vnx1, that contains both transitive and intransitive trees. The trans feature appears on the intransitive ergative trees with the value  and on the transitive trees with the value . This creates the two possibilities needed to account for the data.  intransitive ergativetransitive alternation. These verbs have transitive and intran- sitive variants as shown in sentences (8) and (9). (8) The sun melted the ice cream . (9) The ice cream melted . In the English XTAG grammar, verbs with this behavior are left unspeciﬁed as to value for the trans feature. This lack of speciﬁcation allows these verbs to anchor either type of tree in the Tnx0Vnx1 tree family because the unspeciﬁed trans value of the verb can unify with either  or  values in the trees.  transitive only. Verbs of this type select only the transitive trees and do not allow intransitive ergative variants as in the pattern show in sentences (10) and (11). (10) Elmo borrowed a book . (11) A book borrowed . The restriction to selecting only transitive trees is accomplished by setting the trans feature value to  for these verbs. Sr NP1 VP V trans : - Figure 7.1: Ergative Tree: αEnx1V The declarative ergative tree is shown in Figure 7.1 with the trans feature displayed. Note that the index of the subject NP indicates that it originated as the object of the verb. Chapter 8 Sentential Subjects and Sentential Complements In the XTAG grammar, arguments of a lexical item, including subjects, appear in the initial tree anchored by that lexical item. A sentential argument appears as an S node in the appropriate position within an elementary tree anchored by the lexical item that selects it. This is the case for sentential complements of verbs, prepositions and nouns and for sentential subjects. The distribution of complementizers in English is intertwined with the distribution of embedded sentences. A successful analysis of complementizers in English must handle both the cooccur- rence restrictions between complementizers and various types of clauses, and the distribution of the clauses themselves, in both subject and complement positions. 8.1 S or VP complements? Two comparable grammatical formalisms, Generalized Phrase Structure Grammar (GPSG) [Gazdar et al., 1985] and Head-driven Phrase Structure Grammar (HPSG) [Pollard and Sag, 1994], have rather diﬀerent treatments of sentential complements (S-comps). They both treat embedded sentences as VPs with subjects, which generates the correct structures but misses the generalization that Ss behave similarly in both matrix and embedded environments, and VPs behave quite diﬀerently. Neither account has PRO subjects of inﬁnitival clauses they have subjectless VPs instead. GPSG has a complete complementizer system, which appears to cover the same range of data as our analysis. It is not clear what sort of complementizer analysis could be implemented in HPSG. Following standard GB approach, the English XTAG grammar does not allow VP com- plements but treats verb-anchored structures without overt subjects as having PRO subjects. Thus, indicative clauses, inﬁnitives and gerunds all have a uniform treatment as embedded clauses using the same trees under this approach. Furthermore, our analysis is able to preserve the selectional and distributional distinction between Ss and VPs, in the spirit of GB theories, without having to posit extra empty categories.1 Consider the alternation between that and the null complementizer2, shown in sentences (12) and (13). 1i.e. empty complementizers. We do have PRO and NP traces in the grammar. 2Although we will continue to refer to null complementizers, in our analysis this is actually the absence of 81 82 CHAPTER 8. SENTENTIAL SUBJECTS AND SENTENTIAL COMPLEMENTS (12) He hopes  Muriel wins. (13) He hopes that Muriel wins. In GB both Muriel wins in (12) and that Muriel wins in (13) are CPs even though there is no overt complementizer to head the phrase in (12). Our grammar does not distinguish by category label between the phrases that would be labeled in GB as IP and CP. We label both of these phrases S. The diﬀerence between these two levels is the presence or absence of the complementizer (or extracted WH constituent), and is represented in our system as a diﬀerence in feature values (here, of the comp feature), and the presence of the additional structure contributed by the complementizer or extracted constituent. This illustrates an important distinction in XTAG, that between features and node labels. Because we have a sophisticated feature system, we are able to make ﬁne-grained distinctions between nodes with the same label which in another system might have to be realized by using distinguishing node labels. 8.2 Complementizers and Embedded Clauses in English: The Data Verbs selecting sentential complements (or subjects) place restrictions on their complements, in particular, on the form of the embedded verb phrase.3 Furthermore, complementizers are constrained to appear with certain types of clauses, again, based primarily on the form of the embedded VP. For example, hope selects both indicative and inﬁnitival complements. With an indicative complement, it may only have that or null as possible complementizers; with an inﬁnitival complement, it may only have a null complementizer. Verbs that allow wh comple- mentizers, such as ask, can take whether and if as complementizers. The possible combinations of complementizers and clause types is summarized in Table 8.1. As can be seen in Table 8.1, sentential subjects diﬀer from sentential complements in re- quiring the complementizer that for all indicative and subjunctive clauses. In sentential com- plements, that often varies freely with a null complementizer, as illustrated in (14)-(19). (14) Christy hopes that Mike wins. (15) Christy hopes Mike wins. (16) Dania thinks that Newt is a liar. (17) Dania thinks Newt is a liar. (18) That Helms won so easily annoyed me. (19) Helms won so easily annoyed me. 8.2. COMPLEMENTIZERS AND EMBEDDED CLAUSES IN ENGLISH: THE DATA 83 Complementizer: that whether if for null Clause type indicative subject Yes Yes No No No complement Yes Yes Yes No Yes inﬁnitive subject No Yes No Yes Yes complement No Yes No Yes Yes subjunctive subject Yes No No No No complement Yes No No No Yes gerundive4 complement No No No No Yes base complement No No No No Yes small clause complement No No No No Yes Table 8.1: Summary of Complementizer and Clause Combinations Another fact which must be accounted for in the analysis is that in inﬁnitival clauses, the complementizer for must appear with an overt subject NP, whereas a complementizer-less inﬁnitival clause never has an overt subject, as shown in (20)-(23). (See section 8.5 for more discussion of the case assignment issues relating to this construction.) (20) To lose would be awful. (21) For Penn to lose would be awful. (22) For to lose would be awful. (23) Penn to lose would be awful. In addition, some verbs select wh complements (either questions or clauses with whether or if) [Grimshaw, 1990]: (24) Jesse wondered who left. (25) Jesse wondered if Barry left. (26) Jesse wondered whether to leave. a complementizer. 3Other considerations, such as the relationship between the tenseaspect of the matrix clause and the tenseaspect of a complement clause are also important but are not currently addressed in the current English XTAG grammar. 4Most gerundive phrases are treated as NPs. In fact, all gerundive subjects are treated as NPs, and the only gerundive complements which receive a sentential parse are those for which there is no corresponding NP parse. This was done to reduce duplication of parses. See Chapter 17 for further discussion of gerunds. 84 CHAPTER 8. SENTENTIAL SUBJECTS AND SENTENTIAL COMPLEMENTS (27) Jesse wondered whether Barry left. (28) Jesse thought who left. (29) Jesse thought if Barry left. (30) Jesse thought whether to leave. (31) Jesse thought whether Barry left. 8.3 Features Required As we have seen above, clauses may be wh or wh, may have one of several com- plementizers or no complementizer, and can be of various clause types. The XTAG analysis uses three features to capture these possibilities: comp for the variation in complemen- tizers, wh for the question vs. non-question alternation and mode5 for clause types. In addition to these three features, the assign-comp feature represents complementizer requirements of the embedded verb. More detailed discussion of the assign-comp feature appears below in the discussions of sentential subjects and of inﬁnitives. The four features and their possible values are shown in Table 8.2. Feature Values comp that, if, whether, for, rel, nil mode ind, inf, subjnt, ger, base, ppart, nomprep assign-comp that, if, whether, for, rel, ind nil, inf nil wh , Table 8.2: Summary of Relevant Features 8.4 Distribution of Complementizers Like other non-arguments, complementizers anchor an auxiliary tree (shown in Figure 8.1) and adjoin to elementary clausal trees. The auxiliary tree for complementizers is the only alternative to having a complementizer position built into every sentential tree. The latter choice would mean having an empty complementizer substitute into every matrix sentence and a complementizerless embedded sentence to ﬁll the substitution node. Our choice follows the XTAG principle that initial trees consist only of the arguments of the anchor6  the S tree does not contain a slot for a complementizer, and the βCOMP tree has only one argument, an S with particular features determined by the complementizer. Complementizers select the type of clause to which they adjoin through constraints on the mode feature of the S foot node 8.5. CASE ASSIGNMENT, FOR AND THE TWO TOS 85 Sc comp : 1 inv : - displ-const : 2 wh : 3 mode : 4 indsbjnct Comp wh : 3 comp : 1 wh : - comp : that that Sr NA inv : - assign-comp : 1 wh : - mode : 4 comp : nil sub-conj : nil displ-const : 2 assign-comp : that Figure 8.1: Tree βCOMPs, anchored by that in the tree shown in Figure 8.1. These features also pass up to the root node, so that they are visible to the tree where the embedded sentence adjoinssubstitutes. The grammar handles the following complementizers: that, whether, if , for, and no comple- mentizer, and the clause types: indicative, inﬁnitival, gerundive, past participial, subjunctive and small clause (nomprep). The comp feature in a clausal tree reﬂects the value of the complementizer if one has adjoined to the clause. The comp and wh features receive their root node values from the particular com- plementizer which anchors the tree. The βCOMPs tree adjoins to an S node with the feature compnil; this feature indicates that the tree does not already have a complementizer adjoined to it.7 We ensure that there are no stacked complementizers by requiring the foot node of βCOMPs to have compnil. 8.5 Case assignment, for and the two tos The assign-comp feature is used to represent the requirements of particular types of clauses for particular complementizers. So while the comp feature represents constraints originat- ing from the VP dominating the clause, the assign-comp feature represents constraints originating from the highest VP in the clause. assign-comp is used to control the the appearance of subjects in inﬁnitival clauses (see discussion of ECM constructions in 8.6.1), to block bare indicative sentential subjects (bare inﬁnitival subjects are allowed), and 5mode actually conﬂates several types of information, in particular verb form and mood. 6See section 4.2 for a discussion of the diﬀerence between complements and adjuncts in the XTAG grammar. 7 Because root Ss cannot have complementizers, the parser checks that the root S has compnil at the end of the derivation, when the S is also checked for a tensed verb. 86 CHAPTER 8. SENTENTIAL SUBJECTS AND SENTENTIAL COMPLEMENTS to block that-trace violations. Examples (33), (34) and (35) show that an accusative case subject is obligatory in an inﬁnitive clause if the complementizer for is present. The inﬁnitive clauses in (32) is analyzed in the English XTAG grammar as having a PRO subject. (32) Christy wants to pass the exam. (33) Mike wants for her to pass the exam. (34) Mike wants for she to pass the exam. (35) Christy wants for to pass the exam. The for-to construction is particularly illustrative of the diﬃculties and beneﬁts faced in using a lexicalized grammar. It is commonly accepted that for behaves as a case-assigning complementizer in this construction, assigning accusative case to the subject of the clause since the inﬁnitival verb does not assign case to its subject position. However, in our featurized grammar, the absence of a feature licenses anything, so we must have overt null case assigned by inﬁnitives to ensure the correct distribution of PRO subjects. (See section 4.4 for more dis- cussion of case assignment.) This null case assignment clashes with accusative case assignment if we simply add for as a standard complementizer, since NPs (including PRO) are drawn from the lexicon already marked for case. Thus, we must use the assign-comp feature to pass information about the verb up to the root of the embedded sentence. To capture these facts, two inﬁnitive tos are posited. One inﬁnitive to has assign-casenone which forces a PRO subject, and assign-compinf nil which prevents for from adjoining. The other inﬁnitive to has no value at all for assign-case and has assign-compforecm, so that it can only occur either with the complementizer for or with ECM constructions. In those instances either for or the ECM verb supplies the assign-case value, assigning accusative case to the overt subject. 8.6 Sentential Complements of Verbs Tree families: Tnx0Vs1, Tnx0Vnx1s2, TItVnx1s2, TItVpnx1s2, TItVad1s2. Verbs that select sentential complements restrict the mode and comp values for those complements. Since with very few exceptions8 long distance extraction is possible from sentential complements, the S complement nodes are adjunction nodes. Figure 8.2 shows the declarative tree for sentential complements, anchored by think. The need for an adjunction node rather than a substitution node at S1 may not be obvious until one considers the derivation of sentences with long distance extractions. For example, the declarative in (36) is derived by adjoining the tree in Figure 8.3(b) to the S1 node of the tree in Figure 8.3(a). Since there are no bottom features on S1, the same ﬁnal result could have been achieved with a substitution node at S1. (36) The emu thinks that the aardvark smells terrible. 8.6. SENTENTIAL COMPLEMENTS OF VERBS 87 Sr NP0 VP V think S1 Figure 8.2: Sentential complement tree: βnx0Vs1 Sr NP DetP D the N aardvark VP V smells A terrible Sr NP DetP D the N emu VP V thinks S1 (a) (b) Figure 8.3: Trees for The emu thinks that the aardvark smells terrible. However, adjunction is crucial in deriving sentences with long distance extraction, as in sentences (37) and (38). (37) Who does the emu think smells terrible? (38) Who did the elephant think the panda heard the emu say smells terrible? The example in (37) is derived from the trees for who smells terrible? shown in Figure 8.4 and the emu thinks S shown in Figure 8.3(b), by adjoining the latter at the Sr node of the former.9 This process is recursive, allowing sentences like (38). Such a representation has been shown by [Kroch and Joshi, 1985] to be well-suited for describing unbounded dependencies. In English, a complementizer may not appear on a complement with an extracted subject (the that-trace conﬁguration). This phenomenon is illustrated in (39)-(41): (39) Which animal did the giraﬀe say that he likes? 8For example, long distance extraction is not possible from the S complement in it-clefts. 9See Chapter 20 for a discussion of do-support. 88 CHAPTER 8. SENTENTIAL SUBJECTS AND SENTENTIAL COMPLEMENTS Sq NP N who Sr NP0 NA ε VP V smells A terrible Figure 8.4: Tree for Who smells terrible? (40) Which animal did the giraﬀe say that likes him? (41) Which animal did the giraﬀe say likes him? These sentences are derived in XTAG by adjoining the tree for did the giraﬀe say S at the Sr node of the tree for either which animal likes him (to yield sentence (41)) or which animal he likes (to yield sentence (39)). That-trace violations are blocked by the presence of the feature assign-compinf nilind nilecm feature on the bottom of the Sr node of trees with extracted subjects (W0), i.e. those used in sentences such as (40) and (41). If a complementizer tree, βCOMPs, adjoins to a subject extraction tree at Sr, its assign- comp  thatwhetherforif feature will clash and the derivation will fail. If there is no complementizer, there is no feature clash, and this will permit the derivation of sentences like (41), or of ECM constructions, in which case the ECM verb will have assign-compecm (see section 8.6.1 for more discussion of the ECM case). Complementizers may adjoin normally to object extraction trees such as those used in sentence (39), and so object extraction trees have no value for the assign-comp feature. In the case of indirect questions, subjacency follows from the principle that a given tree cannot contain more than one wh-element. Extraction out of an indirect question is ruled out because a sentence like: (42)  Whoi do you wonder whoj ej loves ei ? would have to be derived from the adjunction of do you wonder into whoi whoj ej loves ei, which is an ill-formed elementary tree.10 10This does not mean that elementary trees with more than one gap should be ruled out across the grammar. Such trees might be required for dealing with parasitic gaps or gaps in coordinated structures. 8.6. SENTENTIAL COMPLEMENTS OF VERBS 89 8.6.1 Exceptional Case Marking Verbs Tree family: TXnx0Vs1 Exceptional Case Marking verbs are those which assign accusative case to the subject of the sentential complement. This is in contrast to verbs in the Tnx0Vnx1s2 family (section 6.6), which assign accusative case to an NP which is not part of the sentential complement. The subject of an ECM inﬁnitive complement is assigned accusative case is a manner anal- ogous to that of a subject in a for-to construction, as described in section 8.5. As in the for-to case, the ECM verb assigns accusative case into the subject of the lower inﬁnitive, and so the in- ﬁnitive uses the to which has no value for assign-case and has assign-compforecm. The ECM verb has assign-compecm and assign-caseacc on its foot. The former allows the assign-comp features of the ECM verb and the to tree to unify, and so be used together, and the latter assigns the accusative case to the lower subject. Figure 8.5 shows the declarative tree for the tree for the TXnx0Vs1 family, in this case anchored by expects. Figure 8.6 shows a parse for Van expects Bob to talk Sr NP0  VP V expects S1  displ-const : set1 : - assign-comp : ecm inv : - extracted : - control : 2 punct : contains : 10 comp: nil mode: inf wh: - Figure 8.5: ECM tree: βXnx0Vs1 The ECM and for-to cases are analogous in how they are used together with the correct inﬁnitival to to assign accusative case to the subject of the lower inﬁnitive. However, they are diﬀerent in that for is blocked along with other complementizers in subject extraction contexts, as discussed in section 8.6, as in (43), while subject extraction is compatible with ECM cases, as in (44). (43) What child did the giraﬀe ask for to leave? (44) Who did Bill expect to eat beans? 90 CHAPTER 8. SENTENTIAL SUBJECTS AND SENTENTIAL COMPLEMENTS Sr NP N Van VP V expects S1 NP N Bob VPr V to VP NA V talk Figure 8.6: Sample ECM parse Sentence (43) is ruled out by the assign-comp inf nilind nilecm feature on the subject extraction tree for ask, since the assign-compfor feature from the for tree will fail to unify. However, (44) will be allowed since assign-compecm feature on the expect tree will unify with the foot of the ECM verb tree. The use of features allows the ECM and for- to constructions to act the same for exceptional case assignment, while also being distinguished for that-trace violations. Verbs that take bare inﬁnitives, as in (45), are also treated as ECM verbs, the only diﬀerence being that their foot feature has modebase instead of modeinf. Since the comple- ment does not have to, there is no question of using the to tree for allowing accusative case to be assigned. Instead, verbs with modebase allow either accusative or nominative case to be assigned to the subject, and the foot of the ECM bare inﬁnitive tree forces it to be accusative by its assign-caseacc value at its foot node uniﬁes with the assign-casenomacc value of the bare inﬁnitive clause. (45) Bob sees the harmonica fall. The trees in the TXnx0Vs1 family are generally parallel to those in the Tnx0Vs1 family, except for the assign-case and assign-comp values on the foot nodes. However, the TXnx0Vs1 family also includes a tree for the passive, which of course is not included in the Tnx0Vs1 family. Unlike all the other trees in the TXnx0Vs1 family, the passive tree is not rooted in S, and is instead a VP auxiliary tree. Since the subject of the inﬁnitive is not thematically selected by the ECM verb, it is not part of the ECM verbs tree, and so it cannot be part of the passive tree. Therefore, the passive acts as a raising verb (see section 9.3). For example, to derive (47), the tree in Figure 8.7 would adjoin into a derivation for Bob to talk at the VP 8.7. SENTENTIAL SUBJECTS 91 node (and the modepassive feature, not shown, forces the auxiliary to adjoin in, as for other passives, as described in chapter 12). (46) Van expects Bob to talk. (47) Bob was expected to talk. VPr V expected VP Figure 8.7: ECM passive It has long been noted that passives of both full and bare inﬁnitive ECM constructions are full inﬁnitives, as in (47) and (49). (48) Bob sees the harmonica fall. (49) The harmonica was seen to fall. (50) The harmonica was seen fall. Under the TAG ECM analysis, this fact is easy to implement. The foot node of the ECM passive tree is simply set to have modeinf, which prevents the derivation of (50). There- fore, for all the other trees in the family, to foot nodes are set to have modebase or modeinf depending on whether it is a bare inﬁnitive or not. These foot nodes are all S nodes. The VP foot node of the passive tree, however, has modeinf regardless. 8.7 Sentential Subjects Tree families: Ts0Vnx1, Ts0Ax1, Ts0N1, Ts0Pnx1, Ts0ARBPnx1, Ts0PPnx1, Ts0PNaPnx1, Ts0V, Ts0Vtonx1, Ts0NPnx1, Ts0APnx1, Ts0A1s1. Verbs that select sentential subjects anchor trees that have an S node in the subject position rather than an NP node. Since extraction is not possible from sentential subjects, they are implemented as substitution nodes in the English XTAG grammar. Restrictions on sentential subjects, such as the required that complementizer for indicatives, are enforced by feature values speciﬁed on the S substitution node in the elementary tree. 92 CHAPTER 8. SENTENTIAL SUBJECTS AND SENTENTIAL COMPLEMENTS Sentential subjects behave essentially like sentential complements, with a few exceptions. In general, all verbs which license sentential subjects license the same set of clause types. Thus, unlike sentential complement verbs which select particular complementizers and clause types, the matrix verbs licensing sentential subjects merely license the S argument. Information about the complementizer or embedded verb is located in the tree features, rather than in the features of each verb selecting that tree. Thus, all sentential subject trees have the same mode, comp and assign-comp values shown in Figure 8.8(a). Sr S0 extracted : - inv : - assign-comp : inf_nil comp : thatforwhethernil mode : infind VP V perplexes NP1 Sr NP0 VP V thinks S1 NA displ-const : set1 : - assign-comp : inf_nilind_nil inv : - control : 6 punct : contains : 14 comp : thatnil mode : ind wh : - (a) (b) Figure 8.8: Comparison of assign-comp values for sentential subjects: αs0Vnx1 (a) and sentential complements: βnx0Vs1 (b) The major diﬀerence in clause types licensed by S-subjs and S-comps is that indicative S- subjs obligatorily have a complementizer (see examples in section 8.2). The assign-comp feature is used here to license a null complementizer for inﬁnitival but not indicative clauses. assign-comp has the same possible values as comp, with the exception that the nil value is split into ind nil and inf nil. This diﬀerence in feature values is illustrated in Figure 8.8. Another minor diﬀerence is that whether but not if is grammatical with S-subjs.11 Thus, if is not among the comp values allowed in S-subjs. The ﬁnal diﬀerence from S-comps is that there are no S-subjs with modeger. As noted in footnote 4 of this chapter, gerundive complements are only allowed when there is no corresponding NP parse. In the case of gerundive S-subjs, there is always an NP parse available. 8.8 Nouns and Prepositions taking Sentential Complements Trees: αNXNs, βvxPs, βPss, βnxPs, Tnx0N1s1, Tnx0A1s1. Prepositions and nouns can also select sentential complements, using the trees listed above. These trees use the mode and comp features as shown in Figure 8.9. For example, the 11Some speakers also ﬁnd if as a complementizer only marginally grammatical in S-comps. 8.9. PRO CONTROL 93 Sr S1 NA P S rel-pron : 9 punct : struct : nil displ-const : set1 : - inv : - extracted : - wh : - Sf NA NPr N S mode : infind comp : thatnil inv : - extracted : - (a) (b) Figure 8.9: Sample trees for preposition: βPss (a) and noun: αNXNs (b) taking sentential complements noun claim takes only indicative complements with that, while the preposition with takes small clause complements, as seen in sentences (51)-(54). (51) Beths claim that Clove was a smart dog.... (52) Beths claim that Clove a smart dog.... (53) Dania wasnt getting any sleep with Doug sick. (54) Dania wasnt getting any sleep with Doug was sick. 8.9 PRO control 8.9.1 Types of control In the literature on control, two types are often distinguished: obligatory control, as in sen- tences (55), (56), (57), and (58) and optional control, as in sentence (59). (55) Srinii promised Mickeyi [PROi to leave]. (56) Srini persuaded Mickeyi [PROi to leave]. (57) Srinii wanted [PROi to leave]. (58) Christyi left the party early [PROi to go to the airport]. 94 CHAPTER 8. SENTENTIAL SUBJECTS AND SENTENTIAL COMPLEMENTS (59) [PROarbi to dance] is important for Billi. At present, an analysis for obligatory control into complement clauses (as in sentences (55), (56), and (57)) has been implemented. An analysis for cases of obligatory control into adjunct clauses and optional control exists and can be found in [Bhatt, 1994]. 8.9.2 A feature-based analysis of PRO control The analysis for obligatory control involves co-indexation of the control feature of the NP anchored by PRO to the control feature of the controller. A feature equation in the tree anchored by the control verb co-indexes the control feature of the controlling NP with the foot node of the tree. All sentential trees have a co-indexed control feature from the root S to the subject NP. When the tree containing the controller adjoins onto the complement clause tree containing the PRO, the features of the foot node of the auxiliary tree are uniﬁed with the bottom features of the root node of the complement clause tree containing the PRO. This leads to the control feature of the controller being co-indexed with the control feature of the PRO. Depending on the choice of the controlling verb, the control propagation paths in the auxil- iary trees are diﬀerent. In the case of subject control (as in sentence (56)), the subject NP and the foot node are have co-indexed control features, while for object control (e.g. sentence (55), the object NP and the foot node are co-indexed for control. Among verbs that belong to the Tnx0Vnx1s2 family, i.e. verbs that take an NP object and a clausal complement, subject-control verbs form a distinct minority, promise being the only commonly used verb in this class. Consider the derivation of sentence (56). The auxiliary tree for persuade, shown in Figure 8.10, has the following feature equation (60). (60) NP1:control  S2.t:control The auxiliary tree adjoins into the tree for leave, shown in Figure 8.11, which has the following feature equation (61). (61) Sr.b:control  NP0.t:control Since the adjunction takes place at the root node (Sr) of the leave tree, after uniﬁcation, NP1 of the persuade tree and NP0 of the leave tree share a control feature. The resulting derived and derivation trees are shown in Figures 8.12 and 8.13. 8.9.3 The nature of the control feature The control feature does not have any value and is used only for co-indexing purposes. If two NPs have their control features co-indexed, it means that they are participating in a relationship of control; the c-commanding NP controls the c-commanded NP. 8.9.4 Long-distance transmission of control features Cases involving embedded inﬁnitival complements with PRO subjects such as (62) can also be handled. 8.9. PRO CONTROL 95 Sr NP0 VP V persuaded NP1 control : 1 S2 NA control : 1 Figure 8.10: Tree for persuaded Sr control : 1 NP0 control : 1 VP V leave Figure 8.11: Tree for leave (62) Johni wants [PROi to want [PROi to dance]]. The control feature of John and the two PROs all get co-indexed. This treatment might appear to lead to a problem. Consider (63): (63) Johni wants [Maryi to want [PROi to dance]]. If both the want trees have the control feature of their subject co-indexed to their foot nodes, we would have a situtation where the PRO is co-indexed for control feature with John, as well as with Mary. Note that the higher want in (62) is wantECM - it assigns case to the subject of the lower clause while the lower want in (62) is not. Subject control is restricted to non-ECM (Exceptional Case Marking) verbs that take inﬁnitival complements. Since the two wants in (62) are diﬀerent with respect to their control (and other) properties, the control feature of PRO stops at Mary and is not transmitted to the higher clause. 8.9.5 Locality constraints on control PRO control obeys locality constraints. The controller for PRO has to be in the immediately higher clause. Consider the ungrammatical sentence (64) ((64) is ungrammatical only with the 96 CHAPTER 8. SENTENTIAL SUBJECTS AND SENTENTIAL COMPLEMENTS r NP N Srini VP V persuaded NP control : 1 N Mickey S2 NA control : 1 NP control : 1 PRO VPr V to VP NA V leave Figure 8.12: Derived tree for Srini persuaded Mickey to leave αnx0V[leave] βnx0Vnx1s2[persuaded] (0) αNXN[Srini] (1) αNXN[Mickey] (2.2) αNX[PRO] (1) βVvx[to] (2) Figure 8.13: Derivation tree for Srini persuaded Mickey to leave co-indexing indicated below). (64)  Johni wants [PROi to persuade Maryi [PROi to dance]] However, such a derivation is ruled out automatically by the mechanisms of a TAG derivation and feature uniﬁcation. Suppose it was possible to ﬁrst compose the want tree with the dance tree and then insert the persuade tree. (This is not possible in the XTAG grammar because of the convention that auxiliary trees have NA (Null Adjunction) constraints on their foot nodes.) Even then, at the end of the derivation the control feature of the subject of want would end up co-indexed with the PRO subject of persuade and the control feature of Mary would be co-indexed with the PRO subject of dance as desired. There is no way to generate the illegal co-indexing in (63). Thus the locality constraints on PRO control fall out from the mechanics of TAG derivation and feature uniﬁcation. 8.10. REPORTED SPEECH 97 8.10 Reported speech Reported speech is handled in the XTAG grammar by having the reporting clause adjoin into the quote. Thus, the reporting clause is an auxiliary tree, anchored by the reporting verb. See [Doran, 1998] for details of the analysis. There are trees in both the Tnx0Vs1 and Tnx0nx1s2 families to handle reporting clauses which precede, follow and come in the middle of the quote. Chapter 9 The English Copula, Raising Verbs, and Small Clauses The English copula, raising verbs, and small clauses are all handled in XTAG by a common analysis based on sentential clauses headed by non-verbal elements. Since there are a number of diﬀerent analyses in the literature of how these phenomena are related (or not), we will present ﬁrst the data for all three phenomena, then various analyses from the literature, ﬁnishing with the analysis used in the English XTAG grammar.1 9.1 Usages of the copula, raising verbs, and small clauses 9.1.1 Copula The verb be as used in sentences (65)-(67) is often referred to as the copula. It can be followed by a noun, adjective, or prepositional phrase. (65) Carl is a jerk . (66) Carl is upset . (67) Carl is in a foul mood . Although the copula may look like a main verb at ﬁrst glance, its syntactic behavior follows the auxiliary verbs rather than main verbs. In particular,  Copula be inverts with the subject. (68) is Beth writing her dissertation ? is Beth upset ? wrote Beth her dissertation ? 1This chapter is strongly based on [Heycock, 1991]. Sections 9.1 and 9.2 are greatly condensed from her paper, while the description of the XTAG analysis in section 9.3 is an updated and expanded version. 98 9.1. USAGES OF THE COPULA, RAISING VERBS, AND SMALL CLAUSES 99  Copula be occurs to the left of the negative marker not. (69) Beth is not writing her dissertation . Beth is not upset . Beth wrote not her dissertation .  Copula be can contract with the negative marker not. (70) Beth isnt writing her dissertation . Beth isnt upset . Beth wrotent her dissertation .  Copula be can contract with pronominal subjects. (71) Shes writing her dissertation . Shes upset . Sheote her dissertation .  Copula be occurs to the left of adverbs in the unmarked order. (72) Beth is often writing her dissertation . Beth is often upset . Beth wrote often her dissertation . Unlike all the other auxiliaries, however, copula be is not followed by a verbal category (by deﬁnition) and therefore must be the rightmost verb. In this respect, it is like a main verb. The semantic behavior of the copula is also unlike main verbs. In particular, any semantic restrictions or roles placed on the subject come from the complement phrase (NP, AP, PP) rather than from the verb, as illustrated in sentences (73) and (74). Because the complement phrases predicate over the subject, these types of sentences are often called predicative sentences. (73) The bartender was garrulous . (74) ?The cliﬀ was garrulous . 9.1.2 Raising Verbs Raising verbs are the class of verbs that share with the copula the property that the complement, rather than the verb, places semantic constraints on the subject. (75) Carl seems a jerk . Carl seems upset . Carl seems in a foul mood . (76) Carl appears a jerk . Carl appears upset . Carl appears in a foul mood . 100 CHAPTER 9. THE ENGLISH COPULA, RAISING VERBS, AND SMALL CLAUSES The raising verbs are similar to auxiliaries in that they order with other verbs, but they are unique in that they can appear to the left of the inﬁnitive, as seen in the sentences in (77). They cannot, however, invert or contract like other auxiliaries (78), and they appear to the right of adverbs (79). (77) Carl seems to be a jerk . Carl seems to be upset . Carl seems to be in a foul mood . (78) seems Carl to be a jerk ? Carl seemnt to be upset . Carlems to be in a foul mood . (79) Carl often seems to be upset . Carl seems often to be upset . 9.1.3 Small Clauses One way of describing small clauses is as predicative sentences without the copula. Since matrix clauses require tense, these clausal structures appear only as embedded sentences. They occur as complements of certain verbs, each of which may allow certain types of small clauses but not others, depending on its lexical idiosyncrasies. (80) I consider [Carl a jerk] . I consider [Carl upset] . ?I consider [Carl in a foul mood] . (81) I prefer [Carl in a foul mood] . ??I prefer [Carl upset] . 9.1.4 Raising Adjectives Raising adjectives are the class of adjectives that share with the copula and raising verbs the property that the complement, rather than the verb, places semantic constraints on the subject. They appear with the copula in a matrix clause, as in (82). However, in other cases, such as that of small clauses (83), they do not have to appear with the copula. (82) Carl is likely to be a jerk . Carl is likely to be upset . Carl is likely to be in a foul mood . Carl is likely to perjure himself . (83) I consider Carl likely to perjure himself . 9.2. VARIOUS ANALYSES 101 9.2 Various Analyses 9.2.1 Main Verb Raising to INFL  Small Clause In [Pollock, 1989] the copula is generated as the head of a VP, like any main verb such as sing or buy. Unlike all other main verbs2, however, be moves out of the VP and into Inﬂ in a tensed sentence. This analysis aims to account for the behavior of be as an auxiliary in terms of inversion, negative placement and adverb placement, while retaining a sentential structure in which be heads the main VP at D-Structure and can thus be the only verb in the clause. Pollock claims that the predicative phrase is not an argument of be, which instead he assumes to take a small clause complement, consisting of a node dominating an NP and a predicative AP, NP or PP. The subject NP of the small clause then raises to become the subject of the sentence. This accounts for the failure of the copula to impose any selectional restrictions on the subject. Raising verbs such as seem and appear, presumably, take the same type of small clause complement. 9.2.2 Auxiliary  Null Copula In [Lapointe, 1980] the copula is treated as an auxiliary verb that takes as its complement a VP headed by a passive verb, a present participle, or a null verb (the true copula). This verb may then take AP, NP or PP complements. The author points out that there are many languages that have been analyzed as having a null copula, but that English has the peculiarity that its null copula requires the co-presence of the auxiliary be. 9.2.3 Auxiliary  Predicative Phrase In GPSG ([Gazdar et al., 1985], [Sag et al., 1985]) the copula is treated as an auxiliary verb that takes an X2 category with a  value for the head feature [PRD] (predicative). AP, NP, PP and VP can all be [PRD], but a Feature Co-occurrence Restriction guarantees that a [PRD] VP will be headed by a verb that is either passive or a present participle. GPSG follows [Chomsky, 1970] in adopting the binary valued features [V] and [N] for de- composing the verb, noun, adjective and preposition categories. In that analysis, verbs are [V,N], nouns are [V,N], adjectives [V,N] and prepositions [V,N]. NP and AP predicative complements generally pattern together; a fact that can be stated economically using this category decomposition. In neither [Sag et al., 1985] nor [Chomsky, 1970] is there any discussion of how to handle the complete range of complements to a verb like seem, which takes AP, NP and PP complements, as well as inﬁnitives. The solution would appear to be to associate the verb with two sets of rules for small clauses, leaving aside the use of the verb with an expletive subject and sentential complement. 9.2.4 Auxiliary  Small Clause In [Moro, 1990] the copula is treated as a special functional category - a lexicalization of tense, which is considered to head its own projection. It takes as a complement the projection of another functional category, Agr (agreement). This projection corresponds roughly to a small 2with the exception of have in British English. See footnote 1 in Chapter 20. 102 CHAPTER 9. THE ENGLISH COPULA, RAISING VERBS, AND SMALL CLAUSES clause, and is considered to be the domain within which predication takes place. An NP must then raise out of this projection to become the subject of the sentence: it may be the subject of the AgrP, or, if the predicate of the AgrP is an NP, this may raise instead. In addition to occurring as the complement of be, AgrP is selected by certain verbs such as consider. It follows from this analysis that when the complement to consider is a simple AgrP, it will always consist of a subject followed by a predicate, whereas if the complement contains the verb be, the predicate of the AgrP may raise to the left of be, leaving the subject of the AgrP to the right. (84) Johni is [AgrP ti the culprit ] . (85) The culpriti is [AgrP John ti ] . (86) I consider [AgrP John the culprit] . (87) I consider [Johni to be [AgrP ti the culprit ]] . (88) I consider [the culpriti to be [AgrP John ti ]] . Moro does not discuss a number of aspects of his analysis, including the nature of Agr and the implied existence of sentences without VPs. 9.3 XTAG analysis Sr NP0 VP V ε NP1 N Sr NP0 VP V ε AP1 A Sr NP0 VP V ε PP1 P NP1 (a) (b) (c) Figure 9.1: Predicative trees: αnx0N1 (a), αnx0Ax1 (b) and αnx0Pnx1 (c) The XTAG grammar provides a uniform analysis for the copula, raising verbs and small clauses by treating the maximal projections of lexical items that can be predicated as predicative 9.3. XTAG ANALYSIS 103 clauses, rather than simply noun, adjective and prepositional phrases. The copula adjoins in for matrix clauses, as do the raising verbs. Certain other verbs (such as consider) can take the predicative clause as a complement, without the adjunction of the copula, to form the embedded small clause. The structure of a predicative clause, then, is roughly as seen in (89)-(91) for NPs, APs and PPs. The XTAG trees corresponding to these structures3 are shown in Figures 9.1(a), 9.1(b), and 9.1(c), respectively. (89) [S NP [V P N . . . ]] (90) [S NP [V P A . . . ]] (91) [S NP [V P P . . . ]] The copula be and raising verbs all get the basic auxiliary tree as explained in the section on auxiliary verbs (section 20.1). Unlike the raising verbs, the copula also selects the inverted auxiliary tree set. Figure 9.2 shows the basic auxiliary tree anchored by the copula be. The mode feature is used to distinguish the predicative constructions so that only the copula and raising verbs adjoin onto the predicative trees. There are two possible values of mode that correspond to the predicative trees, nom and prep. They correspond to a modiﬁed version of the four-valued [N,V] feature described in section 9.2.3. The nom value corresponds to [N], selecting the NP and AP predicative clauses. As mentioned earlier, they often pattern together with respect to constructions using predicative clauses. The remaining prepositional phrase predicative clauses, then, correspond to the prep mode. Figure 9.3 shows the predicative adjective tree from Figure 9.1(b) now anchored by upset and with the features visible. As mentioned, modenom on the VP node prevents auxiliaries other than the copula or raising verbs from adjoining into this tree. In addition, it prevents the predicative tree from occurring as a matrix clause. Since all matrix clauses in XTAG must be mode indicative (ind) or imperative (imp), a tree with modenom or modeprep must have an auxiliary verb (the copula or a raising verb) adjoin in to make it modeind. The distribution of small clauses as embedded complements to some verbs is also man- aged through the mode feature. Verbs such as consider and prefer select trees that take a sentential complement, and then restrict that complement to be modenom andor modeprep, depending on the lexical idiosyncrasies of that particular verb. Many verbs that dont take small clause complements do take sentential complements that are modeind, which includes small clauses with the copula already adjoined. Hence, as seen in sentence sets (92)-(94), consider takes only small clause complements, prefer takes both prep (but not nom) small clauses and indicative clauses, while feel takes only indicative clauses. (92) She considers Carl a jerk . ?She considers Carl in a foul mood . She considers that Carl is a jerk . 3There are actually two other predicative trees in the XTAG grammar. Another predicative noun phrase tree is needed for noun phrases without determiners, as in the sentence They are ﬁremen, and another prepositional phrase tree is needed for exhaustive prepositional phrases, such as The workers are below. 104 CHAPTER 9. THE ENGLISH COPULA, RAISING VERBS, AND SMALL CLAUSES VPr conditional : 9 perfect : 10 progressive : 11 displ-const : set1 : 8 assign-case : 7 mode : 6 tense : 5 agr : 3 neg : 2 assign-comp : 1 mainv : 4 V assign-comp : 1 neg : 2 agr : 3 mainv : 4 - tense : 5 mode : 6 assign-case : 7 displ-const : set1 : 8 mode : ind tense : pres mainv : - assign-comp : ind_niladjthatrelifwhether assign-case : nom agr : 3rdsing :  num : sing pers : 3 is VP NA displ-const : set1 : - progressive : 11 perfect : 10 conditional : 9 mode : nomprep Figure 9.2: Copula auxiliary tree: βVvx (93) She prefers Carl a jerk . She prefers Carl in a foul mood . She prefers that Carl is a jerk . (94) She feels Carl a jerk . She feels Carl in a foul mood . She feels that Carl is a jerk . Figure 9.4 shows the tree anchored by consider that takes the predicative small clauses. Raising verbs such as seems work essentially the same as the auxiliaries, in that they also select the basic auxiliary tree, as in Figure 9.2. The only diﬀerence is that the value of mode 9.3. XTAG ANALYSIS 105 Sr displ-const : set1 : - assign-comp : inf_nilind_nil assign-case : 1 agr : 2 tense : 3 comp : nil mainv : 4 mode : 5 displ-const : set1 : 6 assign-comp : 7 inv : - extracted : - NP0 wh : - case : 1 agr : 2 VP assign-case : 1 agr : 2 tense : 3 mainv : 4 mode : 5 displ-const : set1 : 6 assign-comp : 7 assign-case : acc mode : nom displ-const : set1 : - V ε AP1 A wh : - upset Figure 9.3: Predicative AP tree with features: αnx0Ax1 on the VP foot node might be diﬀerent, depending on what types of complements the raising verb takes. Also, two of the raising verbs take an additional tree, βVpxvx, shown in Figure 9.5, which allows for an experiencer argument, as in John seems to me to be happy. Raising adjectives, such as likely, take the tree shown in Figure 9.6. This tree combines aspects of the auxiliary tree βVvx and the adjectival predicative tree shown in Figure 9.1(b). As with βVvx, it adjoins in as a VP auxiliary tree. However, since it is anchored by an adjective, not a verb, it is similar to the adjectival predicative tree in that it has an ǫ at the V node, and a feature value of modenom which is passed up to the VP root indicates that it is an 106 CHAPTER 9. THE ENGLISH COPULA, RAISING VERBS, AND SMALL CLAUSES Sr NP0 VP V consider S1 displ-const : set1 : - assign-comp : inf_nilind_nil inv : - assign-case : acc comp : nil mode : nomprep Figure 9.4: Consider tree for embedded small clauses VPr V PP P to NP VP NA Figure 9.5: Raising verb with experiencer tree: βVpxvx adjectival predication. This serves the same purpose as in the case of the tree in Figure 9.3, and forces another auxiliary verb, such as the copula, to adjoin in to make it modeind. VPr V NA ε AP A VP NA Figure 9.6: Raising adjective tree: βVvx-adj 9.4. NON-PREDICATIVE BE 107 9.4 Non-predicative BE The examples with the copula that we have given seem to indicate that be is always followed by a predicative phrase of some sort. This is not the case, however, as seen in sentences such as (95)-(100). The noun phrases in these sentences are not predicative. They do not take raising verbs, and they do not occur in embedded small clause constructions. (95) my teacher is Mrs. Wayman . (96) Doug is the man with the glasses . (97) My teacher seems Mrs. Wayman . (98) Doug appears the man with the glasses . (99) I consider [my teacher Mrs. Wayman] . (100) I prefer [Doug the man with the glasses] . In addition, the subject and complement can exchange positions in these type of examples but not in sentences with predicative be. Sentence (101) has the same interpretation as sentence (96) and diﬀers only in the positions of the subject and complement NPs. Similar sentences, with a predicative be, are shown in (102) and (103). In this case, the sentence with the exchanged NPs (103) is ungrammatical. (101) The man with the glasses is Doug . (102) Doug is a programmer . (103) A programmer is Doug . The non-predicative be in (95) and (96), also called equative be, patterns diﬀerently, both syntactically and semantically, from the predicative usage of be. Since these sentences are clearly not predicative, it is not desirable to have a tree structure that is anchored by the NP, AP, or PP, as we have in the predicative sentences. In addition to the conceptual problem, we would also need a mechanism to block raising verbs from adjoining into these sentences (while allowing them for true predicative phrases), and prevent these types of sentence from being embedded (again, while allowing them for true predicative phrases). Although non-predicative be is not a raising verb, it does exhibit the auxiliary verb behavior set out in section 9.1.1. It inverts, contracts, and so forth, as seen in sentences (104) and (105), and therefore can not be associated with any existing tree family for main verbs. It requires a separate tree family that includes the tree for inversion. Figures 9.7(a) and 9.7(b) show the declarative and inverted trees, respectively, for equative be. (104) is my teacher Mrs. Wayman ? (105) Doug isnt the man with the glasses . 108 CHAPTER 9. THE ENGLISH COPULA, RAISING VERBS, AND SMALL CLAUSES Sr NP0 VPr V is VP1 V1 ε1 NP1 Sq V is Sr NP0 VPr NA Vr ε VP1 V1 ε1 NP1 (a) (b) Figure 9.7: Equative BE trees: αnx0BEnx1 (a) and αInvnx0BEnx1 (b) Chapter 10 Ditransitive constructions and dative shift Verbs such as give and put that require two objects, as shown in examples (106)-(109), are termed ditransitive. (106) Christy gave a cannoli to Beth Ann . (107) Christy gave Beth Ann . (108) Christy put a cannoli in the refrigerator . (109) Christy put a cannoli . The indirect objects Beth Ann and refrigerator appear in these examples in the form of PPs. Within the set of ditransitive verbs there is a subset that also allow two NPs as in (110). As can be seen from (110) and (111) this two NP, or double-object, construction is grammatical for give but not for put. (110) Christy gave Beth Ann a cannoli . (111) Christy put the refrigerator the cannoli . The alternation between (106) and (110) is known as dative shift.1 In order to account for verbs with dative shift the English XTAG grammar includes structures for both variants in the tree family Tnx0Vnx1Pnx2. The declarative trees for the shifted and non-shifted alternations are shown in Figure 10.1. The indexing of nodes in these two trees represents the fact that the semantic role of the indirect object (NP2) in Figure 10.1(a) is the same as that of the direct object (NP2) in 1In languages similar to English that have overt case marking indirect objects would be marked with dative case. It has also been suggested that for English the preposition to serves as a dative case marker. 109 110 CHAPTER 10. DITRANSITIVE CONSTRUCTIONS AND DATIVE SHIFT S r NP 0 VP V NP 1 PP 2 P2 to NP 2 S r NP 0 VP V NP 2 NP 1 (a) (b) Figure 10.1: Dative shift trees: αnx0Vnx1Pnx2 (a) and αnx0Vnx2nx1 (b) Figure 10.1(b) (and vice versa). This use of indexing is consistent with our treatment of other constructions such as passive and ergative. Verbs that do not show this alternation and have only the NP PP structure (e.g. put) select the tree family Tnx0Vnx1pnx2. Unlike the Tnx0Vnx1Pnx2 family, the Tnx0Vnx1pnx2 tree family does not contain trees for the NP NP structure. Other verbs such as ask allow only the NP NP structure as shown in (112) and (113). (112) Beth Ann asked Srini a question . (113) Beth Ann asked a question to Srini . Verbs that only allow the NP NP structure select the tree family Tnx0Vnx1nx2. This tree family does not have the trees for the NP PP structure. Notice that in Figure 10.1(a) the preposition to is built into the tree. There are other apparent cases of dative shift with for, such as in (114) and (115), that we have taken to be structurally distinct from the cases with to. (114) Beth Ann baked Dusty a biscuit . (115) Beth Ann baked a biscuit for Dusty . [McCawley, 1988] notes: A for-dative expression in underlying structure is external to the V with which it is combined, in view of the fact that the latter behaves as a unit with regard to all relevant syntactic phenomena. In other words, the for PPs that appear to undergo dative shift are actually adjuncts, not complements. Examples (116) and (117) demonstrate that PPs with for are optional while ditransitive to PPs are not. 111 (116) Beth Ann made dinner . (117) Beth Ann gave dinner . Consequently, in the XTAG grammar the apparent dative shift with for PPs is treated as Tnx0Vnx1nx2 for the NP NP structure, and as a transitive plus an adjoined adjunct PP for the NP PP structure. To account for the ditransitive to PPs, the preposition to is built into the tree family Tnx0Vnx1tonx2. This accounts for the fact that to is the only preposition allowed in dative shift constructions. [McCawley, 1988] also notes that the to and for cases diﬀer with respect to passivization; the indirect objects with to may be the subjects of corresponding passives while the alleged indirect objects with for cannot, as in sentences (118)-(121). Note that the passivisation examples are for NP NP structures of verbs that take to or for PPs. (118) Beth Ann gave Clove dinner . (119) Clove was given dinner (by Beth Ann) . (120) Beth Ann made Clove dinner . (121) ?Clove was made dinner (by Beth Ann) . However, we believe that this to be incorrect, and that the indirect objects in the for case are allowed to be the subjects of passives, as in sentences (122)-(123). The apparent strangeness of sentence (121) is caused by interference from other interpretations of Clove was made dinner . (122) Dania baked Doug a cake . (123) Doug was baked a cake by Dania . Chapter 11 It-clefts There are several varieties of it-clefts in English. All the it-clefts have four major components:  the dummy subject: it,  the main verb: be,  the clefted element: A constituent (XP) compatible with any gap in the clause,  the clause: A clause (e.g. S) with or without a gap. Examples of it-clefts are shown in (124)-(127). (124) it was [XP here XP ] [S that the ENIAC was created . S] (125) it was [XP at MIT XP ] [S that colorless green ideas slept furiously . S] (126) it is [XP happily XP ] [S that Seth quit Reality . S] (127) it was [XP there XP ] [S that she would have to enact her renunciation . S] The clefted element can be of a number of categories, for example NP, PP or adverb. The clause can also be of several types. The English XTAG grammar currently has a separate analysis for only a subset of the speciﬁcational it-clefts1, in particular the ones without gaps in the clause (e.g. (126) and (127)). It-clefts that have gaps in the clause, such as (124) and (125) are currently handled as relative clauses. Although arguments have been made against treating the clefted element and the clause as a constituent ([Delahunty, 1984]), the relative clause approach does capture the restriction that the clefted element must ﬁll the gap in the clause, and does not require any additional trees. In the speciﬁcational it-cleft without gaps in the clause, the clefted element has the role of an adjunct with respect to the clause. For these cases the English XTAG grammar requires 1See e.g. [Ball, 1991], [Delin, 1989] and [Delahunty, 1984] for more detailed discussion of types of it-clefts. 112 113 additional trees. These it-cleft trees are in separate tree families because, although some re- searchers (e.g. [Akmajian, 1970]) derived it-clefts through movement from other sentence types, most current researchers (e.g. [Delahunty, 1984], [Knowles, 1986], [Gazdar et al., 1985], [Delin, 1989] and [Sornicola, 1988]) favor base-generation of the various cleft sentences. Placing the it-cleft trees in their own tree families is consistent with the current preference for base genera- tion, since in the XTAG English grammar, structures that would be related by transformation in a movement-based account will appear in the same tree family. Like the base-generated approaches, the placement of it-clefts in separate tree families makes the claim that there is no derivational relation between it-clefts and other sentence types. The three it-cleft tree families are virtually identical except for the category label of the clefted element. Figure 11.1 shows the declarative tree and an inverted tree for the PP It-cleft tree family. S r NP 0 N VP V VP1 V1 ε PP 1 S 2 S q V S r NA NP 0 N VPr Vr ε VP1 V1 ε1 PP 1 S 2 (a) (b) Figure 11.1: It-cleft with PP clefted element: αItVpnx1s2 (a) and αInvItVpnx1s2 (b) The extra layer of tree structure in the VP represents that, while be is a main verb rather than an auxiliary in these cases, it retains some auxiliary properties. The VP structure for the equativeit-cleft-be is identical to that obtained after adjunction of predicative-be into small- clauses.2 The inverted tree in Figure 11.1(b) is necessary because of bes auxiliary-like behavior. Although be is the main verb in it-clefts, it inverts like an auxiliary. Main verb inversion cannot be accomplished by adjunction as is done with auxiliaries and therefore must be built into the tree family. The tree in Figure 11.1(b) is used for yesno questions such as (128). (128) was it in the forest that the wolf talked to the little girl ? 2For additional discussion of equative or predicative-be see Chapter 9. Part III Sentence Types 114 Chapter 12 Passives In passive constructions such as (129), the subject NP is interpreted as having the same role as the direct object NP in the corresponding active declarative (130). (129) An airline buy-out bill was approved by the House. (WSJ) (130) The House approved an airline buy-out bill. Sr mode : 3 NP1 VP mode : 3 passive : 1 mode : 2 V passive : 1  mode : 2 ppart S2 S r NP 1  VP V  PP P by NP 0  S 2  S r NP 1  VP V  S 2  PP P by NP 0  (a) (b) (c) Figure 12.1: Passive trees in the Sentential Complement with NP tree family: βnx1Vs2 (a), βnx1Vbynx0s2 (b) and βnx1Vs2bynx0 (c) In a movement analysis, the direct object is said to have moved to the subject position. The original declarative subject is either absent in the passive or is in a by headed PP (by phrase). In the English XTAG grammar, passive constructions are handled by having separate trees within the appropriate tree families. Passive trees are found in most tree families that have a direct object in the declarative tree (the light verb tree families, for instance, do not contain 116 117 passive trees). Passive trees occur in pairs - one tree with the by phrase, and another without it. Variations in the location of the by phrase are possible if a subcategorization includes other arguments such as a PP or an indirect object. Additional trees are required for these variations. For example, the Sentential Complement with NP tree family has three passive trees, shown in Figure 12.1: one without the by-phrase (Figure 12.1(a)), one with the by phrase before the sentential complement (Figure 12.1(b)), and one with the by phrase after the sentential complement (Figure 12.1(c)). Figure 12.1(a) also shows the feature restrictions imposed on the anchor1. Only verbs with modeppart (i.e. verbs with passive morphology) can anchor this tree. The mode feature is also responsible for requiring that passive be adjoin into the tree to create a matrix sen- tence. Since a requirement is imposed that all matrix sentences must have modeindimp, an auxiliary verb that selects modeppart and passive (such as was) must adjoin (see Chapter 20 for more information on the auxiliary verb system). 1A reduced set of features are shown for readability. Chapter 13 Extraction The discussion in this chapter covers constructions that are analyzed as having wh-movement in GB, in particular, wh-questions and topicalization. Relative clauses, which could also be considered extractions, are discussed in Chapter 14. Extraction involves a constituent appearing in a linear position to the left of the clause with which it is interpreted. One clause argument position is empty. For example, the position ﬁlled by frisbee in the declarative in sentence (131) is empty in sentence (132). The wh-item what in sentence (132) is of the same syntactic category as frisbee in sentence (131) and ﬁlls the same role with respect to the subcategorization. (131) Clove caught a frisbee. (132) Whati did Clove catch ǫi? The English XTAG grammar represents the connection between the extracted element and the empty position with co-indexing (as does GB). The trace feature is used to implement the co-indexing. In extraction trees in XTAG, the empty position is ﬁlled with an ǫ. The extracted item always appears in these trees as a sister to the Sr tree, with both dominated by a Sq root node. The Sr subtrees in extraction trees have the same structure as the declarative tree in the same tree family. The additional structure in extraction trees of the Sq and NP nodes roughly corresponds to the CP and Spec of CP positions in GB. All sentential trees with extracted components (this does not include relative clause trees) are marked extracted at the top S node, while sentential trees with no extracted components are marked extracted. Items that take embedded sentences, such as nouns, verbs and some prepositions can place restrictions on whether the embedded sentence is allowed to be extracted or not. For instance, sentential subjects and sentential complements of nouns and prepositions are not allowed to be extracted, while certain verbs may allow extracted sentential complements and others may not (e.g. sentences (133)-(136)). (133) The jury wondered [who killed Nicole]. (134) The jury wondered [who Simpson killed]. 118 119 (135) The jury thought [Simpson killed Nicole]. (136) The jury thought [who did Simpson kill]? The extracted feature is also used to block embedded topicalization in inﬁnitival comple- ment clauses as exempliﬁed in (137). (137)  John wants [ Billi [PRO to see ti]] Verbs such as want that take non-wh inﬁnitival complements specify that the extracted feature of their complement clause (i.e. of the foot S node) is . Clauses that involve topical- ization have  as the value of their extracted feature (i.e. of the root S node). Sentences like (137) are thus ruled out. Sq invlink : 1 inv : 1 extracted :  wh : 5 NP case : 2 agr : 3 trace : 4 wh : 5 Sr inv : 1 inv : - NP0 VP V NP1 NA case : acc case : 2 agr : 3 trace : 4 ε Figure 13.1: Transitive tree with object extraction: αW1nx0Vnx1 The tree that is used to derive the embedded sentence in (135) in the English XTAG grammar is shown in Figure 13.11. The important features of extracted trees are:  The subtree that has Sr as its root is identical to the declarative tree or a non-extracted passive tree, except for having one NP position in the VP ﬁlled by ǫ.  The root S node is Sq, which dominates NP and Sr. 1Features not pertaining to this discussion have been taken out to improve readability. 120 CHAPTER 13. EXTRACTION  The trace feature of the ǫ ﬁlled NP is co-indexed with the trace feature of the NP daughter of Sq.  The case and agr features are passed from the empty NP to the extracted NP. This is particularly important for extractions from subject NPs, since case can continue to be assigned from the verb to the subject NP position, and from there be passed to the extracted NP.  The inv feature of Sr is co-indexed to the wh feature of NP through the use of the invlink feature in order to force subject-auxiliary inversion where needed (see section 13.1 for more discussion of the invwh co-indexing and the use of these trees for topicalization). 13.1 Topicalization and the value of the inv feature Our analysis of topicalization uses the same trees as wh-extraction. For any NP complement position a single tree is used for both wh-questions and for topicalization from that position. Wh-questions have subject-auxiliary inversion and topicalizations do not. This diﬀerence be- tween the constructions is captured by equating the values of the Srs inv feature and the extracted NPs wh feature. This means that if the extracted item is a wh-expression, as in wh-questions, the value of inv will be  and an inverted auxiliary will be forced to adjoin. If the extracted item is a non-wh, inv will be  and no auxiliary adjunction will occur. An additional complication is that inversion only occurs in matrix clauses, so the values of inv and wh should only be equated in matrix clauses and not in embedded clauses. In the English XTAG grammar, appropriate equating of the inv and wh features is accom- plished using the invlink feature and a restriction imposed on the root S of a derivation. In particular, in extraction trees that are used for both wh-questions and topicalizations, the value of the inv feature for the top of the Sr node is co-indexed to the value of the inv feature on the bottom of the Sq node. On the bottom of the Sq node the inv feature is co-indexed to the invlink feature. The wh feature of the extracted NP node is co-indexed to the value of the wh feature on the bottom of Sq. The linking between the value of the Sq wh and the invlink features is imposed by a condition on the ﬁnal root node of a derivation (i.e. the top S node of a matrix clause) requires that invlinkwh. For example, the tree in Figure 13.1 is used to derive both (138) and (139). (138) John, I like. (139) Who do you like? For the question in (139), the extracted item who has the feature value wh, so the value of the inv feature on VP is also  and an auxiliary, in this case do, is forced to adjoin. For the topicalization (138) the values for Johns wh feature and for Sqs inv feature are both  and no auxiliary adjoins. 13.2. EXTRACTED SUBJECTS 121 13.2 Extracted subjects The extracted subject trees provide for sentences like (140)-(142), depending on the tree family with which it is associated. (140) Who left? (141) Who wrote the paper? (142) Who was happy? Wh-questions on subjects diﬀer from other argument extractions in not having subject- auxiliary inversion. This means that in subject wh-questions the linear order of the constituents is the same as in declaratives so it is diﬃcult to tell whether the subject has moved out of position or not (see [Heycock and Kroch, 1993] for arguments for and against moved subject). The English XTAG treatment of subject extractions assumes the following:  Syntactic subject topicalizations dont exist; and  Subjects in wh-questions are extracted rather than in situ. The assumption that there is no syntactic subject topicalization is reasonable in English since there is no convincing syntactic evidence and since the interpretability of subjects as topics seems to be mainly aﬀected by discourse and intonational factors rather than syntactic structure. As for the assumption that wh-question subjects are extracted, these questions seem to have more similarities to other extractions than to the two cases in English that have been considered in situ wh: multiple wh questions and echo questions. In multiple wh questions such as sentence (143), one of the wh-items is blocked from moving sentence initially because the ﬁrst wh-item already occupies the location to which it would move. (143) Who ate what? This type of blocking account is not applicable to subject wh-questions because there is no obvious candidate to do the blocking. Similarity between subject wh-questions and echo questions is also lacking. At least one account of echo questions ([Hockey, 1994]) argues that echo questions are not ordinary wh-questions at all, but rather focus constructions in which the wh-item is the focus. Clearly, this is not applicable to subject wh-questions. So it seems that treating subject wh-questions similarly to other wh-extractions is more justiﬁed than an in situ treatment. Given these assumptions, there must be separate trees in each tree family for subject extrac- tions. The declarative tree cannot be used even though the linear order is the same because the structure is diﬀerent. Since topicalizations are not allowed, the wh feature for the extracted NP node is set in these trees to . The lack of subject-auxiliary inversion is handled by the absence of the invlink feature. Without the presence of this feature, the wh and inv are never linked, so inversion can not occur. Like other wh-extractions, the Sq node is marked extracted to constrain the occurrence of these trees in embedded sentences. The tree in Figure 13.2 is an example of a subject wh-question tree. 122 CHAPTER 13. EXTRACTION Sq inv : 9 wh : 8 extracted :  NP agr : 5 case : 6 trace : 7 wh : 8  Sr inv : 9 wh : 8 assign-case : 3 agr : 4 inv : - NP0 NA case : 3 agr : 4 agr : 5 case : 6 trace : 7 ε VP assign-case : 3 agr : 4 agr : 1 assign-case : 2 V agr : 1 assign-case : 2 Figure 13.2: Intransitive tree with subject extraction: αW0nx0V 13.3 Wh-moved NP complement Wh-questions can be formed on every NP object or indirect object that appears in the declar- ative tree or in the passive trees, as seen in sentences (144)-(149). A tree family will contain one tree for each of these possible NP complement positions. Figure 13.3 shows the two extrac- tion trees from the ditransitive tree family for the extraction of the direct (Figure 13.3(a)) and indirect object (Figure 13.3(b)). (144) Dania asked Beth a question. (145) Whoi did Dania ask ǫi a question? (146) Whati did Dania ask Beth ǫi? (147) Beth was asked a question by Dania. (148) Whoi was Beth asked a question by ǫi?? (149) Whati was Beth asked ǫi? by Dania? 13.4. WH-MOVED OBJECT OF A P 123 Sq NP Sr NP0 VP V NP1 NA ε NP2 Sq NP Sr NP0 VP V NP1 NP2 NA ε (a) (b) Figure 13.3: Ditransitive trees with direct object: αW1nx0Vnx1nx2 (a) and indirect object extraction: αW2nx0Vnx1nx2 (b) 13.4 Wh-moved object of a P Wh-questions can be formed on the NP object of a complement PP as in sentence (150). (150) [Which dog]i did Beth Ann give a bone to ǫi? The by phrases of passives behave like complements and can undergo the same type of extraction, as in (151). (151) [Which dog]i was the frisbee caught by ǫi? Tree structures for this type of sentence are very similar to those for the wh-extraction of NP complements discussed in section 13.3 and have the identical important features related to tree structure and trace and inversion features. The tree in Figure 13.4 is an example of this type of tree. Topicalization of NP objects of prepositions is handled the same way as topicalization of complement NPs. 13.5 Wh-moved PP Like NP complements, PP complements can be extracted to form wh-questions, as in sentence (152). (152) [To which dog]i did Beth Ann throw the frisbee ǫi? As can be seen in the tree in Figure 13.5, extraction of PP complements is very similar to extraction of NP complements from the same positions. The PP extraction trees diﬀer from NP extraction trees in having a PP rather than an NP left daughter node under Sq and in having the ǫ ﬁll a PP rather than an NP position in the VP. In other respects these PP extraction structures behave like the NP extractions, including being used for topicalization. 124 CHAPTER 13. EXTRACTION Sq NP Sr NP0 VP V NP1 PP2 P2 NP2 NA ε Figure 13.4: Ditransitive with PP tree with the object of the PP extracted: αW2nx0Vnx1pnx2 Sq PP2 Sr NP0 VP V NP1 PP NA ε Figure 13.5: Ditransitive with PP with PP extraction tree: αpW2nx0Vnx1pnx2 13.6 Wh-moved S complement Except for the node label on the extracted position, the trees for wh-questions on S complements look exactly like the trees for wh-questions on NPs in the same positions. This is because there is no separate wh-lexical item for clauses in English, so the item what is ambiguous between representing a clause or an NP. To illustrate this ambiguity notice that the question in (153) could be answered by either a clause as in (154) or an NP as in (155). The extracted NP in these trees is constrained to be wh, since sentential complements can not be topicalized. (153) What does Clove want? (154) for Beth Ann to play frisbee with her (155) a biscuit 13.7. WH-MOVED ADJECTIVE COMPLEMENT 125 13.7 Wh-moved Adjective complement In subcategorizations that select an adjective complement, that complement can be questioned in a wh-question, as in sentence (156). (156) Howi did he feel ǫi? Sq AP Sr NP0  VP V AP1 NA ε Figure 13.6: Predicative Adjective tree with extracted adjective: αWA1nx0Vax1 The tree families with adjective complements include trees for such adjective extractions that are very similar to the wh-extraction trees for other categories of complements. The adjective position in the VP is ﬁlled by an ǫ and the trace feature of the adjective complement and of the adjective daughter of Sq are co-indexed. The extracted adjective is required to be wh2, so no topicalizations are allowed. An example of this type of tree is shown in Figure 13.6. 2How is the only wh adjective currently in the XTAG English grammar. Chapter 14 Relative Clauses Relative clauses are NP modiﬁers, which involve extraction of an argument or an adjunct. The NP head (the portion of the NP being modiﬁed by the relative clause) is not directly related to the extracted element. For example in (157), the person is the head NP and is modiﬁed by the relative clause whose mother ǫ likes Chris. The person is not interpreted as the subject of the relative clause which is missing an overt subject. In other cases, such as (158), the relationship between the head NP export exhibitions may seem to be more direct but even there we assume that there are two independent relationships: one between the entire relative clause and the NP it modiﬁes, and another between the extracted element and its trace. The extracted element may be an overt wh-phrase as in (157) or a covert element as in (158). (157) the person whose mother likes Chris (158) export exhibitions that included high-tech items Relative clauses are represented in the English XTAG grammar by auxiliary trees that adjoin to NPs. These trees are anchored by the verb in the clause and appear in the appropriate tree families for the various verb subcategorizations. Within a tree family there will be groups of relative clause trees based on the declarative tree and each passive tree. Within each of these groups, there is a separate relative clause tree corresponding to each possible argument that can be extracted from the clause. There is no relationship between the extracted position and the head NP. The relationship between the relative clause and the head NP is treated as a semantic relationship which will be provided by any reasonable compositional theory. The relationship between the extracted element (which can be covert) is captured by co-indexing the trace features of the extracted NP and the NPw node in the relative clause tree. If for example, it is NP0 that is extracted, we have the following feature equations: NPw.t: trace  NP0.t: trace  NPw.t: case  NP0.t: case  NPw.t: agr  NP0.t: agr  1 1 No adjunct traces are represented in the XTAG analysis of adjunct extraction. Relative clauses on adjuncts do not have traces and consequently feature equations of the kind shown here are not present. 126 127 Representative examples from the transitive tree family are shown with a relevant subset of their features in Figures 14.1(a) and 14.1(b). Figure 14.1(a) involves a relative clause with a covert extracted element, while ﬁgure 14.1(b) involves a relative clause with an overt wh-phrase.2 NPr NPf  NA agr : 12 case : 13 wh: 14 case : nomacc Sp NA NPw NA agr : 1 case : 2 trace : 3 εw Sq NA Comp rel-pron : 4 select-mode : 5 infind assign-case : 6 Sr NP0  VP V likes NP1 NA case : acc agr : 1 case : 2 trace : 3 ε NPr NPf  NA case : 10 agr : 11 wh: 12 case : nomacc Sp NA NPw  select-mode : 1 ind wh:  agr : 2 case : 3 trace : 4 Sq NA Comp NA εc Sr NP0 NA case : 8 agr : 9 agr : 2 case : 3 trace : 4 ε VP V likes NP1  (a) (b) Figure 14.1: Relative clause trees in the transitive tree family: βNc1nx0Vnx1 (a) and βN0nx0Vnx1 (b) The above analysis is essentially identical to the GB analysis of relative clauses. One aspect of its implementation is that an covert  wh NP and a covert Comp have to be introduced. See (159) and (160) for example. (159) export exhibitions [ [NPwǫ]i [ that [ ǫi included high-tech items]]] (160) the export exhibition [ [NPwǫ]i [ ǫC [Muriel planned ǫi]]] The lexicalized nature of XTAG makes it problematic to have trees headed by null strings. Of the two null trees, NPw and Comp, that we could postulate, the former is deﬁnitely more undesirable because it would lead to massive overgeneration, as can be seen in (161) and (162). (161)  [NPwǫ] did John eat the apple? (as a wh-question) 2 The convention followed in naming relative clause trees is outlined in Appendix D. 128 CHAPTER 14. RELATIVE CLAUSES (162)  I wonder [[NPwǫ] Mary likes John](as an indirect question) The presence of an initial headed by a null Comp does not lead to problems of overgeneration because relative clauses are the only environment with a Comp substitution node. 3 Consequently. our treatment of relative clauses has diﬀerent trees to handle relative clauses with an overt extracted wh-NP and relative clauses with a covert extracted wh-NP. Relative clauses with an overt extracted wh-NP involve substitution of a wh NP into the NPw node 4 and have a Comp node headed by ǫC built in. Relative clauses with a covert extracted wh-NP have a NPw node headed by ǫw built in and involve substitution into the Comp node. The Comp node that is introduced by substitution can be the ǫC (null complementizer), that, and for. For example, the tree shown in Figure 14.1(b) is used for the relative clauses shown in sentences (163)-(164), while the tree shown in Figure 14.1(a) is used for the relative clauses in sentences (165)-(168). (163) the man who Muriel likes (164) the man whose mother Muriel likes (165) the man Muriel likes (166) the book for Muriel to read (167) the man that Muriel likes (168) the book Muriel is reading Cases of PP pied-piping (cf. 169) are handled in a similar fashion by building in a PPw node. (169) the demon by whom Muriel was chased See the tree in Figure 14.2. 3Complementizers in clausal complementation are introduced by adjunction. See section 8.4. 4The feature equation used is NPw.t:wh . Examples of NPs that could substitute under NPw are whose mother, who, whom, and also which but not when and where which are treated as exhaustive wh PPs. 14.1. COMPLEMENTIZERS AND CLAUSES 129 NPr NPf  NA case : 1 agr : 2 wh: 3 case : nomacc Sp NA PPw  wh:  select-mode : 4 indinf Sq NA Comp NA εc Sr NP0  VP V chased NP1  Figure 14.2: Adjunct relative clause tree with PP-pied-piping in the transitive tree family: βNpxnx0Vnx1 14.1 Complementizers and clauses The co-occurrence constraints that exist between various Comps and the clause type of the clause they occur with are implemented through combinations of diﬀerent clause types using the mode feature, the select-mode feature, and the rel-pron feature. Clauses are speciﬁed for the mode feature which indicates the clause type of that clause. Possible values for the mode feature are ind, inf, ppart, ger etc. Comps are lexically speciﬁed for a feature named select-mode. In addition, the select-mode feature of the Comp is equated with the mode feature of its comple- ment S by the following equation: Sr.t:mode  Comp.t:select-mode The lexical speciﬁcations of the Comps are shown below:  ǫC, Comp.t:select-mode indinfgerppart  that, Comp.t:select-mode ind 130 CHAPTER 14. RELATIVE CLAUSES  for, Comp.t:select-mode inf The following examples display the co-occurence constraints which the select-mode speciﬁcations assigned above implement. For ǫC: (170) the book Muriel likes (S.t:mode ind) (171) a book to like (S.t:mode inf) (172) the girl reading the book (S.t:mode ger) (173) the book read by Muriel (S.t:mode ppart) For for: (174) the book for Muriel likes (S.t:mode ind) (175) a book for Mary to like (S.t:mode inf) (176) the girl for reading the book (S.t:mode ger) (177) the book for read by Muriel (S.t:mode ppart) For that: (178) the book that Muriel likes (S.t:mode ind) (179) a book that (Muriel) to like (S.t:mode inf) (180) the girl that reading the book (S.t:mode ger) (181) the book that read by Muriel (S.t:mode ppart) Relative clause trees that have substitution of NPw have the following feature equations: Sr.t:mode  NPw.t:select-mode NPw.t:select-mode ind The examples that follow are intended to provide the rationale for the above setting of features. (182) the boy whose mother chased the cat (Sr.t:mode ind) 14.1. COMPLEMENTIZERS AND CLAUSES 131 (183) the boy whose mother to chase the cat (Sr.t:mode inf) (184) the boy whose mother eaten the cake (Sr.t:mode ppart) (185) the boy whose mother chasing the cat (Sr.t:mode  ger) (186) the boy [whose mother]i Bill believes ǫi to chase the cat (Sr.t: mode ind) The feature equations that appear in trees which have substitution of PPw are: Sr.t:mode  PPw.t:select-mode PPw.t:mode indinf 5 Examples that justify the above feature setting follow. (187) the person [by whom] this machine was invented (Sr.t:mode ind) (188) a baker [in whom]i PRO to trust ǫi (Sr.t:mode  inf) (189) the fork [with which] (Geoﬀrey) eaten the pudding (Sr.t: mode ppart) (190) the person [by whom] (this machine) inventing (Sr.t:mode  ger) 14.1.1 Further constraints on the null Comp ǫC There are additional constraints on where the null Comp ǫC can occur. The null Comp is not permitted in cases of subject extraction unless there is an intervening clause or or the relative clause is a reduced relative (mode  ppartger). This can be seen in (191-194). (191) the toy [ǫi [ǫC [ ǫi likes Dafna]]] (192) the toy [ǫi [ǫC Fred thinks [ ǫi likes Dafna]]] (193) the boy [ǫi [ǫC [ ǫi eating the guava]]] (194) the guava [ǫi [ǫC [ ǫi eaten by the boy]]] 5As is the case for NPw substitution, any wh-PP can substitute under PPw. This is implemented by the following equation: PPw.t:wh   Not all cases of pied-piping involve substitution of PPw. In some cases, the P may be built in. In cases where part of the pied-piped PP is part of the anchor, it continues to function as an anchor even after pied-piping i.e. the P node and the NPw nodes are represented separately. 132 CHAPTER 14. RELATIVE CLAUSES To model this paradigm, the feature rel-pron is used in conjunction with the following equations:  Sr.t:rel-pron  Comp.t:rel-pron  Sr.b:rel-pron  Sr.b:mode  Comp.b:rel-pron ppartgeradj-clause (for ǫC) The full set of the equations shown above is only present in Comp substitution trees involving subject extraction. So (195) will not be ruled out. (195) the toy [ǫi [ǫC [ Dafna likes ǫi ]]] The feature mismatch induced by the above equations is not remedied by adjunction of just any S-adjunct because all other S-adjuncts are transparent to the rel-pron feature because of the following equation: Sm.b:rel-pron  Sf.t:rel-pron 14.2 Reduced Relatives Reduced relatives are permitted only in cases of subject-extraction. Past participial reduced relatives are only permitted on passive clauses. See (196-203). (196) the toy [ǫi [ǫC [ ǫi playing the banjo]]] (197) the instrument [ǫi [ǫC [ Amis playing ǫi ]]] (198) the day [ǫw [ǫC [ Amis playing the banjo]]] (199) the apple [ǫi [ǫC [ ǫi eaten by Dafna]]] (200) the child [ǫi [ǫC [ the apple eaten by ǫi ]]] (201) the day [ǫw [ǫC [ Amis eaten the apple]]] (202) the apple [ǫi [ǫC [ Dafna eaten ǫi ]]] (203) the child [ǫi [ǫC [ ǫi eaten the apple ]]] These restrictions are built into the mode speciﬁcations of S.t. So non-passive cases of subject extraction have the following feature equation: Sr.t:mode  indgerinf Passive cases of subject extraction have the following feature equation: Sr.t:mode  indgerppartinf Finally, all cases of non-subject extraction have the following feature equation: Sr.t:mode  indinf 14.3. EXTERNAL SYNTAX 133 14.2.1 Restrictive vs. Non-restrictive relatives The English XTAG grammar does not contain any syntactic distinction between restrictive and non-restrictive relatives because we believe this to be a semantic andor pragmatic diﬀerence. 14.3 External syntax A relative clause can combine with the NP it modiﬁes in at least the following two ways: (204) [the [toy [ǫi [ǫC [Dafna likes ǫi ]]]]] (205) [[the toy] [ǫi [ǫC [Dafna likes ǫi ]]]] Based on cases like (206) and (207), which are problematic for the structure in (204), the structure in (205) is adopted. (206) [[the man and the woman] [who met on the bus]] (207) [[the man and the woman] [who like each other]] As it stands, the RC analysis sketched so far will combine in two ways with the Determiner tree shown in Figure (52), 6 giving us both the possiblities shown in (204) and (205). In order to block the structure exempliﬁed in (204), the feature rel-clause is used in combination with the following equations. On the RC: NPr.b:rel-clause   On the Determiner tree: NPf.t:rel-clause   Together, these equations block introduction of the determiner above the relative clause. 14.4 Other Issues 14.4.1 Interaction with adjoined Comps The XTAG analysis now has two diﬀerent ways of introducing a complementizer like that or for, depending upon whether it occurs in a relative clause or in sentential complementation. Relative clause complementizers substitute in (using the tree αComp), while sentential complementizers adjoin in (using the tree βCOMPs). Cases like (208) where both kinds of complementizers illicitly occur together are blocked. (208) the book [ǫwi [that [that [Muriel wrote ǫi]]]] 6The determiner tree shown has the rel-clause feature built in. The RC analysis would give two parses in the absence of this feature. 134 CHAPTER 14. RELATIVE CLAUSES NPr D the NPf NA displ-const : 1 conj : 2 case : 3 nomacc agr : 4 rel-clause :- gerund : - wh : - quan : - gen : - definite : - decreas : - const : - card : - Figure 14.3: Determiner tree with rel-clause feature: βDnx This is accomplished by setting the Sr.t:comp feature in the relative clause tree to nil. The Sr.t:comp feature of the auxiliary tree that introduces (the sentential complementa- tion) that is set to that. This leads to a feature clash ruling out (208). On the other hand, if a sentential complement taking verb is adjoined in at Sr, this feature clash goes away (cf. 209). (209) the book [ǫwi [that Beth thinks [that [Muriel wrote ǫi]]]] 14.4.2 Adjunction on PRO Adjunction on PRO, which would yield the ungrammatical (210) is blocked. (210) I want [[PRO [who Muriel likes] to read a book]]. This is done by specifying the case feature of NPf to be nomacc. The case feature of PRO is null. This leads to a feature clash and blocks adjunction of relative clauses on to PRO. 14.4.3 Adjunct relative clauses Two types of trees to handle adjunct relative clauses exist in the XTAG grammar: one in which there is PPw substitution with a null Comp built in and one in which there is a null NPw 14.5. CASES NOT HANDLED 135 built in and a Comp substitutes in. There is no NPw substitution tree with a null Comp built in. This is because of the contrast between (211) and (212). (211) the day [[on whose predecessor] [ǫC [Muriel left]]] (212) the day [[whose predecessor] [ǫC [Muriel left]]] In general, adjunct relatives are not possible with an overt NPw. We do not consider (213) and (214) to be counterexamples to the above statements because we consider where and when to be exhaustive PPs that head a PP initial tree. (213) the place [where [ǫC [Muriel wrote her ﬁrst book]]] (214) the time [when [ǫC [Muriel lived in Bryn Mawr]]] 14.4.4 ECM Cases where for assigns exceptional case (cf. 215, 216) are handled. (215) a book [ǫwi [for [Muriel to read ǫi]]] (216) the time [ǫwi [for [Muriel to leave Haverford]]] The assignment of case by for is implemented by a combination of the following equations. Comp.t:assign-case acc Sr.t:assign-case Comp.t:assign-case Sr.b:assign-case NP0.t:case 14.5 Cases not handled 14.5.1 Partial treatment of free-relatives Free relatives are only partially handled. All free relatives on non-subject positions and some free relatives on subject positions are handled. The structure assigned to free relatives treats the extracted wh-NP as the head NP of the relative clause. The remaining relative clause modiﬁes this extracted wh-NP (cf. 217-219). (217) what(ever) [ǫwi [ǫC [Mary likes ǫi]]] (218) where(ever) [ǫw [ǫC [Mary lives]]] (219) who(ever) [ǫwi [ǫC [Muriel thinks [ǫi likes Mary]]]] However, simple subject extractions without further emebedding are not handled (cf. 220). (220) who(ever) [ǫwi [ǫC [ǫi likes Bill]]] This is because (219) is treated exactly like the ungrammatical (221). (221) the person [ ǫwi [ǫC [ǫi likes Bill]]] 136 CHAPTER 14. RELATIVE CLAUSES 14.5.2 Adjunct P-stranding The following cases of adjunct preposition stranding are not handled (cf. 222, 223). (222) the pen Muriel wrote this letter with (223) the street Muriel lives on Adjuncts are not built into elementary trees in XTAG. So there is no clean way to represent adjunct preposition stranding. A better solution is, probably , available if we make use of multi-component adjunction. 14.5.3 Overgeneration The following ungrammatical sentences are currently being accepted by the XTAG grammar. This is because no clean and conceptually attractive way of ruling them out is obvious to us. 14.5.3.1 how as wh-NP In standard American English, how is not acceptable as a relative pronoun (cf. 224). (224) the way [how [ǫC [PRO to solve this problem]]] However, (224) is accepted by the current grammar. The only way to rule (224) out would be to introduce a special feature devoted to this purpose. This is unappealing. Further, there exist speech registersdialects of English, where (224) is acceptable. 14.5.3.2 for-trace eﬀects (225) is ungrammatical, being an instance of a violation of the for-trace ﬁlter of early transfor- mational grammar. (225) the person [ǫwi [for [ǫi to read the book]]] The XTAG grammar currently accepts (225).7 14.5.3.3 Internal head constraint Relative clauses in English (and in an overwhelming number of languages) obey a no internal head constraint. This constraint is exempliﬁed in the contrast between (226) and (227). (226) the person [whoi [ǫC Muriel likes ǫi]] (227) the person [[which person]i [ǫC Muriel likes ǫi]] We know of no good way to rule (227) out, while still ruling (228) in. 7It may be of some interest that (225) is acceptable in certain dialects of Belfast English. 14.5. CASES NOT HANDLED 137 (228) the person [[whose mother]i [ǫC Muriel likes ǫi]] Dayal (1996) suggests that full NPs such as which person and whose mother are R- expressions while who and whose are pronouns. R-expressions, unlike pronouns, are subject to Condition C. (226) is, then, ruled out as a violation of Condition C since the person and which person are co-indexed and the person c-commands which person. If we accept Dayals ar- gument, we have a principled reason for allowing overgeneration of relative clauses that violate the internal head constraint, the reason being that the XTAG grammar does generate binding theory violations. 14.5.3.4 Overt Comp constraint on stacked relatives Stacked relatives of the kind in (229) are handled. (229) [[the book [that Bill likes]] [which Mary wrote]] There is a constraint on stacked relatives: all but the relative clause closest to the head-NP must have either an overt Comp or an overt NPw. Thus (230) is ungrammatical. (230) [[the book [that Bill likes]] [Mary wrote]] Again, no good way of handling this constraint is known to us currently. Chapter 15 Adjunct Clauses Adjunct clauses include subordinate clauses (i.e. those with overt subordinating conjunctions), purpose clauses and participial adjuncts. Subordinating conjunctions each select four trees, allowing them to appear in four diﬀerent positions relative to the matrix clause. The positions are (1) before the matrix clause, (2) after the matrix clause, (3) before the VP, surrounded by two punctuation marks, and (4) after the matrix clause, separated by a punctuation mark. Each of these trees is shown in Figure 15.1. Sr S1 NA P because S Sf VPr VPf NA PP P1 P in N order S VPr Punct1 PP P P1 as P2 if S Punct2 VP NA Sr Sf NA Punct PP P when S (1) βPss (2) βvxPNs (3) βpuPPspuvx (4) βspuPs Figure 15.1: Auxiliary Trees for Subordinating Conjunctions Sentence-initial adjuncts adjoin at the root S of the matrix clause, while sentence-ﬁnal adjuncts adjoin at a VP node. In this, the XTAG analysis follows the ﬁndings on the attach- ment sites of adjunct clauses for conditional clauses ([Iatridou, 1991]) and for inﬁnitival clauses ([Browning, 1987]). One compelling argument is based on Binding Condition C eﬀects. As can be seen from examples (231)-(233) below, no Binding Condition violation occurs when the adjunct is sentence initial, but the subject of the matrix clause clearly governs the adjunct clause when it is in sentence ﬁnal position and co-indexation of the pronoun with the subject of the adjunct clause is impossible. (231) Unless shei hurries, Maryi will be late for the meeting. 138 15.1. BARE ADJUNCT CLAUSES 139 (232) Shei will be late for the meeting unless Maryi hurries. (233) Maryi will be late for the meeting unless shei hurries. We had previously treated subordinating conjunctions as a subclass of conjunction, but are now assigning them the POS preposition, as there is such clear overlap between words that function as prepositions (taking NP complements) and subordinating conjunctions (taking clausal complements). While there are some prepositions which only take NP complements and some which only take clausal complements, many take both as shown in examples (234)-(237), and it seems to be artiﬁcial to assign them two diﬀerent parts-of-speech. (234) Helen left before the party. (235) Helen left before the party began. (236) Since the election, Bill has been elated. (237) Since winning the election, Bill has been elated. Each subordinating conjunction selects the values of the mode and comp features of the subordinated S. The mode value constrains the types of clauses the subordinating conjunction may appear with and the comp value constrains the complementizers which may adjoin to that clause. For instance, indicative subordinate clauses may appear with the complementizer that as in (238), while participial clauses may not have any complementizers (239). (238) Midge left that car so that Sam could drive to work. (239) Since that seeing the new VW, Midge could think of nothing else. 15.0.4 Multi-word Subordinating Conjunctions We extracted a list of multi-word conjunctions, such as as if, in order, and for all (that), from [Quirk et al., 1985]. For the most part, the components of the complex are all anchors, as shown in Figures 15.2(a). In one case, as ADV as, there is a great deal of latitude in the choice of adverb, so this is a substitution site (Figures 15.2(b)). This multi-anchor treatment is very similar to that proposed for idioms in [Abeille and Schabes, 1989], and the analysis of light verbs in the XTAG grammar (see section 6.15). 15.1 Bare Adjunct Clauses Bare adjunct clauses do not have an overt subordinating conjunction, but are typically parallel in meaning to clauses with subordinating conjunctions. For this reason, we have elected to handle them using the same trees shown above, but with null anchors. They are selected at the same time and in the same way the PRO tree is, as they all have PRO subjects. Three values of mode are licensed: inf (inﬁnitive), ger (gerundive) and ppart (past participal).1 They 1We considered allowing bare indicative clauses, such as He died that others may live, but these were considered too archaic to be worth the additional ambiguity they would add to the grammar. 140 CHAPTER 15. ADJUNCT CLAUSES VPr VPf NA PP P P1 as Ad soon P2 as S VPr VPf NA PP P P1 as Ad P2 as S (a) (b) Figure 15.2: Trees Anchored by Subordinating Conjunctions: βvxPARBPs and βvxParbPs interact with complementizers as follows:  Participial complements do not license any complementizers:2 (240) [Destroyed by the ﬁre], the building still stood. (241) The ﬁre raged for days [destroying the building]. (242) [That destroyed by the ﬁre], the building still stood.  Inﬁnitival adjuncts, including purpose clauses, are licensed both with and without the complementizer for. (243) Harriet bought a Mustang [to impress Eugene]. (244) [To impress Harriet], Eugene dyed his hair. (245) Traﬃc stopped [for Harriet to cross the street]. 2While these sound a bit like extraposed relative clauses (see [Kroch and Joshi, 1987]), those move only to the right and adjoin to S; as these clauses are equally grammatical both sentence-initially and sentence-ﬁnally, we are analyzing them as adjunct clauses. 15.2. DISCOURSE CONJUNCTION 141 Sm Sr NP PRO VP V destroyed PP P by NP DetP D the N fire Sf NA NP DetP D the N building VPr Ad still VP NA V stood Sr NP DetP D the N fire VPm VP NA VP NA V raged PP P for NP N days Sr NP PRO VP1 V destroying NP DetP D the N building (a) (b) Figure 15.3: Sample Participial Adjuncts 15.2 Discourse Conjunction The CONJs auxiliary tree is used to handle discourse conjunction, as in sentence (246). Only the coordinating conjunctions (and, or and but) are allowed to adjoin to the roots of matrix sentences. Discourse conjunction with and is shown in the derived tree in Figure 15.4. (246) And Truﬀula trees are what everyone needs! [Seuss, 1971] 142 CHAPTER 15. ADJUNCT CLAUSES Sc Conj and Sr NA NP Nr N Truffula Nf NA trees VPr V are VP NA V ε NPr NA NP N what Sr NP N everyone VP V needs NP1 NA ε Figure 15.4: Example of discourse conjunction, from Seuss The Lorax Chapter 16 Imperatives Imperatives in English do not require overt subjects. The subject in imperatives is second person, i.e. you, whether it is overt or not, as is clear from the verbal agreement and the interpretation. Imperatives with overt subjects can be parsed using the trees already needed for declaratives. The imperative cases in which the subject is not overt are handled by the imperative trees discussed in this section. The imperative trees in English XTAG grammar are identical to the declarative tree except that the NP0 subject position is ﬁlled by an ǫ, the NP0 agr pers feature is set in the tree to the value 2nd and the mode feature on the root node has the value imp. The value for agr pers is hardwired into the epsilon node and insures the proper verbal agreement for an imperative. The mode value of imp on the root node is recognized as a valid mode for a matrix clause. The imp value for mode also allows imperatives to be blocked from appearing as embedded clauses. Figure 16.1 is the imperative tree for the transitive tree family. 143 144 CHAPTER 16. IMPERATIVES S r displ-const : set1 : - assign-comp : inf_nilind_nil progressive : 11 perfect : 12 passive : 13 conditional : 14 assign-comp : 15 assign-case : 9 agr : 10 tense : 16 mode : 17 inv : - displ-const : set1 : 18 comp : nil extracted : - NP 0 N A wh : - case : 9 nom agr : 10 num : plursing 3rdsing : - pers : 2 ε VP refl-obj : 8 progressive : 11 perfect : 12 passive : 13 conditional : 14 assign-comp : 15 assign-case : 9 agr : 10 tense : 16 pres mode : 17 displ-const : set1 : 18 refl-obj : 1 mode : imp mainv : 2 tense : 3 assign-comp : 4 assign-case : 5 agr : 6 passive : 7 displ-const : set1 : - V  refl-obj : 1 trans :  mainv : 2 tense : 3 assign-comp : 4 assign-case : 5 agr : 6 passive : 7 - mode : base NP 1  refl : 8 case : acc Figure 16.1: Transitive imperative tree: αInx0Vnx1 Chapter 17 Gerund NPs There are two types of gerunds identiﬁed in the linguistics literature. One is the class of derived nominalizations (also called nominal gerundives or action nominalizations) exempliﬁed in (247), which instantiates the direct object within an of PP. The other is the class of so- called sentential or VP gerundives exempliﬁed in (248). In the English XTAG grammar, the derived nominalizations are termed determiner gerunds, and the sentential or VP gerunds are termed NP gerunds. (247) Some think that the selling of bonds is beneﬁcial. (248) Are private markets approving of Washington bashing Wall Street? Both types of gerunds exhibit a similar distribution, appearing in most places where NPs are allowed.1 The bold face portions of sentences (249)(251) show examples of gerunds as a subject and as the object of a preposition. (249) Avoiding such losses will take a monumental eﬀort. (250) Mr. Nolens wandering doesnt make him a weirdo. (251) Are private markets approving of Washington bashing Wall Street? The motivation for splitting the gerunds into two classes is semantic as well as structural in nature. Semantically, the two gerunds are in sharp contrast with each other. NP gerunds refer to an action, i.e., a way of doing something, whereas determiner gerunds refer to a fact. Structurally, there are a number of properties (extensively discussed in [Lees, 1960]) that show that NP gerunds have the syntax of verbs, whereas determiner gerunds have the syntax of basic nouns. Firstly, the fact that the direct object of the determiner gerund can only appear within an of PP suggests that the determiner gerund, like nouns, is not a case assigner and needs insertion of the preposition of for assignment of case to the direct object. NP gerunds, like 1an exception being the NP positions in equative BE sentences, such as, John is my father. 145 146 CHAPTER 17. GERUND NPS verbs, need no such insertion and can assign case to their direct object. Secondly, like nouns, only determiner gerunds can appear with articles (cf. example (252) and (253)). Thirdly, determiner gerunds, like nouns, can be modiﬁed by adjectives (cf. example (254)), whereas NP gerunds, like verbs, resist such modiﬁcation (cf. example (255)). Fourthly, nouns, unlike verbs, cannot co-occur with aspect (cf. example (256) and (257)). Finally, only NP gerunds, like verbs, can take adverbial modiﬁcation (cf. example (258) and (259)). (252) . . . the proving of the theorem. . . . (det ger with article) (253)  . . . the proving the theorem. . . . (NP ger with article) (254) Johns rapid writing of the book. . . . (det ger with Adj) (255)  Johns rapid writing the book. . . . (NP ger with Adj) (256)  Johns having written of the book. . . . (det ger with aspect) (257) John having written the book. . . . (NP ger with aspect) (258)  His writing of the book rapidly. . . . (det ger with Adverb) (259) His writing the book rapidly. . . . (NP ger with Adverb) In English XTAG, the two types of gerunds are assigned separate trees within each tree family, but in order to capture their similar distributional behavior, both are assigned NP as the category label of their top node. The feature gerund   distinguishes gerund NPs from regular NPs where needed.2 The determiner gerund and the NP gerund trees are discussed in section (17.1) and (17.2) respectively. 17.1 Determiner Gerunds The determiner gerund tree in Figure 17.1 is anchored by a V, capturing the fact that the gerund is derived from a verb. The verb projects an N and instantiates the direct object as an of PP. The nominal category projected by the verb can now display all the syntactic properties of basic nouns, as discussed above. For example, it can be straightforwardly modiﬁed by adjectives; it cannot co-occur with aspect; and it can appear with articles. The only diﬀerence of the determiner gerund nominal with the basic nominals is that the former cannot occur without the determiner, whereas the latter can. The determiner gerund tree therefore has an 17.1. DETERMINER GERUNDS 147 NP agr : 3rdsing :  pers : 3 num : sing case : nomacc wh : 3 decreas : 4 gen : 5 card : 6 quan : 7 definite : 8 const : 9 D wh : 3 decreas : 4 gen : 5 card : 6 quan : 7 definite : 8 const : 9 N V mode : ger PP1 wh : 1 assign-case : 2 P1 assign-case : 2 assign-case : acc of NP1 wh : 1 case : 2 αDnx0Vnx1 Figure 17.1: Determiner Gerund tree from the transitive tree family: αDnx0Vnx1 initial D modifying the N.3 It is used for gerunds such as the ones in bold face in sentences (260), (261) and (262). The D node can take a simple determiner (cf. example (260)), a genitive pronoun (cf. 2This feature is also needed to restrict the selection of gerunds in NP positions. For example, the subject and object NPs in the equative BE tree (Tnx0BEnx1) cannot be ﬁlled by gerunds, and are therefore assigned the feature gerund  , which prevents gerunds (which have the feature gerund  ) from substituting into these NP positions. 3Note that the determiner can adjoin to the gerund only from within the gerund tree. Adjunction of deter- miners to the gerund root node is prevented by constraining determiners to only select NPs with the feature gerund  . This rules out sentences like Private markets approved of (the) [the selling of bonds]. 148 CHAPTER 17. GERUND NPS example (261)), or a genitive NP (cf. example (262)).4 (260) Some think that the selling of bonds is beneﬁcial. (261) His painting of Mona Lisa is highly acclaimed. (262) Are private markets approving of Washingtons bashing of Wall Street? 17.2 NP Gerunds NP gerunds show a number of structural peculiarities, the crucial one being that they have the internal properties of sentences. In the English XTAG grammar, we adopt a position similar to that of [Rosenbaum, 1967] and [Emonds, 1970]  that these gerunds are NPs exhaustively dominating a clause. Consequently, the tree assigned to the transitive NP gerund tree (cf. Figure 17.2) looks exactly like the declarative transitive tree for clauses except for the root node label and the feature values. The anchoring verb projects a VP. Auxiliary adjunction is allowed, subject to one constraint  that the highest verb in the verbal sequence be in gerundive form, regardless of whether it is a main or auxiliary verb. This constraint is implemented by requiring the topmost VP node to be mode  ger. In the absence of any adjunction, the anchoring verb itself is forced to be gerundive. But if the verbal sequence has more than one verb, then the sequence and form of the verbs is limited by the restrictions that each verb in the sequence imposes on the succeeding verb. The nature of these restrictions for sentential clauses, and the manner in which they are implemented in XTAG, are both discussed in Chapter 20. The analysis and implementation discussed there diﬀers from that required for gerunds only in one respect  that the highest verb in the verbal sequence is required to be mode  ger. Additionally, the subject in the NP gerund tree is required to have caseaccnonegen, i.e., it can be either a PRO (cf. example 263), a genitive NP (cf. example 264), or an accusative NP (cf. example 265). The whole NP formed by the gerund can occur in either nominative or accusative positions. (263) . . . John does not like wearing a hat. (264) Are private markets approving of Washingtons bashing Wall Street? (265) Mother disapproved of me wearing such casual clothes. One question that arises with respect to gerunds is whether there is anything special about their distribution as compared to other types of NPs. In fact, it appears that gerund NPs can occur in any NP position. Some verbs might not seem to be very accepting of gerund NP arguments, as in (266) below, but we believe this to be a semantic incompatibility rather than a syntactic problem since the same structures are possible with other lexical items. 4The trees for genitive pronouns and genitive NPs have the root node labelled as D because they seem to function as determiners and as such, sequence with the rest of the determiners. See Chapter 18 for discussion on determiner trees. 17.3. GERUND PASSIVES 149 NPr gerund :  displ-const : set1 : 3 agr : 3rdsing :  pers : 3 num : sing case : nomacc wh : 4 NP0 wh : 4 case : accnonegen VP mode : ger displ-const : set1 : 3 - passive : 1 mode : 2 compar : - displ-const : set1 : - V passive : 1 - mode : 2 NP1 case : acc αGnx0Vnx1 Figure 17.2: NP Gerund tree from the transitive tree family: αGnx0Vnx1 (266) ? [NP Johns tinkeringNP ] ran. (267) [NP Johns tinkeringNP ] worked. By having the root node of gerund trees be NP, the gerunds have the same distribution as any other NP in the English XTAG grammar without doing anything exceptional. The clause structure is captured by the form of the trees and by inclusion in the tree families. 17.3 Gerund Passives It was mentioned above that the NP gerunds display certain clausal properties. It is therefore not surprising that they too have their own set of transformationally related structures. For example, NP gerunds allow passivization just like their sentential counterparts (cf. examples (268) and (269)). (268) The lawyers objected to the slanderous book being written by John. 150 CHAPTER 17. GERUND NPS (269) Susan could not forget having been embarrassed by the vicar. In the English XTAG grammar, gerund passives are treated in an almost exactly similar fashion to sentential passives, and are assigned separate trees within the appropriate tree fam- ilies. The passives occur in pairs, one with the by phrase, and another without it. There are two feature restrictions imposed on the passive trees: (a) only verbs with mode  ppart (i.e., verbs with passive morphology) can be the anchors, and (b) the highest verb in the verb sequence is required to be mode  ger. The two restrictions, together, ensure the selection of only those sequences of auxiliary verb(s) that select mode  ppart and passive   (such as being or having been but NOT having). The passive trees are assumed to be related to only the NP gerund trees (and not the determiner gerund trees), since passive structures involve movement of some object to the subject position (in a movement analysis). Also, like the sentential passives, gerund passives are found in most tree families that have a direct object in the declarative tree. Figure 17.3 shows the gerund passive trees for the transitive tree family. NPr gerund :  wh : 6 displ-const : set1 : 5 agr : 3rdsing :  pers : 3 num : sing case : nomacc NP1 wh : 6 case : accnonegen VP mode : ger displ-const : set1 : 5 - compar : - displ-const : set1 : - passive : 3 mode : 4 V trans :  passive : 3  mode : 4 ppart PP0 wh : 1 assign-case : 2 P0 assign-case : 2 assign-case : acc by NP0 wh : 1 case : 2 NPr gerund :  wh : 4 displ-const : set1 : 3 agr : 3rdsing :  pers : 3 num : sing case : nomacc NP1 case : accnonegen wh : 4 VP mode : ger displ-const : set1 : 3 - passive : 1 mode : 2 displ-const : set1 : - compar : - V trans :  passive : 1  mode : 2 ppart (a) αGnx1Vbynx0 (b) αGnx1V Figure 17.3: Passive Gerund trees from the transitive tree family: αGnx1Vbynx0 (a) and αGnx1V (b) Part IV Other Constructions 151 Chapter 18 Determiners and Noun Phrases In our English XTAG grammar,1 all nouns select the noun phrase (NP) tree structure shown in Figure 18.1. Common nouns do not require determiners in order to form grammatical NPs. Rather than being ungrammatical, singular countable nouns without determiners are restricted in interpretation and can only be interpreted as mass nouns. Allowing all nouns to head determinerless NPs correctly treats the individuation in countable NPs as a property of determiners. Common nouns have negative(-) values for determiner features in the lexicon in our analysis and can only acquire a positive() value for those features if determiners adjoin to them. Other types of NPs such as pronouns and proper nouns have been argued by Abney [Abney, 1987] to either be determiners or to move to the determiner position because they exhibit determiner-like behavior. We can capture this insight in our system by giving pronouns and proper nouns positive values for determiner features. For example pronouns and proper nouns would be marked as deﬁnite, a value that NPs containing common nouns can only obtain by having a deﬁnite determiner adjoin. In addition to the determiner features, nouns also have values for features such as reﬂexive (reﬂ), case, pronoun (pron) and conjunction (conj). A single tree structure is selected by simple determiners, an auxiliary tree which adjoins to NP. An example of this determiner tree anchored by the determiner these is shown in Figure 18.2. In addition to the determiner features the tree in Figure 18.2 has noun features such as case (see section 4.4.2), the conj feature to control conjunction (see Chapter 21), rel-clause (see Chapter 14) and gerund (see Chapter 17) which prevent determiners from adjoining on top of relative clauses and gerund NPs respectively, and the displ-const feature which is used to simulate multi-component adjunction. Complex determiners such as genitives and partitives also anchor tree structures that adjoin to NP. They diﬀer from the simple determiners in their internal complexity. Details of our treatment of these more complex constructions appear in Sections 18.3 and 18.4. Sequences of determiners, as in the NPs all her dogs or those ﬁve dogs are derived by multiple adjunctions of the determiner tree, with each tree anchored by one of the determiners in the sequence. The order in which the determiner trees can adjoin is controlled by features. This treatment of determiners as adjoining onto NPs is similar to that of [Abeille, 1990], and allows us to capture one of the insights of the DP hypothesis, namely that determiners select NPs as complements. In Figure 18.2 the determiner and its NP complement appear in 1A more detailed discussion of this analysis can be found in [Hockey and Mateyak, 1998]. 153 154 CHAPTER 18. DETERMINERS AND NOUN PHRASES NP compl : 1 gen : 2 definite : 3 decreas : 4 quan : 5 const : 6 card : 7 conj : 8 pron : 9 wh : 10 case : 11 refl : 12 agr : 13 N compl : 1 gen : 2 definite : 3 decreas : 4 quan : 5 const : 6 card : 7 conj : 8 pron : 9 wh : 10 case : 11 nomacc refl : 12 agr : 13 Figure 18.1: NP Tree the conﬁguration that is typically used in LTAG to represent selectional relationships. That is, the head serves as the anchor of the tree and its complement is a sister node in the same elementary tree. The XTAG treatment of determiners uses nine features for representing their properties: deﬁniteness (deﬁnite), quantity (quan), cardinality (card), genitive (gen), decreasing (de- creas), constancy (const), wh, agreement (agr), and complement (compl). Seven of these features were developed by semanticists for their accounts of semantic phenomena ([Keenan and Stavi, 1986], [Barwise and Cooper, 1981], [Partee et al., 1990]), another was developed for 155 NP r wh : 1 decreas : 2 compl : 3 gen : 4 card : 5 quan : 6 definite : 7 const : 8 agr : 9 case : 10 nomacc conj : 11 displ-const : 12 D wh : 1 decreas : 2 compl : 3 gen : 4 card : 5 quan : 6 definite : 7 const : 8 wh : - These NP f NA gerund : - rel-clause : - agr : 9 case : 10 conj : 11 displ-const : 12 Figure 18.2: Determiner Trees with Features a semantic account of determiner negation by one of the authors of this determiner analysis ([Mateyak, 1997]), and the last is the familiar agreement feature. When used together these features also account for a substantial portion of the complex patterns of English determiner sequencing. Although we do not claim to have exhaustively covered the sequencing of deter- miners in English, we do cover a large subset, both in terms of the phenomena handled and in terms of corpus coverage. The XTAG grammar has also been extended to include complex determiner constructions such as genitives and partitives using these determiner features. Each determiner carries with it a set of values for these features that represents its own properties, and a set of values for the properties of NPs to which can adjoin. The features are crucial to ordering determiners correctly. The semantic deﬁnitions underlying the features are given below. Deﬁniteness: Possible Values []. 156 CHAPTER 18. DETERMINERS AND NOUN PHRASES A function f is deﬁnite iﬀ f is non-trivial and whenever f(s)   then it is always the intersection of one or more individuals. [Keenan and Stavi, 1986] Quantity: Possible Values []. If A and B are sets denoting an NP and associated predicate, respectively; E is a domain in a model M, and F is a bijection from M1 to M2, then we say that a determiner satisﬁes the constraint of quantity if DetE1AB  DetE2F(A)F(B). [Partee et al., 1990] Cardinality: Possible Values []. A determiner D is cardinal iﬀ D  cardinal numbers  1. Genitive: Possible Values []. Possessive pronouns and the possessive morpheme (s) are marked gen; all other nouns are gen. Decreasing: Possible Values []. A set of Q properties is decreasing iﬀ whenever st and tQ then sQ. A function f is decreasing iﬀ for all properties f(s) is a decreasing set. A non-trivial NP (one with a Det) is decreasing iﬀ its denotation in any model is decreas- ing. [Keenan and Stavi, 1986] Constancy: Possible Values []. If A and B are sets denoting an NP and associated predicate, respectively, and E is a domain, then we say that a determiner displays constancy if (AB)  E  E then DetEAB  DetEAB. Modiﬁed from [Partee et al., 1990] Complement: Possible Values []. A determiner Q is positive complement if and only if for every set X, there exists a continuous set of possible values for the size of the negated determined set, NOT(QX), and the cardinality of QX is the only aspect of QX that can be negated. (adapted from [Mateyak, 1997]) The wh-feature has been discussed in the linguistics literature mainly in relation to wh- movement and with respect to NPs and nouns as well as determiners. We give a shallow but useful working deﬁnition of the wh-feature below: Wh: Possible Values []. Interrogative determiners are wh; all other determiners are wh. The agr feature is inherently a noun feature. While determiners are not morphologically marked for agreement in English many of them are sensitive to number. Many determiners are semantically either singular or plural and must adjoin to nouns which are the same. For example, a can only adjoin to singular nouns (a dog vs a dogs while many must have plurals (many dogs vs many dog). Other determiners such as some are unspeciﬁed for agreement in our analysis because they are compatible with either singulars or plurals (some dog, some dogs). The possible values of agreement for determiners are: [3sg, 3pl, 3]. 157 Det deﬁnite quan card gen wh decreas const agr compl all        3pl  both        3pl  this        3sg  these        3pl  that        3sg  those        3pl  what        3  whatever        3  which        3  whichever        3  the        3  each        3sg  every        3sg  aan        3sg  some1        3  some2        3pl  any        3sg  another        3sg  few        3pl  a few        3pl  many        3pl  many aan        3sg  several        3pl  various        3pl  sundry        3pl  no        3  neither        3  either        3  GENITIVE        UN2  CARDINAL        3pl3 4 PARTITIVE  -5      UN - Table 18.1: Determiner Features associated with D anchors 158 CHAPTER 18. DETERMINERS AND NOUN PHRASES The determiner tree in Figure 18.2 shows the appropriate feature values for the determiner these, while Table 18.1 shows the corresponding feature values of several other common deter- miners. In addition to the features that represent their own properties, determiners also have features to represent the selectional restrictions they impose on the NPs they take as complements. The selectional restriction features of a determiner appear on the NP footnode of the auxiliary tree that the determiner anchors. The NPf node in Figure 18.2 shows the selectional feature restriction imposed by these6, while Table 18.2 shows the corresponding selectional feature restrictions of several other determiners. 18.1 The Wh-Feature A determiner with a wh feature is always the left-most determiner in linear order since no determiners have selectional restrictions that allow them to adjoin onto an NP with a wh feature value. The presence of a wh determiner makes the entire NP wh, and this is correctly represented by the coindexation of the determiner and root NP nodes values for the wh-feature. Wh determiners selectional restrictions on the NP foot node of their tree only allows them adjoin onto NPs that are wh- or unspeciﬁed for the wh-feature. Therefore ungrammatical sequences such as which what dog are impossible. The adjunction of wh  determiners onto wh pronouns is also prevented by the same mechanism. 18.2 Multi-word Determiners The system recognizes the multi-word determiners a few and many a. The features for a multi- word determiner are located on the parent node of its two components (see Figure 18.3). We chose to represent these determiners as multi-word constituents because neither determiner retains the same set of features as either of its parts. For example, the determiner a is 3sg and few is decreasing, while a few is 3pl and increasing. Additionally, many is 3pl and a displays constancy, but many a is 3sg and does not display constancy. Example sentences appear in (270)-(271).  Multi-word Determiners (270) a few teaspoons of sugar should be adequate . (271) many a man has attempted that stunt, but none have succeeded . 2We use the symbol UN to represent the fact that the selectional restrictions for a given feature are unspeciﬁed, meaning the noun phrase that the determiner selects can be either positive or negative for this feature. 3Except one which is 3sg. 4Except one which is compl. 5A partitive can be either quan or quan-, depending upon the nature of the noun that anchors the partitive. If the anchor noun is modiﬁed, then the quantity feature is determined by the modiﬁers quantity value. 6In addition to this tree, these would also anchor another auxiliary tree that adjoins onto card determiners. 7one diﬀers from the rest of CARD in selecting singular nouns 18.2. MULTI-WORD DETERMINERS 159 Det deﬁn quan card gen wh decreas const agr compl e.g.        3pl  dogs all    UN  UN UN 3pl  these dogs UN UN  UN UN UN UN 3pl UN ﬁve dogs        3pl  dogs both    UN  UN UN 3pl  these dogs        3sg  dog   UN UN    3 UN few dogs thisthat   UN UN    3pl  many dogs UN UN  UN UN UN UN 3sg UN ﬁve dogs        3pl  dogs thesethose   UN UN    3pl UN few dogs UN UN  UN UN UN UN 3pl UN ﬁve dogs whatwhich        3  dog(s) whatever   UN UN    3 UN few dogs whichever UN UN  UN UN UN UN 3 UN many dogs        3  dog(s) the   UN UN    3 UN few dogs        UN  the me   UN UN    3pl  many dogs UN UN  UN UN UN UN 3 UN ﬁve dogs        3sg  dog everyeach   UN UN    3 UN few dogs UN UN  UN UN UN UN 3 UN ﬁve dogs aan        3sg  dog some1,2        3  dog(s) some1 UN UN  UN UN UN UN 3pl UN dogs        3sg  dog any   UN UN    3 UN few dogs UN UN  UN UN UN UN 3 UN ﬁve dogs        3sg  dog another   UN UN    3 UN few dogs UN UN  UN UN UN UN 3 UN ﬁve dogs few        3pl  dogs a few        3pl  dogs many        3pl  dogs many aan        3sg  dog several        3pl  dogs various        3pl  dogs sundry        3pl  dogs no        3  dog(s) neither        3sg  dog either        3sg  dog Table 18.2: Selectional Restrictions Imposed by Determiners on the NP foot node 160 CHAPTER 18. DETERMINERS AND NOUN PHRASES Det deﬁnite quan card gen wh decreas const agr compl        3    UN UN    3 UN GENITIVE   UN UN    3pl  UN UN  UN UN UN UN 3 UN        3pl         3pl  CARDINAL        3pl7  PARTITIVE UN UN UN UN  UN UN UN UN Table 18.3: Selectional Restrictions Imposed by Groups of DeterminersDeterminer Construc- tions NPr D const : 5 definite : 6 quan : 7 card : 8 gen : 9 decreas : 10 wh: 11 agr : 4 wh: - quan :  predet : - gen : - definite : - decreas : - card : - agr : 3rdsing :  pers : 3 num: sing D1 many D2 a NPf  NA displ-const : 1 conj : 2 case : 3 nomacc agr : 4 wh: - quan : - gen : - definite : - decreas : - const : - card : - Figure 18.3: Multi-word Determiner tree: βDDnx 18.3 Genitive Constructions There are two kinds of genitive constructions: genitive pronouns, and genitive NPs (which have an explicit genitive marker, s, associated with them). It is clear from examples such as 18.4. PARTITIVE CONSTRUCTIONS 161 her dog returned home and her ﬁve dogs returned home vs dog returned home that genitive pronouns function as determiners and as such, they sequence with the rest of the determiners. The features for the genitives are the same as for other determiners. Genitives are not required to agree with either the determiners or the nouns in the NPs that they modify. The value of the agr feature for an NP with a genitive determiner depends on the NP to which the genitive determiner adjoins. While it might seem to make sense to take their as 3pl, my as 1sg, and Alfonsos as 3sg, this number and person information only eﬀects the genitive NP itself and bears no relationship to the number and person of the NPs with these items as determiners. Consequently, we have represented agr as unspeciﬁed for genitives in Table 18.1. Genitive NPs are particularly interesting because they are potentially recursive structures. Complex NPs can easily be embedded within a determiner. (272) [[[John]s friend from high school]s uncle]s mother came to town. There are two things to note in the above example. One is that in embedded NPs, the genitive morpheme comes at the end of the NP phrase, even if the head of the NP is at the beginning of the phrase. The other is that the determiner of an embedded NP can also be a genitive NP, hence the possibility of recursive structures. In the XTAG grammar, the genitive marker, s, is separated from the lexical item that it is attached to and given its own category (G). In this way, we can allow the full complexity of NPs to come from the existing NP system, including any recursive structures. As with the simple determiners, there is one auxiliary tree structure for genitives which adjoins to NPs. As can be seen in 18.4, this tree is anchored by the genitive marker s and has a branching D node which accomodates the additional internal structure of genitive determiners. Also, like simple determiners, there is one initial tree structure (Figure 18.5) available for substitution where needed, as in, for example, the Determiner Gerund NP tree (see Chapter 17 for discussion on determiners for gerund NPs). Since the NP node which is sister to the G node could also have a genitive determiner in it, the type of genitive recursion shown in (272) is quite naturally accounted for by the genitive tree structure used in our analysis. 18.4 Partitive Constructions The deciding factor for including an analysis of partitive constructions(e.g. some kind of, all of ) as a complex determiner constructions was the behavior of the agreement features. If partitive constructions are analyzed as an NP with an adjoined PP, then we would expect to get agreement with the head of the NP (as in (273)). If, on the other hand, we analyze them as a determiner construction, then we would expect to get agreement with the noun that the determiner sequence modiﬁes (as we do in (274)). (273) a kind [of these machines] is prone to failure. (274) [a kind of] these machines are prone to failure. 162 CHAPTER 18. DETERMINERS AND NOUN PHRASES NP r decreas : 2 gen : 3 card : 4 quan : 5 definite : 6 const : 7 agr : 1 wh : 8 D decreas : 2 gen : 3 card : 4 quan : 5 definite : 6 const : 7 wh : 8 gen : 9 wh : 10 NP wh : 10 case : nomacc G gen : 9 gen :  NP f agr : 1 Figure 18.4: Genitive Determiner Tree D gen : 1 wh: 2 NP wh: 2 case : nomacc G gen : 1 gen :  Figure 18.5: Genitive NP tree for substitution: αDnxG In other words, for partitive constructions, the semantic head of the NP is the second rather than the ﬁrst noun in linear order. That the agreement shown in (274) is possible suggests that 18.5. ADVERBS, NOUN PHRASES, AND DETERMINERS 163 the second noun in linear order in these constructions should also be treated as the syntactic head. Note that both the partitive and PP readings are usually possible for a particular NP. In the cases where either the partitive or the PP reading is preferred, we take it to be just that, a preference, most appropriately modeled not in the grammar but in a component such as the heuristics used with the XTAG parser for reducing the analyses produced to the most likely. In our analysis the partitive tree in Figure 18.6 is anchored by one of a limited group of nouns that can appear in the determiner portion of a partitive construction. A rough semantic characterization of these nouns is that they either represent quantity (e.g. part, half, most, pot, cup, pound etc.) or classiﬁcation (e.g. type, variety, kind, version etc.). In the absence of a more implementable characterization we use a list of such nouns compiled from a descriptive grammar [Quirk et al., 1985], a thesaurus, and from online corpora. In our grammar the nouns on the list are the only ones that select the partitive determiner tree. Like other determiners, partitives can adjoin to an NP consisting of just a noun ([a certain kind of] machine), or adjoin to NPs that already have determiners ([some parts of] these machines. Notice that just as for the genitives, the complexity and the recursion are contained below the D node and rest of the structure is the same as for simple determiners. 18.5 Adverbs, Noun Phrases, and Determiners Many adverbs interact with the noun phrase and determiner system in English. For example, consider sentences (275)-(282) below. (275) Approximately thirty people came to the lecture. (276) Practically every person in the theater was laughing hysterically during that scene. (277) Only Johns crazy mother can make stuﬃng that tastes so good. (278) Relatively few programmers remember how to program in COBOL. (279) Not every martian would postulate that all humans speak a universal language. (280) Enough money was gathered to pay oﬀ the group gift. (281) Quite a few burglaries occurred in that neighborhood last year. (282) I wanted to be paid double the amount they oﬀered. 164 CHAPTER 18. DETERMINERS AND NOUN PHRASES NP r NA decreas : 5 compl : 6 gen : 7 card : 8 quan : 9 definite : 10 const : 11 agr : 13 wh : 12 case : 14 conj : 15 displ-const : 16 D decreas : 5 compl : 6 gen : 7 card : 8 quan : 9 definite : 10 const : 11 wh : 12 wh : 4 NP wh : 4 wh : 1 case : 2 agr : 3 N wh : 1 case : 2 agr : 3 P of NP f NA rel-clause : - agr : 13 case : 14 nomacc conj : 15 displ-const : 16 Figure 18.6: Partitive Determiner Tree Although there is some debate in the literature as to whether these should be classiﬁed as determiners or adverbs, we believe that these items that interact with the NP and determiner system are in fact adverbs. These items exhibit a broader distribution than either determiners or adjectives in that they can modify many other phrasal categories, including adjectives, verb phrases, prepositional phrases, and other adverbs. 18.5. ADVERBS, NOUN PHRASES, AND DETERMINERS 165 Using the determiner feature system, we can obtain a close approximation to an accurate characterization of the behavior of the adverbs that interact with noun phrases and determiners. Adverbs can adjoin to either a determiner or a noun phrase (see ﬁgure 18.7), with the adverbs restricting what types of NPs or determiners they can modify by imposing feature requirements on the foot D or NP node. For example, the adverb approximately, seen in (275) above, selects for determiners that are card. The adverb enough in (280) is an example of an adverb that selects for a noun phrase, speciﬁcally a noun phrase that is not modiﬁed by a determiner. NPr Dr Ad approximately Df NA thirty NPf NA N people NPr Ad double NPf D the NPf NA N amount (a) (b) Figure 18.7: (a) Adverb modifying a determiner; (b) Adverb modifying a noun phrase Most of the adverbs that modify determiners and NPs divide into six classes, with some minor variation within classes, based on the pattern of these restrictions. Three of the classes are adverbs that modify determiners, while the other three modify NPs. The largest of the ﬁve classes is the class of adverbs that modify cardinal determiners. This class includes, among others, the adverbs about, at most, exactly, nearly, and only. These ad- verbs have the single restriction that they must adjoin to determiners that are card. Another class of adverbs consists of those that can modify the determiners every, all, any, and no. The adverbs in this class are almost, nearly, and practically. Closely related to this class are the ad- verbs mostly and roughly, which are restricted to modifying every and all, and hardly, which can only modify any. To select for every, all, and any, these adverbs select for determiners that are [quan, card-, const, compl], and to select for no, the adverbs choose a determiner that is [quan, decreas, const]. The third class of adverbs that modify determiners are those that modify the determiners few and many, representable by the feature sequences [quan, decreas, const-] and [quan, decreas-, const-, 3pl, compl], respectively. Examples of these adverbs are awfully, fairly, relatively, and very. Of the three classes of adverbs that modify noun phrases, one actually consists of a single adverb not, that only modiﬁes determiners that are compl. Another class consists of the focus adverbs, at least, even, only, and just. These adverbs select NPs that are wh- and card-. For the NPs that are card, the focus adverbs actually modify the cardinal determiner, and so these adverbs are also included in the ﬁrst class of adverbs mentioned in the previous paragraph. The last major class that modify NPs consist of the adverbs double and twice, which select NPs that are [deﬁnite] (i.e., the, thisthatthosethese, and the genitives). 166 CHAPTER 18. DETERMINERS AND NOUN PHRASES Although these restrictions succeed in recognizing the correct determineradverb sequences, a few unacceptable sequences slip through. For example, in handling the second class of adverbs mentioned above, every, all, and any share the features [quan, card-, const, compl] with a and another, and so nearly a man is acceptable in this system. In addition to this over-generation within a major class, the adverb quite selects for determiners and NPs in what seems to be a purely idiosyncratic fashion. Consider the following examples. (283) a. Quite a few members of the audience had to leave. b. There were quite many new participants at this years conference. c. Quite few triple jumpers have jumped that far. d. Taking the day oﬀ was quite the right thing to do. e. The recent negotiation ﬁasco is quite another issue. f. Pandora is quite a cat! In examples (283a)-(283c), quite modiﬁes the determiner, while in (283d)-(283f), quite mod- iﬁes the entire noun phrase. Clearly, it functions in a diﬀerent manner in the two sets of sentences; in (283a)-(283c), quite intensiﬁes the amount implied by the determiner, whereas in (283d)-(283f), it singles out an individual from the larger set to which it belongs. To capture the selectional restrictions needed for (283a)-(283c), we utilize the two sets of features mentioned previously for selecting few and many. However, a few cannot be singled out so easily; using the sequence [quan, card-, decreas-, const, 3pl, compl-], we also accept the ungrammatical NPs quite several members and quite some members (where quite modiﬁes some). In selecting the as in (d) with the features [deﬁnite, gen-, 3sg], quite also selects this and that, which are ungrammatical in this position. Examples (283e) and (283f) present yet another obstacle in that in selecting another and a, quite erroneously selects every and any. It may be that there is an undiscovered semantic feature that would alleviate these diﬃcul- ties. However, on the whole, the determiner feature system we have proposed can be used as a surprisingly eﬃcient method of characterizing the interaction of adverbs with determiners and noun phrases. Chapter 19 Modiﬁers This chapter covers various types of modiﬁers: adverbs, prepositions, adjectives, and noun modiﬁers in noun-noun compounds.1 These categories optionally modify other lexical items and phrases by adjoining onto them. In their modiﬁer function these items are adjuncts; they are not part of the subcategorization frame of the items they modify. Examples of some of these modiﬁers are shown in (284)-(286). (284) [ADV certainly ADV ], the October 13 sell-oﬀ didnt settle any stomachs . (WSJ) (285) Mr. Bakes [ADV previously ADV ] had a turn at running Continental . (WSJ) (286) most [ADJ foreign ADJ] [N government N] [N bond N] [prices] rose [P P during the week P P ]. The trees used for the various modiﬁers are quite similar in form. The modiﬁer anchors the tree and the root and foot nodes of the tree are of the category that the particular anchor modiﬁes. Some modiﬁers, e.g. prepositions, select for their own arguments and those are also included in the tree. The foot node may be to the right or the left of the anchoring modiﬁer (and its arguments) depending on whether that modiﬁer occurs before or after the category it modiﬁes. For example, almost all adjectives appear to the left of the nouns they modify, while prepositions appear to the right when modifying nouns. 19.1 Adjectives In addition to being modiﬁers, adjectives in the XTAG English grammar can be also anchor clauses (see Adjective Small Clauses in Chapter 9). There is also one tree family, Intransitive with Adjective (Tnx0Vax1), that has an adjective as an argument and is used for sentences such as Seth felt happy. In that tree family the adjective substitutes into the tree rather than adjoining as is the case for modiﬁers. As modiﬁers, adjectives anchor the tree shown in Figure 19.1. The features of the N node onto which the βAn tree adjoins are passed through to the top node of the resulting N. The 1Relative clauses are discussed in Chapter 14. 167 168 CHAPTER 19. MODIFIERS Nr gen : 1 definite : 2 decreas : 3 quan : 4 const : 5 card : 6 conj : 7 displ-const : 8 wh : 9 pron : 10 assign-comp : 11 agr : 12 case : 13 A Nf NA gen : 1 definite : 2 decreas : 3 quan : 4 const : 5 card : 6 conj : 7 displ-const : 8 wh : 9 pron : 10 assign-comp : 11 agr : 12 case : 13 Figure 19.1: Standard Tree for Adjective modifying a Noun: βAn null adjunction marker (NA) on the N foot node imposes right binary branching such that each subsequent adjective must adjoin on top of the leftmost adjective that has already adjoined. Due to the NA constraint, a sequence of adjectives will have only one derivation in the XTAG grammar. The adjectives morphological features such as superlative or comparative are in- stantiated by the morphological analyzer. See Chapter 22 for a description of how we handle comparatives. At this point, the treatment of adjectives in the XTAG English grammar does not include selectional or ordering restrictions. Consequently, any adjective can adjoin onto 19.2. NOUN-NOUN MODIFIERS 169 any noun and on top of any other adjective already modifying a noun. All of the modiﬁed noun phrases shown in (287)-(290) currently parse with the same structure shown for colorless green ideas in Figure 19.2. (287) big green bugs (288) big green ideas (289) colorless green ideas (290) ?green big ideas NP Nr A colorless Nf NA A green Nf NA ideas Figure 19.2: Multiple adjectives modifying a noun While (288)-(290) are all semantically anomalous, (290) also suﬀers from an ordering prob- lem that makes it seem ungrammatical in the absence of any licensing context. One could argue that the grammar should accept (287)-(289) but not (290). One of the future goals for the grammar is to develop a treatment of adjective ordering similar to that developed by [Hockey and Mateyak, 1998] for determiners2. An adequate implementation of ordering restrictions for adjectives would rule out (290). 19.2 Noun-Noun Modiﬁers Noun-noun compounding in the English XTAG grammar is very similar to adjective-noun modiﬁcation. The noun modiﬁer tree, shown in Figure 19.3, has essentially the same structure 170 CHAPTER 19. MODIFIERS Nr assign-comp : 1 displ-const : 2 pron : 3 - wh : 4 agr : 5 case : 6 nomacc N case : nomacc pron : - Nf NA assign-comp : 1 displ-const : 2 wh : 4 agr : 5 case : 6 pron : 3 Figure 19.3: Noun-noun compounding tree: βNn (not all features displayed) as the adjective modiﬁer tree in Figure 19.1, except for the syntactic category label of the anchor. Noun compounds have a variety of scope possibilities not available to adjectives, as illus- trated by the single bracketing possibility in (291) and the two possibilities for (292). This ambiguity is manifested in the XTAG grammar by the two possible adjunction sites in the noun-noun compound tree itself. Subsequent modifying nouns can adjoin either onto the Nr node or onto the N anchor node of that tree, which results in exactly the two bracketing pos- sibilities shown in (292). This inherent structural ambiguity results in noun-noun compounds regularly having multiple derivations. However, the multiple derivations are not a defect in the grammar because they are necessary to correctly represent the genuine ambiguity of these phrases. (291) [N big [N green design N]N] (292) [N computer [N furniture design N]N] [N [N computer furniture N] design N] 2See Chapter 18 or [Hockey and Mateyak, 1998] for details of the determiner analysis. 19.3. TIME NOUN PHRASES 171 Noun-noun compounds have no restriction on number. XTAG allows nouns to be either singular or plural as in (293)-(295). (293) Hyun is taking an algorithms course . (294) waﬄes are in the frozen foods section . (295) I enjoy the dog shows . 19.3 Time Noun Phrases Although in general NPs cannot modify clauses or other NPs, there is a class of NPs, with meanings that relate to time, that can do so.3 We call this class of NPs Time NPs. Time NPs behave essentially like PPs. Like PPs, time NPs can adjoin at four places: to the right of an NP, to the right and left of a VP, and to the left of an S. Time NPs may include determiners, as in this month in example (296), or may be single lexical items as in today in example (297). Like other NPs, time NPs can also include adjectives, as in example (301). (296) Elvis left the building this week (297) Elvis left the building today (298) It has no bearing on our work force today (WSJ) (299) The ﬁre yesterday claimed two lives (300) Today it has no bearing on our work force (301) Michael late yesterday announced a buy-back program The XTAG analysis for time NPs is fairly simple, requiring only the creation of proper NP auxiliary trees. Only nouns that can be part of time NPs will select the relevant auxiliary trees, and so only these type of NPs will behave like PPs under the XTAG analysis. Currently, about 60 words select Time NP trees, but since these words can form NPs that include determiners and adjectives, a large number of phrases are covered by this class of modiﬁers. Corresponding to the four positions listed above, time NPs can select one of the four trees shown in Figure 19.4. Determiners can be added to time NPs by adjunction in the same way that they are added to NPs in other positions. The trees in Figure 19.5 show that the structures of examples (296) and (297) diﬀer only in the adjunction of this to the time NP in example (296). The sentence 3 There may be other classes of NPs, such as directional phrases, such as north, south etc., which behave similarly. We have not yet analyzed these phrases. 172 CHAPTER 19. MODIFIERS Sr NP N S NA VPr NP N VP NA VPr VP NA NP N NPr NPf  NA NP N βNs βNvx βvxN βnxN Figure 19.4: Time Phrase Modiﬁer trees: βNs, βNvx, βvxN, βnxN S r NP N Elvis VPr VP NA V left NP r D the NP f NA N building NP r D this NP f NA N week Sr NP N Elvis VPr VP NA V left NPr D the NPf NA N building NP N today Figure 19.5: Time NPs with and without a determiner (302) Esso said the Whiting ﬁeld started production Tuesday (WSJ) has (at least) two diﬀerent interpretations, depending on whether Tuesday attaches to said or to started. Valid time NP analyses are available for both these interpretations and are shown in Figure 19.6. Derived tree structures for examples (298)  (301), which show the four possible time NP positions are shown in Figures 19.7 and 19.8. The derivation tree for example (301) is also shown in Figure 19.8. 19.4. PREPOSITIONS 173 Sr NP N Esso VPr VP NA V said S1 NA NPr D the NPf NA Nr N Whiting Nf NA field VP V started NP N production NP N Tuesday Sr NP N Esso VP V said S1 NA NPr D the NPf NA Nr N Whiting Nf NA field VPr VP NA V started NP N production NP N Tuesday Figure 19.6: Time NP trees: Two diﬀerent attachments Sr NP N it VPr VP NA V has NPr NPf NA D no NPf NA N bearing PP P on NPr D our NPf NA Nr N work Nf NA force NP N today Sr NPr NPf NA D the NPf NA N fire NP N yesterday VP V claimed NPr D two NPf NA N lives Sr NP N today S NA NP N it VP V has NPr NPf NA D no NPf NA N bearing PP P on NPr D our NPf NA Nr N work Nf NA force Figure 19.7: Time NPs in diﬀerent positions (βvxN, βnxN and βNs) 19.4 Prepositions There are three basic types of prepositional phrases, and three places at which they can adjoin. The three types of prepositional phrases are: Preposition with NP Complement, Preposition 174 CHAPTER 19. MODIFIERS Sr NP N Michael VPr NP Nr A late Nf NA yesterday VP NA V announced NPr D a NPf NA Nr N buy-back Nf NA program αnx0Vnx1[announced] αNXN[Michael] (1) βNvx[yesterday] (2) βAn[late] (1.1) αNXN[program] (2.2) βDnx[a] (0) βNn[buy-back] (1) Figure 19.8: Time NPs: Derived tree and Derivation (βNvx position) with Sentential Complement, and Exhaustive Preposition. The three places are to the right of an NP, to the right of a VP, and to the left of an S. Each of the three types of PP can adjoin at each of these three places, for a total of nine PP modiﬁer trees. Table 19.1 gives the tree family names for the various combinations of type and location. (See Section 23.4.2 for discussion of the βspuPnx, which handles post-sentential comma-separated PPs.) position and category modiﬁed pre-sentential post-NP post-VP Complement type S modiﬁer NP modiﬁer VP modiﬁer S-complement βPss βnxPs βvxPs NP-complement βPnxs βnxPnx βvxPnx no complement βPs βnxP βvxP (exhaustive) Table 19.1: Preposition Anchored Modiﬁers The subset of preposition anchored modiﬁer trees in Figure 19.9 illustrates the locations and the four PP types. Example sentences using the trees in Figure 19.9 are shown in (303)-(306). There are also more trees with multi-word prepositions as anchors. Examples of these are: ahead of, contrary to, at variance with and as recently as. 19.5. ADVERBS 175 Sr PP P S Sf NA NPr NPf NA PP P NP VPr NA VP NA PP P VPr VP NA PP P P1 outside P2 of NP βPss βnxPnx βvxP βvxPPnx Figure 19.9: Selected Prepositional Phrase Modiﬁer trees: βPss, βnxPnx, βvxP and βvxPPnx (303) [P P with Clove healthy P P], the veterinarians bill will be more aﬀordable . (βPss4) (304) The frisbee [P P in the brambles P P] was hidden . (βnxPnx) (305) Clove played frisbee [P P outside P P] . (βvxP) (306) Clove played frisbee [P P outside of the house P P] . (βvxPPnx) Prepositions that take NP complements assign accusative case to those complements (see section 4.4.3.1 for details). Most prepositions take NP complements. Subordinating conjunc- tions are analyzed in XTAG as Preps (see Section 15 for details). Additionally, a few non- conjunction prepositions take S complements (see Section 8.8 for details). 19.5 Adverbs In the English XTAG grammar, VP and S-modifying adverbs anchor the auxiliary trees βARBs, βsARB, βvxARB and βARBvx,5 allowing pre and post modiﬁcation of Ss and VPs. Besides the VP and S-modifying adverbs, the grammar includes adverbs that modify other categories. Examples of adverbs modifying an adjective, an adverb, a PP, an NP, and a determiner are shown in (307)-(314). (See Sections 23.1.5 and 23.4.1 for discussion of the βpuARBpuvx and βspuARB, which handle pre-verbal parenthetical adverbs and post-sentential comma-separated adverbs.)  Modifying an adjective (307) extremely good 4Clove healthy is an adjective small clause 5In the naming conventions for the XTAG trees, ARB is used for adverbs. Because the letters in A, Ad, and Adv are all used for other parts of speech (adjective, determiner and verb), ARB was chosen to eliminate ambiguity. Appendix D contains a full explanation of naming conventions. 176 CHAPTER 19. MODIFIERS (308) rather tall (309) rich enough  Modifying an adverb (310) oddly enough (311) very well  Modifying a PP (312) right through the wall  Modifying a NP (313) quite some time  Modifying a determiner (314) exactly ﬁve men XTAG has separate trees for each of the modiﬁed categories and for pre and post modiﬁ- cation where needed. The kind of treatment given to adverbs here is very much in line with the base-generation approach proposed by [Ernst, 1983], which assumes all positions where an adverb can occur to be base-generated, and that the semantics of the adverb speciﬁes a range of possible positions occupied by each adverb. While the relevant semantic features of the adverbs are not currently implemented, implementation of semantic features is scheduled for future work. The trees for adverb anchored modiﬁers are very similar in form to the adjec- tive anchored modiﬁer trees. Examples of two of the basic adverb modiﬁer trees are shown in Figure 19.10. 19.5. ADVERBS 177 Like the adjective anchored trees, these trees also have the NA constraint on the foot node to restrict the number of derivations produced for a sequence of adverbs. Features of the modiﬁed category are passed from the foot node to the root node, reﬂecting correctly that these types of properties are unaﬀected by the adjunction of an adverb. A summary of the categories modiﬁed and the position of adverbs is given in Table 19.2. In the English XTAG grammar, no traces are posited for wh-adverbs, in-line with the base-generation approach ([Ernst, 1983]) for various positions of adverbs. Since convincing arguments have been made against traces for adjuncts of other types (e.g. [Baltin, 1989]), and since the reasons for wanting traces do not seem to apply to adjuncts, we make the general assumption in our grammar that adjuncts do not leave traces. Sentence initial wh-adverbs select the same auxiliary tree used for other sentence initial adverbs (βARBs) with the feature wh. Under this treatment, the derived tree for the sentence How did you fall? is as in Figure (19.11), with no trace for the adverb. S r inv : - wh : 1 displ-const : 2 agr : 3 assign-case : 4 mode : 5 tense : 6 assign-comp : 7 comp : 8 nil Ad wh : 1 S  NA inv : 1 displ-const : 2 agr : 3 assign-case : 4 mode : 5 tense : 6 assign-comp : 7 comp : 8 sub-conj : nil sub-conj : nil comp : nil VPr passive : 1 displ-const : 2 assign-case : 3 assign-comp : 4 tense : 5 agr : 6 mode : 7 VP NA passive : 1 displ-const : 2 assign-case : 3 assign-comp : 4 tense : 5 agr : 6 mode : 7 Ad (a) (b) Figure 19.10: Adverb Trees for pre-modiﬁcation of S: βARBs (a) and post-modiﬁcation of a VP: βvxARB (b) 178 CHAPTER 19. MODIFIERS S r Ad how S NA V did S NA NP N you VPr V ε VP NA V fall Figure 19.11: Derived tree for How did you fall? 19.5. ADVERBS 179 Sr invlink : 3  inv : 3 wh : 1 displ-const : 4 agr : 5 assign-case : 6 mode : 7 tense : 8 assign-comp : 9 comp : 10 nil Adr wh : 1 wh : 2  Ad wh : 2 Adc S NA inv : 3 displ-const : 4 agr : 5 assign-case : 6 mode : 7 tense : 8 assign-comp : 9 comp : 10 sub-conj : nil sub-conj : nil comp : nil Figure 19.12: Complex adverb phrase modiﬁer: βARBarbs 180 CHAPTER 19. MODIFIERS Position with respect to item modiﬁed Category Modiﬁed Pre Post S βARBs βsARB VP βARBvx,βpuARBpuvx βvxARB A βARBa βaARB PP βARBpx βpxARB ADV βARBarb βarbARB NP βARBnx Det βARBd Table 19.2: Simple Adverb Anchored Modiﬁers There is one more adverb modiﬁer tree in the grammar which is not included in Table 19.2. This tree, shown in Figure 19.12, has a complex adverb phrase and is used for wh two-adverb phrases that occur sentence initially, such as in sentence (315). Since how is the only wh adverb, it is the only adverb that can anchor this tree. (315) how quickly did Srini ﬁx the problem ? Focus adverbs such as only, even, just and at least are also handled by the system. Since the syntax allows focus adverbs to appear in practically any position, these adverbs select most of the trees listed in Table 19.2. It is left up to the semantics or pragmatics to decide the correct scope for the focus adverb for a given instance. In terms of the ability of the focus adverbs to modify at diﬀerent levels of a noun phrase, the focus adverbs can modify either cardinal determiners or noun-cardinal noun phrases, and cannot modify at the level of noun. The tree for adverbial modiﬁcation of noun phrases is in shown Figure 19.13(a). In addition to at least, the system handles the other two-word adverbs, at most and up to, and the three-word as-as adverb constructions, where an adjective substitutes between the two occurrences of as. An example of a three-word as-as adverb is as little as. Except for the ability of at least to modify many diﬀerent types of constituents as noted above, the multi-word adverbs are restricted to modifying cardinal determiners. Example sentences using the trees in Figure 19.13 are shown in (316)-(320).  Focus Adverb modifying an NP (316) only a member of our crazy family could pull oﬀ that kind of a stunt . (317) even a ﬂying saucer sighting would seem interesting in comparison with your story . (318) The report includes a proposal for at least a partial impasse in negotiations .  Multi-word adverbs modifying cardinal determiners (319) at most ten people came to the party . 19.5. ADVERBS 181 NPr Ad NPf  Dr NA Ad P1  A P2  Df  NA Dr NA Ad1 P Ad Df  NA βARBnx βPaPd βPARBd (a) (b) (c) Figure 19.13: Selected Focus and Multi-word Adverb Modiﬁer trees: βARBnx, βPARBd and βPaPd (320) They gave monetary gifts of as little as ﬁve dollars . The grammar also includes auxiliary trees anchored by multi-word adverbs like a little, a bit, a mite, sort of, kind of, etc.. Multi-word adverbs like sort of and kind of can pre- modify almost any non-clausal category. The only strict constraint on their occurrence is that they cant modify nouns (in which case an adjectival interpretation would obtain)6. The category which they scope over can be directly determined from their position, except for when they occur sentence ﬁnally in which case they are assumed to modify VPs. The complete list of auxiliary trees anchored by these adverbs are as follows: βNPax, βNPpx, βNPnx, βNPvx, βvxNP, βNParb. Selected trees are shown in Figure 19.14, and some examples are given in (321)-(324). APr Ad1 NA N P APf  NA VPr Ad1 NA N P VPf  NA VPr VPf  NA Ad1 NA N P βNPax βNPvx βvxNP (a) (b) (c) Figure 19.14: Selected Multi-word Adverb Modiﬁer trees (for adverbs like sort of, kind of): βNPax, βNPvx, βvxNP. (321) John is sort of [AP tired]. 6Note that there are semanticlexical constraints even for the categories that these adverbs can modify, and no doubt invite a more in-depth analysis. 182 CHAPTER 19. MODIFIERS (322) John is sort of [P P to the right]. (323) John could have been sort of [V P eating the cake]. (324) John has been eating his cake sort of [ADV slowly]. There are some multi-word adverbs that are, however, not so free in their distribution. Adverbs like a little, a bit, a mite modify APs in predicative constructions (sentences with the copula and small clauses, AP complements in sentences with raising verbs, and APs when they are subcategorized for by certain verbs (e.g., John felt angry). They can also post-modify VPs and PPs, though not as freely as APs7. Finally, they also function as downtoners for almost all adverbials8. Some examples are provided in (325)-(328). (325) Mickey is a little [AP tired]. (326) The medicine [V P has eased Johns pain] a little. (327) John is a little [P P to the right]. (328) John has been reading his book a little [ADV loudly]. Following their behavior as described above, the auxiliary trees they anchor are βDAax, βDApx, βvxDA, βDAarb, βDNax, βDNpx, βvxDN, βDNarb. Selected trees are shown in Figure 19.15). 19.6 Locative Adverbial Phrases Locative adverbial phrases are multi-word adverbial modiﬁers whose meanings relate to spatial location. Locatives consist of a locative adverb (such as ahead or downstream) preceded by an NP, an adverb, or nothing, as in Examples (329)(331) respectively. The modiﬁer as a whole describes a position relative to one previously speciﬁed in the discourse. The nature of the relation, which is usually a direction, is speciﬁed by the anchoring locative adverb(behind, east). If an NP or a second adverb is present in the phrase, it speciﬁes the degree of the relation (for example: three city blocks, many meters, and far). (329) The accident three blocks ahead stopped traﬃc 7They can also appear before NPs, as in, John wants a little sugar. However, here they function as multi-word determiners and should not be analyzed as adverbs. 8It is to be noted that this analysis, which allows these multiword adverbs to modify adjectival phrases as well as adverbials, will yield (not necessarily desirable) multiple derivations for a sentence like John is a little unecessarily stupid. In one derivation, a little modiﬁes the AP and in the other case, it modiﬁes the adverb. 19.6. LOCATIVE ADVERBIAL PHRASES 183 VPr VPf  NA Ad1 NA D A APr Ad1 NA D A APf  NA PPr Ad1 NA D N PPf  NA βvxDA βDAax βDNpx (a) (b) (c) Figure 19.15: Selected Multi-word Adverb Modiﬁer trees (for adverbs like a little, a bit): βvxDA, βDAax, βDNpx. (330) The ship sank far oﬀshore (331) The trouble ahead distresses me Locatives can modify NPs, VPs and Ss. They modify NPs only by right-adjoining post- positively, as in Example (329). Post-positive is also the more common position when a locative modiﬁes either of the other categories. Locatives pre-modify VPs only when separated by balanced punctuation (commas or dashes). The trees locatives select when modifying NPs are shown in Figure 19.16. NP r NP 1 NA AdvP NP 2 Ad NP r NP 1 NA AdvP Ad Figure 19.16: Locative Modiﬁer Trees: βnxnxARB, βnxARB When the locative phrase consists of only the anchoring locative adverb, as in Example (330), it uses the βnxARB tree, shown in Figure 19.16, and its VP analogue, βvxARB. In addition, these are the trees selected when the locative anchor is modiﬁed by an adverb expressing degree, as in Example 330. The degree adverb adjoins on to the anchor using the βARBarb tree, which is described in Section 19.5. Figure 19.17 shows an example of these trees in action. Though there is a tree for a pre-sentential locative phrase, βnxARBs, there is no corresponding post- sentential tree, as it is highly debatable whether the post-sentential version actually has the entire sentence or just the preceding verb phrase as its scope. Thus, in accordance with XTAG practice, which considers ambiguous post-sentential modiﬁers to be VP-modiﬁers rather than S-modiﬁers, there is only a βvxnxARB tree, as shown in Figure 19.17. 184 CHAPTER 19. MODIFIERS S r S NA NP N I VP V left NP r D my NP f NA N toupee AdvP NP r D three NP f NA N holes Ad back S r NP N I VPr VP NA V left NP r D my NP f NA N toupee Adr Ad NA far Ad1 NA back Figure 19.17: Locative Phrases featuring NP and Adverb Degree Speciﬁcations One possible analysis of locative phrases with NPs might maintain that the NP is the head, with the locative adverb modifying the NP. This is initially attractive because of the similarity to time NPs, which also feature NPs that can modify clauses. This analysis seems insuﬃcient, however, in light of the fact that virtually any NP can occur in locative phrases, as in example (332). Therefore, in the XTAG analysis the locative adverb anchors the locative phrase trees. A complete summary of all trees selected by locatives is contained in Table 19.3. 269 adverbs select the locative trees. (332) I left my toupee and putter three holes back Degree Phrase Type Category Modiﬁed NP AdNone NP βnxnxARB βnxARB VP (post) βvxnxARB βvxARB VP (pre, punct-separated) βpunxARBpuvx βpuARBpuvx S βnxARBs βARBs Table 19.3: Locative Modiﬁers 9Though nearly all of these adverbs are spatial in nature, this number also includes a few temporal adverbs, such as ago, that also select these trees. Chapter 20 Auxiliaries Although there has been some debate about the lexical category of auxiliaries, the English XTAG grammar follows [McCawley, 1988], [Haegeman, 1991], and others in classifying auxil- iaries as verbs. The category of verbs can therefore be divided into two sets, main or lexical verbs, and auxiliary verbs, which can co-occur in a verbal sequence. Only the highest verb in a verbal sequence is marked for tense and agreement regardless of whether it is a main or auxiliary verb. Some auxiliaries (be, do, and have) share with main verbs the property of having overt morphological marking for tense and agreement, while the modal auxiliaries do not. However, all auxiliary verbs diﬀer from main verbs in several crucial ways.  Multiple auxiliaries can occur in a single sentence, while a matrix sentence may have at most one main verb.  Auxiliary verbs cannot occur as the sole verb in the sentence, but must be followed by a main verb.  All auxiliaries precede the main verb in verbal sequences.  Auxiliaries do not subcategorize for any arguments.  Auxiliaries impose requirements on the morphological form of the verbs that immediately follow them.  Only auxiliary verbs invert in questions (with the sole exception in American English of main verb be1).  An auxiliary verb must precede sentential negation (e.g. John not goes).  Auxiliaries can form contractions with subjects and negation (e.g. hell, wont). The restrictions that an auxiliary verb imposes on the succeeding verb limits the sequence of verbs that can occur. In English, sequences of up to ﬁve verbs are allowed, as in sentence (333). 1Some dialects, particularly British English, can also invert main verb have in yesno questions (e.g. have you any Grey Poupon ?). This is usually attributed to the inﬂuence of auxiliary have, coupled with the historic fact that English once allowed this movement for all verbs. 185 186 CHAPTER 20. AUXILIARIES (333) The music should have been being played [for the president] . The required ordering of verb forms when all ﬁve verbs are present is: modal base perfective progressive passive The rightmost verb is the main verb of the sentence. While a main verb subcategorizes for the arguments that appear in the sentence, the auxiliary verbs select the particular morphological forms of the verb to follow each of them. The auxiliaries included in the English XTAG grammar are listed in Table 20.1 by type. The third column of Table 20.1 lists the verb forms that are required to follow each type of auxiliary verb. TYPE LEX ITEMS SELECTS FOR modals can, could, may, might, will, base form2 would, ought, shall, should (e.g. will go, might come) need perfective have past participle (e.g. has gone) progressive be gerund (e.g. is going, was coming) passive be past participle (e.g. was helped by Jane) do support do base form (e.g. did go, does come) inﬁnitive to to base form (e.g. to go, to come) Table 20.1: Auxiliary Verb Properties 20.1 Non-inverted sentences This section and the sections that follow describe how the English XTAG grammar accounts for properties of the auxiliary system described above. In our grammar, auxiliary trees are added to the main verb tree by adjunction. Figure 20.1 shows the adjunction tree for non-inverted sentences.3 The restrictions outlined in column 3 of Table 20.1 are implemented through the features mode, perfect, progressive and passive. The syntactic lexicon entries for the auxiliaries gives values for these features on the foot node (VP) in Figure 20.1. Since the top features of the foot node must eventually unify with the bottom features of the node it adjoins 2There are American dialects, particularly in the South, which allow double modals such as might could and might should. These constructions are not allowed in the XTAG English grammar. 3We saw this tree brieﬂy in section 4.4.3.2, but with most of its features missing. The full tree is presented here. 20.1. NON-INVERTED SENTENCES 187 VPr assign-comp : 1 neg : 2 agr : 3 mainv : 4 tense : 5 mode : 6 assign-case : 7 displ-const : set1 : 8 progressive : 9 perfect : 10 conditional : 11 V assign-comp : 1 neg : 2 agr : 3 mainv : 4 tense : 5 mode : 6 assign-case : 7 displ-const : set1 : 8 VP NA displ-const : set1 : - progressive : 9 perfect : 10 conditional : 11 Figure 20.1: Auxiliary verb tree for non-inverted sentences: βVvx onto for the sentence to be valid, this enforces the restrictions made by the auxiliary node. In addition to these feature values, each auxiliary also gives values to the anchoring node (V), to be passed up the tree to the root VP (VPr) node; there they will become the new features for the top VP node of the sentential tree. Another auxiliary may now adjoin on top of it, and so forth. These feature values thereby ensure the proper auxiliary sequencing. Figure 20.2 shows the auxiliary trees anchored by the four auxiliary verbs in sentence (333). Figure 20.3 shows the ﬁnal tree created for this sentence. The general English restriction that matrix clauses must have tense (or be imperatives) is enforced by requiring the top S-node of a sentence to have modeindimp (indicative or imperative). Since only the indicative and imperative sentences have tense, non-tensed clauses are restricted to occurring in embedded environments. Noun-verb contractions are labeled NVC in their part-of-speech ﬁeld in the morphological database and then undergo special processing to split them apart into the noun and the reduced verb before parsing. The noun then selects its trees in the normal fashion. The contraction, say 188 CHAPTER 20. AUXILIARIES VPr conditional : 1 perfect : 2 progressive : 3 displ-const : set1 : 4 assign-case : 5 mainv : 6 agr : 7 neg : 8 assign-comp : 9 tense : 10 pres mode : 11 ind V assign-comp : 9 neg : 8 agr : 7 mainv : 6 tense : 10 mode : 11 assign-case : 5 displ-const : set1 : 4 displ-const : set1 : - assign-case : nom assign-comp : ind_nilthatrelifwhether mainv : - tense : pres mode : ind should VP NA displ-const : set1 : - progressive : 3 perfect : 2 conditional : 1 mode : base VPr conditional : 1 progressive : 2 displ-const : set1 : 3 assign-case : 4 mode : 5 tense : 6 agr : 7 neg : 8 assign-comp : 9 perfect : 10  mainv : 11 - V assign-comp : 9 neg : 8 agr : 7 mainv : 11 tense : 6 mode : 5 assign-case : 4 displ-const : set1 : 3 displ-const : set1 : - mode : base have VP NA displ-const : set1 : - progressive : 2 perfect : 10 conditional : 1 mode : ppart passive : - VPr conditional : 1 perfect : 2 displ-const : set1 : 3 assign-case : 4 mode : 5 tense : 6 agr : 7 neg : 8 assign-comp : 9 progressive : 10  mainv : 11 - V assign-comp : 9 neg : 8 agr : 7 mainv : 11 tense : 6 mode : 5 assign-case : 4 displ-const : set1 : 3 displ-const : set1 : - weak : - mode : ppart been VP NA displ-const : set1 : - progressive : 10 perfect : 2 conditional : 1 mode : ger VPr conditional : 1 perfect : 2 progressive : 3 displ-const : set1 : 4 assign-case : 5 mode : 6 tense : 7 agr : 8 neg : 9 assign-comp : 10 mainv : 11 - V assign-comp : 10 neg : 9 agr : 8 mainv : 11 tense : 7 mode : 6 assign-case : 5 displ-const : set1 : 4 displ-const : set1 : - assign-case : none mode : ger being VP NA displ-const : set1 : - progressive : 3 perfect : 2 conditional : 1 mode : ppart passive :  mainv :  Figure 20.2: Auxiliary trees for The music should have been being played . 20.2. INVERTED SENTENCES 189 Sr NP D the N music VPr V should VP NA V have VP NA V been VP NA V being VP NA V played Figure 20.3: The music should have been being played . ll or d, likewise selects the normal auxiliary verb tree, βVvx. However, since the contracted form, rather than the verb stem, is given in the morphology, the contracted form must also be listed as a separate syntactic entry. These entries have all the same features of the full form of the auxiliary verbs, with tense constraints coming from the morphological entry (e.g. its is listed as it s NVC 3sg PRES). The ambiguous contractions d (hadwould) and s (hasis) behave like other ambiguous lexical items; there are simply multiple entries for those lexical items in the lexicon, each with diﬀerent features. In the resulting parse, the contracted form is shown with features appropriate to the full auxiliary it represents. 20.2 Inverted Sentences In inverted sentences, the two trees shown in Figure 20.4 adjoin to an S tree anchored by a main verb. The tree in Figure 20.4(a) is anchored by the auxiliary verb and adjoins to the S node, while the tree in Figure 20.4(b) is anchored by an empty element and adjoins at the VP 190 CHAPTER 20. AUXILIARIES node. Figure 20.5 shows these trees (anchored by will) adjoined to the declarative transitive tree4 (anchored by main verb buy). Sr displ-const : set1 : - neg : 1 agr : 2 tense : 3 progressive : 7 perfect : 8 conditional : 9 assign-case : 5 mode : 4 inv :  V neg : 1 agr : 2 tense : 3 mode : 4 ind assign-case : 5 agr : 6 S NA progressive : 7 perfect : 8 conditional : 9 assign-case : 5 agr : 6 displ-const : set1 :  comp : nil inv : - VPr conditional : 1 perfect : 2 progressive : 3 displ-const : set1 : 7 assign-case : 8 tense : 9 neg : 10 assign-comp : 11 agr : 4 passive : 5 mode : 6 mainv : 12 - V assign-comp : 11 neg : 10 agr : 4 mainv : 12 tense : 9 mode : 6 assign-case : 8 displ-const : set1 : 7 displ-const : set1 :  ε VP NA conditional : 1 perfect : 2 progressive : 3 displ-const : set1 : - agr : 4 passive : 5 mode : 6 (a) (b) Figure 20.4: Trees for auxiliary verb inversion: βVs (a) and βVvx (b) The feature displ-const ensures that both of the trees in Figure 20.4 must adjoin to an elementary tree whenever one of them does. For more discussion on this mechanism, which simulates tree local multi-component adjunction, see [Hockey and Srinivas, 1993]. The tree in Figure 20.4(b), anchored by ǫ, represents the originating position of the inverted auxiliary. Its adjunction blocks the assign-case values of the VP it dominates from being co-indexed with the case value of the subject. Since assign-case values from the VP are blocked, the case value of the subject can only be co-indexed with the assign-case value of the inverted auxiliary (Figure 20.4(a)). Consequently, the inverted auxiliary functions as the case-assigner for the subject in these inverted structures. This is in contrast to the situation in uninverted structures where the anchor of the highest (leftmost) VP assigns case to the subject 4The declarative transitive tree was seen in section 6.2. 20.3. DO-SUPPORT 191 Sr V will S NA NP N John VPr V ε VP NA V buy NP D a N backpack Figure 20.5: will John buy a backpack ? (see section 4.4.3.2 for more on case assignment). The XTAG analysis is similar to GB accounts where the inverted auxiliary plus the ǫ-anchored tree are taken as representing I to C movement. 20.3 Do-Support It is well-known that English requires a mechanism called do-support for negated sentences and for inverted yes-no questions without auxiliaries. (334) John does not want a car . (335) John not wants a car . (336) John will not want a car . (337) Do you want to leave home ? (338) want you to leave home ? (339) will you want to leave home ? 192 CHAPTER 20. AUXILIARIES 20.3.1 In negated sentences The GB analysis of do-support in negated sentences hinges on the separation of the INFL and VP nodes (see [Chomsky, 1965], [Jackendoﬀ, 1972] and [Chomsky, 1986]). The claim is that the presence of the negative morpheme blocks the main verb from getting tense from the INFL node, thereby forcing the addition of a verbal lexeme to carry the inﬂectional elements. If an auxiliary verb is present, then it carries tense, but if not, periphrastic or dummy, do is required. This seems to indicate that do and other auxiliary verbs would not co-occur, and indeed this is the case (see sentences (340)-(341)). Auxiliary do is allowed in English when no negative morpheme is present, but this usage is marked as emphatic. Emphatic do is also not allowed to co-occur with auxiliary verbs (sentences (342)-(345)). (340) We will have do bought a sleeping bag . (341) We do will have bought a sleeping bag . (342) You do have a backpack, dont you ? (343) I do want to go ! (344) You do can have a backpack, dont you ? (345) I did have had a backpack ! At present, the XTAG grammar does not have analyses for emphatic do. In the XTAG grammar, do is prevented from co-occurring with other auxiliary verbs by a requirement that it adjoin only onto main verbs (mainv  ). It has indicative mode, so no other auxiliaries can adjoin above it.5 The lexical item not is only allowed to adjoin onto a non-indicative (and therefore non-tensed) verb. Since all matrix clauses must be indicative (or imperative), a negated sentence will fail unless an auxiliary verb, either do or another auxiliary, adjoins somewhere above the negative morpheme, not. In addition to forcing adjunction of an auxiliary, this analysis of not allows it freedom to move around in the auxiliaries, as seen in the sentences (346)-(349). (346) John will have had a backpack . (347) John not will have had a backpack . (348) John will not have had a backpack . (349) John will have not had a backpack . 5Earlier, we said that indicative mode carries tense with it. Since only the topmost auxiliary carries the tense, any subsequent verbs must not have indicative mode. 20.4. INFINITIVES 193 20.3.2 In inverted yesno questions In inverted yesno questions, do is required if there is no auxiliary verb to invert, as seen in sentences (337)-(339), replicated here as (350)-(352). (350) do you want to leave home ? (351) want you to leave home ? (352) will you want to leave home ? (353) do you will want to leave home ? In English, unlike other Germanic languages, the main verb cannot move to the beginning of a clause, with the exception of main verb be.6 In a GB account of inverted yesno questions, the tense feature is said to be in C0 at the front of the sentence. Since main verbs cannot move, they cannot pick up the tense feature, and do-support is again required if there is no auxiliary verb to perform the role. Sentence (353) shows that do does not interact with other auxiliary verbs, even when in the inverted position. In XTAG, trees anchored by a main verb that lacks tense are required to have an auxiliary verb adjoin onto them, whether at the VP node to form a declarative sentence, or at the S node to form an inverted question. Do selects the inverted auxiliary trees given in Figure 20.4, just as other auxiliaries do, so it is available to adjoin onto a tree at the S node to form a yesno question. The mechanism described in section 20.3.1 prohibits do from co-occurring with other auxiliary verbs, even in the inverted position. 20.4 Inﬁnitives The inﬁnitive to is considered an auxiliary verb in the XTAG system, and selects the auxiliary tree in Figure 20.1. To, like do, does not interact with the other auxiliary verbs, adjoining only to main verb base forms, and carrying inﬁnitive mode. It is used in embedded clauses, both with and without a complementizer, as in sentences (354)-(356). Since it cannot be inverted, it simply does not select the trees in Figure 20.4. (354) John wants to have a backpack . (355) John wants Mary to have a backpack . (356) John wants for Mary to have a backpack . The usage of inﬁnitival to interacts closely with the distribution of null subjects (PRO), and is described in more detail in section 8.5. 6The inversion of main verb have in British English was previously noted. 194 CHAPTER 20. AUXILIARIES 20.5 Semi-Auxiliaries Under the category of semi-auxiliaries, we have placed several verbs that do not seem to closely follow the behavior of auxiliaries. One of these auxiliaries, dare, mainly behaves as a modal and selects for the base form of the verb. The other semi-auxiliaries all select for the inﬁnitival form of the verb. Examples of this second type of semi-auxiliary are used to, ought to, get to, have to, and BE to. 20.5.1 Marginal Modal dare The auxiliary dare is unique among modals in that it both allows DO-support and exhibits a past tense form. It clearly falls in modal position since no other auxiliary (except do) may precede it in linear order7. Examples appear below. (357) she dare not have been seen . (358) she does not dare succeed . (359) Jerry dared not look left or right . (360) only models dare wear such extraordinary outﬁts . (361) dare Dale tell her the secret ? (362) Louise had dared not tell a soul . As mentioned above, auxiliaries are prevented from having DO-support within the XTAG system. To allow for DO-support in this case, we had to create a lexical entry for dare that allowed it to have the feature mainv and to have base mode (this measure is what also allows dare to occur in double-modal sequences). A second lexical entry was added to handle the regular modal occurrence of dare. Additionally, all other modals are classiﬁed as being present tense, while dare has both present and past forms. To handle this behavior, dare was given similar features to the other modals in the morphology minus the speciﬁcation for tense. 7Some speakers accept dare preceded by a modal, as in I might dare ﬁnish this report today. In the XTAG analysis, this particular double modal usage is accounted for. Other cases of double modal occurrence exist in some dialects of American English, although these are not accounted for in the system, as was mentioned earlier. 20.5. SEMI-AUXILIARIES 195 20.5.2 Other semi-auxiliaries The other semi-auxiliaries all select for the inﬁnitival form of the verb. Many of these auxiliaries allow for DO-support and can appear in both base and past participle forms, in addition to being able to stand alone (indicative mode). Examples of this type appear below. (363) Alex used to attend karate workshops . (364) Angelina might have used to believe in fate . (365) Rich did not used to want to be a physical therapist . (366) Mick might not have to play the game tonight . (367) Singer had to have been there . (368) Heather has got to ﬁnish that project before she goes insane . The auxiliaries ought to and BE to may not be preceded by any other auxiliary. (369) Biﬀ ought to have been working harder . (370) Carson does ought to have been working harder . (371) the party is to take place this evening . (372) the party had been to take place this evening . The trickiest element in this group of auxiliaries is used to. While the other verbs behave according to standard inﬂection for auxiliaries, used to has the same form whether it is in mode base, past participle, or indicative forms. The only connection used to maintains with the inﬁnitival form use is that occasionally, the bare form use will appear with DO-support. Since the three modes mentioned above are mutually exclusive in terms of both the morphology and the lexicon, used has three entries in each. 196 CHAPTER 20. AUXILIARIES 20.5.3 Other Issues There is a lingering problem with the auxiliaries that stems from the fact that there currently is no way to distinguish between the main verb and auxiliary verb behaviors for a given letter string within the morphology. This situation results in many unacceptable sentences being successfully parsed by the system. Examples of the unacceptable sentences are given below. (373) the miller cans tell a good story . (vs the farmer cans peaches in July .) (374) David wills have ﬁnished by noon . (vs the old man wills his fortune to me .) (375) Sarah needs not leave . (vs Sarah needs to leave .) (376) Jennifer dares not be seen . (vs the young woman dares him to do the stunt .) (377) Lila does use to like beans . (vs Lila does use her new cookware .) Chapter 21 Conjunction 21.1 Introduction The XTAG system can handle sentences with conjunction of two constituents of the same syntactic category. The coordinating conjunctions which select the conjunction trees are and, or and but.1 There are also multi-word conjunction trees, anchored by either-or, neither-nor, both-and, and as well as. There are eight syntactic categories that can be coordinated, and in each case an auxiliary tree is used to implement the conjunction. These eight categories can be considered as four diﬀerent cases, as described in the following sections. In all cases the two constituents are required to be of the same syntactic category, but there may also be some additional constraints, as described below. 21.2 Adjective, Adverb, Preposition and PP Conjunction Each of these four categories has an auxiliary tree that is used for conjunction of two constituents of that category. The auxiliary tree adjoins into the left-hand-side component, and the right- hand-side component substitutes into the auxiliary tree. Figure 21.1(a) shows the auxiliary tree for adjective conjunction, and is used, for example, in the derivation of the parse tree for the noun phrase the dark and dreary day, as shown in Figure 21.1(b). The auxiliary tree adjoins onto the node for the left adjective, and the right adjective substitutes into the right hand side node of the auxiliary tree. The analysis for adverb, preposition and PP conjunction is exactly the same and there is a corresponding auxiliary tree for each of these that is identical to that of Figure 21.1(a) except, of course, for the node labels. 21.3 Noun Phrase and Noun Conjunction The tree for NP conjunction, shown in Figure 21.2(a), has the same basic analysis as in the previous section except that the wh and case features are used to force the two noun phrases to have the same wh and case values. This allows, for example, he and she wrote the book together while disallowing he and her wrote the book together. Agreement is 1We believe that the restriction of but to conjoining only two items is a pragmatic one, and our grammars accepts sequences of any number of elements conjoined by but. 197 198 CHAPTER 21. CONJUNCTION A A1 Conj A2 NP DetP D the Nr A A1 dark Conj and A dreary Nf NA day (a) (b) Figure 21.1: Tree for adjective conjunction: βa1CONJa2 and a resulting parse tree lexicalized, since the various conjunctions behave diﬀerently. With and, the root agr num value is plural, no matter what the number of the two conjuncts. With or, however, the root agr num is co-indexed with the agr num feature of the right conjunct. This ensures that the entire conjunct will bear the number of both conjuncts if they agree (Figure 21.2(b)), or of the most recent one if they diﬀer (Either the boys or John is going to help you.). There is no rule per se on what the agreement should be here, but people tend to make the verb agree with the last conjunct (cf. [Quirk et al., 1985], section 10.41 for discussion). The tree for N conjunction is identical to that for the NP tree except for the node labels. (The multi-word conjunctions do not select the N conjunction tree - the both dogs and cats). 21.4 Determiner Conjunction In determiner coordination, all of the determiner feature values are taken from the left deter- miner, and the only requirement is that the wh feature is the same, while the other features, such as card, are unconstrained. For example, which and what and all but one are both acceptable determiner conjunctions, but which and all is not. (378) how many and which people camp frequently ? (379) some or which people enjoy nature . 21.5 Sentential Conjunction The tree for sentential conjunction, shown in Figure 21.4, is based on the same analysis as the conjunctions in the previous two sections, with a slight diﬀerence in features. The mode 21.6. COMMA AS A CONJUNCTION 199 NP decreas : 4 card : 5 quan : 6 definite : 7 gen : 8 const : 9 conj : 1 displ-const : 10 wh : 2 case : 3 Conj1  conj : 1 NP1  NA decreas : 4 card : 5 quan : 6 definite : 7 gen : 8 const : 9 displ-const : 10 wh : 2 case : 3 Conj2  NP2  wh : 2 case : 3 nomacc NP case : 1 nomacc wh : 2 - displ-const : 3 conj : 4 or const : 5 - gen : 6 - definite : 7 - quan : 8 - card : 9 - decreas : 10 - predet : 11 agr : num : 12 plur Conj1 either NP1 NA N aardvarks Conj2 or NP N emus (a) (b) Figure 21.2: Tree for NP conjunction: βCONJnx1CONJnx2 and a resulting parse tree feature2 is used to constrain the two sentences being conjoined to have the same mode so that the day is dark and the phone never rang is acceptable, but the day dark and the phone never rang is not. Similarly, the two sentences must agree in their wh, comp and extracted features. Co-indexation of the comp feature ensures that either both conjuncts have the same complementizer, or there is a single complementizer adjoined to the complete conjoined S. The assign-comp feature3 feature is used to allow conjunction of inﬁnitival sentences, such as to read and to sleep is a good life. 21.6 Comma as a conjunction We treat comma as a conjunction in conjoined lists. It anchors the same trees as the lexical conjunctions, but is considerably more restricted in how it combines with them. The trees anchored by commas are prohibited from adjoining to anything but another comma conjoined element or a non-coordinate element. (All scope possibilities are allowed for elements coordi- nated with lexical conjunctions.) Thus, structures such as Tree 21.5(a) are permitted, with each element stacking sequentially on top of the ﬁrst element of the conjunct, while structures such as Tree 21.5(b) are blocked. 2See section 8.3 for an explanation of the mode feature. 3See section 8.5 for an explanation of the assign-comp feature. 200 CHAPTER 21. CONJUNCTION D conj : 1 wh : 2 const : 3 decreas : 4 gen : 5 card : 6 quan : 7 agr : 8 definite : 9 predet : 10 displ-const : 11 D1  NA wh : 2 const : 3 decreas : 4 gen : 5 card : 6 quan : 7 agr : 8 definite : 9 predet : 10 displ-const : 11 Conj conj : 1 D2  wh : 2 Figure 21.3: Tree for determiner conjunction: βd1CONJd2.ps 21.6. COMMA AS A CONJUNCTION 201 S comp : 2 extracted : 3 wh : 4 conj : 1 displ-const : 5 mode : 6 indinfgernomprepimp S1 NA comp : 2 extracted : 3 wh : 4 displ-const : 5 assign-comp : inf_nilind_nil mode : 6 Conj conj : 1 S2 comp : 2 extracted : 3 wh : 4 assign-comp : inf_nilind_nil mode : 6 Figure 21.4: Tree for sentential conjunction: βs1CONJs2 NP Nr A A1 NA A1 NA beautiful Conj , A red Conj , A fragrant Nf NA roses NP Nr A A1 NA beautiful Conj , A A1 NA red Conj , A fragrant Nf NA roses (a) Valid tree with comma conjunction (b) Invalid tree Figure 21.5: This is accomplished by using the conj feature, which has the values andorbut and comma to diﬀerentiate the lexical conjunctions from commas. The conj values for a comma-anchored tree and and-anchored tree are shown in Figure 21.6. The feature conj  commanone on A1 in (a) only allows comma conjoined or non-conjoined elements as the left-adjunct, and conj  none on A in (a) allows only a non-conjoined element as the right 202 CHAPTER 21. CONJUNCTION conjunct. We also need the feature conj  andorbutnone on the right conjunct of the trees anchored by lexical conjunctions like (b), to block comma-conjoined elements from substituting there. Without this restriction, we would get multiple parses of the NP in Tree 21.5; with the restrictions we only get the derivation with the correct scoping, shown as (a). Since comma-conjoined lists can appear without a lexical conjunction between the ﬁnal two elements, as shown in example (380), we cannot force all comma-conjoined sequences to end with a lexical conjunction. (380) So it is too with many other spirits which we all know: the spirit of Nazism or Com- munism, school spirit , the spirit of a street corner gang or a football team, the spirit of Rotary or the Ku Klux Klan. [Brown cd01] Nr A neg : - conj : 14 comma displ-const : 15 A1 NA wh : - displ-const : 15 neg : - conj : commanone large Conj conj : 14 , A wh : - neg : - conj : none white Nf NA Nr A neg : - conj : 14 and displ-const : 15 A1 NA wh : - neg : - displ-const : 15 red Conj conj : 14 and A wh : - neg : - conj : andorbutnone white Nf NA Figure 21.6: βa1CONJa2 (a) anchored by comma and (b) anchored by and 21.7 But-not, not-but, and-not and ǫ-not We are analyzing conjoined structures such as The women but not the men with a multi-anchor conjunction tree anchored by the conjunction plus the adverb not. The alternative is to allow not to adjoin to any constituent. However, this is the only construction where not can freely occur onto a constituent other than a VP or adjective (cf. βNEGvx and βNEGa trees). It can also adjoin to some determiners, as discussed in Section 18. We want to allow sentences like (381) and rule out those like (382). The tree for the good example is shown in Figure 21.7. There are similar trees for and-not and ǫ-not, where ǫ is interpretable as either and or but, and a tree with not on the ﬁrst conjunct for not-but. (381) Beth grows basil in the house (but) not in the garden . (382) Beth grows basil (but) not in the garden . 21.8. TO AS A CONJUNCTION 203 PP PP1 NA Conj but PP0 NA Ad not PP2 Figure 21.7: Tree for conjunction with but-not: βpx1CONJARBpx2 Although these constructions sound a bit odd when the two conjuncts do not have the same number, they are sometimes possible. The agreement information for such NPs is always that of the non-negated conjunct: his sons, and not Bill, are in charge of doing the laundry or not Bill, but his sons, are in charge of doing the laundry (Some people insist on having the commas here, but they are frequently absent in corpus data.) The agreement feature from the non-negated conjunct in passed to the root NP, as shown in Figure 21.8. Aside from agreement, these constructions behave just like their non-negated counterparts. 21.8 To as a Conjunction To can be used as a conjunction for adjectives (Fig. 21.9) and determiners, when they denote points on a scale: (383) two to three degrees (384) high to very high temperatures As far as we can tell, when the conjuncts are determiners they must be cardinal. 21.9 Predicative Coordination This section describes the method for predicative coordination (including VP coordination of various kinds) used in XTAG. The description is derived from work described in ([Sarkar and Joshi, 1996]). It is important to say that this implementation of predicative coordination is not part of the XTAG release at the moment due massive parsing ambiguities. This is partly because of the current implementation and also the inherent ambiguities due to VP coordination that cause a combinatorial explosion for the parser. We are trying to remedy both of these 204 CHAPTER 21. CONJUNCTION NP neg :  wh : 1 case : 2 nomacc displ-const : 3 conj : 4 but agr : num : 5 NP0 NA displ-const : 3 case : 2 wh : 1 Ad not NP1 NA displ-const : 3 wh : 1 case : 2 Conj conj : 4 but NP2 wh : 1 case : 2 agr : num : 5 conj : andorbutnone Figure 21.8: Tree for conjunction with not-but: βARBnx1CONJnx2 limitations using a probability model for coordination attachments which will be included as part of a later XTAG release. This extended domain of locality in a lexicalized Tree Adjoining Grammar causes problems when we consider the coordination of such predicates. Consider (385) for instance, the NP the beans that I bought from Alice in the Right-Node Raising (RNR) construction has to be shared by the two elementary trees (which are anchored by cooked and ate respectively). (385) (((Harry cooked) and (Mary ate)) the beans that I bought from Alice) We use the standard notion of coordination which is shown in Figure 21.10 which maps two constituents of like type, but with diﬀerent interpretations, into a constituent of the same type. We add a new operation to the LTAG formalism (in addition to substitution and adjunction) called conjoin (later we discuss an alternative which replaces this operation by the traditional operations of substitution and adjunction). While substitution and adjunction take two trees to give a derived tree, conjoin takes three trees and composes them to give a derived tree. One of the trees is always the tree obtained by specializing the schema in Figure 21.10 for a particular category. The tree obtained will be a lexicalized tree, with the lexical anchor as the conjunction: and, but, etc. The conjoin operation then creates a contraction between nodes in the contraction sets of the trees being coordinated. The term contraction is taken from the graph-theoretic notion of edge contraction. In a graph, when an edge joining two vertices is contracted, the nodes are 21.9. PREDICATIVE COORDINATION 205 NP Nr A A1 NA high Conj to Ar Ad very A NA high Nf NA temperatures Figure 21.9: Example of conjunction with to 110 X X Conj X Figure 21.10: Coordination schema merged and the new vertex retains edges to the union of the neighbors of the merged vertices. The conjoin operation supplies a new edge between each corresponding node in the contraction set and then contracts that edge. For example, applying conjoin to the trees Conj(and), α(eats) and α(drinks) gives us the derivation tree and derived structure for the constituent in 386 shown in Figure 21.11. (386) . . . eats cookies and drinks beer Another way of viewing the conjoin operation is as the construction of an auxiliary structure from an elementary tree. For example, from the elementary tree α(drinks), the conjoin oper- ation would create the auxiliary structure β(drinks) shown in Figure 21.12. The adjunction operation would now be responsible for creating contractions between nodes in the contraction sets of the two trees supplied to it. Such an approach is attractive for two reasons. First, it uses only the traditional operations of substitution and adjunction. Secondly, it treats conj X as a kind of modiﬁer on the left conjunct X. This approach reduces some of the parsing ambiguities introduced by the predicative coordination trees and forms the basis of the XTAG implementation. 206 CHAPTER 21. CONJUNCTION 110 VP Conj(and) VP and VP Derived structure and S NP VP V NP VP eats S VP V NP cookies beer drinks α Conj(and) α α 1 2.2 (cookies) (eats){1} (drinks) {1} α (beer) 2.2 3 Derivation tree Figure 21.11: An example of the conjoin operation. {1} denotes a shared dependency. 110 NP VP and VP S NP S VP V NP eats VP V NP drinks α (eats) β (drinks) {1} {1} Figure 21.12: Coordination as adjunction. 21.10. PSEUDO-COORDINATION 207 More information about predicative coordination can be found in ([Sarkar and Joshi, 1996]), including an extension to handle gapping constructions. 21.10 Pseudo-coordination The XTAG grammar does handle one sort of verb pseudo-coordination. Semi-idiomatic phrases such as try and and up and (as in they might try and come today) are handled as multi- anchor modiﬁers rather than as true coordination. These items adjoin to a V node, using the βVCONJv tree. This tree adjoins only to verbs in their base morphological (non-inﬂected) form. The verb anchor of the βVCONJv must also be in its base form, as shown in examples (387)- (389). This blocks 3rd-person singular derivations, which are the only person morphologically marked in the present, except when an auxiliary verb is present or the verb is in the inﬁnitive. (387) He tried and came yesterday. (388) They try and exercise three times a week. (389) He wants to try and sell the puppies. Chapter 22 Comparatives 22.1 Introduction Comparatives in English can manifest themselves in many ways, acting on many diﬀerent grammatical categories and often involving ellipsis. A distinction must be made at the outset between two very diﬀerent sorts of comparativesthose which make a comparison between two propositions and those which compare the extent to which an entity has one property to a greater or lesser extent than another property. The former, which we will refer to as proposi- tional comparatives, is exempliﬁed in (390), while the latter, which we will call metalinguistic comparatives (following Hellan 1981), is seen in (391): (390) Ronaldo is more angry than Romario. (391) Ronaldo is more angry than upset. In (390), the extent to which Ronaldo is angry is greater than the extent to which Romario is angry. Sentence (391) indicates that the extent to which Ronaldo is angry is greater than the extent to which he is upset. Apart from certain of the elliptical cases, both kinds of comparatives can be handled straight- forwardly in the XTAG system. Elliptical cases which are not presently covered include those exempliﬁed by the following sentences, which would presumably be handled in the same way as other sorts of VP ellipsis would. (392) Ronaldo is more angry than Romario is. (393) Bill eats more broccoli than George eats. (394) Bill eats more broccoli than George does. We turn to the analysis of metalinguistic comparatives ﬁrst. 208 22.2. METALINGUISTIC COMPARATIVES 209 22.2 Metalinguistic Comparatives A metalinguistic comparison can be performed on basically all of the predicational categories adjectives, verb phrases, prepositional phrases, and nounsas in the following examples: (395) The table is more long than wide. (AP) (396) Clark more makes the rules than follows them. (VP) (397) Calvin is more in the living room than in the kitchen. (PP) (398) That unindentiﬁed amphibian in the bush is more frog than toad, I would say. (NP) At present, we only deal with the adjectival metalinguistic comparatives as in (395). The analysis given here for these can be easily extended to prepositional phrases and nominal com- paratives of the metalinguistic sort, but, as with coordination in XTAG, verb phrases will prove more diﬃcult. Adjectival comparatives appear to distribute with simple adjectives, as in the following examples: (399) Herbert is more livid than angry. (400) Herbert is more livid and furious than angry. (401) The more innovative than conventional medication cured everyone in the sick ward. (402) The elephant, more wobbly than steady, fell from the circus ball. This patterning indicates that we can give these comparatives a tree that adjoins quite freely onto adjectives, as in Figure 22.1. This tree is anchored by moreless - than. To avoid grammatically incorrect comparisons such as more brighter than dark, the feature compar is used to block this tree from adjoining onto morphologically comparative adjectives. The foot node is compar-, while brighter and its comparative siblings are compar1. We also wish to block strings like more brightest than dark, which is accomplished with the feature super, indicating superlatives. This feature is negative at the foot node so that βARBaPa cannot adjoin to superlatives like nicest, which are speciﬁed as super from the morphology. Furthermore, the root node is super so that βARBaPa cannot adjoin onto itself and produce monstrosities such as (403): (403) Herbert is more less livid than angry than furious. 1The analysis given later for adjectival propositional comparatives produces aggregated compar adjectives such as more bright, which will also be incompatible (as desired) with βARBaPa. 210 CHAPTER 22. COMPARATIVES Ar Ad Af PP P A Figure 22.1: Tree for Metalinguistic Adjective Comparative: βARBaPa Thus, the use of the super feature is less to indicate superlativeness speciﬁcally, but rather to indicate that the subtree below a super node contains a full-ﬂeshed comparison. In the case of lexical superlatives, the comparison is against everything, implicitly. A beneﬁt of the multiple-anchor approach here is that we will never allow sentences such as (404), which would be permissible if we split the comparative component and the than component of metalinguistic comparatives into two separate trees. (404) Ronaldo is angrier than upset. We also see another variety of adjectival comparatives of the form moreless than X, which indicates some property which is more or less extreme than the property X. In a sentence such as (405), some property is being said to hold of Francis such that it is of a kind with stupid and that it exceeds stupid on some scale (intelligence, for example). Quirk et al. also note that these constructions remark on the inadequacy of the lexical item. Thus, in (404), it could be that stupid is a starting point from which the speaker makes an approximation for some property which the speaker feels is beyond the range of the English lexicon, but which expresses the supreme lack of intellect of the individual it is predicated of. (405) Francis is more than stupid. (406) Romario is more than just upset. Taking our inspiration from βARBaPa, we can handle these comparatives, which have the same distribution but contain an empty adjective, by using the tree shown in Figure 22.2. This sort of metalinguistic comparative also occurs with the verb phrase, prepositional phrase, and noun varieties. (407) Clark more than makes the rules. (VP) (408) Calvins hands are more than near the cookie jar. (PP) (409) That stuﬀ on her face is more than mud. (NP) Presumably, the analysis for these would parallel that for adjectives, though it has not yet been implemented. 22.3. PROPOSITIONAL COMPARATIVES 211 Ar Ad A NA ε PP P Af Figure 22.2: Tree for Adjective-Extreme Comparative: βARBPa 22.3 Propositional Comparatives 22.3.1 Nominal Comparatives Nominal comparatives are considered here to be those which compare the cardinality of two sets of entities denoted by nominal phrases. The following data lay out a basic distribution of these comparatives. (410) More vikings than mongols eat spam. (411) More the vikings than mongols eat spam. (412) Vikings eat less spaghetti than spam. (413) More men that walk to the store than women who despise spam enjoyed the football game. (414) More men than James like scotch on the rocks. (415) Elmer knows fewer martians than rabbits. Looking at these examples, we are tempted to produce a tree for this construction that is similar to βARBaPa. However, it is quite common for the than portion of these comparatives to be left out, as in the following sentences: (416) More vikings eat spam. (417) Mongols eat less spam. 212 CHAPTER 22. COMPARATIVES Furthermore, than NP cannot occur without more. These facts indicate that we can and should build up nominal comparatives with two separate trees. The ﬁrst, which allows a comparative adverb to adjoin to a noun, is given in Figure 22.3(a). The second is the noun-phrase modifying prepositional tree. The tree βCARBn is anchored by morelessfewer and βCnxPnx is anchored by than. The feature compar is used to ensure that only one βCARBn tree can adjoin to any given nounits foot node is compar- and the root node is compar. All nouns are compar-, and the compar value is passed up through all trees which adjoin to N or NP. In order to ensure that we do not allow sentences like Vikings than mongols eat spam, the compar feature is used. The NP foot node of βCnxPnx is compar; thus, βCnxPnx will adjoin only to NPs which have been already modiﬁed by βCARBn (and thereby comparativized). In this way, we capture sentences like (416) en route to deriving sentences like (410), in a principled and simple manner. Nr equiv : 1 super : - compar :  Ad equiv : 1 compar :  super : - Nf compar : - super : - NP r NP f PP P NP  (a) βCARBn tree (b) βCnxPnx tree Figure 22.3: Nominal comparative trees Further evidence for this approach comes from comparative clauses which are missing the noun phrase which is being compared against something, as in the following: (418) The vikings ate more.2 (419) The vikings ate more than a boar.3 Sometimes the missing noun refers to an entity or set available in the prior discourse, while at other times it is a reference to some anonymous, unspeciﬁed set. The former is exempliﬁed in a mini-discourse such as the following: 2We ignore here the interpretation in which the comparison covers the eating event, focussing only on the one which the comparison involves the stuﬀ being eaten. 3This sentence diﬀers from the metalinguistic comparison That stuﬀ on her face is more than mud in that it involves a comment on the quantity andor type of the compared NP, whereas the other expresses that the property denoted by the compared noun is an inadequate characterization of the thing being described. 22.3. PROPOSITIONAL COMPARATIVES 213 Calvin: The mongols ate spam. Hobbes: The vikings ate more. The latter can be seen in the following example: Calvin: The vikings ate a a boar. Hobbes: Indeed. But in fact, the vikings ate more than a boar. Since the lone comparatives morelessfewer have the same basic distribution as noun phrases, the tree in Figure 22.4 is employed to capture this fact. The root node of αCARB is compar. Not only does this accord with our intuitions about what the compar feature is supposed to indicate, it also permits βnxPnx to adjoin, giving us strings such as more than NP for free. NP Ad N NA ε Figure 22.4: Tree for Lone Comparatives: αCARB Thus, by splitting nominal comparatives into multiple trees, we make correct predictions about their distribution with a minimal number of simple trees. Furthermore, we now also get certain comparative coordinations for free, once we place the requirement that nouns and noun phrases must match for compar if they are to be coordinated. This yields strings such as the following: (420) Julius eats more grapes and fewer boars than avocados. (421) Were there more or less than ﬁfty people (at the party)? The structures are given in Figure 22.5. Also, it will block strings like more men and women than children under the (impossible) interpretation that there are more men than children but the comparison of the quantity of women to children is not performed. Unfortunately, it will permit comparative clauses such as more grapes and fewer than avocados under the interpretation in which there are more grapes than avocados and fewer of some unspeciﬁed thing than avocados (see Figure 22.6). One aspect of this analysis is that it handles the elliptical comparatives such as the following: 214 CHAPTER 22. COMPARATIVES NP r NP NP 1 NA Nr Ad more Nf grapes Conj and NP Nr Ad fewer Nf boars PP P than NP N avocados NP r NP f NP 1 NA Ad more N NA ε Conj or NP Ad less N NA ε PP P than NP Nr N fifty Nf NA people Figure 22.5: Comparative conjunctions. (422) Arnold kills more bad guys than Steven. In a sense, this is actually only simulating the ellipsis of these constructions indirectly. However, consider the following sentences: (423) Arnold kills more bad guys than I do. (424) Arnold kills more bad guys than I. 22.3. PROPOSITIONAL COMPARATIVES 215 NP r NP f NA NP 1 NA Nr Ad more Nf NA grapes Conj and NP Ad fewer N NA ε PP P than NP N avocados Figure 22.6: Comparative conjunctions. (425) Arnold kills more bad guys than me. The ﬁrst of these has a pro-verb phrase which has a nominative subject. If we totally drop the second verb phrase, we ﬁnd that the second NP can be in either the nominative or the accusative case. Prescriptive grammars disallow accusative case, but it actually is more common to ﬁnd accusative caseuse of the nominative in conversation tends to sound rather stiﬀ and unnatural. This accords with the present analysis in which the second noun phrase in these comparatives is the complement of than in βnxPnx, and receives its case-marking from than. This does mean that the grammar will not currently accept (424), and indeed such sentences will only be covered by an analysis which really deals with the ellipsis. Yet the fact that most speakers produce (425) indicates that some sort of restructuring has occured that results in the kind of structure the present analysis oﬀers. There is yet another distributional fact which falls out of this analysis. When comparative or comparativized adjectives modify a noun phrase, they can stand alone or occur with a than phrase; furthermore, they are obligatory when a than-phrase is present. (426) Hobbes is a better teacher. (427) Hobbes is a better teacher than Bill. (428) A more exquisite horse launched onto the racetrack. (429) A more exquisite horse than Black Beauty launched onto the racetrack. 216 CHAPTER 22. COMPARATIVES (430) Hobbes is a teacher than Bill. Comparative adjectives such as better come from the lexicon as compar. By having trees such as βAn transmit the compar value of the A node to the root N node, we can signal to βCnxPnx that it may adjoin when a comparative adjective has adjoined. An example of such an adjunction is given in Figure 22.7. Of course, if no comparative element is present in the lower part of the noun phrase, βnxPnx will not be able to adjoin since nouns themselves are compar-. In order to capture the fact that a comparative element blocks further modiﬁcation to N, βAn must only adjoin to N nodes which are compar- in their lower feature matrix. NP r NP f NA Nr A better Nf NA teacher PP P than NP N Bill Figure 22.7: Adjunction of βnxPnx to NP modiﬁed by comparative adjective. In order to obtain this result for phrases like more exquisite horse, we need to provide a way for more and less to modify adjectives without a than-clause as we have with βARBaPa. Actually, we need this ability independently for comparative adjectival phrases, as discussed in the next section. 22.3.2 Adjectival Comparatives With nominal comparatives, we saw that a single analysis was amenable to both pure com- paratives and elliptical comparatives. This is not possible for adjectival comparatives, as the following examples demonstrate: (431) The dog is less patient. (432) The dog is less patient than the cat. (433) The dog is as patient. 22.3. PROPOSITIONAL COMPARATIVES 217 (434) The dog is as patient as the cat. (435) The less patient dog waited eagerly for its master. (436) The less patient than the cat dog waited eagerly for its master. The last example shows that comparative adjectival phrases cannot distribute quite as freely as comparative nominals. The analysis of elliptical comparative adjectives follows closely to that of comparative nom- inals. We build them up by ﬁrst adjoining the comparative element to the A node, which then signals to the AP node, via the compar feature, that it may allow a than-clause to adjoin. The relevant trees are given in Figure 22.8. βCARBa is anchored by more, less and as, and βaxPnx is anchored by both than and as. Ar Ad Af NA APr APf PP P NP  (a) βCARBa tree (b) βaxPnx tree Figure 22.8: Elliptical adjectival comparative trees The advantages of this analysis are many. We capture the distribution exhibited in the examples given in (431) - (436). With βCARBa, comparative elements may modify adjectives wherever they occur. However, than clauses for adjectives have a more restricted distribution which coincides nicely with the distribution of APs in the XTAG grammar. Thus, by making them adjoin to AP rather than A, ill-formed sentences like (436) are not allowed. There are two further advantages to this analysis. One is that βCARBa interacts with βnxPnx to produce sequences like more exquisite horse than Black Beauty, a result alluded to at the end of Section 22.3.1. We achieve this by ensuring that the comparativeness of an adjective is controlled by a comparative adverb which adjoins to it. A sample derivation is given in Figure 22.9. The second advantage is that we get sentences such as (437) for free. (437) Hobbes is better than Bill. Since better comes from the lexicon as compar and this value is passed up to the AP node, βaxPnx can adjoin as desired, giving us the derivation given in Figure 22.10. Notice that the root AP node of Figure 22.10 is compar-, so we are basically saying that strings such as better than Bill are not comparative. This accords with our use of the compar 218 CHAPTER 22. COMPARATIVES NP r NP f Nr Ar Ad more Af NA exquisite Nf NA horse PP P than NP Nr A Black Nf NA Beauty Figure 22.9: Comparativized adjective triggering βCnxPnx. featurea positive value for compar signals that the clause beneath it is to be compared against something else. In the case of better than Bill, the comparison has been fulﬁlled, so we do not want it to signal for further comparisons. A nice result which follows is that βaxPnx cannot adjoin more than once to any given AP spine, and we have no need for the NA constraint on the trees root node. Also, this treatment of the comparativeness of various strings proves important in getting the coordination of comparative constructions to work properly. A note needs to be made about the analysis regarding the interaction of the equivalence comparative construction as ... as and the inequivalence comparative construction moreless ... than. In the grammar, more, less, and as all anchor βCARBa, and both than and as anchor βaxPnx. Without further modiﬁcations, this of course will give us sentences such as the following: (438) ?Hobbes is as patient than Bill. (439) ?Hobbes is more patient as Bill. Such cases are blocked with the feature equiv: more, less, fewer and than are equiv- while as (in both adverbial and prepositional uses) is equiv. The prepositional trees then require that their P node and the node to which they are adjoining match for equiv. An interesting phenomena in which comparisons seem to be paired with an inappropriate asthan-clause is exhibited in (440) and (441). 22.3. PROPOSITIONAL COMPARATIVES 219 APr NA compar : - APf NA wh : 1 - compar : 2  A compar : 2 wh : 1 better PP P than NP N Bill Figure 22.10: Adjunction of βaxPnx to comparative adjective. (440) Hobbes is as patient or more patient than Bill. (441) Hobbes is more patient or as patient as Bill. Though prescriptive grammars disfavor these sentences, these are perfectly acceptable. We can capture the fact that the asthan-clause shares the equiv value with the latter of the comparison phrases by passing the equiv value for the second element to the root of the coordination tree. 22.3.3 Adverbial Comparatives The analysis of adverbial comparatives encouragingly parallels the analysis for nominal and elliptical adjectival comparativeswith, however, some interesting diﬀerences. Some examples of adverbial comparatives and their distribution are given in the following: (442) Albert works more quickly. (443) Albert works more quickly than Richard. (444) Albert works more. 220 CHAPTER 22. COMPARATIVES (445) Albert more works. (446) Albert works more than Richard. (447) Hobbes eats his supper more quickly than Calvin. (448) Hobbes more quickly eats his supper than Calvin. (449) Hobbes more quickly than Calvin eats his supper. When more is used alone as an adverb, it must also occur after the verb phrase. Also, it appears that adverbs modiﬁed by more and less have the same distribution as when they are not modiﬁed. However, the than portion of an adverbial comparative is restricted to post verb phrase positions. The ﬁrst observation can be captured by having more and less select only βvxARB from the set of adverb trees. Comparativization of adverbs looks very similar to that of other categories, and we follow this trend by giving the tree in Figure 22.11(a), which parallels the adjectival and nominal trees, for these instances. This handles the quite free distribution of adverbs which have been comparativized, while the tree in Figure 22.11(b), βvxPnx, allows the than portion of an adverbial comparative to occur only after the verb phrase, blocking examples such as (449). Adr Ad Adf NA VPr VPf PP P NP  (a) βCARBarb tree (b) βvxPnx tree Figure 22.11: Adverbial comparative trees The usage of the compar feature parallels that of the adjectives and nominals; however, trees which adjoin to VP are compar- on their root VP node. In this way, βvxPnx anchored by than or as (which must adjoin to a compar VP) can only adjoin immediately above a comparative or comparativized adverb. This avoids extra parses in which the comparative adverb adjoins at a VP node lower than the than-clause. A ﬁnal note is that as may anchor βvxPnx non-comparatively, as in sentence (450). This means that there will be two parses for sentences such as (451). 22.4. FUTURE WORK 221 (450) John works as a carpenter. (451) John works as quickly as a carpenter. This appears to be a legitimate ambiguity. One is that John works as quickly as a carpenter (works quickly), and the other is that John works quickly when he is acting as a carpenter (but maybe he is slow when he acting as a plumber). 22.4 Future Work  Interaction with determiner sequencing (e.g., several more men than women but not every more men than women).  Handle sentential complement comparisons (e.g., Bill eats more pasta than Angus drinks beer).  Add partitives.  Deal with constructions like as many and as much.  Look at so...as construction. Chapter 23 Punctuation Marks Many parsers require that punctuation be stripped out of the input. Since punctuation is often optional, this sometimes has no eﬀect. However, there are a number of constructions which must obligatorily contain punctuation and adding analyses of these to the grammar without the punctuation would lead to severe overgeneration. An especially common example is noun appositives. Without access to punctuation, one would have to allow every combinatorial possibility of NPs in noun sequences, which is clearly undesirable (especially since there is already unavoidable noun-noun compounding ambiguity). Aside from coverage issues, it is also preferable to take input as is and do as little editing as possible. With the addition of punctuation to the XTAG grammar, we need only doassume the conversion of certain sequences of punctuation into the British order (this is discussed in more detail below in Section 23.2). The XTAG POS tagger currently tags every punctuation mark as itself. These tags are all converted to the POS tag Punct before parsing. This allows us to treat the punctuation marks as a single POS class. They then have features which distinguish amongst them. Wherever possible we have the punctuation marks as anchors, to facilitate early ﬁltering. The full set of punctuation marks is separated into three classes: balanced, separating and terminal. The balanced punctuation marks are quotes and parentheses, separating are commas, dashes, semi-colons and colons, and terminal are periods, exclamation points and question marks. Thus, the punct feature is complex (like the agr feature), yielding feature equations like Punct bal  paren or Punct term  excl. Separating and terminal punctuation marks do not occur adjacent to other members of the same class, but may occasionally occur adjacent to members of the other class, e.g. a question mark on a clause which is separated by a dash from a second clause. Balanced punctuation marks are sometimes adjacent to one another, e.g. quotes immediately inside of parentheses. The punct feature allows us to control these local interactions. We also need to control non-local interaction of punctuation marks. Two cases of this are so-called quote alternation, wherein embedded quotation marks must alternate between single and double, and the impossibility of embedding an item containing a colon inside of another item containing a colon. Thus, we have a fourth value for punct, contains colondquoteetc. -, which indicates whether or not a constituent contains a particular punctuation mark. This feature is percolated through all auxiliary trees. Things which may not embed are: colons under colons, semi-colons, dashes or commas; semi-colons under semi- 222 23.1. APPOSITIVES, PARENTHETICALS AND VOCATIVES 223 colon or commas. Although it is rare, parentheses may appear inside of parentheses, say with a bibliographic reference inside a parenthesized sentence. 23.1 Appositives, parentheticals and vocatives These trees handle constructions where additional lexical material is only licensed in conjunction with particular punctuation marks. Since the lexical material is unconstrained (virtually any noun can occur as an appositive), the punctuation marks are anchors and the other nodes are substitution sites. There are cases where the lexical material is restricted, as with parenthetical adverbs like however, and in those cases we have the adverb as the anchor and the punctuation marks as substitution sites. When these constructions can appear inside of clauses (non-peripherally), they must be sep- arated by punctuation marks on both sides. However, when they occur peripherally they have either a preceding or following punctuation mark. We handle this by having both peripheral and non-peripheral trees for the relevant constructions. The alternative is to insert the second (following) punctuation mark in the tokenization process (i.e. insert a comma before the period when an appositive appears on the last NP of a sentence). However, this is very diﬃcult to do accurately. 23.1.1 βnxPUnxPU The symmetric (non-peripheral) tree for NP appositives, anchored by: comma, dash or paren- theses. It is shown in Figure 23.1 anchored by parentheses. (452) The music here , Russell Smiths Tetrameron  , sounded good . [Brown:cc09] (453) ...cost 2 million pounds (3 million dollars) (454) Sen. David Boren (D., Okla.)... (455) ...some analysts believe the two recent natural disasters  Hurricane Hugo and the San Francisco earthquake  will carry economic ramiﬁcations.... [WSJ] The punctuation marks are the anchors and the appositive NP is substituted. The appositive can be conjoined, but only with a lexical conjunction (not with a comma). Appositives with commas or dashes cannot be pronouns, although they may be conjuncts containing pronouns. When used with parentheses this tree actually presents an alternative rather than an appositive, so a pronoun is possible. Finally, the appositive position is restricted to having nominative or accusative case to block PRO from appearing here. Appositives can be embedded, as in (456), but do not seem to be able to stack on a single NP. In this they are more like restrictive relatives than appositive relatives, which typically can stack. (456) ...noted Simon Briscoe, UK economist for Midland Montagu, a unit of Midland Bank PLC. 224 CHAPTER 23. PUNCTUATION MARKS NPr NPf NA Punct1 ( NPr D 3 NPf NA Nr N million Nf NA dollars Punct2 ) Figure 23.1: The βnxPUnxPU tree, anchored by parentheses 23.1.2 βnPUnxPU The symmetric (non-peripheral) tree for N-level NP appositives, is anchored by comma. The modiﬁer is typically an address. It is clear from examples such as (457) that these are attached at N, rather than NP. Carrier is not an appositive on Menlo Park, as it would be if these were simply stacked appositives. Rather, Calif. modiﬁes Menlo Park, and that entire complex is compounded with carrier, as shown in the correct derivation in Figure 23.2. Because this distinction is less clear when the modiﬁer is peripheral (e.g. ends the sentence), and it would be diﬃcult to distinguish between NP and N attachment, we do not currently allow a peripheral N-level attachment. (457) An oﬃcial at Consolidated Freightways Inc., a Menlo Park, Calif., less-than-truckload carrier , said... (458) Rep. Ronnie Flippo (D., Ala.), of the delegation, says... 23.1.3 βnxPUnx This tree, which can be anchored by a comma, dash or colon, handles asymmetric (peripheral) NP appositives and NP colon expansions of NPs. Figure 23.3 shows this tree anchored by a dash and a colon. Like the symmetric appositive tree, βnxPUnxpu, the asymmetric appositive cannot be a pronoun, while the colon expansion can. Thus, this constraint comes from the syntactic entry in both cases rather than being built into the tree. (459) the banks 90 shareholder  Petroliam Nasional Bhd. [Brown] 23.1. APPOSITIVES, PARENTHETICALS AND VOCATIVES 225 NPr NPf NA N Consolidated Punct1 , NPr D a NPf NA Nr Nr Nf NA Menlo-Park Punct1 , NP N Calif Punct2 , Nf NA carrier Punct2 , Figure 23.2: An N-level modiﬁer, using the βnPUnx tree (460) ...said Chris Dillow, senior U.K. economist at Nomura Research Institute . (461) ...qualities that are seldom found in one work: Scrupulous scholarship, a fund of personal experience,... [Brown:cc06] (462) I had eyes for only one person : him . The colon expansion cannot itself contain a colon, so the foot S has the feature NP.t: punctcontainscolon  . 23.1.4 βPUpxPUvx Tree for pre-VP parenthetical PP, anchored by commas or dashes - (463) John , in a ﬁt of anger , broke the vase (464) Mary , just within the last year , has totalled two cars These are clearly not NP modiﬁers. Figures 23.4 and 23.5 show this tree alone and as part of the parse for (463). 226 CHAPTER 23. PUNCTUATION MARKS NPr D NPr D the NPf NA N bank G s NPr NPf NA Nr N 90 Nf NA shareholder Punct - NP Nr N Paetroliam Nf NA N Naasional Nf NA Bhd Sr NPr D our NPf NA N family VPr Ad always VP NA V HATED NPr NPf NA N cats Punct : NP Nr A A1 NA A1 NA nasty Conj , A low Conj , A vulgar Nf NA things (a) (b) Figure 23.3: The derived trees for an NP with (a) a peripheral, dash-separated appositive and (b) an NP colon expansion (uttered by the Mouse in Alices Adventures in Wonderland) VPr Punct1 , PP Punct2 , VP NA Figure 23.4: The βPUpxPUvx tree, anchored by commas 23.1. APPOSITIVES, PARENTHETICALS AND VOCATIVES 227 Sr NP N John VPr Punct1 , PP P in NPr D a NPf NA NPf NA N fit PP P of NP N anger Punct2 , VP NA V broke NPr D the NPf NA N vase Figure 23.5: Tree illustrating the use of βPUpxPUvx 23.1.5 βpuARBpuvx Parenthetical adverbs - however, though, etc. Since the class of adverbs is highly restricted, this tree is anchored by the adverb and the punctuation marks substitute. The punctuation marks may be either commas or dashes. Like the parenthetical PP above, these are clearly not NP modiﬁers. (465) The new argument over the notiﬁcation guideline , however , could sour any atmosphere of cooperation that existed . [WSJ] 23.1.6 βsPUnx Sentence ﬁnal vocative, anchored by comma: (466) You were there , Stanleymy boy . Also, when anchored by colon, NP expansion on S. These often appear to be extraposed modiﬁers of some internal NP. The NP must be quite heavy, and is usually a list: (467) Of the major expansions in 1960, three were ﬁnanced under the R. I. Industrial Building Authoritys 100 guaranteed mortgage plan: Collyer Wire, Leesona Corporation, and American Tube  Controls. 228 CHAPTER 23. PUNCTUATION MARKS A simpliﬁed version of this sentence is shown in ﬁgure 23.6. The NP cannot be a pronoun in either of these cases. Both vocatives and colon expansions are restricted to appear on tensed clauses (indicative or imperative). Sr S NA NPr D three NPf NA N expansions VPr V were VP NA VP NA V financed PP P under NPr D the NPf NA N plan Punct : NP NP1 NA NP1 NA Nr N Collyer Nf NA Wire Conj , NP Nr N Leesona Nf NA Corporation Conj and NP Nr N American Nf NA Controls Figure 23.6: A tree illustrating the use of sPUnx for a colon expansion attached at S. 23.1.7 βnxPUs Tree for sentence initial vocatives, anchored by a comma: (468) Stanleymy boy , you were there . The noun phrase may be anything but a pronoun, although it is most commonly a proper noun. The clause adjoined to must be indicative or imperative. 23.2 Bracketing punctuation 23.2.1 Simple bracketing Trees: βPUsPU, βPUnxPU, βPUnPU, βPUvxPU, βPUvPU, βPUarbPU, βPUaPU, βPUdPU, βPUpxPU, βPUpPU 23.2. BRACKETING PUNCTUATION 229 These trees are selected by parentheses and quotes and can adjoin onto any node type, whether a head or a phrasal constituent. This handles things in parentheses or quotes which are syn- tactically integrated into the surrounding context. Figure 23.7 shows the βPUsPU anchored by parentheses, and this tree along with βPUnxPU in a derived tree. (469) Dick Carroll and his accordion (which we now refer to as Freida) held over at Bahia Cabana where Sir Judson Smith brings in his calypso capers Oct. 13 . [Brown:ca31] (470) ...noted that the term teacher-employee (as opposed to, e.g., maintenance employee) was a not inapt description. [Brown:ca35] Sr Punct1 ( Sf NA Punct2 ) NPr NPf NA D his NPf NA N accordion Sr Punct1 ( Sf NA Comp which Sr NA NP N we VPr VP NA V refer PP1 P to NP1 NA ε PP P as NPr Punct1  NPf NA N Freida Punct2  Punct2 ) (a) (b) Figure 23.7: βPUsPU anchored by parentheses, and in a derivation, along with βPUnxPU There is a convention in English that quotes embedded in quotes alternate between single and double; in American English the outermost are double quotes, while in British English they are single. The contains feature is used to control this alternation. The trees anchored by double quotation marks have the feature punct contains dquote  - on the foot node and the feature punct contains dquote   on the root. All adjunction trees are transparent to the contains feature, so if any tree below the double quote is itself enclosed in double quotes the derivation will fail. Likewise with the trees anchored by single quotes. The quote trees in eﬀect toggle the contains Xquote feature. Immediate proximity is handled by the punct balanced feature, which allows quotes inside of parentheses, but not vice-versa. 230 CHAPTER 23. PUNCTUATION MARKS In addition, American English typically placesmoves periods (and commas) inside of quo- tation marks when they would logically occur outside, as in example 471. The comma in the ﬁrst part of the quote is not part of the quote, but rather part of the parenthetical quoting clause. However, by convention it is shifted inside the quote, as is the ﬁnal period. British English does not do this. We assume here that the input has already been tokenized into the British format. (471) You cant do this to us , Diane screamed . We are Americans. The βPUsPU can handle quotation marks around multiple sentences, since the sPUs tree allows us to join two sentences with a period, exclamation point or question mark. Currently, however, we cannot handle the style where only an open quote appears at the beginning of a paragraph when the quotation extends over multiple paragraphs. We could allow a lone open quote to select the βPUs tree, if this is deemed desirable. Also, the βPUsPU is selected by a pair of commas to handle non-peripheral appositive relative clauses, such as in example (472). Restrictive and appositive relative clauses are not syntactically diﬀerentiated in the XTAG grammar (cf. Chapter 14). (472) This news , announced by Jerome Toobin , the orchestras administrative director , brought applause ... [Brown:cc09] The trees discussed in this section will only allow balanced punctuation marks to adjoin to constituents. We will not get them around non-constituents, as in (473). (473) Mary asked him to leave (and he left) 23.2.2 βsPUsPU This tree allows a parenthesized clause to adjoin onto a non-parenthesized clause. (474) Innumerable motels from Tucson to New York boast swimming pools (  swim at your own risk  is the hospitable sign poised at the brink of most pools ) . [Brown:ca17] 23.3 Punctuation trees containing no lexical material 23.3.1 αPU This is the elementary tree for substitution of punctuation marks. This tree is used in the quoted speech trees, where including the punctuation mark as an anchor along with the verb of saying would require a new entry for every tree selecting the relevant tree families. It is also used in the tree for parenthetical adverbs (βpuARBpuvx), and for S-adjoined PPs and adverbs (βspuARB and βspuPnx). 23.3. PUNCTUATION TREES CONTAINING NO LEXICAL MATERIAL 231 23.3.2 βPUs Anchored by comma: allows comma-separated clause initial adjuncts, (475-476). (475) Here , as in Journal , Mr. Louis has given himself the lions share of the dancing... [Brown:cc09] (476) Choreographed by Mr. Nagrin, the work ﬁlled the second half of a program To keep this tree from appearing on root Ss (i.e. , sentence), we have a root constraint that punct struct  nil (similar to the requirement that root Ss be tensed, i.e. mode  indimp). The punct struct  nil feature on the foot blocks stacking of multiple punctuation marks. This feature is shown in the tree in Figure 23.8. Sr invlink : 1 inv : 1 punct : struct : 2 displ-const : 3 agr : 4 assign-case : 5 mode : 6 sub-conj : 7 extracted : 8 tense : 9 assign-comp : 10 comp : 11 Punct punct : struct : 2 punct : struct : comma , S NA inv : 1 punct : struct : nil displ-const : 3 agr : 4 assign-case : 5 mode : 6 sub-conj : 7 extracted : 8 tense : 9 assign-comp : 10 comp : 11 Figure 23.8: βPUs, with features displayed This tree can be also used by adjuncts on embedded clauses: 232 CHAPTER 23. PUNCTUATION MARKS (477) One might expect that in a poetic career of seventy-odd years, some changes in style and method would have occurred, some development taken place. [Brown:cj65] These adjuncts sometimes have commas on both sides of the adjunct, or, like (477), only have them at the end of the adjunct. Finally, this tree is also used for peripheral appositive relative clauses. (478) Interest may remain limited into tomorrows U.K. trade ﬁgures, which the market will be watching closely to see if there is any improvement after disappointing numbers in the previous two months. 23.3.3 βsPUs This tree handles clausal coordination with comma, dash, colon, semi-colon or any of the terminal punctuation marks. The ﬁrst clause must be either indicative or imperative. The second may also be inﬁnitival with the separating punctuation marks, but must be indicative or imperative with the terminal marks; with a comma, it may only be indicative. The two clauses need not share the same mode. NB: Allowing the terminal punctuation marks to anchor this tree allows us to parse sequences of multiple sentences. This is not the usual mode of parsing; if it were, this sort of sequencing might be better handled by a higher level of processing. (479) For critics , Hardy has had no poetic periods  one does not speak of early Hardy or late Hardy , or of the London or Max Gate period.... (480) Then there was exercise , boating and hiking , which was not only good for you but also made you more virile : the thought of strenuous activity left him exhausted. This construction is one of the few where two non-bracketing punctuation marks can be adjacent. It is possible (if rare) for the ﬁrst clause to end with a question mark or exclamation point, when the two clauses are conjoined with a semi-colon, colon or dash. Features on the foot node, as shown in Figure 23.9, control this interaction. Complementizers are not permitted on either conjunct. Subordinating conjunctions some- times appear on the right conjunct, but seem to be impossible on the left: (481) Killpath would just have to go out and drag Gun back by the heels once an hour ; because hed be damned if he was going to be a mid-watch pencil-pusher . [Brown:cl17] (482) The best rule of thumb for detecting corked wine (provided the eye has not already spotted it) is to smell the wet end of the cork after pulling it : if it smells of wine , the bottle is probably all right ; if it smells of cork , one has grounds for suspicion. [Brown:cf27] 23.3. PUNCTUATION TREES CONTAINING NO LEXICAL MATERIAL 233 Sr wh : 1 displ-const : 2 agr : 3 assign-case : 4 mode : 5 indimp tense : 6 assign-comp : 7 comp : 8 nil punct : contains : 9 Sf NA mode : 5 comp : 8 assign-comp : 7 tense : 6 assign-case : 4 agr : 3 displ-const : 2 wh : 1 sub-conj : nil punct : struct : none term : exclqmark contains : colon : - Punct punct : contains : 9 punct : contains : scolon :  struct : scolon ; S1 comp : nil punct : struct : none contains : colon : - mode : indimpinf Figure 23.9: βsPUs, with features displayed 23.3.4 βsPU This tree handles the sentence ﬁnal punctuation marks when selected by a question mark, exclamation point or period. One could also require a ﬁnal punctuation mark for all clauses, but such an approach would not allow non-periods to occur internally, for instance before a semi- colon or dash as noted above in Section 23.3.3. This tree currently only adjoins to indicative or imperative (root) clauses. (483) He left ! (484) Get lost . (485) Get lost ? The feature punct bal nil on the foot node ensures that this tree only adjoins inside of parentheses or quotes completely enclosing a sentence (486), but does not restrict it from adjoining to clause which ends with balanced punctuation if only the end of the clause is contained in the parentheses or quotes (487). 234 CHAPTER 23. PUNCTUATION MARKS (486) (John then left .) (487) (John then left) . (488) Mary asked him to leave (immediately) . This tree is also selected by the colon to handle a colon expansion after adjunct clause  (489) Expressed diﬀerently : if the price for becoming a faithful follower... [Brown:cd02] (490) Expressing it diﬀerently : if the price for becoming a faithful follower... (491) To express it diﬀerently : if the price for becoming a faithful follower... [Brown:cd02] This tree is only used after adjunct (untensed) clauses, which adjoin to the tensed clause using the adjunct clause trees (cf Section 15 ); the mode of the complete clause is that of the matrix rather than the adjunct. Indicative or imperative (i.e. root) clauses separated by a colon use the βsPUs tree (Section 23.3.3). 23.3.5 βvPU This tree is anchored by a colon or a dash, and occurs between a verb and its complement. These typically are lists. (492) Printed material Available , on request , from U.S. Department of Agriculture , Wash- ington 25 , D.C. , are : Cooperative Farm Credit Can Assist...... [Brown:ch01] 23.3.6 βpPU This tree is anchored by a colon or a dash, and occurs between a preposition and its complement. It typically occurs with a sequence of complements. As with the tree above, this typically occurs with a conjoined complement. (493) ...and utilization such as : (A) the protection of forage... (494) ...can be represented as : Af. 23.4 Other trees 23.4.1 βspuARB In general, we attach post-clausal modiﬁers at the VP node, as you typically get scope ambiguity eﬀects with negation (John didnt leave today  did he leave or not?). However, with post- sentential, comma-separated adverbs, there is no ambiguity - in John didnt leave, today he deﬁnitely did not leave. Since this tree is only selected by a subset of the adverbs (namely, those which can appear pre-sententially, without a punctuation mark), it is anchored by the adverb. (495) The names of some of these products dont suggest the risk involved in buying them , either . [WSJ] 23.4. OTHER TREES 235 23.4.2 βspuPnx Clause-ﬁnal PP separated by a comma. Like the adverbs described above, these diﬀer from VP adjoined PPs in taking widest scope. (496) ...gold for current delivery settled at 367.30 an ounce , up 20 cents . (497) It increases employee commitment to the company , with all that means for eﬃciency and quality control . 23.4.3 βnxPUa Anchored by colon or dash, allows for post-modiﬁcation of NPs by adjectives. (498) Make no mistake , this Gorky Studio drama is a respectable import  aptly grave , carefully written , performed and directed . Part V Appendices 236 Appendix A Future Work A.1 Adjective ordering At this point, the treatment of adjectives in the XTAG English grammar does not include selectional or ordering restrictions.1 Consequently, any adjective can adjoin onto any noun and on top of any other adjective already modifying a noun. All of the modiﬁed noun phrases shown in (499)-(502) currently parse. (499) big green bugs (500) big green ideas (501) colorless green ideas (502) green big ideas While (500)-(502) are all semantically anomalous, (502) also suﬀers from an ordering prob- lem that makes it seem ungrammatical as well. Since the XTAG grammar focuses on syntactic constructions, it should accept (499)-(501) but not (502). Both the auxiliary and determiner ordering systems are structured on the idea that certain types of lexical items (speciﬁed by fea- tures) can adjoin onto some types of lexical items, but not others. We believe that an analysis of adjectival ordering would follow the same type of mechanism. A.2 More work on Determiners In addition to the analysis described in Chapter 18, there remains work to be done to complete the analysis of determiner constructions in English.2 Although constructions such as determiner coordination are easily handled if overgeneration is allowed, blocking sequences such as one and some while allowing sequences such as ﬁve or ten still remains to be worked out. There are 1This section is a repeat of information found in section 19.1. 2This section is from [Hockey and Mateyak, 1998]. 238 A.3. -ING ADJECTIVES 239 still a handful of determiners that are not currently handled by our system. We do not have an analysis to handle most, such, certain, other and own3. In addition, there is a set of lexical items that we consider adjectives (enough, less, more and much) that have the property that they cannot cooccur with determiners. We feel that a complete analysis of determiners should be able to account for this phenomenon, as well. A.3 -ing adjectives An analysis has already been provided for past participal (-ed) adjectives (as in sentence (503)), which are restricted to the Transitive Verb family.4 A similar analysis needs to take place for the present participle (-ing) used as a pre-nominal modiﬁer. This type of adjective, however, does not seem to be as restricted as the -ed adjectives, since verbs in other tree families seem to exhibit this alternation as well (e.g. sentences (504) and (505)). (503) The murdered man was a doctoral student at UPenn . (504) The man died . (505) The dying man pleaded for his life . A.4 Verb selectional restrictions Although we explicitly do not want to model semantics in the XTAG grammar, there is some work along the syntaxsemantics interface that would help reduce syntactic ambiguity and thus decrease the number of semantically anomalous parses. In particular, verb selectional restrictions, particularly for PP arguments and adjuncts, would be quite useful. With the exception of the required to in the Ditransitive with PP Shift tree family (Tnx0Vnx1Pnx2), any preposition is allowed in the tree families that have prepositions as their arguments. In addition, there are no restrictions as to which prepositions are allowed to adjoin onto a given verb. The sentences in (506)-(508) are all currently accepted by the XTAG grammar. Their violations are stronger than would be expected from purely semantic violations, however, and the presence of verb selectional restrictions on PPs would keep these sentences from being accepted. (506) survivors walked of the street . (507) The man about the earthquake survived . (508) The president arranged on a meeting . 3The behavior of own is suﬃciently unlike other determiners that it most likely needs a tree of its own, adjoining onto the right-hand side of genitive determiners. 4This analysis may need to be extended to the Transitive Verb particle family as well. 240 APPENDIX A. FUTURE WORK A.5 Thematic Roles Elementary trees in TAGs capture several notions of locality, with the most primary of these being locality of θ-role assignment. Each elementary tree has associated with it the θ-roles assigned by the anchor of that elementary tree. In the current XTAG system, while the notion of locality of θ-role assignment within an elementary tree has been implicit, the θ-roles assigned by a head have not been explicitly represented in the elementary tree. Incorporating θ-role information will make the elementary trees more informative and will enable eﬃcient pruning of spurious derivations when embedded into a speciﬁc context. In the case of a Synchronous TAG, θ-roles can also be used to automatically establish links between two elementary trees, one in the object language and one in the target language. Appendix B Metarules B.1 Introduction XTAG has now a collection of functions accessible from the user interface that helps the user in the construction and maintenance of a tag tree-grammar. This subsystem is based on the idea of metarules ([Becker, 1993]). Here our primary purpose is to describe the facilities implemented under this metarule-based subsystem. For a discussion of the metarules as a method for compact representation of the Lexicon see [Becker, 1993] and [Srinivas et al., 1994]. The basic idea of using metarules is to take proﬁt of the similarities of the relations involving related pairs of XTAG elementary trees. For example, in the English grammar described in this technical report, comparing the XTAG trees for the basic form and the wh-subject moved form, the relation between this two trees for transitive verbs (αnx0V nx1, αW0nx0V nx1) is similar to the relation for the intransitive verbs (αnx0V , αW0nx0V ) and also to the relation for the ditransitives (αnx0V nx1nx2, αW0nx0V nx1nx2). Hence, instead of generating by hand the six trees mentioned above, a more natural and robust way would be generating by hand only the basic trees for the intransitive, transitive and ditransitive cases, and letting the wh-subject moved trees to be automatically generated by the application of a unique transformation rule that would account exactly for the identical relation involved in each of the three pairs above. Notice that the degree of generalization can be much higher than it might be thought in principle from the above paragraph. For example, once a rule for passivization is applied to the tree diﬀerent basic trees above, the wh-subject moved rule could be again applied to generate the wh-moved subject versions for the passive form. Depending on the degree of regularity that one can ﬁnd in the grammar being built, the reduction in the number of original trees can be exponential. We still make here a point that the reduction of eﬀort in grammar construction is not the only advantage of the approach. Robustness, reliability and maintainability of the grammar achieved by the use of metarules are equally or even more important. In the next section we deﬁne a metarule in XTAG. Section 3 gives some linguistically motivated examples of metarule for the English grammar described in this technical report and their application. Section 4 describes the access through the user interface. 241 242 APPENDIX B. METARULES B.2 The deﬁnition of a metarule in XTAG A metarule speciﬁes a rule for transforming grammar rules into grammar rules. In XTAG the grammar rules are lexicalized trees. Hence an XTAG metarule mr is a pair (lhs, rhs) of XTAG trees, where:  lhs, the left-hand side of the metarule, is a pattern tree, i.e., it is intended to present a speciﬁc pattern of tree to look for in the trees submitted to the application of the metarule.  When a metarule mr is applied to an input tree inp, the ﬁrst step is to verify if the input tree matches the pattern speciﬁed by the lhs. If there is no match, the application fails.  rhs, the right-hand side of the metarule, speciﬁes (together with lhs) the transformation that will be done in inp, in case of successful matching, thus generating the output tree of the metarule application1. B.2.1 Node names, variable instantiation, and matches We will use the terms (lhs, rhs and inp) as introduced above to refer to the parts of a generic metarule being applied to an input tree. The nodes at lhs can take three diﬀerent forms: a constant node, a typed variable node, and a non-typed variable node. The naming conventions for these diﬀerent classes of nodes is given below.  Constant Node: Its name must not initiate by a question mark (? character). They are like we expect for names to be in normal XTAG trees; for instance, inp is expected to have only constant nodes. Some examples of constant nodes are NP, V , NP0, NP1, Sr. We will call the two parts that compose such names the stem and the subscript. In the examples above NP, V and S are stems and 0, 1, r are subscripts. Notice that the subscript part can also be empty as in two of the above examples.  Non-Typed Variable Node: Its name initiates by a question mark (?), followed by a sequence of digits (i.e. a number) which uniquely identiﬁes the variable. Examples: ?1, ?3, ?34522. We assume that there is no stem and no subscript in this names, i.e., ? is just a meta-character to introduce a variable, and the number is the variable identiﬁer.  Typed Variable Node: Its name initiates by a question mark (?) followed by a sequence of digits, but is additionally followed by a type speciﬁers deﬁnition. A type speciﬁers deﬁnition is a sequence of one or more type speciﬁer separated by a slash (). A type speciﬁer has the same form of a regular XTAG node name (like the constant nodes), except that the subscript can be also a question mark. Examples of typed variables are: ?1V P (a single type speciﬁer with stem V P and no subscript), ?3NP1PP (two type speciﬁers, NP1 and PP), ?1NP? (one type speciﬁer, NP? with undetermined subscript). 1actually more than one output tree can be generated from the successful application of a rule to an input tree, as will be seen soon 2Notice however that having the sole purpose of distinguishing between variables, a number like the one in the last example is not very likely to occur, and a metarule with more than three thousand variables can give you a place in the Guinness TagBook of Records. B.2. THE DEFINITION OF A METARULE IN XTAG 243 Well see ahead that each type speciﬁer represents an alternative for matching, and the presence of ? in subscript position of a type speciﬁer means that matching will only check for the stem 3. During the process of matching, variables are associated (we use the term instantiated) with tree material. According to its class a variable can be instantiated with diﬀerent kinds of tree material:  A typed variable will be instantiated with exactly one node of the input tree, which is in accordance to one of its type speciﬁers (The full rule is in the following subsection).  A non-typed variable will be instantiated with a range of subtrees. These subtrees will be taken from one of the nodes of the input tree inp. Hence, there will a node n in inp, with subtrees n.t1, n.t2, ..., n.tk, in this order, where the variable will be instantiated with some subsequence of these subtrees (e.g., n.t2, n.t3, n.t4). Note however, that some of these subtrees, may be incomplete, i.e., they may not go all the way to the bottom leaves. Entire subtrees may be removed. Actually for each child of the non-typed variable node, one subtree that matches this child subtree will be removed from some of the n.ti(maybe an entire n.ti), leaving in place a mark for inserting material during the substitution of occurences at rhs. Notice still that the variable can be instantiated with a single tree and even with no tree. We deﬁne a match to be a complete instantiation of all variables appearing in the metarule. In the process of matching, there may be several possible ways of instantiating the set of variables of the metarule, i.e., several possible matches. This is due to the presence of non- typed variables. Now, we are ready to deﬁne what we mean by a successful matching. The process of matching is successful if the number of possible matches is greater then 0. When there is no possible match the process is said to fail. In addition to return success or failure, the process also return the set of all possible matches, which will be used for generating the output. B.2.2 Structural Matching The process of matching lhs and inp can be seen as a recursive procedure for matching trees, starting at their roots and proceeding in a top-down style along with their subtrees. In the explanation of this process that follows we have used the term lhs not only to refer to the whole tree that contains the pattern but to any of its subtrees that is being considered in a given recursive step. The same applies to inp. By now we ignore feature equations, which will be accounted for in the next subsection. The process described below returns at the end the set of matches (where an empty set means the same as failure). We ﬁrst give one auxiliary deﬁnition, of valid Mapping, and one recursive function Match, that matches lists of trees instead of trees, and then deﬁne the process of matching two trees as a special case of call to Match. 3This is diﬀerent from not having a subscript which is interpreted as checking that the input tree have no subscript for matching 244 APPENDIX B. METARULES Given a list listlhs  [lhs1, lhs2, ..., lhsl] of nodes of lhs and a list listinp  [inp1, inp2, ..., inpi] of nodes of inp, we deﬁne a mapping from listlhs to listinp to be a function Mapping, that for each element of listlhs assigns a list of elements of listinp, deﬁned by the following condition: concatenation (Mapping(lhs1), Mapping(lhs2), ..., Mapping(lhsl))  listinp , i.e., the elements of listinp are split into sublists and assigned in order of appearance in the list to the elements of listlhs. We say that a mapping is a valid mapping if for all j, 1  j  l (where l is the length of listlhs), the following restrictions apply: 1. if lhsj is a constant node, then Mapping(lhsj) must have a single element, say, rhsg(j), and the two nodes must have the same name and agree on the markers (foot, substitution, head and NA), i.e., if lhsj is NA, then rhsg(j) must be NA, if lhsj has no markers, then rhsg(j) must have no markers, etc. 2. if lhsj is a type variable node, then Mapping(lhsj) must have a single element, say, rhsg(j), and rhsg(j) must be marker-compatible and type-compatible with lhsj. rhsg(j) is marker-compatible with lhsj if any marker (foot, substitution, head and NA) present in lhsj is also present in rhsg(j)4. rhsg(j) is type-compatible with lhsj if there is at least one of the alternative type speciﬁers for the typed variable that satisﬁes the conditions below.  rhsg(j) has the stem deﬁned in the type speciﬁer.  if the type speciﬁer doesnt have subscript, then rhsg(j) must have no subscript.  if the type speciﬁer has a subscript diﬀerent of ?, then rhsg(j) must have the same subscript as in the type speciﬁer 5. 3. if lhsj is a non-typed variable node, then theres actually no requirement: Mapping(lhsj) may have any length and even be empty. The following algorithm, Match, takes as input a list of nodes of lhs and a list of nodes of inp, and returns the set of possible matches generated in the attempt of match this two lists. If the result is an empty set, this means that the matching failed. Function Match (listlhs, listrhs) Let MAPPINGS be the list of all valid mappings from listlhs to listrhs Make MATCHES   For each mapping Mapping  MAPPINGS do: Make Matches  {} For each j, 1  j  l, where l  length(listlhs), do: if lhsj is a constant node, then 4Notice that, unlike the case for the constant node, the inverse is not required, i.e., if lhsj has no marker, rhsg(j) is still allowed to have some. 5If the type speciﬁer has a ? subscript, there is no restriction, and that is exactly its function: to allow for the matching to be independent of the subscript B.2. THE DEFINITION OF A METARULE IN XTAG 245 let childrenlhs be the list of children of lhsj lhrg(j) be the single element in Mapping(lhsj) childrenrhs be the list of children of lhrg(j) Make Matches  {m  mj  m  Matches and mj  Match(childrenlhs, childrenrhs)} if lhsj is a typed variable node, then let childrenlhs be the list of children of lhsj lhrg(j) be the single element in Mapping(lhsj) childrenrhs be the list of children of lhrg(j) Make Matches  {{(lhsj, lhrg(j))}  m  mj  m  Matches and mj  Match(childrenlhs, childrenrhs)} if lhsj is a non-typed variable node, then let childrenlhs be the list of children of lhsj sl be the number of nodes in childrenlhs DESCs be the set of s-size lists given by: DESCs  {[dr1, dr2, ..., drs]  for every 1  k  s, drk is a descendant of some node in Mapping(lhsj)}6 for every 1  k  s, drl is to the right of drk17. For every list Desc  [dr1, dr2, ..., drs]  DESCs do: Let Tree-Material be the list of subtrees dominated by the nodes in Mapping(lhsj), but, with the subtrees dominated by the nodes in DESCs cut out from these trees Make Matches  {{(lhsj, Tree  Struct)}  m  mj  m  Matches and mj  Match(childrenlhs, Desc)} Make MATCHES  MATCHES  Matches Return MATCHES Finally we can deﬁne the process of structurally matching lhs to inp as the evaluation of Match([root(lhs)], [root(inp)]. If the result is an empty set, the matching failed, otherwise the resulting set is the set of possible matches that will be used for generating the new trees (after being pruned by the feature equation matching). B.2.3 Output Generation Although nothing has yet been said about the feature equations, which is the subject of the next subsection, we assume that only matches that meet the additional constraints imposed by feature equations are considered for output. If no structural match survives feature equations checking, that matching has failed. If the process of matching lhs to inp fails, there are two alternative behaviors according to the value of a parameter8. If the parameter is set to false, which is the default value, no output is generated. On the other hand, if it is set to true, then the own inp tree is copied to the output. 8the parameter is accessible at the Lisp interface by the name XTAG::metarules-copy-unmatched-trees 246 APPENDIX B. METARULES If the process of matching succeeds, as many trees will be generated in the output as the number of possible matches obtained in the process. For a given match, the output tree is generated by substituting in the rhs tree of the metarule the occurrences of variables by the material to which they have been instantiated in the match. The case of the typed-variable is simple. The name of the variable is just substituted by the name of the node to which it has been instantiated from inp. A very important detail is that the marker (foot, substitution, head, NA, or none) at the output tree node comes from what is speciﬁed in the rhs node, which can be diﬀerent of the marker at the variable node in inp and of the associated node from inp. The case of the non-typed variable, not surpringly, is not so simple. In the output tree, this node will be substituted by the subtree list that was associated to this node, in the same other, attaching to the parent of this non-typed variable node. But remember, that some subtrees may have been removed from some of the trees in this list, maybe entire elements of this list, due to the eﬀect of the children of the metavariable in lhs. It is a requirement that any occurence of a non-typed variable node at the rhs tree has exactly the same number of children than the unique occurence of this non-typed variable node in lhs. Hence, when generating the output tree, the subtrees at rhs will be inserted exactly at the points where subtrees were removed during matching, in a positional, one to one correspondance. For feature equations in the output trees see the next subsection. The comments at the output are the comments at the lhs tree of the metarule followed by the coments at inp, both parts introduced by appropriate headers, allowing the user to have a complete history of each tree. B.2.4 Feature Matching In the previous subsections we have considered only the aspects of a metarule involving the structural part of the XTAG trees. In a feature based grammar as XTAG is, accounting for features is essential. A metarule is not really worth if it doesnt account for the proper change of feature equations9 from the input to the output tree. The aspects that have to be considered here are:  Which feature equations should be required to be present in inp in order for the match to succeed.  Which feature equations should be generated in the output tree as a function of the feature equations in the input tree. Based on the possible combinations of these requirements we partition the feature equations into the following ﬁve classes10:  Require  Retain: Feature equations in this class are required to be in inp in order for matching to succeed. Upon matching, these equations will be copied to the output tree. 9Notice that what is really important is not the features themselves, but the feature equations that relate the feature values of nodes of the same tree 10This classiﬁcation is really a partition, i.e., no equation may be conceptually in more than one class at the same time. B.3. EXAMPLES 247 To achieve this behaviour, the equation must be placed in the lhs tree of the metarule preceded by a plus character (e.g. V.t : trans  )11  Require  Dont Copy: The equation is required to be in inp for matching, but should not be copied to the output tree. Those equations must be in lhs preceded by minus character (e.g. NP1 : case  acc).  Optional  Dont Copy: The equation is not required for matching, but we have to make sure not to copy it to the output tree set of equations, regardless of it being present or not in inp. Those equations must be in lhs in raw form, i.e. neither preceded by a plus nor minus character (e.g. Sr.b : perfect  V P.t : perfect ).  Optional  Retain: The equation is not required for matching but, in case it is found in inp it must be copied to the output tree. This is the default case, and hence these equations should not be present in the metarule speciﬁcation.  Add: The equation is not required for matching but we want it to be put in the output tree anyway. These equations are placed in raw form in the rhs (notice in this case it is the right hand side). Typed variables can be used in feature equations in both lhs and rhs. They are intended to represent the nodes of the input tree to which they have been instantiated. For each resulting match from the structural matching process the following is done:  The (typed) variables in the equations at lhs and rhs are substituted by the names of the nodes they have been instantiated to.  The requirements concerning feature equations are checked, according to the above rules.  If the match survives feature equation checking, the proper output tree is generated, according to Section B.2.3 and to the rules described above for the feature equations. Finally, a new kind of metavariable, which is not used at the nodes, can be introduced in the feature equations part. They have the same form of the non-typed variables, i.e. quotation mark, followed by a number, and are used in the place of feature values and feature names. Hence, if the equation ?NP?.b :?2 ?3 appears in lhs, this means, that all feature equations of inp that match a bottom attribute of some NP to any feature value (but not to a feature path) will not be copied to the output. B.3 Examples Figure B.1 shows a metarule for wh-movement of the subject. Among the trees to which it have been applied are the basic trees of intransitive, transitive and ditransitive families (including prepositional complements), passive trees of the same families, and ergative. 11Commutativity of equations is accounted for in the system. Hence an equation x  y can also be speciﬁed as y  x. Associativity is not accounted for and its need by an user is viewed as indicating misspeciﬁcation at the input trees. 248 APPENDIX B. METARULES lhs rhs Figure B.1: Metarule for wh-movement of subject Figure B.2 shows a metarule for wh-movement of an NP in object position. Among the trees to which it have been applied are the basic and passive trees of transitive and ditransitive families. lhs rhs Figure B.2: Metarule for wh-movement of object Figure B.3 shows a metarule for general wh-movement of an NP. It can be applied to generate trees with either subject or object NP moved. We show in Figure B.4, the basic tree for the family Tnx0Vnx1Pnx2 and the tree wh-trees generated by the application of the rule. B.4 The Access to the Metarules through the XTAG Interface We ﬁrst describe the access to the metarules subsystem using buﬀers with single metarule applications. Then we proceed by describing the application of multiple metarules in what we call the parallel, sequential, and cumulative modes to input tree ﬁles. We have deﬁned conceptually a metarule as an ordered pair of trees. In the implementation of the metarule subsystem it works the same: a metarule is a buﬀer with two trees. The name of the metarule is the name of the buﬀer. The ﬁrst tree that appear in the main window under B.4. THE ACCESS TO THE METARULES THROUGH THE XTAG INTERFACE 249 lhs rhs Figure B.3: Metarule for general wh movement of an NP Tnx0Vnx1Pnx2 subject moved NP object moved NP object moved from PP Figure B.4: Application of wh-movement rule to Tnx0Vnx1Pnx2 the metarule buﬀer is the left hand side, the next appearing below is the right hand side12. The positional approach allows us to have naming freedom: the tree names are irrelevant13. Since 12Although a buﬀer is intended to implement the concept of a set (not a sequence) of trees we take proﬁt of the actual organization of the system to realize the concept of (ordered) tree pair in the implementation. 13so that even if we want to have mnemonic names resembling their distinct character - left or right hand side, 250 APPENDIX B. METARULES we can save buﬀers into text ﬁles, we can talk also about metarule ﬁles. The available options for applying a metarule which is in a buﬀer are:  For applying it to a single input tree, click in the name of the tree in the main window, and choose the option apply metarule to tree. You will be prompted for the name of the metarule to apply to the tree which should be, as we mentioned before, the name of the buﬀer that contains the metarule trees. The output trees will be generated at the end of the buﬀer that contains the input tree. The names of the trees depend of a LISP parameter metarules-change-name . If the value of the parameter is false  the default value  then the new trees will have the same name as the input, otherwise, the name of the input tree followed by a dash (-) and the name of the right hand side of the tree14. The value of the parameter can be changed by choosing Tools at the menu bar and then either name mr output trees  input or append rhs name to mr output trees.  For applying it to all the trees of a buﬀer, click in the name of the buﬀer that contains the trees and proceed as above. The output will be a new buﬀer with all the output trees. The name of the new buﬀer will be the same as the input buﬀer preﬁxed by MR-. The names of the trees follow the conventions above. The other options concern application to ﬁles (instead of buﬀers). Lets ﬁrst deﬁne the concepts of parallel, sequential and cumulative application of metarules. One metarule ﬁle can contain more than one metarule. The ﬁrst two trees, i.e., the ﬁrst tree pair, form one metarule - lets call it mr0. Subsequent pairs in the sequence of trees deﬁne additional metarules  mr1, mr2, ..., mrn.  We say that a metarule ﬁle is applied in parallel to a tree (see Figure B.5) if each of the metarules is applied independently to the input generating its particular output trees15. We generalize the concept to the application in parallel of a metarule ﬁle to a tree ﬁle (with possibly more than one tree), generating all the trees as if each metarule in the metarule ﬁle was applied to each tree in the input ﬁle. MR0 Input Trees MR1 MRn Output Trees Figure B.5: Parallel application of metarules - we have some naming ﬂexibility to call them e.g. lhs23 or lhs-passive, ... 14the reason why we do not use the name of the metarule, i.e. the name of the buﬀer, is because in some forms of application the metarules do not carry individual names, as well see soon is the case when a set of metarules from a ﬁle is applied. 15remember a metarule application generates as many output trees as the number of matches B.4. THE ACCESS TO THE METARULES THROUGH THE XTAG INTERFACE 251  We say that a metarule ﬁle mr0, mr1, mr2, ..., mrn is applied in sequence to a input tree ﬁle (see Figure B.6) if we apply mr0 to the trees of the input ﬁle, and for each 0  i  n apply metarule mri to the trees generated as a result of the application of mri1. Output Trees MR1 MRn MR0 Input Trees Figure B.6: Sequential application of metarules  Finally, the cumulative application is similar to the sequential, except that the input trees at each stage are by-passed to the output together with the newly generated ones (see Figure B.7). Output Trees MR0 MR1 MRn Input Trees Figure B.7: Cumulative application of metarules Remember that in case of matching failure the output result is decided as explained in subsection B.2.3 either to be empty or to be the input tree. The reﬂex here of having the parameter set for copying the input is that for the parallel application the output will have as many copies of the input as matching failures. For the sequential case the decision apply at each level, and setting the parameter for copying, in a certain sense, guarantees for the pipe not to break. Due to its nature and unlike the two other modes, the cumulative application is not aﬀected by this parameter. The options for application of metarules to ﬁles are available by clicking at the menu item Tools and then choosing the appropriate function among:  Apply metarule to ﬁles: Youll be prompted for the metarule ﬁle name which should contain one metarule16, and for input ﬁle names. Each input ﬁle name inpﬁle will be independently submitted to the application of the metarule generating an output ﬁle with the name MR-inpﬁle.  Apply metarules in parallel to ﬁles: Youll be prompted for the metarules ﬁle name with one or more metarules and for input ﬁle names. Each input ﬁle name inpﬁle will be independently submitted to the application of the metarules in parallel. For each parallel application to a ﬁle inpﬁle an output ﬁle with the name MRP-inpﬁle will be generated.  Apply metarules in sequence to ﬁles: The interaction is as described for the application in parallel, except that the application of the metarules are in sequence and that the output ﬁles are preﬁxed by MRS- instead of MRP-. 16if it contains more than 2 trees, the additional trees are ignored 252 APPENDIX B. METARULES  Apply metarules cumulatively to ﬁles: The interaction is as described for the applications in parallel and in sequence, except that the mode of application is cumulative and that the output ﬁles are preﬁxed by MRC-. Finally still under the Tools menu we can change the setting of the parameter that controls the output result on matching failure (see Subsection B.2.3) by choosing either copy input on mr matching failure or no output on mr matching failure. Appendix C Lexical Organization C.1 Introduction An important characteristic of an FB-LTAG is that it is lexicalized, i.e., each lexical item is anchored to a tree structure that encodes subcategorization information. Trees with the same canonical subcategorizations are grouped into tree families. The reuse of tree substructures, such as wh-movement, in many diﬀerent trees creates redundancy, which poses a problem for grammar development and maintenance [Vijay-Shanker and Schabes, 1992]. To consistently implement a change in some general aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, [Becker, 1994], Evans et al. [Evans et al., 1995], and Candito[Candito, 1996] have developed systems for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, [Vijay-Shanker and Schabes, 1992], to use partial-tree descriptions in specifying a grammar by separately deﬁning pieces of tree structures to encode independent syntactic principles. Various individual speciﬁcations are then combined to form the elementary trees of the grammar. The chapter begins with a description of our grammar development system, and its implementation. We will then show the main results of using this tool to generate the Penn English grammar as well as a Chinese TAG. We describe the signiﬁcant properties of both grammars, pointing out the major diﬀerences between them, and the methods by which our system is informed about these language-speciﬁc properties. The chapter ends with the conclusion and future work. C.2 System Overview In our approach, three types of components  subcategorization frames, blocks and lexical redistribution rules  are used to describe lexical and syntactic information. Actual trees are generated automatically from these abstract descriptions, as shown in Figure C.1. In maintaining the grammar only the abstract descriptions need ever be manipulated; the tree descriptions and the actual trees which they subsume are computed deterministically from these high-level descriptions. 253 254 APPENDIX C. LEXICAL ORGANIZATION Subcategorization frames Transformation blocks Subcategorization blocks Lexical redistribution rules(LRRs) Our System Sets of Trees Figure C.1: Lexical Organization: System Overview C.2.1 Subcategorization frames Subcategorization frames specify the category of the main anchor, the number of arguments, each arguments category and position with respect to the anchor, and other information such as feature equations or node expansions. Each tree family has one canonical subcategorization frame. C.2.2 Blocks Blocks are used to represent the tree substructures that are reused in diﬀerent trees, i.e. blocks subsume classes of trees. Each block includes a set of nodes, dominance relation, parent relation, precedence relation between nodes, and feature equations. This follows the deﬁnition of the tree descriptions speciﬁed in a logical language patterned after Rogers and Vijay-Shanker[Rogers and Vijay-Shankar, 1994]. Blocks are divided into two types according to their functions: subcategorization blocks and transformation blocks. The former describes structural conﬁgurations incorporating the various information in a subcategorization frame. For example, some of the subcategorization blocks used in the development of the English grammar are shown in Figure C.2.1 When the subcategorization frame for a verb is given by the grammar developer, the system will automatically create a new block (of code) by essentially selecting the appropriate primi- tive subcategorization blocks corresponding to the argument information speciﬁed in that verb frame. The transformation blocks are used for various transformations such as wh-movement. These transformation blocks do not encode rules for modifying trees, but rather describe the properties of a particular syntactic construction. Figure C.3 depicts our representation of phrasal extraction. This can be specialized to give the blocks for wh-movement, topicaliza- tion, relative clause formation, etc. For example, the wh-movement block is deﬁned by further 1 In order to focus on the use of tree descriptions and to make the ﬁgures less cumbersome, we show only the structural aspects and do not show the feature value speciﬁcation. The parent, (immediate dominance), relationship is illustrated by a plain line and the dominance relationship by a dotted line. The arc between nodes shows the precedence order of the nodes are unspeciﬁed. The nodes categories are enclosed in parentheses. C.2. SYSTEM OVERVIEW 255 specifying that the ExtractionRoot is labeled S, the NewSite has a wh feature, and so on. Root Pred PredP (a) is_main_frame VP(VP) Verb(V) Root Subject (b)pred_has_subject VP Subject(NP) (c) subject_is_NP PredP Pred Object (e)pred_has_object (f)object_is_NP Object(NP) V, Pred (d) main_pred_is_V VP,PredP Figure C.2: Some subcategorization blocks Root(S) NewSite ε (c) relative clause (b) wh-movement Root(S) NewSite ε ε Root(S) NewSite SB(S) COMP(COMP) ε NPf(NP) NPr(NP) ExtNode ExtRoot ExtNode ExtRoot(S) ExtRoot(S) ExtNode (a) extraction Figure C.3: Transformation blocks for extraction 256 APPENDIX C. LEXICAL ORGANIZATION C.2.3 Lexical Redistribution Rules (LRRs) The third type of machinery available for a grammar developer is the Lexical Redistribution Rule (LRR). An LRR is a pair (rl, rr) of subcategorization frames, which produces a new frame when applied to a subcategorization frame s, by ﬁrst matching2 the left frame rl of r to s, then combining information in rr and s. LRRs are introduced to incorporate the connection between subcategorization frames. For example, most transitive verbs have a frame for active(a subject and an object) and another frame for passive, where the object in the former frame becomes the subject in the latter. An LRR, denoted as passive LRR, is built to produce the passive subcategorization frame from the active one. Similarly, applying dative-shift LRR to the frame with one NP subject and two NP objects will produce a frame with an NP subject and an PP object. Besides the distinct content, LRRs and blocks also diﬀer in several aspects:  They have diﬀerent functionalities: Blocks represent the substructures that are reused in diﬀerent trees. They are used to reduce the redundancy among trees; LRRs are introduced to incorporate the connections between the closely related subcategorization frames.  Blocks are strictly additive and can be added in any order. LRRs, on the other hand, produce diﬀerent results depending on the order they are applied in, and are allowed to be non-additive, i.e., to remove information from the subcategorization frame they are being applied to, as in the procedure of passive from active. to to to Sq S 1 VP NP NP V (a) (b) (c) 2 2 2 PP NP P ε Sq S 0 VP 1 ε 2 2 2 NP NP V NP PP NP P Sq S 0 VP 1 2 2 2 NP NP V NP PP NP P ε Figure C.4: Elementary trees generated from combining blocks C.2.4 Tree generation To generate elementary trees, we begin with a canonical subcategorization frame. The system will ﬁrst generate related subcategorization frames by applying LRRs, then select subcate- gorization blocks corresponding to the information in the subcategorization frames, next the combinations of these blocks are further combined with the blocks corresponding to various 2Matching occurs successfully when frame s is compatible with rl in the type of anchors, the number of arguments, their positions, categories and features. In other words, incompatible features etc. will block certain LRRs from being applied. C.3. IMPLEMENTATION 257 transformations, ﬁnally, a set of trees are generated from those combined blocks, and they are the tree family for this subcategorization frame. Figure C.4 shows some of the trees produced in this way. For instance, the last tree is obtained by incorporating information from the di- transitive verb subcategorization frame, applying the dative-shift and passive LRRs, and then combining them with the wh-non-subject extraction block. Besides, in our system the hierarchy for subcategorization frames is implicit as shown in Figure C.5. . . . . Transitive(buy) Ditrans. with NP (ask) Intransitive(walk) (force sb to do sth) Ditrans. with S (put) Ditrans. with PP object_is_AP main_anchor_is_verb object_is_NP subject_is_NP (take a walk) Trans. light verb (kick the bucket) Trans. idioms Intrans. particle(add up) Intrans. with adj (feel happy) Trans. particle(pick up) Figure C.5: Partial inheritance lattice in English C.3 Implementation The input of our system is the description of the language, which includes the subcategorization frame list, LRR list, subcategorization block list and transformation lists. The output is a list of trees generated automatically by the system, as shown in Figure C.6. The tree generation module is written in Prolog, and the rest part is in C. We also have a graphic interface to input the language description. Figure C.7 and C.8 are two snapshots of the interface. C.4 Generating grammars We have used our tool to specify a grammar for English in order to produce the trees used in the current English XTAG grammar. We have also used our tool to generate a large grammar for Chinese. In designing these grammars, we have tried to specify the grammars to reﬂect the similarities and the diﬀerences between the languages. The major features of our speciﬁcation of these two grammars3 are summarized in Table C.1 and C.2. 3Both grammars are still under development, so the contents of these two tables might change a lot in the future according to the analyses we choose for certain phenomenon. For example, the majority of work on Chinese grammar treat ba-construction as some kind of object-fronting where the character ba is either an object marker or a preposition. According to this analysis, an LRR rule for ba-construction is used in our grammar to generate the preverbal-object frame from the postverbal frame. However, there has been some argument for treating ba as a verb. If we later choose that analysis, the main verbs in the patterns NP0 VP and NP0 ba NP1 VP will be diﬀerent, therefore no LRR will be needed for it. As a result, the numbers of LRRs, subcat frames and tree generated will change accordingly. 258 APPENDIX C. LEXICAL ORGANIZATION LRR list subcat frame list subcat block list trans block list Select blocks subcat LRRs generation subcat blocks subsets of subcat blocks and trans. blocks trees generated elementary trees for the language Language description Lexorg system Apply Tree combinations of derived frames subsets of Combine w trans blocks Figure C.6: Implementation of the system Figure C.7: Interface for creating a grammar By focusing on the speciﬁcation of individual grammatical information, we have been able to generate nearly all of the trees from the tree families used in the current English grammar C.4. GENERATING GRAMMARS 259 Figure C.8: Part of the Interface for creating blocks English Chinese examples passive bei-construction of LRRs dative-shift object fronting ergative ba-construction examples wh-question topicalization of transformation relativization relativization blocks declarative argument-drop  LRRs 6 12  subcat blocks 34 24  trans blocks 8 15  subcat frames 43 23  trees generated 638 280 Table C.1: Major features of English and Chinese grammars developed at Penn4. Our approach, has also exposed certain gaps in the Penn grammar. We are encouraged with the utility of our tool and the ease with which this large-scale grammar was developed. We are currently working on expanding the contents of subcategorization frame to include trees for other categories of words. For example, a frame which has no speciﬁer and one NP complement and whose predicate is a preposition will correspond to PP  P NP tree. Well also introduce a modiﬁer ﬁeld and semantic features, so that the head features will propagate 4We have not yet attempted to extend our coverage to include punctuation, it-clefts, and a few idiosyncratic analyses. 260 APPENDIX C. LEXICAL ORGANIZATION both grammars English Chinese causative long passive VO-inversion LRRs short passive ergative ba-const dative-shift topicalization trans blocks relativization gerund argument-drop declarative NPS subject zero-subject subcat blocks SNPPP object PL object preverbal object V predicate prep predicate Table C.2: Comparison of the two grammars from modiﬁee to modiﬁed node, while non-head features from the predicate as the head of the modiﬁer will be passed to the modiﬁed node. C.5 Summary We have described a tool for grammar development in which tree descriptions are used to pro- vide an abstract speciﬁcation of the linguistic phenomena relevant to a particular language. In grammar development and maintenance, only the abstract speciﬁcations need to be edited, and any changes or corrections will automatically be proliferated throughout the grammar. In addition to lightening the more tedious aspects of grammar maintenance, this approach also allows a unique perspective on the general characteristics of a language. Deﬁning hierarchical blocks for the grammar both necessitates and facilitates an examination of the linguistic as- sumptions that have been made with regard to feature speciﬁcation and tree-family deﬁnition. This can be very useful for gaining an overview of the theory that is being implemented and exposing gaps that remain unmotivated and need to be investigated. The type of gaps that can be exposed could include a missing subcategorization frame that might arise from the au- tomatic combination of blocks and which would correspond to an entire tree family, a missing tree which would represent a particular type of transformation for a subcategorization frame, or inconsistent feature equations. By focusing on syntactic properties at a higher level, our approach allows new opportunities for the investigation of how languages relate to themselves and to each other. Appendix D Tree Naming conventions The various trees within the XTAG grammar are named more or less according to the following tree naming conventions. Although these naming conventions are generally followed, there are occasional trees that do not strictly follow these conventions. D.1 Tree Families Tree families are named according to the basic declarative tree structure in the tree family (see section D.2), but with a T as the ﬁrst character instead of an α or β. D.2 Trees within tree families Each tree begins with either an α (alpha) or a β (beta) symbol, indicating whether it is an initial or auxiliary tree, respectively. Following an α or a β the name may additionally contain one of: I imperative E ergative N0,1,2 relative clause{position} G NP gerund D Determiner gerund pW0,1,2 wh-PP extraction{position} W0,1,2 wh-NP extraction{position} X ECM (eXceptional case marking) Numbers are assigned according to the position of the argument in the declarative tree, as follows: 0 subject position 1 ﬁrst argument (e.g. direct object) 2 second argument (e.g. indirect object) The body of the name consists of a string of the following components, which corresponds to the leaves of the tree. The anchor(s) of the trees is(are) indicated by capitalizing the part of speech corresponding to the anchor. 261 262 APPENDIX D. TREE NAMING CONVENTIONS s sentence a adjective arb adverb be be c relative complementizer x phrasal category d determiner v verb lv light verb conj conjunction comp complementizer it it n noun p preposition to to pl particle by by neg negation As an example, the transitive declarative tree consists of a subject NP, followed by a verb (which is the anchor), followed by the object NP. This translates into αnx0Vnx1. If the subject NP had been extracted, then the tree would be αW0nx0Vnx1. A passive tree with the by phrase in the same tree family would be αnx1Vbynx0. Note that even though the object NP has moved to the subject position, it retains the object encoding (nx1). D.3 Assorted Initial Trees Trees that are not part of the tree families are generally gathered into several ﬁles for conve- nience. The various initial trees are located in lex.trees. All the trees in this ﬁle should begin with an α, indicating that they are initial trees. This is followed by the root category which follows the naming conventions in the previous section (e.g. n for noun, x for phrasal category). The root category is in all capital letters. After the root category, the node leaves are named, beginning from the left, with the anchor of the tree also being capitalized. As an example, the αNXN tree is rooted by an NP node (NX) and anchored by a noun (N). D.4 Assorted Auxiliary Trees The auxiliary trees are mostly located in the buﬀers prepositions.trees, conjunctions.trees, determiners.trees, advs-adjs.trees, and modifiers.trees, although a couple of other ﬁles also contain auxiliary trees. The auxiliary trees follow a slightly diﬀerent naming convention from the initial trees. Since the root and foot nodes must be the same for the auxiliary trees, the root nodes are not explicitly mentioned in the names of auxiliary trees. The trees are named according to the leaf nodes, starting from the left, and capitalizing the anchor node. All auxiliary trees begin with a β, of course. For example, βARBs, indicates a tree anchored by D.4. ASSORTED AUXILIARY TREES 263 an adverb (ARB), that adjoins onto the left of an S node (Note that S must be the foot node, and therefore also the root node). D.4.1 Relative Clause Trees For relative clause trees, the following naming conventions have been adopted: if the wh- moved NP is overt, it is not explicitly represented. Instead the index of the site of movement (0 for subject, 1 for object, 2 for indirect object) is appended to the N. So βN0nx0Vnx1 is a subject extraction relative clause with NPw substitution and βN1nx0Vnx1 is an object extraction relative clause. If the wh-moved NP is covert and Comp substitutes in, the Comp node is represented by c in the tree name and the index of the extraction site follows c. Thus βNc0nx0Vnx1 is a subject extraction relative clause with Comp substitution. Adjunct trees are similar, except that since the extracted material is not co-indexed to a trace, no index is speciﬁed (cf. βNpxnx0Vnx1, which is an adjunct relative clause with PP pied-piping, and βNcnx0Vnx1, which is an adjunct relative clause with Comp substitution). Cases of pied- piping, in which the pied-piped material is part of the anchor have the anchor capitalized or spelled-out (cf. βNbynx0nx1Vbynx0 which is a relative clause with by-phrase pied-piping and NPw substitution.). Appendix E Features Table E.1 contains a comprehensive list of the features in the XTAG grammar and their possible values. This section consists of short biographical sketches of the various features currently in use in the XTAG English grammar. E.1 Agreement agr is a complex feature. It can have as its subfeatures: agr 3rdsing, possible values:  agr num, possible values: plur, sing agr pers, possible values: 1, 2, 3 agr gen, possible values: masc, fem, neut These features are used to ensure agreement between a verb and its subject. Where does it occur: Nouns comes speciﬁed from the lexicon with their agr features. e.g. books is agr 3rdsing: , agr num: plur, and agr pers: 3. Only pronouns use the gen (gender) feature. The agr features of a noun are transmitted up the NP tree by the following equation: NP.b:agr  N.t:agr Agreement between a verb and its subject is mediated by the following feature equations: (509) NPsubj:agr  VP.t:agr (510) VP.b:agr  V.t:agr Agreement has to be done as a two step process because whether the verb agrees with the subject or not depends upon whether some auxiliary verb adjoins in and upon what the agr speciﬁcation of the verb is. Verbs also come speciﬁed from the lexicon with their agr features, e.g. the agr features of the verb sings are agr 3rdsing: , agr num: sing, and agr pers: 3; Non-ﬁnite forms of the verb sing e.g. singing do not come with an agr feature speciﬁcation. 264 E.1. AGREEMENT 265 Feature Value agr 3rdsing ,  agr num plur,sing agr pers 1,2,3 agr gen fem,masc,neuter assign-case nom,acc,none assign-comp that,whether,if,for,ecm,rel,inf nil,ind nil,ppart nil,none card ,  case nom,acc,gen,none comp that,whether,if,for,rel,inf nil,ind nil,nil compar ,  compl ,  conditional ,  conj and,or,but,comma,scolon,to,disc,nil const ,  contr ,  control no value, indexing only decreas ,  deﬁnite ,  displ-const ,  equiv ,  extracted ,  gen ,  gerund ,  inv ,  invlink no value, indexing only irrealis ,  mainv ,  mode base,ger,ind,inf,imp,nom,ppart,prep,sbjunt neg ,  passive ,  perfect ,  pred ,  progressive ,  pron ,  punct bal dquote,squote,paren,nil punct contains colon ,  punct contains dash ,  punct contains dquote ,  punct contains scolon ,  punct contains squote ,  punct struct comma,dash,colon,scolon,nil punct term per,qmark,excl,nil quan ,  reﬂ ,  rel-clause ,  rel-pron ppart,ger,adj-clause select-mode ind,inf,ppart,ger super ,  tense pres,past trace no value, indexing only trans ,  weak ,  wh ,  Table E.1: List of features and their possible values 266 APPENDIX E. FEATURES E.1.1 Agreement and Movement The agr features of a moved NP and its trace are co-indexed. This captures the fact that movement does not disrupt a pre-existing agreement relationship between an NP and a verb. (511) [Which boys]i does John think [ti areis intelligent]? E.2 Case There are two features responsible for case-assignment: case, possible values: nom, acc, gen, none assign-case, possible values: nom, acc, none Case assigners (prepositions and verbs) as well as the VP, S and PP nodes that dominate them have an assign-case case feature. Phrases and lexical items that have case i.e. Ns and NPs have a case feature. Case assignment by prepositions involves the following equations: (512) PP.b:assign-case  P.t:case (513) NP.t:case  P.t:case Prepositions come speciﬁed from the lexicon with their assign-case feature. (514) P.b:assign-case  acc Case assignment by verbs has two parts: assignment of case to the object(s) and assignment of case to the subject. Assignment of case to the object is simpler. English verbs always assign accusative case to their NP objects (direct or indirect). Hence this is built into the tree and not put into the lexical entry of each individual verb. (515) NPobject.t:case  acc Assignment of case to the subject involves the following two equations. (516) NPsubj:case  VP.t:assign-case (517) VP.b:assign-case  V.t:assign-case This is a two step process  the ﬁnal case assigned to the subject depends upon the assign- case feature of the verb as well as whether an auxiliary verb adjoins in. Finite verbs like sings have nom as the value of their assign-case feature. Non-ﬁnite verbs have none as the value of their assign-case feature. So if no auxiliary adjoins in, the only subject they can have is PRO which is the only NP with none as the value its case feature. E.3. EXTRACTION AND INVERSION 267 E.2.1 ECM Certain verbs e.g. want, believe, consider etc. and one complementizer for are able to assign case to the subject of their complement clause. The complementizer for, like the preposition for, has the assign-case feature of its com- plement set to acc. Since the assign-case feature of the root Sr of the complement tree and the case feature of its NP subject are co-indexed, this leads to the subject being assigned accusative case. ECM verbs have the assign-case feature of their foot S node set to acc. The co-indexation between the assign-case feature of the root Sr and the case feature of the NP subject leads to the subject being assigned accusative case. E.2.2 Agreement and Case The case features of a moved NP and its trace are co-indexed. This captures the fact that movement does not disrupt a pre-existing relationship of case-assignment between a verb and an NP. (518) HeriShei, I think that Odo like ti. E.3 Extraction and Inversion extracted, possible vales are  All sentential trees with extracted components, with the exception of relative clauses are marked S.bextracted   at their top S node. The extracted element may be a wh-NP or a topicalized NP. The extracted feature is currently used to block embedded topicalizations as exempliﬁed by the following example. (519)  John wants [Billi [PRO to leave ti]] trace: this feature is not assigned any value and is used to co-index moved NPs and their traces which are marked by ǫ. wh: possible values are  NPs like who, what etc. come marked from the lexicon with a value of  for the feature wh. Non wh-NPs have  as the value of their wh feature. Note that wh   NPs are not restricted to occurring in extracted positions, to allow for the correct treatment of echo questions. The wh feature is propagated up by possessives  e.g. the  wh feature of the determiner which in which boy is propagated up to the level of the NP so that the value of the wh feature of the entire NP is wh. This process is recursive e.g. which boys mother, which boys mothers sister. The wh feature is also propagated up PPs. Thus the PP to whom has  as the value of its wh feature. In trees with extracted NPs, the wh feature of the root node S node is equated with the wh feature of the extracted NPs. The wh feature is used to impose subcategorizational constraints. Certain verbs like wonder can only take interrogative complements, other verbs such as know can take both 268 APPENDIX E. FEATURES interrogative and non-interrogative complements, and yet other verbs like think can only take non-interrogative complements (cf. the extracted and mode features also play a role in imposing subcategorizational constraints). The wh feature is also used to get the correct inversion patterns. E.3.1 Inversion, Part 1 The following three features are used to ensure the correct pattern of inversion: wh: possible values are  inv: possible values are  invlink: possible values are  Facts to be captured: 1. No inversion with topicalization 2. No inversion with matrix extracted subject wh-questions 3. Inversion with matrix extracted object wh-questions 4. Inversion with all matrix wh-questions involving extraction from an embedded clause 5. No inversion in embedded questions 6. No matrix subject topicalizations. Consider a tree with object extraction, where NP is extracted. The following feature equa- tions are used: (520) Sq.b:wh  NP.t:wh (521) Sq.b:invlink  Sq.b:inv (522) Sq.b:inv  Sr.t:inv (523) Sr.b:inv   Root restriction: A restriction is imposed on the ﬁnal root node of any XTAG derivation of a tensed sentence which equates the wh feature and the invlink feature of the ﬁnal root node. If the extracted NP is not a wh-word i.e. its wh feature has the value , at the end of the derivation, Sq.b:wh will also have the value . Because of the root constraint Sq.b:wh will be equated to Sq.b:invlink which will also come to have the value . Then, by (522), Sr.t:inv will acquire the value . This will unify with Sr.b:inv which has the value  (cf. 523). Consequently, no auxiliary verb adjunction will be forced. Hence, there will never be inversion in topicalization. If the extracted NP is a wh-word i.e. its wh feature has the value , at the end of the derivation, Sq.b:wh will also have the value . Because of the root constraint Sq.b:wh will be equated to Sq.b:invlink which will also come to have the value . Then, by (522), Sr.t:inv will acquire the value . This will not unify with Sr.b:inv which has the value  (cf. 523). Consequently, the adjunction of an inverted auxiliary verb is required for the derivation to succeed. Inversion will still take place even if the extraction is from an embedded clause. E.4. CLAUSE TYPE 269 (524) Whoi does Loida think [Miguel likes ti] This is because the adjoined trees root node will also have its Sr.b:inv set to . Note that inversion is only forced upon us because Sq is the ﬁnal root node and the Root restriction applies. In embedded environments, the root restriction would not apply and the feature clash that forces adjunction would not take place. The invlink feature is not present in subject extractions. Consequently there is no inver- sion in subject questions. Subject topicalizations are blocked by setting the wh feature of the extracted NP to  i.e. only wh-phrases can go in this location. E.3.2 Inversion, Part 2 displ-const: Possible values: [set1: ], [set1: ] In the previous section, we saw how inversion is triggered using the invlink, inv, wh features. Inversion involves movement of the verb from V to C. This movement process is represented using the displ-const feature which is used to simulate Multi-Component TAGs.1 The sub-value set1 indicates the inversion multi-component set; while there are not currently any other uses of this mechanism, it could be expanded with other sets receiving diﬀerent set values. The displ-const feature is used to ensure adjunction of two trees, which in this case are the auxiliary tree corresponding to the moved verb (S adjunct) and the auxiliary tree corresponding to the trace of the moved verb (VP adjunct). The following equations are used: (525) Sr.b:displ-const set1   (526) S.t:displ-const set1   (527) VP.b:displ-const set1  V.t:displ-const set1 (528) V.b:displ-const set1   (529) Sr.b:displ-const set1  VP.t:displ-const set1 E.4 Clause Type There are several features that mark clause type.2 They are: mode passive: possible values are  1The displ-const feature is also used in the ECM analysis. 2We have already seen one instance of a feature that marks clause-type: extracted, which marks whether a certain S involves extraction or not. 270 APPENDIX E. FEATURES mode: possible values are base, ger, ind, inf, imp, nom, ppart, prep, sbjnct The mode feature of a verb in its root form is base. The mode feature of a verb in its past participial form is ppart, the mode feature of a verb in its progressivegerundive form is ger, the mode feature of a tensed verb is ind, and the mode feature of a verb in the imperative is imp. nom is the mode value of APNP predicative trees headed by a null copula. prep is the mode value of PP predicative trees headed by a null copula. Only the copula auxiliary tree, some sentential complement verbs (such as consider and raising verb auxiliary trees have nomprep as the mode feature speciﬁcation of their foot node. This allow them, and only them, to adjoin onto APNPPP predicative trees with null copulas. E.4.1 Auxiliary Selection The mode feature is also used to state the subcategorizational constraints between an aux- iliary verb and its complement. We model the following constraints: have takes past participial complements passive be takes past participial complements active be takes progressive complements modal verbs, do, and to take VPs headed by verbs in their base form as their complements. An auxiliary verb transmits its own mode to its root and imposes its subcategorizational restrictions on its complement i.e. on its foot node. e.g. the auxiliary have in its inﬁnitival form involves the following equations: (530) VPr.b:mode  V.t:mode (531) V.t:mode  base (532) VP.b:mode  ppart passive: This feature is used to ensure that passives only have be as their auxiliary. Passive trees start out with their passive feature as . This feature starts out at the level of the verb and is percolated up to the level of the VP. This ensures that only auxiliary verbs whose foot node has  as their passive feature can adjoin on a passive. Passive trees have ppart as the value of their mode feature. So the only auxiliary trees that we really have to worry about blocking are trees whose foot nodes have ppart as the value of their mode feature. There are two such trees  the be tree and the have tree. The be tree is ﬁne because its foot node has  as its passive feature, so both the passive and mode values unify; the have tree is blocked because its foot node has  as its passive feature. E.5 Relative Clauses Features that are peculiar to the relative clause system are: select-mode, possible values are ind, inf, ppart, ger E.5. RELATIVE CLAUSES 271 rel-pron, possible values are ppart, ger, adj-clause rel-clause, possible values are  select-mode: Comps are lexically speciﬁed for select-mode. In addition, the select-mode feature of a Comp is equated to the mode feature of its sister S node by the following equation: (533) Comp.t:select-mode  St.t:mode The lexical speciﬁcations of the Comps are shown below:  ǫC, Comp.t:select-mode indinfgerppart  that, Comp.t:select-mode ind  for, Comp.t:select-mode inf rel-pron: There are additional constraints on where the null Comp ǫC can occur. The null Comp is not permitted in cases of subject extraction unless there is an intervening clause or or the relative clause is a reduced relative (mode  ppartger). To model this paradigm, the feature rel-pron is used in conjunction with the following equations. (534) Sr.t:rel-pron  Comp.t:rel-pron (535) Sr.b:rel-pron  Sr.b:mode (536) Comp.b:rel-pron ppartgeradj-clause (for ǫC) The full set of the equations above is only present in Comp substitution trees involving subject extraction. So the following will not be ruled out. (537) the toy [ǫi [ǫC [ Dafna likes ti ]]] The feature mismatch induced by the above equations is not remedied by adjunction of just any S-adjunct because all other S-adjuncts are transparent to the rel-pron feature because of the following equation: (538) Sm.b:rel-pron  Sf.t:rel-pron rel-clause: The XTAG analysis forces the adjunction of the determiner below the relative clause. This is done by using the rel-clause feature. The relevant equations are: (539) On the root of the RC: NPr.b:rel-clause   (540) On the foot node of the Determiner tree: NPf.t:rel-clause   272 APPENDIX E. FEATURES E.6 Complementizer Selection The following features are used to ensure the appropriate distribution of complementizers: comp, possible values: that, if, whether, for, rel, inf nil, ind nil, nil assign-comp, possible values: that, if, whether, for, ecm, rel, ind nil, inf nil, none mode, possible values: ind, inf, sbjnct, ger, base, ppart, nom, prep wh, possible values: ,  The value of the comp feature tells us what complementizer we are dealing with. The trees which introduce complementizers come speciﬁed from the lexicon with their comp feature and assign-comp feature. The comp of the Comp tree regulates what kind of tree goes above the Comp tree, while the assign-comp feature regulates what kind of tree goes below. e.g. the following equations are used for that: (541) Sc.b:comp  Comp.t:comp (542) Sc.b:wh  Comp.t:wh (543) Sc.b:mode  indsbjnct (544) Sr.t:assign-comp  Comp.t:comp (545) Sr.b:comp  nil By specifying Sr.b:comp  nil, we ensure that complementizers do not adjoin onto other complementizers. The root node of a complementizer tree always has its comp feature set to a value other than nil. Trees that take clausal complements specify with the comp feature on their foot node what kind of complementizer(s) they can take. The assign-comp feature of an S node is determined by the highest VP below the S node and the syntactic conﬁguration the S node is in. E.6.1 Verbs with object sentential complements Finite sentential complements: (546) S1.t:comp  thatwhetherifnil (547) S1.t:mode  indsbjnct or S1.t:mode  ind (548) S1.t:assign-comp  ind nilinf nil The presence of an overt complementizer is optional. Non-ﬁnite sentential complements, do not permit for: E.6. COMPLEMENTIZER SELECTION 273 (549) S1.t:comp  nil (550) S1.t:mode  inf (551) S1.t:assign-comp  ind nilinf nil Non-ﬁnite sentential complements, permit for: (552) S1.t:comp  fornil (553) S1.t:mode  inf (554) S1.t:assign-comp  ind nilinf nil Cases like I want for to win are independently ruled out due to a case feature clash between the acc assigned by for and the intrinsic case feature none on the PRO. Non-ﬁnite sentential complements, ECM: (555) S1.t:comp  nil (556) S1.t:mode  inf (557) S1.t:assign-comp  ecm E.6.2 Verbs with sentential subjects The following contrast involving complementizers surfaces with sentential subjects: (558) (That) John is crazy is likely. Indicative sentential subjects obligatorily have complementizers while inﬁnitival sentential subjects may or may not have a complementizer. Also if is possible as the complementizer of an object clause but not as the complementizer of a sentential subject. (559) S0.t:comp  thatwhetherfornil (560) S0.t:mode  infind (561) S0.t:assign-comp  inf nil If the sentential subject is ﬁnite and a complementizer does not adjoin in, the assign- comp feature of the S0 node of the embedding clause and the root node of the embedded clause will fail to unify. If a complementizer adjoins in, there will be no feature-mismatch because the root of the complementizer tree is not speciﬁed for the assign-comp feature. The comp feature nil is split into two assign-comp features ind nil and inf nil to capture the fact that there are certain conﬁgurations in which it is acceptable for an inﬁnitival clause to lack a complementizer but not acceptable for an indicative clause to lack a comple- mentizer. 274 APPENDIX E. FEATURES E.6.3 That-trace and for-trace eﬀects (562) Whoi do you think (that) ti ate the apple? That trace violations are blocked by the presence of the following equation: (563) Sr.b:assign-comp  inf nilind nilecm on the bottom of the Sr nodes of trees with extracted subjects (W0). The ind nil feature speciﬁcation permits the above example while the inf nilecm feature speciﬁcation allows the following examples to be derived: (564) Whoi do you want [ ti to win the World Cup]? (565) Whoi do you consider [ ti intelligent]? The feature equation that ruled out the that-trace ﬁlter violations will also serve to rule out the for-trace violations above. E.7 Determiner ordering card, possible values are ,  compl, possible values are ,  const, possible values are ,  decreas, possible values are ,  deﬁnite, possible values are ,  gen, possible values are ,  quan, possible values are ,  For detailed discussion see Chapter 18. E.8 Punctuation punct is a complex feature. It has the following as its subfeatures: punct bal, possible values are dquote, squote, paren, nil punct contains colon, possible values are ,  punct contains dash, possible values are ,  punct contains dquote, possible values are ,  punct contains scolon, possible values are ,  punct contains squote, possible values are ,  punct struct, possible values are comma, dash, colon, scolon, none, nil punct term, possible values are per, qmark, excl, none, nil For detailed discussion see Chapter 23. E.9. CONJUNCTION 275 E.9 Conjunction conj, possible values are but, and, or, comma, scolon, to, disc, nil The conj feature is speciﬁed in the lexicon for each conjunction and is passed up to the root node of the conjunction tree. If the conjunction is and, the root agr num is plural, no matter what the number of the two conjuncts. With or, the the root agr num is equated to the agr num feature of the right conjunct. The conjdisc feature is only used at the root of the βCONJs tree. It blocks the adjunction of one βCONJs tree on another. The following equations are used, where Sr is the substitution node and Sc is the root node: (566) Sr.t:conj  disc (567) Sc.b:conj  andorbutnil E.10 Comparatives compar, possible values are ,  equiv, possible values are ,  super, possible values are ,  For detailed discussion see Chapter 22. E.11 Control control has no value and is used only for indexing purposes. The root node of every clausal tree has its control feature coindexed with the control feature of its subject. This allows adjunct control to take place. In addition, clauses that take inﬁnitival clausal complements have the control feature of their subjectobject coindexed with the control feature of their complement clause S, depending upon whether they are subject control verbs or object control verbs respectively. E.12 Other Features neg, possible values are ,  Used for controlling the interaction of negation and auxiliary verbs. pred, possible values are ,  The pred feature is used in the following tree families: Tnx0N1.trees and Tnx0nx1ARB.trees. In the Tnx0N1.trees family, the following equations are used: for αW1nx0N1: (568) NP1.t:pred   (569) NP1.b:pred   276 APPENDIX E. FEATURES (570) NP.t:pred   (571) N.t:pred  NP.b:pred This is the only tree in this tree family to use the pred feature. The other tree family where the pred feature is used is Tnx0nx1ARB.trees. Within this family, this feature (and the following equations) are used only in the αW1nx0nx1ARB tree. (572) AdvP1.t:pred   (573) AdvP1.b:pred   (574) NP.t:pred   (575) AdvP.b:pred  NP.t:pred pron, possible values are ,  This feature indicates whether a particular NP is a pronoun or not. Certain constructions which do not permit pronouns use this feature to block pronouns. tense, possible values are pres, past It does not seem to be the case that the tense feature interacts with other featuressyntactic processes. It comes from the lexicon with the verb and is transmitted up the tree in such a way that the root S node ends up with the tense feature of the highest verb in the tree. The equations used for this purpose are: (576) Sr.b:tense  VP.t:tense (577) VP.b:tense  V.t:tense trans, possible values are ,  Many but not all English verbs can anchor both transitive and intransitive trees. (578) The sun melted the ice cream. (579) The ice cream melted. (580) Elmo borrowed a book. (581)  A book borrowed. Transitive trees have the trans feature of their anchor set to  and intransitive trees have the trans feature of their anchor set to . Verbs such as melt which can occur in both transitive and intransitive trees come unspeciﬁed for the trans feature from the lexicon. Verbs which can only occur in transitive trees e.g. borrow have their trans feature speciﬁed in the lexicon as  thus blocking their anchoring of an intransitive tree. Appendix F Evaluation and Results In this appendix we describe various evaluations done of the XTAG grammar. Some of these evaluations were done on an earlier version of the XTAG grammar (the 1995 release), while other were done more recently. We will try to indicate in each section which version was used. F.1 Parsing Corpora In the XTAG project, we have used corpus analysis in two main ways: (1) to measure the performance of the English grammar on a given genre and (2) to identify gaps in the grammar. The second type of evaluation involves performing detailed error analysis on the sentences rejected by the parser, and we have done this several times on WSJ and Brown data. Based on the results of such analysis, we prioritize upcoming grammar development eﬀorts. The results of a recent error analysis are shown in Table F.1. The table does not show errors in parsing due to mistakes made by the POS tagger which contributed the largest number of errors: 32. At this point, we have added a treatment of punctuation to handle 1, an analysis of time NPs (2), a large number of multi-word prepositions (part of 3), gapless relative clauses (7), bare inﬁnitives (14) and have added the missing subcategorization (3) and missing lexical entry (12). We are in the process of extending the parser to handle VP coordination (9) (See Section 21 on recent work to handle VP and other predicative coordination). We ﬁnd that this method of error analysis is very useful in focusing grammar development in a productive direction. To ensure that we are not losing coverage of certain phenomena as we extend the gram- mar, we have a benchmark set of grammatical and ungrammatical sentences from this technical report. We parse these sentences periodically to ensure that in adding new features and con- structions to the grammar, we are not blocking previous analyses. There are approximately 590 example sentences in this set. F.2 TSNLP In addition to corpus-based evaluation, we have also run the English Grammar on the Test Suites for Natural Language Processing (TSNLP) English corpus [Lehmann et al., 1996]. The corpus is intended to be a systematic collection of English grammatical phenomena, including 277 278 APPENDIX F. EVALUATION AND RESULTS Rank No of errors Category of error 1 11 Parentheticals and appositives 2 8 Time NP 3 8 Missing subcat 4 7 Multi-word construction 5 6 Ellipsis 6 6 Not sentences 7 3 Relative clause with no gap 8 2 Funny coordination 9 2 VP coordination 10 2 Inverted predication 11 2 Who knows 12 1 Missing entry 13 1 Comparative? 14 1 Bare inﬁnitive Table F.1: Results of Corpus Based Error Analysis complementation, agreement, modiﬁcation, diathesis, modality, tense and aspect, sentence and clause types, coordination, and negation. It contains 1409 grammatical sentences and phrases and 3036 ungrammatical ones. Error Class  Example POS Tag 19.7 She adds toV it , He noisesN him abroad Missing lex item 43.3 used as an auxiliary V, calm NP down Missing tree 21.2 shouldve, bet NP NP S, regard NP as Adj Feature clashes 3 My every ﬁrm, All money Rest 12.8 approx, e.g. Table F.2: Breakdown of TSNLP Errors There were 42 examples which we judged ungrammatical, and removed from the test corpus. These were sentences with conjoined subject pronouns, where one or both were accusative, e.g. Her and him succeed. Overall, we parsed 61.4 of the 1367 remaining sentences and phrases. The errors were of various types, broken down in Table F.2. As with the error analysis described above, we used this information to help direct our grammar development eﬀorts. It also highlighted the fact that our grammar is heavily slanted toward American Englishour grammar did not handle dare or need as auxiliary verbs, and there were a number of very British particle constructions, e.g. She misses him out. One general problem with the test-suite is that it uses a very restricted lexicon, and if there is one problematic lexical item it is likely to appear a large number of times and cause a disproportionate amount of grief. Used to appears 33 times and we got all 33 wrong. However, it must be noted that the XTAG grammar has analyses for syntactic phenomena that were not represented in the TSNLP test suite such as sentential subjects and subordinating clauses among others. This eﬀort was, therefore, useful in highlighting some deﬁciencies in our grammar, but F.3. CHUNKING AND DEPENDENCIES IN XTAG DERIVATIONS 279 did not provide the same sort of general evaluation as parsing corpus data. F.3 Chunking and Dependencies in XTAG Derivations We evaluated the XTAG parser for the text chunking task [Abney, 1991]. In particular, we compared NP chunks and verb group (VG) chunks1 produced by the XTAG parser with the NP and VG chunks from the Penn Treebank [Marcus et al., 1993]. The test involved 940 sentences of length 15 words or less from sections 17 to 23 of the Penn Treebank, parsed using the XTAG English grammar. The results are given in Table F.3. NP Chunking VG Chunking Recall 82.15 74.51 Precision 83.94 76.43 Table F.3: Text Chunking performance of the XTAG parser System Training Size Recall Precision Ramshaw  Marcus Baseline 81.9 78.2 Ramshaw  Marcus 200,000 90.7 90.5 (without lexical information) Ramshaw  Marcus 200,000 92.3 91.8 (with lexical information) Supertags Baseline 74.0 58.4 Supertags 200,000 93.0 91.8 Supertags 1,000,000 93.8 92.5 Table F.4: Performance comparison of the transformation based noun chunker and the supertag based noun chunker As described earlier, the results cannot be directly compared with other results in chunking such as in [Ramshaw and Marcus, 1995] since we do not train from the Treebank before testing. However, in earlier work, text chunking was done using a technique called supertagging [Srinivas, 1997b] (which uses the XTAG English grammar) which can be used to train from the Treebank. The comparative results of text chunking between supertagging and other methods of chunking is shown in Figure F.4.2 We also performed experiments to determine the accuracy of the derivation structures pro- duced by XTAG on WSJ text, where the derivation tree produced after parsing XTAG is interpreted as a dependency parse. We took sentences that were 15 words or less from the Penn Treebank [Marcus et al., 1993]. The sentences were collected from sections 1723 of the Tree- bank. 9891 of these sentences were given at least one parse by the XTAG system. Since XTAG typically produces several derivations for each sentence we simply picked a single derivation 1We treat a sequence of verbs and verbal modiﬁers, including auxiliaries, adverbs, modals as constituting a verb group. 2It is important to note in this comparison that the supertagger uses lexical information on a per word basis only to pick an initial set of supertags for a given word. 280 APPENDIX F. EVALUATION AND RESULTS from the list for this evaluation. Better results might be achieved by ranking the output of the parser using the sort of approach described in [Srinivas et al., 1995]. There were some striking diﬀerences in the dependencies implicit in the Treebank and those given by XTAG derivations. For instance, often a subject NP in the Treebank is linked with the ﬁrst auxiliary verb in the tree, either a modal or a copular verb, whereas in the XTAG derivation, the same NP will be linked to the main verb. Also XTAG produces some dependencies within an NP, while a large number of words in NPs in the Treebank are directly dependent on the verb. To normalize for these facts, we took the output of the NP and VG chunker described above and accepted as correct any dependencies that were completely contained within a single chunk. For example, for the sentence Borrowed shares on the Amex rose to another record, the XTAG and Treebank chunks are shown below. XTAG chunks: [Borrowed shares] [on the Amex] [rose] [to another record] Treebank chunks: [Borrowed shares on the Amex] [rose] [to another record] Using these chunks, we can normalize for the fact that in the dependencies produced by XTAG borrowed is dependent on shares (i.e. in the same chunk) while in the Treebank borrowed is directly dependent on the verb rose. That is to say, we are looking at links between chunks, not between words. The dependencies for the sentence are given below. XTAG dependency Treebank dependency Borrowed::shares Borrowed::rose shares::rose shares::rose on::shares on::shares the::Amex the::Amex Amex::on Amex::on rose::NIL rose::NIL to::rose to::rose another::record another::record record::to record::to After this normalization, testing simply consisted of counting how many of the dependency links produced by XTAG matched the Treebank dependency links. Due to some tokenization and subsequent alignment problems we could only test on 835 of the original 9891 parsed sentences. There were a total of 6135 dependency links extracted from the Treebank. The XTAG parses also produced 6135 dependency links for the same sentences. Of the dependencies produced by the XTAG parser, 5165 were correct giving us an accuracy of 84.2. F.4. COMPARISON WITH IBM 281 F.4 Comparison with IBM The evaluation in this section was done with the earlier 1995 release of the grammar. This section describes an experiment to measure the crossing bracket accuracy of the XTAG-parsed IBM-manual sentences. In this experiment, XTAG parses of 1100 IBM-manual sentences have been ranked using certain heuristics. The ranked parses have been compared3 against the bracketing given in the Lancaster Treebank of IBM-manual sentences4. Table F.5 shows the results of XTAG obtained in this experiment, which used the highest ranked parse for each system. It also shows the results of the latest IBM statistical grammar ([Jelinek et al., 1994]) on the same genre of sentences. Only the highest-ranked parse of both systems was used for this evaluation. Crossing Brackets is the percentage of sentences with no pairs of brackets crossing the Treebank bracketing (i.e. ( ( a b ) c ) has a crossing bracket measure of one if compared to ( a ( b c ) ) ). Recall is the ratio of the number of constituents in the XTAG parse to the number of constituents in the corresponding Treebank sentence. Precision is the ratio of the number of correct constituents to the total number of constituents in the XTAG parse. System  of Crossing Bracket Recall Precision sentences Accuracy XTAG 1100 81.29 82.34 55.37 IBM Statistical 1100 86.20 86.00 85.00 grammar Table F.5: Performance of XTAG on IBM-manual sentences As can be seen from Table F.5, the precision ﬁgure for the XTAG system is considerably lower than that for IBM. For the purposes of comparative evaluation against other systems, we had to use the same crossing-brackets metric though we believe that the crossing-brackets measure is inadequate for evaluating a grammar like XTAG. There are two reasons for the inadequacy. First, the parse generated by XTAG is much richer in its representation of the internal structure of certain phrases than those present in manually created treebanks (e.g. IBM: [N your personal computer], XTAG: [NP [G your] [N [N personal] [N computer]]]). This is reﬂected in the number of constituents per sentence, shown in the last column of Table F.6.5 System Sent.  of Av.  of Av.  of Length sent wordssent Constituentssent XTAG 1-10 654 7.45 22.03 1-15 978 9.13 30.56 IBM Stat. 1-10 447 7.50 4.60 Grammar 1-15 883 10.30 6.40 Table F.6: Constituents in XTAG parse and IBM parse A second reason for considering the crossing bracket measure inadequate for evaluating 3We used the parseval program written by Phil Harison (philatc.boeing.com). 4The Treebank was obtained through Salim Roukos (roukoswatson.ibm.com) at IBM. 5We are aware of the fact that increasing the number of constituents also increases the recall percentage. However we believe that this a legitimate gain. 282 APPENDIX F. EVALUATION AND RESULTS XTAG is that the primary structure in XTAG is the derivation tree from which the bracketed tree is derived. Two identical bracketings for a sentence can have completely diﬀerent derivation trees (e.g. kick the bucket as an idiom vs. a compositional use). A more direct measure of the performance of XTAG would evaluate the derivation structure, which captures the dependencies between words. F.5 Comparison with Alvey The evaluation in this section was done with the earlier 1995 release of the grammar. This section compares XTAG to the Alvey Natural Language Tools (ANLT) Grammar. We parsed the set of LDOCE Noun Phrases presented in Appendix B of the technical report ([Carroll, 1993]) using XTAG. Table F.7 summarizes the results of this experiment. A total of 143 noun phrases were parsed. The NPs which did not have a correct parse in the top three derivations were considered failures for either system. The maximum and average number of derivations columns show the highest and the average number of derivations produced for the NPs that have a correct derivation in the top three. We show the performance of XTAG both with and without the tagger since the performance of the POS tagger is signiﬁcantly degraded on the NPs because the NPs are usually shorter than the sentences on which it was trained. It would be interesting to see if the two systems performed similarly on a wider range of data. System  of  parsed  parsed Maximum Average NPs derivations derivations ANLT Parser 143 127 88.81 32 4.57 XTAG Parser with 143 93 65.03 28 3.45 POS tagger XTAG Parser without 143 120 83.91 28 4.14 POS tagger Table F.7: Comparison of XTAG and ANLT Parser F.6 Comparison with CLARE The evaluation in this section was done with the earlier 1995 release of the grammar. This section compares the performance of XTAG against that of the CLARE-2 system ([Alshawi et al., 1992]) on the ATIS corpus. Table F.8 shows the performance results. The percentage parsed column for both systems represents the percentage of sentences that produced any parse. It must be noted that the performance result shown for CLARE-2 is without any tuning of the grammar for the ATIS domain. The performance of CLARE-3, a later version of the CLARE system, is estimated to be 10 higher than that of the CLARE-2 system.6 In an attempt to compare the performance of the two systems on a wider range of sentences (from similar genres), we provide in Table F.9 the performance of CLARE-2 on LOB corpus and 6When CLARE-3 is tuned to the ATIS domain, performance increases to 90. However XTAG has not been tuned to the ATIS domain. F.6. COMPARISON WITH CLARE 283 System Mean length  parsed CLARE-2 6.53 68.50 XTAG 7.62 88.35 Table F.8: Performance of CLARE-2 and XTAG on the ATIS corpus the performance of XTAG on the WSJ corpus. The performance was measured on sentences of up to 10 words for both systems. System Corpus Mean length  parsed CLARE-2 LOB 5.95 53.40 XTAG WSJ 6.00 55.58 Table F.9: Performance of CLARE-2 and XTAG on LOB and WSJ corpus respectively Bibliography [Abeille and Schabes, 1989] Anne Abeille and Yves Schabes. Parsing Idioms in Lexicalized TAGs. In Proceedings of EACL 89, pages 16165, 1989. [Abeille, 1990] Anne Abeille. French and english determiners: Interaction of morphology, syntax, and semantics in lexicalized tree adjoining grammars. In Tree Adjoining Gram- mar,First International Workshop on TAGs: Formal Theory and Applications (abstracts), Shloss Dagstuhl, Sarrbrucken, 1990. [Abney, 1987] Steven Abney. The English Noun Phrase in its Sentential Aspects. PhD thesis, MIT, 1987. [Abney, 1991] Steven Abney. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-based parsing. Kluwer Academic Publishers, 1991. [Akmajian, 1970] A. Akmajian. On deriving cleft sentences from pseudo-cleft sentences. Lin- guistic Inquiry, 1:149168, 1970. [Alshawi et al., 1992] Hiyan Alshawi, David Carter, Richard Crouch, Steve Pullman, Manny Rayner, and Arnold Smith. CLARE  A Contextual Reasoning and Cooperative Response Framework for the Core Language Engine. SRI International, Cambridge, England, 1992. [Ball, 1991] Catherine N. Ball. The historical development of the it-cleft. PhD thesis, University of Pennsylvania, 1991. [Baltin, 1989] Mark Baltin. Heads and projections. In Mark Baltin and Anthony S. Kroch, editors, Alternative conceptions of phrase structure, pages 116. University of Chicago Press, Chicago, Illinois, 1989. [Barwise and Cooper, 1981] John Barwise and Robin Cooper. Generalized Quantiﬁers and Natural Language. Linguistics and Philosophy, 4, 1981. [Becker, 1993] Tilman Becker. HyTAG: A new Type of Tree Adjoining Grammars for Hybrid Syntactic Representation of Free Word Order Languages. PhD thesis, Universitat des Saar- landes, 1993. [Becker, 1994] Tilman Becker. Patterns in metarules. In Proceedings of the 3rd TAG Confer- ence, Paris, France, 1994. [Bhatt, 1994] Rajesh Bhatt. Pro-control in tags. Unpublished paper, University of Pennsylva- nia, 1994. 284 BIBLIOGRAPHY 285 [Browning, 1987] Marguerite Browning. Null Operator Constructions. PhD thesis, MIT, 1987. [Burzio, 1986] Luigi Burzio. Italian syntax. A Government-Binding approach. Studies in nat- ural language and linguistic theory. Reidel, Dordrecht, 1986. [Candito, 1996] Marie-Helene Candito. A Principle-Based Hierarchical Representation of LT- AGs. In Proceedings of COLING-96, Copenhagen, Denmark, 1996. [Carroll, 1993] John Carroll. Practical Uniﬁcation-based Parsing of Natural Language. Univer- sity of Cambridge, Computer Laboratory, Cambridge, England, 1993. [Chomsky and Lasnik, 1993] Noam Chomsky and Howard Lasnik. The minimalist program. manuscript, 1993. [Chomsky, 1965] Noam Chomsky. Aspects of the Theory of Syntax. MIT Press, Cambridge, Massachusetts, 1965. [Chomsky, 1970] Noam Chomsky. Remarks on Nominalization. In Readings in English Trans- formational Grammar, pages 184221. Ginn and Company, Waltham, Massachusetts, 1970. [Chomsky, 1986] Noam Chomsky. Barriers. MIT Press, Cambridge, Massachusetts, 1986. [Chomsky, 1992] Noam Chomsky. A Minimalist Approach to Linguistic Theory. MIT Working Papers in Linguistics, Occasional Papers in Linguistics No. 1, 1992. [Church, 1988] Kenneth Ward Church. A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In 2nd Applied Natural Language Processing Conference, Austin, Texas, 1988. [Cinque, 1990] G. Cinque. Types of A-bar-Dependencies. MIT Press, Cambridge, Mas- sachusetts; London, England, 1990. [Cowie and Mackin, 1975] A. P. Cowie and R. Mackin, editors. Oxford Dictionary of Current Idiomatic English, volume 1. Oxford University Press, London, England, 1975. [Delahunty, 1984] Gerald P. Delahunty. The Analysis of English Cleft Sentences. Linguistic Analysis, 13(2):63113, 1984. [Delin, 1989] Judy L. Delin. Cleft Constructions in Discourse. PhD thesis, University of Edin- burgh, 1989. [Doran, 1998] Christine Doran. Incorporating Punctuation into the Sentence Grammar: A Lexicalized Tree Adjoining Grammar Perspective. PhD thesis, University of Pennsylvania, 1998. [Egedi and Martin, 1994] Dania Egedi and Patrick Martin. A Freely Available Syntactic Lexi- con for English. In Proceedings of the International Workshop on Sharable Natural Language Resources, Nara, Japan, August 1994. [Emonds, 1970] J. Emonds. Root and structure-preserving transformations. PhD thesis, Mas- sachusetts Institute of Technology, 1970. 286 BIBLIOGRAPHY [Ernst, 1983] T. Ernst. More on Adverbs and Stressed Auxiliaries. Linguistic Inquiry 13, pages 54248, 1983. [Evans et al., 1995] Roger Evans, Gerald Gazdar, and David Weir. Encoding Lexicalized Tree Adjoining Grammars with a Nonmonotonic Inheritance Hierarchy. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Cambridge, MA, 1995. [Gazdar et al., 1985] G. Gazdar, E. Klein, G. Pullum, and I. Sag. Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, Massachusetts, 1985. [Grimshaw, 1990] Jane Grimshaw. Argument Structure. MIT Press, Cambridge, Massachusetts, 1990. [Haegeman, 1991] Liliane Haegeman. Introduction to Government and Binding Theory. Black- well Publishers, Cambridge, Massachusetts, Oxford, U.K., 1991. [Hale and Keyser, 1986] Ken Hale and Samuel Jay Keyser. Some transitivity alternations in English. Technical Report 7, MITCCS, July 1986. [Hale and Keyser, 1987] Ken Hale and Samuel Jay Keyser. A view from the middle. Technical Report 10, MITCCS, January 1987. [Hanks, 1979] Patrick Hanks, editor. Collins Dictionary of the English Language. Collins, London, England, 1979. [Heycock and Kroch, 1993] Caroline Heycock and Anthony S. Kroch. Verb movement and the status of subjects: implications for the theory of licensing. GAGL, 36:75102, 1993. [Heycock, 1991] Caroline Heycock. Progress Report: The English Copula in a LTAG. Internal XTAG project report, University of Pennsylvania, Summer 1991. [Hockey and Mateyak, 1998] Beth Ann Hockey and Heather Mateyak. Determining determiner sequencing: A syntactic analysis for english. Technical Report 98-10, Institute for Research in Cognitive Science, University of Pennsylvania, 1998. [Hockey and Srinivas, 1993] Beth Ann Hockey and B. Srinivas. Feature-Based TAG in Place of Multi-component Adjunction: Computational Implications. In Proceedings of the Natural Language Processing Paciﬁc Rim Symposium (NLPRS), Fukuoka, Japan, December 1993. [Hockey, 1994] Beth Ann Hockey. Echo questions, intonation and focus. In Proceedings of the Interdisciplinary Conference on Focus and Natural Language Processing in Celebration of the 10th anniversary of the Journal of Semantics, Eschwege, Germany, 1994. [Hornby, 1974] A. S. Hornby, editor. Oxford Advanced Learners Dictionary of Current English. Oxford University Press, London, England, third edition, 1974. [Iatridou, 1991] Sabine Iatridou. Topics in Conditionals. PhD thesis, MIT, 1991. [Jackendoﬀ, 1972] R. Jackendoﬀ. Semantic Interpretation in Generative Grammar. MIT Press, Cambridge, Massachusetts, 1972. BIBLIOGRAPHY 287 [Jackendoﬀ, 1990] R. Jackendoﬀ. On Larsons Analysis of the Double Object Construction. Linguistic Inquiry, 21:427456, 1990. [Jelinek et al., 1994] Fred Jelinek, John Laﬀerty, David M. Magerman, Robert Mercer, Adwait Ratnaparkhi, and Salim Roukos. Decision Tree Parsing using a Hidden Derivation Model. In Proceedings from the ARPA Workshop on Human Language Technology Workshop, March 1994. [Joshi et al., 1975] Aravind K. Joshi, L. Levy, and M. Takahashi. Tree Adjunct Grammars. Journal of Computer and System Sciences, 1975. [Joshi, 1985] Aravind K. Joshi. Tree Adjoining Grammars: How much context Sensitivity is required to provide a reasonable structural description. In D. Dowty, I. Karttunen, and A. Zwicky, editors, Natural Language Parsing, pages 206250. Cambridge University Press, Cambridge, U.K., 1985. [Kaplan and Bresnan, 1983] Ronald Kaplan and Joan Bresnan. Lexical-functional Grammar: A Formal System for Grammatical Representation. In J. Bresnan, editor, The Mental Rep- resentation of Grammatical Relations. MIT Press, Cambridge, Massachusetts, 1983. [Karp et al., 1992] Daniel Karp, Yves Schabes, Martin Zaidel, and Dania Egedi. A Freely Available Wide Coverage Morphological Analyzer for English. In Proceedings of the 15th International Conference on Computational Linguistics (COLING 92), Nantes, France, Au- gust 1992. [Keenan and Stavi, 1986] E. L. Keenan and J. Stavi. A Semantic Characterization of Natural Language Determiners. Linguistics and Philosophy, 9, August 1986. [Knowles, 1986] John Knowles. The Cleft Sentence: A Base-Generated Perspective. Lingua: International Review of General Linguistics, 69(4):295317, August 1986. [Kroch and Joshi, 1985] Anthony S. Kroch and Aravind K. Joshi. The Linguistic Relevance of Tree Adjoining Grammars. Technical Report MS-CIS-85-16, Department of Computer and Information Science, University of Pennsylvania, 1985. [Kroch and Joshi, 1987] Anthony S. Kroch and Aravind K. Joshi. Analyzing Extraposition in a Tree Adjoining Grammar. In G. Huck and A. Ojeda, editors, Discontinuous Constituents, Syntax and Semantics, volume 20. Academic Press, 1987. [Lapointe, 1980] S. Lapointe. A lexical analysis of the English auxiliary verb system. In T. Hoekstra et al, editor, Lexical Grammar, pages 215254. Foris, Dordrecht, 1980. [Larson, 1988] R. Larson. On the Double Object construction. Linguistic Inquiry, 19:335391, 1988. [Larson, 1990] R. Larson. Double Objects Revisited: Reply to Jackendoﬀ. Linguistic Inquiry, 21:589612, 1990. [Lasnik and Saito, 1984] H. Lasnik and M. Saito. On the Nature of Proper Government. Lin- guistic Inquiry, 15:235289, 1984. 288 BIBLIOGRAPHY [Lasnik and Uriagereka, 1988] H. Lasnik and J. Uriagereka. A Course in GB Syntax. MIT Press, Cambridge, Massachusetts, 1988. [Lees, 1960] Robert B. Lees. The Grammar of English Nominalizations. Indiana University Research Center in Anthropology, Folklore, and Linguistics, Indiana University, Bloomington, Indiana, 1960. [Lehmann et al., 1996] Sabine Lehmann, Stephan Oepen, Sylvie Regnier-Prost, Klaus Netter, Veronika Lux, Judith Klein, Kirsten Falkedal, Frederik Fouvry, Dominique Estival, Eva Dauphin, Herve Compagnion, Judith Baur, Lorna Balkan, and Doug Arnold. tsnlp  Test Suites for Natural Language Processing. In Proceedings of COLING 1996, Kopenhagen, 1996. [Liberman, 1989] Mark Liberman. Tex on tap: the ACL Data Collection Initiative. In Proceed- ings of the DARPA Workshop on Speech and Natural Language Processing. Morgan Kaufman, 1989. [Marcus et al., 1993] Mitchell M. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Lin- guistics, 19.2:313330, June 1993. [Mateyak, 1997] Heather Mateyak. Negation of noun phrases with not. Technical Report 97-18, UPENN-IRCS, October 1997. [McCawley, 1988] James D. McCawley. The Syntactic Phenomena of English. The University of Chicago Press, Chicago, Illinois, 1988. [Moro, 1990] A. Moro. There as a Raised Predicate. Paper presented at GLOW, 1990. [Napoli, 1988] Donna Jo Napoli. Ergative Verbs in English. Journal of the Linguistic Society of America (Language), 64:1(1):130142, March 1988. [Obernauer, 1984] H. Obernauer. On the Identiﬁcation of Empty Categories. Linguistic Review 4, 2:153202, 1984. [Paroubek et al., 1992] Patrick Paroubek, Yves Schabes, and Aravind K. Joshi. Xtag  a graph- ical workbench for developing tree-adjoining grammars. In Third Conference on Applied Natural Language Processing, Trento, Italy, 1992. [Partee et al., 1990] Barbara Partee, Alice ter Meulen, and Robert E. Wall. Mathematical Methods in Linguistics. Kluwer Academic Publishers, 1990. [Pollard and Sag, 1994] Carl Pollard and Ivan A. Sag. Head-Driven Phrase Structure Grammar. CSLI, 1994. [Pollock, 1989] J-Y. Pollock. Verb Movement, UG, and the Structure of IP. Linguistic Inquiry, 20.3:365424, 1989. [Quirk et al., 1985] Randolph Quirk, Sidney Greenbaum, Geoﬀrey Leech, and Jan Svartvik. A Comprehensive Grammar of the English Language. Longman, London, 1985. BIBLIOGRAPHY 289 [Ramshaw and Marcus, 1995] Lance Ramshaw and Mitchell P. Marcus. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, MIT, Cambridge, Boston, 1995. [Rizzi, 1990] Luigi Rizzi. Relativized Minimality. MIT Press, Cambridge, Massachusetts; Lon- don, England, 1990. [Rogers and Vijay-Shankar, 1994] James Rogers and K. Vijay-Shankar. Obtaining Trees from their Descriptions: An Application to Tree Adjoining Grammars. Computational Intelligence, 10(4), 1994. [Rosenbaum, 1967] Peter S. Rosenbaum. The grammar of English predicate complement con- structions. MIT press, Cambridge, Massachusetts, 1967. [Sag et al., 1985] I. Sag, G. Gazdar, T. Wasow, and S. Weisler. Coordination and How to distinguish categories. Natural Language and Linguistic Theory, 3:117171, 1985. [Sarkar and Joshi, 1996] Anoop Sarkar and Aravind Joshi. Coordination in Tree Adjoining Grammars: Formalization and Implementation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 94), Copenhagen, Denmark, August 1996. [Schabes and Joshi, 1988] Yves Schabes and Aravind K. Joshi. An Early-Type Parsing Algo- rithm for Tree Adjoining Grammars. In Proceedings of the 26th Meeting of the Association for Computational Linguistics, Buﬀalo, June 1988. [Schabes et al., 1988] Yves Schabes, Anne Abeille, and Aravind K. Joshi. Parsing strategies with lexicalized grammars: Application to Tree Adjoining Grammars. In Proceedings of the 12th International Conference on Computational Linguistics (COLING88), Budapest, Hungary, August 1988. [Schabes, 1990] Yves Schabes. Mathematical and Computational Aspects of Lexicalized Gram- mars. PhD thesis, Computer Science Department, University of Pennsylvania, 1990. [Seuss, 1971] Dr. Seuss. The Lorax. Random House, New York, New York, 1971. [Soong and Huang, 1990] Frank K. Soong and Eng-Fong Huang. Fast Tree-Trellis Search for Finding the N-Best Sentence Hypothesis in Continuous Speech Recognition. Journal of Acoustic Society, AM., May 1990. [Sornicola, 1988] R. Sornicola. IT-clefts and WH-clefts: two awkward sentence types. Journal of Linguistics, 24:343379, 1988. [Srinivas et al., 1994] B. Srinivas, D. Egedi, C. Doran, and T. Becker. Lexicalization and gram- mar development. In Proceedings of KONVENS 94, pages 3109, Vienna, Austria, 1994. [Srinivas et al., 1995] B. Srinivas, Christine Doran, and Seth Kulick. Heuristics and parse rank- ing. In Proceedings of the 4th Annual International Workshop on Parsing Technologies, Prague, September 1995. 290 BIBLIOGRAPHY [Srinivas, 1997a] B. Srinivas. Complexity of Lexical Descriptions and its Relevance to Partial Parsing. PhD thesis, University of Pennsylvania, Philadelphia, PA, August 1997. [Srinivas, 1997b] B. Srinivas. Performance Evaluation of Supertagging for Partial Parsing. In Proceedings of Fifth International Workshop on Parsing Technology, Boston, USA, September 1997. [Vijay-Shanker and Joshi, 1991] K. Vijay-Shanker and Aravind K. Joshi. Uniﬁcation Based Tree Adjoining Grammars. In J. Wedekind, editor, Uniﬁcation-based Grammars. MIT Press, Cambridge, Massachusetts, 1991. [Vijay-Shanker and Schabes, 1992] K. Vijay-Shanker and Yves Schabes. Structure sharing in lexicalized tree adjoining grammar. In Proceedings of the 15th International Conference on Computational Linguistics (COLING 92), Nantes, France, August 1992. [Watanabe, 1993] Akira Watanabe. The Notion of Finite Clauses in AGR-Based Case Theory. MIT Working Papers in Linguistics, 18:281296, 1993. [XTAG-Group, 1995] The XTAG-Group. A Lexicalized Tree Adjoining Grammar for English. Technical Report IRCS 95-03, University of Pennsylvania, 1995.",
  "4.pdf": "arXiv:cs9809026v1 [cs.CL] 18 Sep 1998 Preﬁx Probabilities from Stochastic Tree Adjoining Grammars Mark-Jan Nederhof DFKI Stuhlsatzenhausweg 3, D-66123 Saarbrucken, Germany nederhofdfki.de Anoop Sarkar Dept. of Computer and Info. Sc. Univ of Pennsylvania 200 South 33rd Street, Philadelphia, PA 19104 USA anooplinc.cis.upenn.edu Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6A, 35131 Padova, Italy sattadei.unipd.it Abstract Language models for speech recognition typ- ically use a probability model of the form Pr(ana1, a2, . . . , an1). Stochastic grammars, on the other hand, are typically used to as- sign structure to utterances. A language model of the above form is constructed from such grammars by computing the preﬁx probabil- ity  wΣ Pr(a1    anw), where w represents all possible terminations of the preﬁx a1    an. The main result in this paper is an algorithm to compute such preﬁx probabilities given a stochastic Tree Adjoining Grammar (TAG). The algorithm achieves the required computa- tion in O(n6) time. The probability of sub- derivations that do not derive any words in the preﬁx, but contribute structurally to its deriva- tion, are precomputed to achieve termination. This algorithm enables existing corpus-based es- timation techniques for stochastic TAGs to be used for language modelling. 1 Introduction Given some word sequence a1    an1, speech recognition language models are used to hy- pothesize the next word an, which could be any word from the vocabulary Σ. This is typically done using a probability model Pr(ana1, . . . , an1). Based on the assumption that modelling the hidden structure of nat-  Part of this research was done while the ﬁrst and the third authors were visiting the Institute for Research in Cognitive Science, University of Pennsylvania. The ﬁrst author was supported by the German Federal Min- istry of Education, Science, Research and Technology (BMBF) in the framework of the Verbmobil Project un- der Grant 01 IV 701 V0, and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientiﬁc Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94- G-0426. The authors wish to thank Aravind Joshi for his support in this research. ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Laﬀerty, 1991; Stolcke, 1995). The probability model used for a stochas- tic grammar was  wΣ Pr(a1    anw). How- ever, language models that are based on tri- gram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are impor- tant in this respect since they are easily lexical- ized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees as- sociated with words can be used to encode argu- ment and adjunct relations in various syntactic environments. This paper assumes some famil- iarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good intro- ductions to the formalism and its linguistic rele- vance. TAGs have been shown to have relations with both phrase-structure grammars and de- pendency grammars (Rambow and Joshi, 1995), which is relevant because recent work on struc- tured language models (Chelba et al., 1997) have used dependency grammars to exploit their lex- icalization. We use stochastic TAGs as such a structured language model in contrast with ear- lier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute preﬁx probabilities  wΣ Pr(a1    anw). The algorithm assumes as input a stochastic TAG G and a string which is a preﬁx of some string in L(G), the language generated by G. This algo- rithm enables existing corpus-based estimation techniques (Schabes, 1992) in stochastic TAGs to be used for language modelling. 2 Notation A stochastic Tree Adjoining Grammar (STAG) is represented by a tuple (NT, Σ, I, A, φ) where NT is a set of nonterminal symbols, Σ is a set of terminal symbols, I is a set of initial trees and A is a set of auxiliary trees. Trees in I A are also called elementary trees. We refer to the root of an elementary tree t as Rt. Each auxiliary tree has exactly one distin- guished leaf, which is called the foot. We refer to the foot of an auxiliary tree t as Ft. We let V denote the set of all nodes in the elementary trees. For each leaf N in an elementary tree, except when it is a foot, we deﬁne label(N) to be the label of the node, which is either a terminal from Σ or the empty string ǫ. For each other node N, label(N) is an element from NT. At a node N in a tree such that label(N)  NT an operation called adjunction can be ap- plied, which excises the tree at N and inserts an auxiliary tree. Function φ assigns a probability to each ad- junction. The probability of adjunction of t  A at node N is denoted by φ(t, N). The probabil- ity that at N no adjunction is applied is denoted by φ(nil, N). We assume that each STAG G that we consider is proper. That is, for each N such that label(N)  NT,  tA{nil} φ(t, N)  1. For each non-leaf node N we construct the string cdn(N)   N1     Nm from the (ordered) list of children nodes N1, . . . , Nm by deﬁning, for each d such that 1  d  m,  Nd  label(Nd) in case label(Nd)  Σ  {ǫ}, and  Nd  Nd oth- erwise. In other words, children nodes are re- placed by their labels unless the labels are non- terminal symbols. To simplify the exposition, we assume an ad- ditional node for each auxiliary tree t, which we denote by . This is the unique child of the actual foot node Ft. That is, we change the def- inition of cdn such that cdn(Ft)   for each auxiliary tree t. We set V   {N  V  label(N)  NT}  Σ  {}. We use symbols a, b, c, . . . to range over Σ, symbols v, w, x, . . . to range over Σ, sym- bols N, M, . . . to range over V , and symbols α, β, γ, . . . to range over (V ). We use t, t, . . . to denote trees in I  A or subtrees thereof. We deﬁne the predicate dft on elements from V  as dft(N) if and only if (i) N  V and N dominates , or (ii) N  . We extend dft to strings of the form N1 . . . Nm  (V ) by deﬁning dft(N1 . . . Nm) if and only if there is a d (1  d  m) such that dft(Nd). For some logical expression p, we deﬁne δ(p)  1 iﬀ p is true, δ(p)  0 otherwise. 3 Overview The approach we adopt in the next section to derive a method for the computation of preﬁx probabilities for TAGs is based on transforma- tions of equations. Here we informally discuss the general ideas underlying equation transfor- mations. Let w  a1a2    an  Σ be a string and let N  V . We use the following representation which is standard in tabular methods for TAG parsing. An item is a tuple [N, i, j, f1, f2] rep- resenting the set of all trees t such that (i) t is a subtree rooted at N of some derived elementary tree; and (ii) ts root spans from position i to position j in w, ts foot node spans from posi- tion f1 to position f2 in w. In case N does not dominate the foot, we set f1  f2  . We gen- eralize in the obvious way to items [t, i, j, f1, f2], where t is an elementary tree, and [α, i, j, f1, f2], where cdn(N)  αβ for some N and β. To introduce our approach, let us start with some considerations concerning the TAG pars- ing problem. When parsing w with a TAG G, one usually composes items in order to con- struct new items spanning a larger portion of the input string. Assume there are instances of auxiliary trees t and t in G, where the yield of t, apart from its foot, is the empty string. If φ(t, N)  0 for some node N on the spine of t, and we have recognized an item [Rt, i, j, f1, f2], then we may adjoin t at N and hence deduce the existence of an item [Rt, i, j, f1, f2] (see Fig. 1(a)). Similarly, if t can be adjoined at a node N to the left of the spine of t and f1  f2, we may deduce the existence of an item [Rt, i, j, j, j] (see Fig. 1(b)). Importantly, one or more other auxiliary trees with empty yield could wrap the tree t before t adjoins. Adjunc- tions in this situation are potentially nontermi- nating. One may argue that situations where auxil- iary trees have empty yield do not occur in prac- tice, and are even by deﬁnition excluded in the PSfrag replacements i i j j f1 f1 f2 f2 Rt Rt Ft Ft N N Rt Rt Ft Ft ǫ ǫ ǫ ǫ t t t t spine (a) (b) Figure 1: Wrapping in auxiliary trees with empty yield case of lexicalized TAGs. However, in the com- putation of the preﬁx probability we must take into account trees with non-empty yield which behave like trees with empty yield because their lexical nodes fall to the right of the right bound- ary of the preﬁx string. For example, the two cases previously considered in Fig. 1 now gen- eralize to those in Fig. 2. Sfrag replacements i i j n n f1 f1 f2 f2 Rt Rt Ft Ft N Rt Rt Ft Ft ǫ ǫ ǫ ǫ spine (a) (b) Figure 2: Wrapping of auxiliary trees when computing the preﬁx probability To derive a method for the computation of preﬁx probabilities, we give some simple recur- sive equations. Each equation decomposes an item into other items in all possible ways, in the sense that it expresses the probability of that item as a function of the probabilities of items associated with equal or smaller portions of the input. In specifying the equations, we exploit tech- niques used in the parsing of incomplete in- put (Lang, 1988). This allows us to compute the preﬁx probability as a by-product of com- puting the inside probability. In order to avoid the problem of nontermi- nation outlined above, we transform our equa- tions to remove inﬁnite recursion, while preserv- ing the correctness of the probability computa- tion. The transformation of the equations is explained as follows. For an item I, the span of I, written σ(I), is the 4-tuple representing the 4 input positions in I. We will deﬁne an equivalence relation on spans that relates to the portion of the input that is covered. The trans- formations that we apply to our equations pro- duce two new sets of equations. The ﬁrst set of equations are concerned with all possible de- compositions of a given item I into set of items of which one has a span equivalent to that of I and the others have an empty span. Equations in this set represent endless recursion. The sys- tem of all such equations can be solved indepen- dently of the actual input w. This is done once for a given grammar. The second set of equations have the property that, when evaluated, recursion always termi- nates. The evaluation of these equations com- putes the probability of the input string modulo the computation of some parts of the derivation that do not contribute to the input itself. Com- bination of the second set of equations with the solutions obtained from the ﬁrst set allows the eﬀective computation of the preﬁx probability. 4 Computing Preﬁx Probabilities This section develops an algorithm for the com- putation of preﬁx probabilities for stochastic TAGs. 4.1 General equations The preﬁx probability is given by:  wΣ Pr(a1    anw)   tI P([t, 0, n, , ]), where P is a function over items recursively de- ﬁned as follows: P([t, i, j, f1, f2])  P([Rt, i, j, f1, f2]); (1) P([αN, i, j, , ])  (2)  k(i  k  j) P([α, i, k, , ])  P([N, k, j, , ]), if α  ǫ  dft(αN); P([αN, i, j, f1, f2])  (3)  k(i  k  f1) P([α, i, k, , ])  P([N, k, j, f1, f2]), if α  ǫ  dft(N); P([αN, i, j, f1, f2])  (4)  k(f2  k  j) P([α, i, k, f1, f2])  P([N, k, j, , ]), if α  ǫ  dft(α); P([N, i, j, f1, f2])  (5) φ(nil, N)  P([cdn(N), i, j, f1, f2])   f 1, f 2(i  f 1  f1  f2  f 2  j) P([cdn(N), f  1, f  2, f1, f2])   tA φ(t, N)  P([t, i, j, f  1, f  2]), if N  V  dft(N); P([N, i, j, , ])  (6) φ(nil, N)  P([cdn(N), i, j, , ])   f 1, f 2(i  f 1  f 2  j) P([cdn(N), f  1, f  2, , ])   tA φ(t, N)  P([t, i, j, f  1, f  2]), if N  V  dft(N); P([a, i, j, , ])  (7) δ(i  1  j  aj  a)  δ(i  j  n); P([, i, j, f1, f2])  δ(i  f1  j  f2); (8) P([ǫ, i, j, , ])  δ(i  j). (9) Term P([t, i, j, f1, f2]) gives the inside probabil- ity of all possible trees derived from elementary tree t, having the indicated span over the input. This is decomposed into the contribution of each single node of t in equations (1) through (6). In equations (5) and (6) the contribution of a node N is determined by the combination of the inside probabilities of Ns children and by all possible adjunctions at N. In (7) we rec- ognize some terminal symbol if it occurs in the preﬁx, or ignore its contribution to the span if it occurs after the last symbol of the preﬁx. Cru- cially, this step allows us to reduce the compu- tation of preﬁx probabilities to the computation of inside probabilities. 4.2 Terminating equations In general, the recursive equations (1) to (9) are not directly computable. This is because the value of P([A, i, j, f, f ]) might indirectly de- pend on itself, giving rise to nontermination. We therefore rewrite the equations. We deﬁne an equivalence relation over spans, that expresses when two items are associated with equivalent portions of the input: (i, j, f  1, f  2)  (i, j, f1, f2) if and only if ((i, j)  (i, j)) ((f  1, f  2)  (f1, f2) ((f  1  f  2  if  1  f  2  j f  1  f  2  ) (f1  f2  if1  f2  j f1  f2  ))) We introduce two new functions Plow and Psplit. When evaluated on some item I, Plow re- cursively calls itself as long as some other item I with a given elementary tree as its ﬁrst com- ponent can be reached, such that σ(I)  σ(I). Plow returns 0 if the actual branch of recursion cannot eventually reach such an item I, thus removing the contribution to the preﬁx proba- bility of that branch. If item I is reached, then Plow switches to Psplit. Complementary to Plow, function Psplit tries to decompose an argument item I into items I such that σ(I)  σ(I). If this is not possible through the actual branch of recursion, Psplit returns 0. If decomposition is indeed possible, then we start again with Plow at items produced by the decomposition. The eﬀect of this intermixing of function calls is the simulation of the original function P, with Plow being called only on potentially nonterminating parts of the computation, and Psplit being called on parts that are guaranteed to terminate. Consider some derivation tree spanning some portion of the input string, and the associated derivation tree τ. There must be a unique ele- mentary tree which is represented by a node in τ that is the lowest one that entirely spans the portion of the input of interest. (This node might be the root of τ itself.) Then, for each t  A and for each i, j, f1, f2 such that i  j and i  f1  f2  j, we must have: P([t, i, j, f1, f2])  (10)  t  A, f 1, f 2((i, j, f 1, f 2)  (i, j, f1, f2)) Plow([t, i, j, f1, f2], [t, f  1, f  2]). Similarly, for each t  I and for each i, j such that i  j, we must have: P([t, i, j, , ])  (11)  t  {t}  A, f  {, i, j} Plow([t, i, j, , ], [t, f, f]). The reason why Plow keeps a record of indices f  1 and f  2, i.e., the spanning of the foot node of the lowest tree (in the above sense) on which Plow is called, will become clear later, when we introduce equations (29) and (30). We deﬁne Plow([t, i, j, f1, f2], [t, f  1, f  2]) and Plow([α, i, j, f1, f2], [t, f  1, f  2]) for i  j and (i, j, f1, f2)  (i, j, f  1, f  2), as follows. Plow([t, i, j, f1, f2], [t, f  1, f  2])  (12) Plow([Rt, i, j, f1, f2], [t, f  1, f  2])  δ((t, f1, f2)  (t, f  1, f  2))  Psplit([Rt, i, j, f1, f2]); Plow([αN, i, j, , ], [t, f  1, f  2])  (13) Plow([α, i, j, , ], [t, f  1, f  2])  P([N, j, j, , ])  P([α, i, i, , ])  Plow([N, i, j, , ], [t, f  1, f  2]), if α  ǫ  dft(αN); Plow([αN, i, j, f1, f2], [t, f  1, f  2])  (14) δ(f1  j)  Plow([α, i, j, , ], [t, f  1, f  2])  P([N, j, j, f1, f2])  P([α, i, i, , ])  Plow([N, i, j, f1, f2], [t, f  1, f  2]), if α  ǫ  dft(N); Plow([αN, i, j, f1, f2], [t, f  1, f  2])  (15) Plow([α, i, j, f1, f2], [t, f  1, f  2])  P([N, j, j, , ])  δ(i  f2)  P([α, i, i, f1, f2])  Plow([N, i, j, , ], [t, f  1, f  2]), if α  ǫ  dft(α); Plow([N, i, j, f1, f2], [t, f  1, f  2])  (16) φ(nil, N)  Plow([cdn(N), i, j, f1, f2], [t, f  1, f  2])  Plow([cdn(N), i, j, f1, f2], [t, f  1, f  2])   tA φ(t, N)  P([t, i, j, i, j])  P([cdn(N), f1, f2, f1, f2])   tA φ(t, N)Plow([t, i, j, f1, f2], [t, f  1, f  2]), if N  V  dft(N); Plow([N, i, j, , ], [t, f  1, f  2])  (17) φ(nil, N)  Plow([cdn(N), i, j, , ], [t, f  1, f  2])  Plow([cdn(N), i, j, , ], [t, f  1, f  2])   tA φ(t, N)  P([t, i, j, i, j])   f 1 , f 2 (f 1  f 2  i  f 1  f 2  j) P([cdn(N), f  1 , f  2 , , ])   tA φ(t, N)Plow([t, i, j, f  1 , f  2 ], [t, f  1, f  2]), if N  V  dft(N); Plow([a, i, j, , ], [t, f  1, f  2])  0; (18) Plow([, i, j, f1, f2], [t, f  1, f  2])  0; (19) Plow([ǫ, i, j, , ], [t, f  1, f  2])  0. (20) The deﬁnition of Plow parallels the one of P given in 4.1. In (12), the second term in the right-hand side accounts for the case in which the tree we are visiting is the lowest one on which Plow should be called. Note how in the above equations Plow must be called also on nodes that do not dominate the footnode of the elementary tree they belong to (cf. the deﬁnition of ). Since no call to Psplit is possible through the terms in (18), (19) and (20), we must set the right-hand side of these equations to 0. The speciﬁcation of Psplit([α, i, j, f1, f2]) is given below. Again, the deﬁnition parallels the one of P given in 4.1. Psplit([αN, i, j, , ])  (21)  k(i  k  j) P([α, i, k, , ])  P([N, k, j, , ])  Psplit([α, i, j, , ])  P([N, j, j, , ])  P([α, i, i, , ])  Psplit([N, i, j, , ]), if α  ǫ  dft(αN); Psplit([αN, i, j, f1, f2])  (22)  k(i  k  f1  k  j) P([α, i, k, , ])  P([N, k, j, f1, f2])  δ(f1  j)  Psplit([α, i, j, , ])  P([N, j, j, f1, f2])  P([α, i, i, , ])  Psplit([N, i, j, f1, f2]), if α  ǫ  dft(N); Psplit([αN, i, j, f1, f2])  (23)  k(i  k  f2  k  j) P([α, i, k, f1, f2])  P([N, k, j, , ])  Psplit([α, i, j, f1, f2])  P([N, j, j, , ])  δ(i  f2)  P([α, i, i, f1, f2])  Psplit([N, i, j, , ]), if α  ǫ  dft(α); Psplit([N, i, j, f1, f2])  (24) φ(nil, N)  Psplit([cdn(N), i, j, f1, f2])   f 1, f 2 (i  f 1  f1  f2  f 2  j  (f 1, f 2)  (i, j)  (f 1, f 2)  (f1, f2)) P([cdn(N), f  1, f  2, f1, f2])   tA φ(t, N)  P([t, i, j, f  1, f  2])  Psplit([cdn(N), i, j, f1, f2])   tA φ(t, N)  P([t, i, j, i, j]), if N  V  dft(N); Psplit([N, i, j, , ])  (25) φ(nil, N)  Psplit([cdn(N), i, j, , ])   f 1, f 2 (i  f 1  f 2  j  (f 1, f 2)  (i, j)  (f 1  f 2  i  f 1  f 2  j)) P([cdn(N), f  1, f  2, , ])   tA φ(t, N)  P([t, i, j, f  1, f  2])  Psplit([cdn(N), i, j, , ])   tA φ(t, N)  P([t, i, j, i, j]), if N  V  dft(N); Psplit([a, i, j, , ])  δ(i  1  j  aj  a); (26) Psplit([, i, j, f1, f2])  0; (27) Psplit([ǫ, i, j, , ])  0. (28) We can now separate those branches of re- cursion that terminate on the given input from the cases of endless recursion. We assume be- low that Psplit([Rt, i, j, f  1, f  2])  0. Even if this is not always valid, for the purpose of deriving the equations below, this assumption does not lead to invalid results. We deﬁne a new function Pouter, which accounts for probabilities of sub- derivations that do not derive any words in the preﬁx, but contribute structurally to its deriva- tion: Pouter([t, i, j, f1, f2], [t, f  1, f  2])  (29) Plow([t, i, j, f1, f2], [t, f  1, f  2]) Psplit([Rt, i, j, f  1, f  2]) ; Pouter([α, i, j, f1, f2], [t, f  1, f  2])  (30) Plow([α, i, j, f1, f2], [t, f  1, f  2]) Psplit([Rt, i, j, f  1, f  2]) . We can now eliminate the inﬁnite recur- sion that arises in (10) and (11) by rewriting P([t, i, j, f1, f2]) in terms of Pouter: P([t, i, j, f1, f2])  (31)  t  A, f 1, f 2((i, j, f 1, f 2)  (i, j, f1, f2)) Pouter([t, i, j, f1, f2], [t, f  1, f  2])  Psplit([Rt, i, j, f  1, f  2]); P([t, i, j, , ])  (32)  t  {t}  A, f  {, i, j} Pouter([t, i, j, , ], [t, f, f])  Psplit([Rt, i, j, f, f]). Equations for Pouter will be derived in the next subsection. In summary, terminating computation of pre- ﬁx probabilities should be based on equa- tions (31) and (32), which replace (1), along with equations (2) to (9) and all the equations for Psplit. 4.3 Oﬀ-line Equations In this section we derive equations for function Pouter introduced in 4.2 and deal with all re- maining cases of equations that cause inﬁnite recursion. In some cases, function P can be computed independently of the actual input. For any i  n we can consistently deﬁne the following quantities, where t  I  A and α  V  or cdn(N)  αβ for some N and β: Ht  P([t, i, i, f, f]); Hα  P([α, i, i, f , f ]), where f  i if t  A, f   otherwise, and f   i if dft(α), f   otherwise. Thus, Ht is the probability of all derived trees obtained from t, with no lexical node at their yields. Quantities Ht and Hα can be computed by means of a sys- tem of equations which can be directly obtained from equations (1) to (9). Similar quantities as above must be introduced for the case i  n. For instance, we can set H t  P([t, n, n, f, f]), f speciﬁed as above, which gives the probabil- ity of all derived trees obtained from t (with no restriction at their yields). Function Pouter is also independent of the actual input. Let us focus here on the case f1, f2  {i, j, } (this enforces (f1, f2)  (f  1, f  2) below). For any i, j, f1, f2  n, we can consis- tently deﬁne the following quantities. Lt,t  Pouter([t, i, j, f1, f2], [t, f  1, f  2]); Lα,t  Pouter([α, i, j, f1, f2], [t, f  1, f  2]). In the case at hand, Lt,t is the probability of all derived trees obtained from t such that (i) no lexical node is found at their yields; and (ii) at some unﬁnished node dominating the foot of t, the probability of the adjunction of t has al- ready been accounted for, but t itself has not been adjoined. It is straightforward to establish a system of equations for the computation of Lt,t and Lα,t, by rewriting equations (12) to (20) according to (29) and (30). For instance, combining (12) and (29) gives (using the above assumptions on f1 and f2): Lt,t  LRt,t  δ(t  t). Also, if α  ǫ and dft(N), combining (14) and (30) gives (again, using previous assump- tions on f1 and f2; note that the Hαs are known terms here): LαN,t  Hα  LN,t. For any i, f1, f2  n and j  n, we also need to deﬁne: L t,t  Pouter([t, i, n, f1, f2], [t, f  1, f  2]); L α,t  Pouter([α, i, n, f1, f2], [t, f  1, f  2]). Here L t,t is the probability of all derived trees obtained from t with a node dominating the foot node of t, that is an adjunction site for t and is unﬁnished in the same sense as above, and with lexical nodes only in the portion of the tree to the right of that node. When we drop our assumption on f1 and f2, we must (pre)compute in addition terms of the form Pouter([t, i, j, i, i], [t, i, i]) and Pouter([t, i, j, i, i], [t, j, j]) for i  j  n, Pouter([t, i, n, f1, n], [t, f  1, f  2]) for i  f1  n, Pouter([t, i, n, n, n], [t, f  1, f  2]) for i  n, and similar. Again, these are independent of the choice of i, j and f1. Full treatment is omitted due to length restrictions. 5 Complexity and concluding remarks We have presented a method for the computa- tion of the preﬁx probability when the underly- ing model is a Tree Adjoining Grammar. Func- tion Psplit is the core of the method. Its equa- tions can be directly translated into an eﬀective algorithm, using standard functional memoiza- tion or other tabular techniques. It is easy to see that such an algorithm can be made to run in time O(n6), where n is the length of the input preﬁx. All the quantities introduced in 4.3 (Ht, Lt,t, etc.) are independent of the input and should be computed oﬀ-line, using the system of equations that can be derived as indicated. For quantities Ht we have a non-linear system, since equations (2) to (6) contain quadratic terms. Solutions can then be approximated to any de- gree of precision using standard iterative meth- ods, as for instance those exploited in (Stolcke, 1995). Under the hypothesis that the grammar is consistent, that is Pr(L(G))  1, all quanti- ties H t and H α evaluate to one. For quantities Lt,t and the like, 4.3 provides linear systems whose solutions can easily be obtained using standard methods. Note also that quantities Lα,t are only used in the oﬀ-line computation of quantities Lt,t, they do not need to be stored for the computation of preﬁx probabilities (com- pare equations for Lt,t with (31) and (32)). We can easily develop implementations of our method that can compute preﬁx probabilities incrementally. That is, after we have computed the preﬁx probability for a preﬁx a1    an, on in- put an1 we can extend the calculation to preﬁx a1    anan1 without having to recompute all intermediate steps that do not depend on an1. This step takes time O(n5). In this paper we have assumed that the pa- rameters of the stochastic TAG have been pre- viously estimated. In practice, smoothing to avoid sparse data problems plays an important role. Smoothing can be handled for preﬁx prob- ability computation in the following ways. Dis- counting methods for smoothing simply pro- duce a modiﬁed STAG model which is then treated as input to the preﬁx probability com- putation. Smoothing using methods such as deleted interpolation which combine class-based models with word-based models to avoid sparse data problems have to be handled by a cognate interpolation of preﬁx probability models. References C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu- danpur, L. Mangu, H. Printz, E. Ristad, A. Stolcke, R. Rosenfeld, and D. Wu. 1997. Structure and per- formance of a dependency language model. In Proc. of Eurospeech 97, volume 5, pages 27752778. F. Jelinek and J. Laﬀerty. 1991. Computation of the probability of initial substring generation by stochas- tic context-free grammars. Computational Linguis- tics, 17(3):315323. A. K. Joshi and Y. Schabes. 1992. Tree-adjoining gram- mars and lexicalized grammars. In M. Nivat and A. Podelski, editors, Tree automata and languages, pages 409431. Elsevier Science. A. K. Joshi. 1988. An introduction to tree adjoining grammars. In A. Manaster-Ramer, editor, Mathemat- ics of Language. John Benjamins, Amsterdam. B. Lang. 1988. Parsing incomplete sentences. In Proc. of the 12th International Conference on Computational Linguistics, volume 1, pages 365371, Budapest. O. Rambow and A. Joshi. 1995. A formal look at de- pendency grammars and phrase-structure grammars, with special consideration of word-order phenomena. In Leo Wanner, editor, Current Issues in Meaning- Text Theory. Pinter, London. Y. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proc. of COLING 92, volume 2, pages 426432, Nantes, France. B. Srinivas. 1996. Almost Parsing technique for lan- guage modeling. In Proc. ICSLP 96, volume 3, pages 11731176, Philadelphia, PA, Oct 3-6. A. Stolcke. 1995. An eﬃcient probabilistic context-free parsing algorithm that computes preﬁx probabilities. Computational Linguistics, 21(2):165201. J. H. Wright and E. N. Wrigley. 1989. Probabilistic LR parsing for speech recognition. In IWPT 89, pages 105114.",
  "5.pdf": "arXiv:cs9809027v1 [cs.CL] 18 Sep 1998 Conditions on Consistency of Probabilistic Tree Adjoining Grammars Anoop Sarkar Dept. of Computer and Information Science University of Pennsylvania 200 South 33rd Street, Philadelphia, PA 19104-6389 USA anooplinc.cis.upenn.edu Abstract Much of the power of probabilistic methods in modelling language comes from their ability to compare several derivations for the same string in the language. An important starting point for the study of such cross-derivational proper- ties is the notion of consistency. The probabil- ity model deﬁned by a probabilistic grammar is said to be consistent if the probabilities assigned to all the strings in the language sum to one. From the literature on probabilistic context-free grammars (CFGs), we know precisely the con- ditions which ensure that consistency is true for a given CFG. This paper derives the conditions under which a given probabilistic Tree Adjoin- ing Grammar (TAG) can be shown to be con- sistent. It gives a simple algorithm for checking consistency and gives the formal justiﬁcation for its correctness. The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deﬁciency (i.e. whether any probability mass is assigned to strings that cannot be generated). 1 Introduction Much of the power of probabilistic methods in modelling language comes from their abil- ity to compare several derivations for the same string in the language. This cross-derivational power arises naturally from comparison of vari- ous derivational paths, each of which is a prod- uct of the probabilities associated with each step in each derivation. A common approach used to assign structure to language is to use a prob- abilistic grammar where each elementary rule  This research was partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94-G-0426. The author would like to thank Aravind Joshi, Jeﬀ Rey- nar, Giorgio Satta, B. Srinivas, Fei Xia and the two anonymous reviewers for their valuable comments. or production is associated with a probability. Using such a grammar, a probability for each string in the language is computed. Assum- ing that the probability of each derivation of a sentence is well-deﬁned, the probability of each string in the language is simply the sum of the probabilities of all derivations of the string. In general, for a probabilistic grammar G the lan- guage of G is denoted by L(G). Then if a string v is in the language L(G) the probabilistic gram- mar assigns v some non-zero probability. There are several cross-derivational proper- ties that can be studied for a given probabilis- tic grammar formalism. An important starting point for such studies is the notion of consis- tency. The probability model deﬁned by a prob- abilistic grammar is said to be consistent if the probabilities assigned to all the strings in the language sum to 1. That is, if Pr deﬁned by a probabilistic grammar, assigns a probability to each string v  Σ, where Pr(v)  0 if v  L(G), then  vL(G) Pr(v)  1 (1) From the literature on probabilistic context- free grammars (CFGs) we know precisely the conditions which ensure that (1) is true for a given CFG. This paper derives the conditions under which a given probabilistic TAG can be shown to be consistent. TAGs are important in the modelling of nat- ural language since they can be easily lexical- ized; moreover the trees associated with words can be used to encode argument and adjunct re- lations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Sch- abes, 1992) are good introductions to the for- malism and its linguistic relevance. TAGs have been shown to have relations with both phrase- structure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies. Consistency of probabilistic TAGs has prac- tical signiﬁcance for the following reasons:  The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deﬁciency.  Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992). EM based algorithms begin with an initial (usually random) value for each pa- rameter. If the initial assignment causes the grammar to be inconsistent, then it- erative re-estimation might converge to an inconsistent grammar1.  Techniques used in this paper can be used to determine consistency for other proba- bility models based on TAGs (Carroll and Weir, 1997). 2 Notation In this section we establish some notational con- ventions and deﬁnitions that we use in this pa- per. Those familiar with the TAG formalism only need to give a cursory glance through this section. A probabilistic TAG is represented by (N, Σ, I, A, S, φ) where N, Σ are, respectively, non-terminal and terminal symbols. I  A is a set of trees termed as elementary trees. We take V to be the set of all nodes in all the elementary trees. For each leaf A  V , label(A) is an ele- ment from Σ  {ǫ}, and for each other node A, label(A) is an element from N. S is an element from N which is a distinguished start symbol. The root node A of every initial tree which can start a derivation must have label(A)  S. I are termed initial trees and A are auxil- iary trees which can rewrite a tree node A  V . This rewrite step is called adjunction. φ is a function which assigns each adjunction with a probability and denotes the set of parameters 1 Note that for CFGs it has been shown in (Chaud- hari et al., 1983; Sanchez and Benedi, 1997) that inside- outside reestimation can be used to avoid inconsistency. We will show later in the paper that the method used to show consistency in this paper precludes a straightfor- ward extension of that result for TAGs. in the model. In practice, TAGs also allow a leaf nodes A such that label(A) is an element from N. Such nodes A are rewritten with ini- tial trees from I using the rewrite step called substitution. Except in one special case, we will not need to treat substitution as being dis- tinct from adjunction. For t  I  A, A(t) are the nodes in tree t that can be modiﬁed by adjunction. For label(A)  N we denote Adj(label(A)) as the set of trees that can adjoin at node A  V . The adjunction of t into N  V is denoted by N  t. No adjunction at N  V is denoted by N  nil. We assume the following proper- ties hold for every probabilistic TAG G that we consider: 1. G is lexicalized. There is at least one leaf node a that lexicalizes each elementary tree, i.e. a  Σ. 2. G is proper. For each N  V , φ(N  nil)   t φ(N  t)  1 3. Adjunction is prohibited on the foot node of every auxiliary tree. This condition is imposed to avoid unnecessary ambiguity and can be easily relaxed. 4. There is a distinguished non-lexicalized ini- tial tree τ such that each initial tree rooted by a node A with label(A)  S substitutes into τ to complete the derivation. This en- sures that probabilities assigned to the in- put string at the start of the derivation are well-formed. We use symbols S, A, B, . . . to range over V , symbols a, b, c, . . . to range over Σ. We use t1, t2, . . . to range over I  A and ǫ to denote the empty string. We use Xi to range over all i nodes in the grammar. 3 Applying probability measures to Tree Adjoining Languages To gain some intuition about probability assign- ments to languages, let us take for example, a language well known to be a tree adjoining lan- guage: L(G)  {anbncndnn  1} It seems that we should be able to use a func- tion ψ to assign any probability distribution to the strings in L(G) and then expect that we can assign appropriate probabilites to the adjunc- tions in G such that the language generated by G has the same distribution as that given by ψ. However a function ψ that grows smaller by repeated multiplication as the inverse of an exponential function cannot be matched by any TAG because of the constant growth property of TAGs (see (Vijay-Shanker, 1987), p. 104). An example of such a function ψ is a simple Pois- son distribution (2), which in fact was also used as the counterexample in (Booth and Thomp- son, 1973) for CFGs, since CFGs also have the constant growth property. ψ(anbncndn)  1 e  n! (2) This shows that probabilistic TAGs, like CFGs, are constrained in the probabilistic languages that they can recognize or learn. As shown above, a probabilistic language can fail to have a generating probabilistic TAG. The reverse is also true: some probabilis- tic TAGs, like some CFGs, fail to have a corresponding probabilistic language, i.e. they are not consistent. There are two reasons why a probabilistic TAG could be inconsistent: dirty grammars, and destructive or incorrect probability assignments. Dirty grammars. Usually, when applied to language, TAGs are lexicalized and so prob- abilities assigned to trees are used only when the words anchoring the trees are used in a derivation. However, if the TAG allows non- lexicalized trees, or more precisely, auxiliary trees with no yield, then looping adjunctions which never generate a string are possible. How- ever, this can be detected and corrected by a simple search over the grammar. Even in lexi- calized grammars, there could be some auxiliary trees that are assigned some probability mass but which can never adjoin into another tree. Such auxiliary trees are termed unreachable and techniques similar to the ones used in detecting unreachable productions in CFGs can be used here to detect and eliminate such trees. Destructive probability assignments. This problem is a more serious one, and is the main subject of this paper. Consider the prob- abilistic TAG shown in (3)2. PSfrag replacements t1 t2 ǫ S1 S2 S3 S φ(S1  t2)  1.0 φ(S2  t2)  0.99 φ(S2  nil)  0.01 φ(S3  t2)  0.98 φ(S3  nil)  0.02 a (3) Consider a derivation in this TAG as a genera- tive process. It proceeds as follows: node S1 in t1 is rewritten as t2 with probability 1.0. Node S2 in t2 is 99 times more likely than not to be rewritten as t2 itself, and similarly node S3 is 49 times more likely than not to be rewritten as t2. This however, creates two more instances of S2 and S3 with same probabilities. This continues, creating multiple instances of t2 at each level of the derivation process with each instance of t2 creating two more instances of itself. The gram- mar itself is not malicious; the probability as- signments are to blame. It is important to note that inconsistency is a problem even though for any given string there are only a ﬁnite number of derivations, all halting. Consider the prob- ability mass function (pmf) over the set of all derivations for this grammar. An inconsistent grammar would have a pmf which assigns a large portion of probability mass to derivations that are non-terminating. This means there is a ﬁ- nite probability the generative process can enter a generation sequence which has a ﬁnite proba- bility of non-termination. 4 Conditions for Consistency A probabilistic TAG G is consistent if and only if:  vL(G) Pr(v)  1 (4) where Pr(v) is the probability assigned to a string in the language. If a grammar G does not satisfy this condition, G is said to be incon- sistent. To explain the conditions under which a prob- abilistic TAG is consistent we will use the TAG 2 The subscripts are used as a simple notation to uniquely refer to the nodes in each elementary tree. They are not part of the node label for purposes of adjunction. in (5) as an example. A1 a1 B2 B a3 A2 B1 A3 a2 A ag replacements t1 t2 t3 φ(A1  t2)  0.8 φ(A1  nil)  0.2 φ(A2  t2)  0.2 φ(A2  nil)  0.8 φ(B1  t3)  0.2 φ(B1  nil)  0.8 φ(A3  t2)  0.4 φ(A3  nil)  0.6 φ(B2  t3)  0.1 φ(B2  nil)  0.9 (5) From this grammar, we compute a square ma- trix M which of size V , where V is the set of nodes in the grammar that can be rewrit- ten by adjunction. Each Mij contains the ex- pected value of obtaining node Xj when node Xi is rewritten by adjunction at each level of a TAG derivation. We call M the stochastic ex- pectation matrix associated with a probabilistic TAG. To get M for a grammar we ﬁrst write a ma- trix P which has V  rows and I  A columns. An element Pij corresponds to the probability of adjoining tree tj at node Xi, i.e. φ(Xi  tj)3. P  A1 A2 B1 A3 B2 t1 t2 t3   0 0.8 0 0 0.2 0 0 0 0.2 0 0.4 0 0 0 0.1   We then write a matrix N which has I  A rows and V  columns. An element Nij is 1.0 if node Xj is a node in tree ti. N  t1 t2 t3 A1 A2 B1 A3 B2   1.0 0 0 0 0 0 1.0 1.0 1.0 0 0 0 0 0 1.0   Then the stochastic expectation matrix M is simply the product of these two matrices. 3 Note that P is not a row stochastic matrix. This is an important diﬀerence in the construction of M for TAGs when compared to CFGs. We will return to this point in 5. M  P  N  A1 A2 B1 A3 B2 A1 A2 B1 A3 B2   0 0.8 0.8 0.8 0 0 0.2 0.2 0.2 0 0 0 0 0 0.2 0 0.4 0.4 0.4 0 0 0 0 0 0.1   By inspecting the values of M in terms of the grammar probabilities indicates that Mij con- tains the values we wanted, i.e. expectation of obtaining node Aj when node Ai is rewritten by adjunction at each level of the TAG derivation process. By construction we have ensured that the following theorem from (Booth and Thomp- son, 1973) applies to probabilistic TAGs. A formal justiﬁcation for this claim is given in the next section by showing a reduction of the TAG derivation process to a multitype Galton- Watson branching process (Harris, 1963). Theorem 4.1 A probabilistic grammar is con- sistent if the spectral radius ρ(M)  1, where M is the stochastic expectation matrix com- puted from the grammar. (Booth and Thomp- son, 1973; Soule, 1974) This theorem provides a way to determine whether a grammar is consistent. All we need to do is compute the spectral radius of the square matrix M which is equal to the modulus of the largest eigenvalue of M. If this value is less than one then the grammar is consistent4. Comput- ing consistency can bypass the computation of the eigenvalues for M by using the following theorem by Gerˇsgorin (see (Horn and Johnson, 1985; Wetherell, 1980)). Theorem 4.2 For any square matrix M, ρ(M)  1 if and only if there is an n  1 such that the sum of the absolute values of the elements of each row of Mn is less than one. Moreover, any n  n also has this prop- erty. (Gerˇsgorin, see (Horn and Johnson, 1985; Wetherell, 1980)) 4 The grammar may be consistent when the spectral radius is exactly one, but this case involves many special considerations and is not considered in this paper. In practice, these complicated tests are probably not worth the eﬀort. See (Harris, 1963) for details on how this special case can be solved. This makes for a very simple algorithm to check consistency of a grammar. We sum the values of the elements of each row of the stochas- tic expectation matrix M computed from the grammar. If any of the row sums are greater than one then we compute M2, repeat the test and compute M22 if the test fails, and so on un- til the test succeeds5. The algorithm does not halt if ρ(M)  1. In practice, such an algorithm works better in the average case since compu- tation of eigenvalues is more expensive for very large matrices. An upper bound can be set on the number of iterations in this algorithm. Once the bound is passed, the exact eigenvalues can be computed. For the grammar in (5) we computed the fol- lowing stochastic expectation matrix: M    0 0.8 0.8 0.8 0 0 0.2 0.2 0.2 0 0 0 0 0 0.2 0 0.4 0.4 0.4 0 0 0 0 0 0.1   The ﬁrst row sum is 2.4. Since the sum of each row must be less than one, we compute the power matrix M2. However, the sum of one of the rows is still greater than 1. Continuing we compute M22. M22    0 0.1728 0.1728 0.1728 0.0688 0 0.0432 0.0432 0.0432 0.0172 0 0 0 0 0.0002 0 0.0864 0.0864 0.0864 0.0344 0 0 0 0 0.0001   This time all the row sums are less than one, hence ρ(M)  1. So we can say that the gram- mar deﬁned in (5) is consistent. We can conﬁrm this by computing the eigenvalues for M which are 0, 0, 0.6, 0 and 0.1, all less than 1. Now consider the grammar (3) we had con- sidered in Section 3. The value of M for that grammar is computed to be: M(3)  S1 S2 S3 S1 S2 S3   0 1.0 1.0 0 0.99 0.99 0 0.98 0.98   5 We compute M22 and subsequently only successive powers of 2 because Theorem 4.2 holds for any n  n. This permits us to use a single matrix at each step in the algorithm. The eigenvalues for the expectation matrix M computed for the grammar (3) are 0, 1.97 and 0. The largest eigenvalue is greater than 1 and this conﬁrms (3) to be an inconsistent grammar. 5 TAG Derivations and Branching Processes To show that Theorem 4.1 in Section 4 holds for any probabilistic TAG, it is suﬃcient to show that the derivation process in TAGs is a Galton- Watson branching process. A Galton-Watson branching process (Harris, 1963) is simply a model of processes that have objects that can produce additional objects of the same kind, i.e. recursive processes, with cer- tain properties. There is an initial set of ob- jects in the 0-th generation which produces with some probability a ﬁrst generation which in turn with some probability generates a second, and so on. We will denote by vectors Z0, Z1, Z2, . . . the 0-th, ﬁrst, second, . . . generations. There are two assumptions made about Z0, Z1, Z2, . . .: 1. The size of the n-th generation does not inﬂuence the probability with which any of the objects in the (n  1)-th generation is produced. In other words, Z0, Z1, Z2, . . . form a Markov chain. 2. The number of objects born to a parent object does not depend on how many other objects are present at the same level. We can associate a generating function for each level Zi. The value for the vector Zn is the value assigned by the n-th iterate of this gen- erating function. The expectation matrix M is deﬁned using this generating function. The theorem attributed to Galton and Wat- son speciﬁes the conditions for the probability of extinction of a family starting from its 0-th generation, assuming the branching process rep- resents a family tree (i.e, respecting the condi- tions outlined above). The theorem states that ρ(M)  1 when the probability of extinction is 1.0. t1 t2 (0) t2 (0) t2 (1.1) t3 (1) t3 (0) t2 (1.1) PSfrag replacements level 0 level 1 level 2 level 3 level 4 (6) A2 B1 A2 B1 A3 a2 A a2 A B2 B B A2 B1 A3 a2 A a2 a3 a3 A a1 (7) The assumptions made about the generating process intuitively holds for probabilistic TAGs. (6), for example, depicts a derivation of the string a2a2a2a2a3a3a1 by a sequence of adjunc- tions in the grammar given in (5)6. The parse tree derived from such a sequence is shown in Fig. 7. In the derivation tree (6), nodes in the trees at each level i are rewritten by adjunction to produce a level i  1. There is a ﬁnal level 4 in (6) since we also consider the probability that a node is not rewritten further, i.e. Pr(A  nil) for each node A. We give a precise statement of a TAG deriva- tion process by deﬁning a generating function for the levels in a derivation tree. Each level i in the TAG derivation tree then corresponds to Zi in the Markov chain of branching pro- 6 The numbers in parentheses next to the tree names are node addresses where each tree has adjoined into its parent. Recall the deﬁnition of node addresses in Section 2. cesses. This is suﬃcient to justify the use of Theorem 4.1 in Section 4. The conditions on the probability of extinction then relates to the probability that TAG derivations for a proba- bilistic TAG will not recurse inﬁnitely. Hence the probability of extinction is the same as the probability that a probabilistic TAG is consis- tent. For each Xj  V , where V is the set of nodes in the grammar where adjunction can occur, we deﬁne the k-argument adjunction generating function over variables s1, . . . , sk corresponding to the k nodes in V . gj(s1, . . . , sk)   tAdj(Xj){nil} φ(Xj  t)  sr1(t) 1    srk(t) k where, rj(t)  1 iﬀ node Xj is in tree t, rj(t)  0 otherwise. For example, for the grammar in (5) we get the following adjunction generating functions taking the variable s1, s2, s3, s4, s5 to represent the nodes A1, A2, B1, A3, B2 respectively. g1(s1, . . . , s5)  φ(A1  t2)  s2  s3  s4  φ(A1  nil) g2(s1, . . . , s5)  φ(A2  t2)  s2  s3  s4  φ(A2  nil) g3(s1, . . . , s5)  φ(B1  t3)  s5  φ(B1  nil) g4(s1, . . . , s5)  φ(A3  t2)  s2  s3  s4  φ(A3  nil) g5(s1, . . . , s5)  φ(B2  t3)  s5  φ(B2  nil) The n-th level generating function Gn(s1, . . . , sk) is deﬁned recursively as fol- lows. G0(s1, . . . , sk)  s1 G1(s1, . . . , sk)  g1(s1, . . . , sk) Gn(s1, . . . , sk)  Gn1[g1(s1, . . . , sk), . . . , gk(s1, . . . , sk)] For the grammar in (5) we get the following level generating functions. G0(s1, . . . , s5)  s1 G1(s1, . . . , s5)  g1(s1, . . . , s5)  φ(A1  t2)  s2  s3  s4  φ(A1  nil)  0.8  s2  s3  s4  0.2 G2(s1, . . . , s5)  φ(A2  t2)[g2(s1, . . . , s5)][g3(s1, . . . , s5)] [g4(s1, . . . , s5)]  φ(A2  nil)  0.08s2 2s2 3s2 4s5  0.03s2 2s2 3s2 4  0.04s2s3s4s5  0.18s2s3s4  0.04s5  0.196 . . . Examining this example, we can express Gi(s1, . . . , sk) as a sum Di(s1, . . . , sk)  Ci, where Ci is a constant and Di() is a polyno- mial with no constant terms. A probabilistic TAG will be consistent if these recursive equa- tions terminate, i.e. iﬀ limiDi(s1, . . . , sk)  0 We can rewrite the level generation functions in terms of the stochastic expectation matrix M, where each element mi,j of M is computed as follows (cf. (Booth and Thompson, 1973)). mi,j  gi(s1, . . . , sk) sj  s1,...,sk1 (8) The limit condition above translates to the con- dition that the spectral radius of M must be less than 1 for the grammar to be consistent. This shows that Theorem 4.1 used in Sec- tion 4 to give an algorithm to detect inconsis- tency in a probabilistic holds for any given TAG, hence demonstrating the correctness of the al- gorithm. Note that the formulation of the adjunction generating function means that the values for φ(X  nil) for all X  V do not appear in the expectation matrix. This is a crucial diﬀer- ence between the test for consistency in TAGs as compared to CFGs. For CFGs, the expecta- tion matrix for a grammar G can be interpreted as the contribution of each non-terminal to the derivations for a sample set of strings drawn from L(G). Using this it was shown in (Chaud- hari et al., 1983) and (Sanchez and Benedi, 1997) that a single step of the inside-outside algorithm implies consistency for a probabilis- tic CFG. However, in the TAG case, the inclu- sion of values for φ(X  nil) (which is essen- tial if we are to interpret the expectation ma- trix in terms of derivations over a sample set of strings) means that we cannot use the method used in (8) to compute the expectation matrix and furthermore the limit condition will not be convergent. 6 Conclusion We have shown in this paper the conditions under which a given probabilistic TAG can be shown to be consistent. We gave a simple al- gorithm for checking consistency and gave the formal justiﬁcation for its correctness. The re- sult is practically signiﬁcant for its applications in checking for deﬁciency in probabilistic TAGs. References T. L. Booth and R. A. Thompson. 1973. Applying prob- ability measures to abstract languages. IEEE Trans- actions on Computers, C-22(5):442450, May. J. Carroll and D. Weir. 1997. Encoding frequency in- formation in lexicalized grammars. In Proc. 5th Intl Workshop on Parsing Technologies IWPT-97, Cam- bridge, Mass. R. Chaudhari, S. Pham, and O. N. Garcia. 1983. Solu- tion of an open problem on probabilistic grammars. IEEE Transactions on Computers, C-32(8):748750, August. T. E. Harris. 1963. The Theory of Branching Processes. Springer-Verlag, Berlin. R. A. Horn and C. R. Johnson. 1985. Matrix Analysis. Cambridge University Press, Cambridge. A. K. Joshi and Y. Schabes. 1992. Tree-adjoining gram- mar and lexicalized grammars. In M. Nivat and A. Podelski, editors, Tree automata and languages, pages 409431. Elsevier Science. A. K. Joshi. 1988. An introduction to tree adjoining grammars. In A. Manaster-Ramer, editor, Mathemat- ics of Language. John Benjamins, Amsterdam. O. Rambow and A. Joshi. 1995. A formal look at de- pendency grammars and phrase-structure grammars, with special consideration of word-order phenomena. In Leo Wanner, editor, Current Issues in Meaning- Text Theory. Pinter, London. J.-A. Sanchez and J.-M. Benedi. 1997. Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations. IEEE Transactions on Pattern Analysis and Machine Intel- ligence, 19(9):10521055, September. Y. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proc. of COLING 92, volume 2, pages 426432, Nantes, France. S. Soule. 1974. Entropies of probabilistic grammars. Inf. Control, 25:5574. K. Vijay-Shanker. 1987. A Study of Tree Adjoining Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania. C. S. Wetherell. 1980. Probabilistic languages: A re- view and some open questions. Computing Surveys, 12(4):361379.",
  "6.pdf": "arXiv:cs9809028v1 [cs.CL] 18 Sep 1998 Separating Dependency from Constituency in a Tree Rewriting System Anoop Sarkar Department of Computer and Information Science University of Pennsylvania 200 South 33rd St, Philadelphia PA 19104 anooplinc.cis.upenn.edu 1 Introduction We deﬁne a new grammar formalism called Link- Sharing Tree Adjoining Grammar (LSTAG) which arises directly out of a concern for distinguishing the notion of constituency from the notion of relating lex- ical items in terms of linguistic dependency1(Melˆcuk, 1988; Rambow and Joshi, 1992). This work de- rives directly from work on Tree Adjoining Gram- mars (TAG) (Joshi, Levy, and Takahashi, 1975) where these two notions are conﬂated. The set of derived trees for a TAG correspond to the traditional notions of constituency while the derivation trees of a TAG are closely related to dependency structure (Rambow and Joshi, 1992). A salient feature of TAG is the ex- tended domain of locality it provides for stating these dependencies. Each elementary tree can be associ- ated with a lexical item giving us a lexicalized TAG (LTAG)(Joshi and Schabes, 1991). Properties related to the lexical item such as subcategorization, agree- ment, and certain types of word-order variation can be expressed directly in the elementary tree (Kroch, 1987; Frank, 1992). Thus, in an LTAG all of these linguistic dependencies are expressed locally in the elementary trees of the grammar. This means that Thanks to Christy Doran, Aravind Joshi, Nobo Komagata, Owen Rambow, and B. Srinivas for their helpful comments and discussion. 1 The term dependency is used here broadly to include for- mal relationships such as case and agreement and other rela- tionships such as ﬁller-gap. the predicate and its arguments are always topologi- cally situated in the same elementary tree. However, in coordination of predicates, e.g. (1), the dependencies between predicate and argument cannot be represented in a TAG elementary tree di- rectly, since several elementary trees seem to be shar- ing their arguments. (1) a. Kiki frolics, sings and plays all day. b. Kiki likes and Bill thinks Janet likes soccer. The idea behind LSTAG is that the non-local na- ture of coordination as in (1) (for TAG-like gram- mar formalisms) can be captured by introducing a restricted degree of synchronized parallelism into the TAG rewriting system while retaining the existing independent parallelism2(Engelfriet, Rozenberg, and Slutzki, 1980; Rambow and Satta, to appear). We believe that an approach towards coordination that explicitly distinguishes the dependencies from the constituency gives a better formal understanding of its representation when compared to previous ap- proaches that use tree-rewriting systems which con- ﬂate the two issues, as in (Joshi, 1990; Joshi and Schabes, 1991; Sarkar and Joshi, 1996) which have 2It is important to note that while the adjunction opera- tion in TAGs is context-free, synchronized parallelism could be attributed to the TAG formalism due to the string wrap- ping capabilities of adjunction, since synchronized parallelism is concerned with how strings are derived in a rewriting sys- tem. We note this as a conjecture but will not attempt to prove it here. 1 to represent sentences such as (1) with either un- rooted trees or by performing structure merging on the derived tree. Other formalisms for coordination have similar motivations: however their approaches diﬀer, e.g. CCG (Steedman, 1985; Steedman, 1997b) extends the notion of constituency, while generative syntacticians (Moltmann, 1992; Muadz, 1991) work with three-dimensional syntactic trees. 2 Synchronized Parallelism The terms synchronized parallelism and independent parallelism arise from work done on a family of formalisms termed parallel rewriting systems that extend context-free grammars (CFG) by the addi- tion of various restrictive devices (see (Engelfriet, Rozenberg, and Slutzki, 1980))). Synchronized par- allelism allows derivations which include substrings which have been generated by a common (or shared) underlying derivation process3. Independent paral- lelism corresponds to the instantiations of indepen- dent derivation processes which are then combined to give the entire derivation of a string4. What we are exploring in this paper is an example of a mixed system with both independent and synchronous par- allelism. In (Rambow and Satta, to appear) it is shown that by allowing an unbounded degree of synchro- nized parallelism we get systems that are too uncon- strained. However, interesting subfamilies arise when the synchronous parallelism is bounded to a ﬁnite de- gree, i.e. only a bounded number of subderivations can be synchronized in a given grammar. The system we deﬁne has this property. 3 LSTAG We ﬁrst look at the formalism of Synchronous TAG (STAG)(Shieber and Schabes, 1990) since it is an ex- 3 The Lindenmayer systems are examples of systems with only synchronous parallelism and it is interesting to note that these L systems have the anti-AFL property (where none of the standard closures apply). 4 CFG is a formalism that only has independent parallelism. ample of a tree-rewriting system that has synchro- nized parallelism. As a preliminary we ﬁrst informally deﬁne Tree Adjoining Grammars (TAG). For example, Figure 1 shows an example of a tree for a transitive verb cooked. Each node in the tree has a unique address obtained by applying a Gorn tree addressing scheme. For instance, the object NP has address 2.2. In the TAG formalism, trees can be composed using the two operations of substitution (corresponds to string concatenation) and adjunction (corresponds to string wrapping). A history of these operations on elemen- tary trees in the form of a derivation tree can be used to reconstruct the derivation of a string recognized by a TAG. Figure 2 shows an example of a deriva- tion tree and the corresponding parse tree for the de- rived structure obtained when α(John) and α(beans) substitute into α(cooked) and β(dried) adjoins into α(beans) giving us a derivation tree for John cooked dried beans. Trees that adjoin are termed as auxiliary trees, trees that are not auxiliary are called initial. Each node in the derivation tree is the name of an elementary tree. The labels on the edges denote the address in the parent node where a substitution or adjunction has occured. Deﬁnition 1 In a TAG G  {γ  γ is either an ini- tial tree or an auxiliary tree }, we will notate adjunc- tion (similarly substitution) of trees γ1 . . . γk into tree γ at addresses a1 . . . ak giving a derived tree γ as γ  γ[a1, γ1] . . . [ak, γk] Deﬁnition 2 Given two standard TAGs GL and GR we deﬁne (from (Shieber, 1994)) a STAG as {γ, γ,   γ  GL, γ  GR}, where  is a set of links from a node address in γ to a node address in γ. A derivation proceeds as follows:  for γ  γL, γR, , pick a link member aL i aR, where the as are node addresses and i  . For simplicity, we refer to  as link and its elements i as link members.  adjunction (similarly substitution) of βL, βR,  into γ is given by γ L, γ R,   γL[aL, βL], γR[aR, βR],  2 α (cooked) S VP NP V NP 1 0 2.1 2.2 2 cooked α (John) NP N John α(beans) N NP beans (dried) β N ADJ dried N Figure 1: Example of a TAG S NP VP N V NP John cooked N ADJ N dried beans 1 2.2 (beans) 1 α β α(John) α (cooked) (dried) Derivation Tree Figure 2: Example of a derivation tree and corresponding parse tree where all links in  and  are included in  except i.  γ L, γ R,  is now a derived structure which can be further operated upon. In (Abeille, 1992; Abeille, 1994) STAGs have been used in handling non-local dependencies and to seper- ate syntactic attachment from semantic roles. How- ever, STAG cannot be used to seperate the dependen- cies created in (pairs of) derivation trees for coordi- nate structures from the constituency represented in these derivation trees. In this particular sense, STAG has the same shortcomings of a TAG. Also the above deﬁnition of the inheritance of links in derived struc- tures allows STAG to derive strings not generable by TAG (Shieber, 1994). We look at a modiﬁed ver- sion of STAGs which is weaker in power than STAGs as deﬁned in Defn 2. We call this formalism Link- Sharing TAG (LSTAG). Deﬁnition 3 An LSTAG G is deﬁned as a 4-tuple GL, GR, , Φ where GL, GR are standard TAGs,  and Φ are disjoint sets of sets of links and for each pair γ  γL, γR, where γL  GL and γR  GR, δγ   is a subset of links in γ and φγR  Φ is a distinguished subset of links with the following prop- erties:  for each link   φγR, η  η, where η is a node address in γR. i.e. φγR is a set of reﬂexive links.  δR and φγR have some canonical order .  adjunction (similarly substitution) of βL, βR into γ is given by γ L, γ R  γL[aL, βL], γR[aR, βR] and for all γi  δγ, βi  φβR(1  i  n) (card(δγ)  card(βR)) δγ  φβR def  γ1  β1  . . .  γn  βn 3 where γ1γ2, . . . , γn1γn and βR1βR2, . . . , βRn1βRn  i  j is a set of links deﬁned as follows. If aLi i aRi and aRj j aRj, then i  j def  {aLi  aRi}  {aLi  aRj}  γ L, γ R is the new derived structure with new set of links δγ  φβR. Φ is used to derive synchronized parallelism in GR. The ordering  is simply used to match up the links being shared via the (non-local) sharing operation . This ordering  can be deﬁned in terms of node addresses or ﬁrst argument  second argument, i.e. ordering the arguments of the two predicates being coordinated. It is important to note that only the links in Φ are used non-locally and they are always exhausted in a single adjunction (or substitution) operation. No links from  are ever inherited unlike STAGs. Hence, non-locality is only used in a restricted fashion for the notion of sharing. 4 Linguistic Relevance To explain how the formalism works consider sen- tence (2). (2) John cooks and eats beans. Consider a LSTAG G  {γ, β, α, υ} partially shown in Fig. 3(a) and Fig. 3(b). α and υ are analogously deﬁned for John and beans respectively (see Fig. 1). In Fig. 3(a) δγ  {1, 2}5and φγR  {}, while for Fig. 3(b) δγ  {} and φγR  {1, 2}. 5 We are just using numbers 1, 2, . . . to denote the links rather than use the Gorn notation to make the trees easier to read. Here, link number 1 stands for 1  1 and 2 stands for 2.2  2.2 It is important to note that our initial motivation about seperating dependency from the constituency information is highlighted in β (see Fig. 3(b)) where the ﬁrst projection will only contribute information about constituency in a derivation tree while the sec- ond projection will contribute only dependency infor- mation in a derivation tree. We conjecture that this is true for all the structures deﬁned in an LSTAG. the kind of questions addressed in (Rambow, Vijay- Shanker, and Weir, 1995) can perhaps be answered within the framework of LSTAG6. (a)  :  S NP  VP V co oks NP  S NP  VP V co oks' NP   (b)  :  V V and V eats S S and' S NP  VP V eats NP   Figure 3: Trees γ and β from LSTAG G The derived structure after β adjoins onto γ is shown in Fig. 7(a). Fig. 5(a) shows the derived tree after the tree α (for John) substitutes into γ. Notice that due to link sharing, substitution is shared, eﬀec- tively forming a tangled derived tree7. In Figs. 7 6 In (Rambow, Vijay-Shanker, and Weir, 1995) a new for- malism called D-Tree Grammars was introduced in order to bring together the notion of derivation tree in a TAG with the notion of dependency grammar (Melˆcuk, 1988). Perhaps the kind of questions addressed in (Rambow, Vijay-Shanker, and Weir, 1995) can also be handled using the current framework. Such an application of the formalism would motivate the need for trees like γ in Fig. 3 independent of the coordination facts since they would be required to get the dependencies right. 7 While this notion of sharing bears some resemblance to the notion of joining node in the three-dimensional trees used in (Moltmann, 1992; Muadz, 1991) the rules for semantic in- terpretation of the derivations produced in a LSTAG is con- 4 and 5 the derivation trees are also given (associated with each element). The derivation structure for the second element in Fig. 5(b) is a directed acyclic derivation graph which gives us information about dependency we expect. The derivation tree of the ﬁrst element in Fig. 5(b), on the other hand, gives us information about constituency. The notion of link sharing is closely related to the schematization of the coordination rule in (Steedman, 1997b) shown below in combinatory notation. bxy  bxy bfg  λx.b(fx)(gx) bfg  λx.λy.b(fxy)(gxy)    Link sharing is used to combine the interpretation of the predicate arguments f and g (e.g. cooks, eats) of the conjunction b with the interpretation of the arguments of those predicates x, y, . . .. However, it does this within a tree-rewriting system, unlike the use of combinators in (Steedman, 1997b). 5 Restrictions Having deﬁned the formalism of LSTAG, we now de- ﬁne certain restrictions on the grammar that can be written in this formalism in order to capture correctly certain facts about coordinate structures in English. For instance, we need to prohibit elementary struc- tures like the one in Fig. 6 because they give rise to ungrammatical sentences like (3). (3) Peanuts John likes and almonds hates. (Joshi, 1990) However, such restrictions in the context of TAGs have been discussed before. (Joshi, 1990) rules out (3) by stating a requirement on the lexical string spelled out by the elementary tree. If the lexical string spelled out is not contiguous then it cannot siderably less obscure than the rules needed to interpret 3D trees; crucially because elementary structures in a TAG-like formalisms are taken to be semantically minimal without be- ing semantically void. coordinate. This requirement is stated to be a phono- logical condition and relates the notion of an intona- tional phrase (IP) to the notion of appropriate frag- ments for coordination (in the spirit of (Steedman, 1997a)). It is important to note that the notions of phrase structure for coordination and intonational phrases deﬁned in (Joshi, 1990) for TAG are not iden- tical, whereas they are identical for CCG (Steedman, 1997a). We can state an analogous restriction on the for- mation of elementary structures in a LSTAG, one that is motivated by the notion of link sharing. The left element of an elementary structure in a LSTAG cannot be composed of discontinuous parts of the right element. For example, in Fig. 6 the segment [S[NP][V P ]] from the right element has been excised in the left element. This restriction corresponds to the notion that the left element of a structure in a LSTAG represents constituency. 6 Conclusion We have presented a new tree-rewriting formal- ism called Link-Sharing Tree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs (STAG). Using LSTAG we deﬁned an approach to- wards coordination where linguistic dependency is distinguished from the notion of constituency. Ap- propriate restrictions on the nature of elementary structures in a LSTAG were also deﬁned. Such an approach towards coordination that explicitly distin- guishes dependencies from constituency gives a better formal understanding of its representation when com- pared to previous approaches that use tree-rewriting systems which conﬂate the two issues (see (Joshi and Schabes, 1991; Sarkar and Joshi, 1996)). The pre- vious approaches had to represent coordinate struc- tures either with unrooted trees or by performing structure merging on the parse tree. Moreover, the linguistic analyses presented in (Joshi and Schabes, 1991; Sarkar and Joshi, 1996) can be easily adopted in the current formalism. 5 (a)  S NP  VP V V co oks and V eats NP  S S NP  VP V co oks' NP  and' S NP  VP V eats' NP   (b)    .   0  Figure 4: Derived and derivation structures after β adjoins into γ. (a)  S NP John VP V V co oks and V eats NP  S S NP John' VP V co oks' NP  and' S VP V eats' NP   (b)      .     0   Figure 5: Substitution of α (a)  S S and S NP almonds VP V hates NP  S S and S NP almonds S NP  VP V hates NP   Figure 6: Discontiguous elementary structure 6 References [Abeille1992] Abeille, Anne. 1992. Synchronous TAGs and French Pronominal Clitics. In Proc. of COLING- 92, pages 6066, Nantes, Aug 2328. [Abeille1994] Abeille, Anne. 1994. Syntax or Seman- tics? Handling Nonlocal Dependencies with MCTAGs or Synchronous TAGs. Computational Intelligence, 10(4):471485. [Engelfriet, Rozenberg, and Slutzki1980] Engelfriet, J., G. Rozenberg, and G. Slutzki. 1980. Tree transduc- ers, L systems, and two-way machines. Journal of Computer and System Science, 43:328360. [Frank1992] Frank, Robert. 1992. Syntactic locality and Tree Adjoining Grammar: grammatical, acquisition and processing perspectives. Ph.D. thesis, University of Pennsylvania,IRCS-92-47. [Joshi and Schabes1991] Joshi, A. and Y. Schabes. 1991. Tree adjoining grammars and lexicalized grammars. In M. Nivat and A. Podelski, editors, Tree automata and languages. North-Holland. [Joshi1990] Joshi, Aravind. 1990. Phrase Structure and Intonational Phrases: Comments on the papers by Marcus and Steedman. In G. Altmann, editor, Com- putational and Cognitive Models of Speech. MIT Press. [Joshi and Schabes1991] Joshi, Aravind and Yves Sch- abes. 1991. Fixed and ﬂexible phrase structure: Coor- dination in Tree Adjoining Grammar. In Presented at the DARPA Workshop on Spoken Language Systems, Asilomar, CA. [Joshi, Levy, and Takahashi1975] Joshi, Aravind K., L. Levy, and M. Takahashi. 1975. Tree Adjunct Grammars. Journal of Computer and System Sci- ences. [Kroch1987] Kroch, A. 1987. Subjacency in a tree adjoin- ing grammar. In A. Manaster-Ramer, editor, Mathe- matics of Language. J. Benjamins Pub. Co., pages 143 172. [Melˆcuk1988] Melˆcuk, I. 1988. Dependency Syntax: The- ory and Practice. State University of New York Press, Albany. [Moltmann1992] Moltmann, Friederike. 1992. On the In- terpretation of Three-Dimensonal Syntactic Trees. In Chris Barker and David Dowty, editors, Proc. of SALT- 2, pages 261281, May 1-3. [Muadz1991] Muadz, H. 1991. A Planar Theory of Coor- dination. Ph.D. thesis, University of Arizona, Tucson, Arizona. [Rambow and Joshi1992] Rambow, O. and A. Joshi. 1992. A formal look at dependency grammars and phrase- structure grammars, with special consideration to word-order phenomena. In Intern. Workshop on the Meaning-Text Theory, pages 4766, Arbeitspapiere der GMD 671. Darmstadt. [Rambow and Sattato appear] Rambow, O. and G. Satta. to appear. Independent parallelism in ﬁnite copying parallel rewriting systems. Theor. Comput. Sc. [Rambow, Vijay-Shanker, and Weir1995] Rambow, O., K. Vijay-Shanker, and D. Weir. 1995. D-Tree Grammars. In Proceedings of the 33rd Meeting of the ACL. [Sarkar and Joshi1996] Sarkar, Anoop and Aravind Joshi. 1996. Coordination in TAG: Formalization and im- plementation. In Proceedings of the 16th Interna- tional Conference on Computational Linguistics (COL- ING96), Copenhagen. [Shieber1994] Shieber, S. 1994. Restricting the weak generative capacity of synchronous tree adjoining grammars. Computational Intelligence, 10(4):371385, November. [Shieber and Schabes1990] Shieber, Stuart and Yves Sch- abes. 1990. Synchronous Tree Adjoining Grammars. In Proceedings of the 13th International Conference on Computational Linguistics (COLING90), Helsinki, Finland. [Steedman1985] Steedman, Mark. 1985. Dependency and coordination in the grammar of Dutch and English. Language, 61:523568. [Steedman1997a] Steedman, Mark. 1997a. Informa- tion Structure and the Syntax-Phonology Interface. manuscript. Univ. of Pennsylvania. [Steedman1997b] Steedman, Mark. 1997b. Surface Struc- ture and Interpretation: Unbounded and Bounded De- pendency in Combinatory Grammar. Linguistic In- quiry monograph. MIT Press. 7",
  "7.pdf": "arXiv:cs9809029v1 [cs.CL] 18 Sep 1998 Incremental Parser Generation for Tree Adjoining Grammars Anoop Sarkar University of Pennsylvania Department of Computer and Information Science Philadelphia PA 19104 anooplinc.cis.upenn.edu 2 February 1996 Abstract This paper describes the incremental generation of parse tables for the LR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented handles modiﬁcations to the input grammar by updating the parser generated so far. In this paper, a lazy generation of LR-type parsers for TALs is deﬁned in which parse tables are created by need while parsing. We then describe an incremental parser generator for TALs which responds to modiﬁcation of the input grammar by updating parse tables built so far. 1 Introduction Tree Adjoining Grammars (TAGs) are tree rewriting systems which combine trees with the single operation of adjunction (see Figure 1). The construction of deterministic bottom-up left to right parsing of Tree Adjoining Languages (TALs)1(Schabes and Vijay-Shanker, 1990) is an extension of the LR pars- ing strategy for context free languages (Aho et al., 1986). Parser generation Thanks to Dania Egedi, Aravind Joshi, B. Srinivas and the student session reviewers. 1Familiarity with Tree Adjoining Grammars (TAGs) and their parsing techniques is assumed throughout the paper. For an introduction to TAGs, see (Joshi, 1987). We shall assume that our deﬁnition of TAG does not have the substitution operation. Refer to (Schabes, 1991) for a background on the parsing of TAGs. 1 involves precompiling as much top-down information as possible into a parse table which is used by the LR parsing algorithm. This paper gives an algo- rithm for the incremental generation of parse tables for the LR-type parsing of TAGs. X x, w, y, u, v are terminal symbols X is a non-terminal symbol Adjunction w x y Initial Tree α X X u v Auxiliary Tree X X x y u v w α β β Figure 1: The Adjunction Operation Parser generation provides a fast solution to the parsing of input sen- tences as certain information about the grammar is precompiled and avail- able while parsing. However, if the grammar used to generate the parser is either dynamic or needs frequent modiﬁcation then the time needed to parse the input is determined by both the parser and the parser generator. The main application area for TAGs has been the description of natural languages. In such an area grammars are very rarely static, and modiﬁ- cations to the original grammar are commonplace. In such an interactive environment, conventional LR-type parsing suﬀers from the following disad- vantages:  Some parts of the grammar might never be used in the parsing of sentences actually given to the parser. The time taken by the parser generator over such parts is wasted.  Usually, only a small part of the grammar is modiﬁed. So a parser gen- erator should also correspondingly make a small change to the parser rather than generate a new one from scratch. The algorithm described here allows the incremental incorporation of modiﬁcations to the grammar in a LR-type parser for TALs. This paper extends the work done on the incremental modiﬁcation of LR(0) parser generators for CFGs in (Heering et al., 1990; Heering et al., 1989). We deﬁne a lazy and incremental parser generator having the following characteristics: 2  The parse tables are generated in a lazy fashion from the grammar, i.e. generation occurs while parsing the input. Information previously precompiled is now generated depending on the input.  The parser generator is incremental. Changes in the grammar trigger a corresponding change in the already generated parser. Parts of the parser not aﬀected by the modiﬁcations in the grammar are reused.  Once the needed parts of the parser have been generated, the parsing process is as eﬃcient as a conventionally generated one. Incremental generation of parsers gives us the following beneﬁts:  The LR-type parsing of lexicalized TAGs (Schabes, 1991). With the use of the lazy and incremental parser generation, lexicalized descrip- tions of TAGs can be parsed using LR-type parsing techniques. Parse tables can be generated without exhaustively considering all lexical items that anchor each tree.  Modular composition of parsers, where various modules of TAG de- scriptions are integrated with recompilation of only the necessary parts of the parse table of the combined parser. 2 LR Parser Generation (Schabes and Vijay-Shanker, 1990) describe the construction of an LR pars- ing algorithm for TAGs. Parser generation here is taken to be the construc- tion of LR(0) tables (i.e. without any lookahead) for a particular TAG2. The moves made by the parser can be most succinctly explained by looking at an automaton which is weakly equivalent to TAGs called Bottom-Up Embed- ded Pushdown Automata (BEPDA) (Schabes and Vijay-Shanker, 1990)3. The storage of a BEPDA is a sequence of stacks (or pushdown stores) where stacks can be introduced above and below the top stack in the automaton. Recognition of adjunction can be informally seen to be equivalent to the unwrap move shown in Figure 2. 2 The algorithm described here can be extended to a parser with SLR(1) tables (Schabes and Vijay-Shanker, 1990). 3 Note that the LR(0) tables considered here are deterministic and hence correspond to a subset of the TALs. Techniques developed in (Tomita, 1986) can be used to resolve nondeterminism in the parser. 3 X X β foot of β spine α unwrap β β spine of right of foot of left of foot of β Figure 2: Recognition of adjunction in a BEPDA. The LR parser uses a parsing table and a sequence of stacks (see Figure 2) to parse the input. The parsing table encodes the actions taken by the parser as follows (with the help of two GOTO functions):  Shift to a new state which is pushed onto a new stack which appears on top of the current sequence of stacks.  Resume Right where the parser has reached right and below a node on which an auxiliary tree has been adjoined. Figure 3 gives the two cases where the string beneath the foot node of an auxiliary tree has been recognized (in some other tree) and where the GOTOfoot function encodes the proper state such that the right part of an auxiliary tree can be recognized.  Reduce Root which causes the parser to execute an unwrap move to recognize adjunction (see Figure 2). The proper state for the parser after adjunction is given by the GOTOright function.  Accept and Error functions as in conventional LR parsing. Figure 4 shows how the concept of dotted rules for CFGs is extended to trees. There are four positions for a dot associated with a symbol: left above, left below, right below and right above. A dotted tree has one such dotted symbol. The tree traversal in Figure 4 scans the frontier of the tree from left to right while trying to recognize possible adjunctions between the 4 GOTO foot is being adjoined into α β resume right B A k k k B α α β (a) (b) B B Figure 3: The resume right action in the parser. above and below positions of the dot. If an adjunction has been performed on a node then it is marked with a star (e.g. B). A B C D E F Figure 4: Left to right dotted tree traversal. Construction of a LR(0) parsing table is an extension of the technique used for CFGs. The parse table is built as a ﬁnite state automaton (FSA) with each state deﬁned to be a set of dotted trees. The closure operations on states in the parse table are deﬁned in Figure 5. All the states in the parse table must be closed under these operations. Figure 9 is a partial FSA constructed for the grammar in Figure 7. The FSA is built as follows: in state 0 put all the initial trees with the dot left and above the root. The state is then closed. New states are built by the transitions deﬁned in Figure 6. Entries in the parse table are determined 5 A A A A A B A C B A C B A A B A A A A A B A B A Adjunction Prediction Move Dot Up Skip Node Left Completion Move Dot Down Figure 5: Closure Operations. as follows:  a shift for each transition in the FSA.  resume right iﬀ there is a node B with the dot right and below it.  reduce root iﬀ there is a rootnode in an auxiliary tree with the dot right and above it.  accept and error with the usual interpretation. The items created in each state before closure applies, i.e. the right hand sides in Figure 6 are called the kernels of each state in the FSA. The initial trees with the dot left and above the root form the kernel for state 0. A state which has not been closed is said to be in kernel form. 3 Lazy Parser Generation The algorithm described so far assumes that the parse table is precompiled before the parser is used. Lazy parser generation spreads the generation of the parse table over the parsing of several sentences to obtain a faster response time in the parser generation stage. It generates only those parts of the parser that are needed to parse the sentences given to it. Lazy parser 6 S i S j β right A a a A β foot A A a a is a terminal symbol can adjoin at node A β Figure 6: Transitions in the ﬁnite state automaton. generation is useful in cases where typical input sentences are parsed with a small part of the total grammar. We deﬁne lazy parser generation mainly as a step towards incremen- tal parser generation. The approach is an extension of the algorithm for CFGs given in (Heering et al., 1990; Heering et al., 1989). To modify the LR parsing strategy given earlier we move the closure and computation of transitions (Figure 5 and Figure 6) from the table generation stage to the LR parser. The lazy technique expands a kernel state only when the parser, looking at the current input, indicates that the state needs expansion. For example, the TAG in Figure 7 (na rules out adjunction) produces the FSA in Figure 84. Computation of closure and transitions in the state occurs while parsing as seen in Figure 9 which is the result of the LR parser expanding the FSA in Figure 8 while parsing the string aec. The only extra statement in the modiﬁed parse function is a check on the type of the state and possible expansion of kernel states takes place while parsing a sentence. Memory use in the lazy technique is greater as the FSA is needed during parsing as well. 4 As a convention in our FSAs we mark unexpanded kernel states with a boldfaced outline and a double-lined outline as the acceptance states. 7 S e Sna Sna S a c α: β: Figure 7: TAG G where L(G)  {anecn} S e 0 Figure 8: The FSA after parse table generation. 4 Incremental Parser Generation The lazy parser generator described reacts to modiﬁcations to the grammar by throwing away all parts of the parser that it has generated and creates a FSA containing only the start state. In this section we describe an in- cremental parser generator which retains as much of the original FSA as it can. It throws away only that information from the FSA of the old grammar which is incorrect with respect to the updated grammar. The incremental behaviour is obtained by selecting the states in the parse table aﬀected by the change in the grammar and returning them to their kernel form (i.e. remove items added by the closure operations). The parse table FSA will now become a disconnected graph. The lazy parser will expand the states using the new grammar. All states in the disconnected graph are kept as the lazy parser will reconnect with those states (when the transitions in Figure 6 are computed) that are unaﬀected by the change in the grammar. Consider the addition of a tree to the grammar5.  for an initial tree α return state 0 to kernel form adding α with the dot left and above the root node. Also return all states where a possible Left Completion on α can occur to their kernel form. 5 Deletion of a tree will be similar. 8 S S S na c a na S S S na c a na S S S na c a na S e e e S S S e e e S S S S S na c a na S S S na c a na S S S na c a na S S S na c a na S S S na c a na e e S S e S e a a foot β c 1 0 2 3 4 S S S na c na S S S na c a na S S S na c a na S S S na c a na S S S na c a na S S S na c a na S S na c a na S S na c a na S S na c a na S S S a S S S na c a na S S S na c a na S S S na c a na 7 e βright 5 6 βfoot e e S S S S S na c a na Figure 9: The FSA after parsing the string aec. γ: Sna d S b na S Figure 10: New tree added to G with L(G)  {anbmecndm} 9  for an auxiliary tree β return all states where a possible Adjunction Prediction on β can occur and all states with a βright transition to their kernel form. For example, the addition of the tree in Figure 10 causes the FSA to fragment into the disconnected graph in Figure 11. It is crucial that the disconnected states are kept around as can be seen from the re-expansion of a single state in Figure 12. All states compatible with the modiﬁed grammar are eventually reused. S e 4 0 1 2 7 5 6 right β e S S e e e S S S S na c a na S S S S na c a na e e S S S S S na c a na S S S na c a na S S S na c a na c 3 S S S na c a na S S S na c a na S S S na c a na S S S na c a na S S S na c a na Figure 11: The parse table after the addition of γ. The approach presented above causes certain states to become unreach- able from the start state. Frequent modiﬁcations of a grammar can cause many unreachable states. A garbage collection scheme deﬁned in (Heering et al., 1990) can be used here which avoids overregeneration by retaining unreachable states. 5 Conclusion What we have described above is work in progress in implementing a LR- type parser for a wide-coverage lexicalized grammar of English in the TAG framework (XTAG Group, 1995). The algorithm for incremental parse ta- 10 S S na c a na S S S na c a na S S S na c a na S S S na c a na S S S na c a na S S S na c a na S S S na c a na S S e e e S S na S b Sna S d na S b Sna S d na S b Sna S d S S na c a na S S na c a na S S na S b Sna S d na S b Sna S d 0 S S na c na S a 1 na S Sna S d b S e e e S S 8 4 e b a 7 2 S S na c a na S S S na c a na S S S na c a na S c right β e S e e S S 6 5 3 Figure 12: The parse table after expansion of state 0 with the modiﬁed grammar. ble generation for TAGs given here extends a similar result for CFGs. The parse table generator was built on a lazy parser generator which generates the parser only when the input string uses parts of the parse table not previ- ously generated. The technique for incremental parser generation allows the addition and deletion of elementary trees from a TAG without recompilation of the parse table for the updated grammar. This allows us to combine the speed-up obtained by precompiling top-down dependencies such as the pre- diction of adjunction with the ﬂexibility in lexical description usually given by Earley-style parsers. References [Aho et al.1986] Aho, Alfred V., Ravi Sethi and Jeﬀrey D. Ullman, Compil- ers: Principles, Techniques and Tools, Addison Wesley, Reading, MA, 1986. [Heering et al.1990] Heering, Jan, Paul Klint and Jan Rekers, Incremental 11 Generation of Parsers, In IEEE Transactions on Software Engineering, vol. 16, no. 12, pp. 1344-1350, 1990. [Heering et al.1989] Heering, Jan, Paul Klint and Jan Rekers, Incremental Generation of Parsers, In ACM SIGPLAN Notices (SIGPLAN 89 Con- ference on Programming Language Design and Implementation), vol. 24, no. 7, pp. 179191, 1989. [Joshi1987] Joshi, Aravind K., An Introduction to Tree Adjoining Gram- mars, In Manaster-Ramer, Alexis (ed.) Mathematics of Language, John Benjamins, Amsterdam, 1987. [Schabes and Vijay-Shanker1990] Schabes, Yves and K. Vijay-Shanker, De- terministic Left to Right Parsing of Tree Adjoining Languages, In 28th Meeting of the Association for Computational Linguistics (ACL 90), Pittsburgh, PA, 1990. [Schabes1991] Schabes, Yves and Aravind K. Joshi, Parsing with Lexicalized Tree Adjoining Grammars, In Tomita, Masaru (ed.) Current Issues in Parsing Technologies, Kluwer Academic, Dordrecht, The Netherlands, 1991. [Tomita1986] Tomita, Masaru, Eﬃcient Parsing for Natural Language: A Fast Algorithm for Practical Systems, Kluwer Academic, Dordrecht, The Netherlands, 1986. [XTAG Group1995] XTAG Research Group, A Lexicalized Tree Adjoining Grammar for English, IRCS Technical Report 95-03, University of Penn- sylvania, Philadelphia, PA. 1995. 12",
  "8.pdf": "cs.CL9809050 23 Sep 1998 A Freely Available Morphological Analyzer, Disambiguator and Context Sensitive Lemmatizer for German1 Wolfgang Lezius University of Paderborn Cognitive Psychology D-33098 Paderborn leziuspsycho.uni- paderborn.de Reinhard Rapp University of Mainz Faculty of Applied Linguistics D-76711 Germersheim rappusun1.fask.uni- mainz.de Manfred Wettler University of Paderborn Cognitive Psychology D-33098 Paderborn wettlerpsycho.uni- paderborn.de 1 This paper was published in the Proceedings of the COLING-ACL 1998. Abstract In this paper we present Morphy, an inte- grated tool for German morphology, part-of- speech tagging and context-sensitive lem- matization. Its large lexicon of more than 320,000 word forms plus its ability to pro- cess German compound nouns guarantee a wide morphological coverage. Syntactic ambiguities can be resolved with a standard statistical part-of-speech tagger. By using the output of the tagger, the lemmatizer can determine the correct root even for ambi- guous word forms. The complete package is freely available and can be downloaded from the World Wide Web. Introduction Morphological analysis is the basis for many NLP applications, including syntax parsing, machine translation and automatic indexing. However, most morphology systems are com- ponents of commercial products. Often, as for example in machine translation, these systems are presented as black boxes, with the morpho- logical analysis only used internally. This makes them unsuitable for research purposes. To our knowledge, the only wide coverage morpho- logical lexicon readily available is for the Eng- lish language (Karp, Schabes, et al., 1992). There have been attempts to provide free mor- phological analyzers to the research community for other languages, for example in the MULTEXT project (Armstrong, Russell, et al., 1995), which developed linguistic tools for six European languages. However, the lexicons provided are rather small for most languages. In the case of German, we hope to significantly improve this situation with the development of a new version of our morphological analyzer Morphy. In addition to the morphological analyzer, Morphy includes a statistical part-of-speech tag- ger and a context-sensitive lemmatizer. It can be downloaded from our web site as a complete package including documentation and lexicon (http:www-psycho.uni-paderborn.delezius). The lexicon comprises 324,000 word forms based on 50,500 stems. Its completeness has been checked using Wahrig Deutsches Wörter- buch, a standard dictionary of German (Wahrig, 1997). Since Morphy is intended not only for linguists, but also for second language learners of German, the current version has been imple- mented with Delphi for a standard Windows 95 or Windows NT platform and great effort has been put in making it as user friendly as possi- ble. For UNIX users, an export facility is pro- vided which allows generating a lexicon of full forms together with their morphological de- scriptions in text format. 1 The Morphology System Since German is a highly inflectional language, the morphological algorithms used in Morphy are rather complex and can not be described here in detail (see Lezius, 1996). In essence, Morphy is a computer implementation of the morpho- logical system described in the Duden grammar (Drosdowsky, 1984). An overview on other German morphology systems, namely GERTWOL, LA-Morph, Morph, Morphix, Morphy, MPRO, PC-Kimmo and Plain, is given in the documentation for the Morpholympics (Hausser, 1996). The Morpho- lympics were an attempt to compare and evalu- ate morphology systems in a standardized com- petition. Since then, many of the systems have been further developed. The version of Morphy as described here is a new release. Improve- ments over the old version include an integrated part-of-speech tagger, a context-sensitive lem- matizer, a 2.5 times larger lexicon and more user-friendliness through an interactive Win- dows-environment. The following subsections describe the three submodules of the morphological analyzer. These are the lexical system, the generation module and the analysis module. 1.1 Lexical System The lexicon of Morphy is very compact as it only stores the base form for each word together with its inflection class. Therefore, the complete morphological information for 324,000 word forms takes less than 2 Megabytes of disk space. In comparison, the text representation of the same lexicon, which can be generated via Mor- phys export facility, requires 125 MB when full morphological descriptions are given. Since the lexical system has been specifically designed to allow a user-friendly extension of the lexicon, new words can be added easily. To our knowledge, Morphy is the only morphology system for German whose lexicon can be ex- panded by users who have no specialist know- ledge. When entering a new word, the user is asked the minimal number of questions neces- sary to infer the grammatical features of the new word and which any native speaker of German should be able to answer. 1.2 Generation Starting from the root form of a word and its inflection type as stored in the lexicon, the gen- eration system produces all inflected forms. Morphy's generation algorithms were designed with the aim of producing 100 correct output. Among other morphological characteristics, the algorithms consider vowel mutation (Haus - Häuser), shift between ß and ss (Faß - Fässer), e- omission (segeln - segle), infixation of infinitive markers (weggehen - wegzugehen), as well as pre- and infixation of markers of participles (gehen - gegangen; weggehen - weggegangen). 1.3 Analysis For each word form of a text, the analysis sys- tem determines its root, part of speech, and - if appropriate - its gender, case, number, person, tense, and comparative degree. It also segments compound nouns using a longest-matching rule which works from right to left and takes linking letters into account. To compound German nouns is not trivial: it can involve base forms andor inflected forms (e.g. Haus-meister but Häuser-meer); in some cases the compounding is morphologically ambiguous (e.g. Stau-becken means water reservoir, but Staub-ecken means dust corners); and the linking letters e and s are not always determined phonologically, but in some cases simply occur by convention (e.g. Schwein-e-bauch but Schwein-s-blase and Schwein-kram). Since the analysis system treats each word separately, ambiguities can not be resolved at this stage. For ambiguous word forms, all pos- sible lemmata and their morphological descrip- tions are given (see Table 1 for the example Winde). If a word form can not be recognized, its part of speech is predicted by a guesser which makes use of statistical data derived from Ger- man suffix frequencies (Rapp, 1996). morphological description lemma SUB NOM SIN FEM Winde SUB GEN SIN FEM Winde SUB DAT SIN FEM Winde SUB AKK SIN FEM Winde SUB DAT SIN MAS Wind SUB NOM PLU MAS Wind SUB GEN PLU MAS Wind SUB AKK PLU MAS Wind VER SIN 1PE PRÄ winden VER SIN 1PE KJ1 winden VER SIN 3PE KJ1 winden VER SIN IMP winden Table 1: Morphological analysis for Winde. Morphy's algorithm for analysis is motivated by linguistic considerations. When analyzing a word form, Morphy first builds up a list of pos- sible roots by cutting off all possible prefixes and suffixes and reverses the process of vowel mutation if umlauts are found (shifts between ß and ss are treated analogously). Each root is looked up in the lexicon, and - if found - all possible inflected forms are generated. Only those roots which lead to an inflected form identical to the original word form are selected (Lezius, 1996). Naturally, this procedure is much slower than a simple algorithm for the lookup of word forms in a full form lexicon. It results in an analysis speed of about 300 word forms per second on a fast PC, compared to many thousands using a full form lexicon. However, there are also ad- vantages: First, as mentioned above, the lexicon can be kept very small, which is an important consideration for a PC-based system intended for Internet-distribution. More importantly, the processing of German compound nouns and the implementation of derivation rules - although only partially completed at this stage - fits better into this concept. For the processing of very large corpora under UNIX, we have imple- mented a lookup algorithm which operates on the Morphy-generated full form lexicon. The coverage of the current version of Mor- phy was evaluated with the same test corpus that had been used at the Morpholympics. This cor- pus comprises about 7.800 word forms in total and consists of two political speeches, a frag- ment of the LIMAS-corpus, and a list of special word forms. The present version of Morphy recognized 94.3, 98.4, 96.2, and 88.9 of the word forms respectively. The corresponding values for the old version of Morphy, with a 2.5 times smaller lexicon, had been 89.2, 95.9, 86.9, and 75.8. 2 The Disambiguator Since the morphology system only looks at iso- lated word forms, words with more than one reading can not be disambiguated. This is done by the disambiguator or tagger, which takes context into account by considering the condi- tional probabilities of tag sequences. For exam- ple, in the sentence \"he opens the can\" the verb- reading of can may be ruled out because a verb can not follow an article. After the success of statistical part-of-speech taggers for English, there have been quite a few attempts to apply the same methods to German. Lezius, Rapp  Wettler (1996) give an overview on some German tagging projects. Although we considered a number of algorithms, we decided to use the trigram algorithm described by Church (1988) for tagging. It is simple, fast, robust, and - among the statistical taggers - still more or less unsurpassed in terms of accuracy. Conceptually, the Church-algorithm works as follows: For each sentence of a text, it generates all possible assignments of part-of-speech tags to words. It then selects that assignment which optimizes the product of the lexical and contex- tual probabilities. The lexical probability for word N is the probability of observing part of speech X given the (possibly ambiguous) word N. The contextual probability for tag Z is the probability of observing part of speech Z given the preceding two parts of speech X and Y. It is estimated by dividing the trigram frequency XYZ by the bigram frequency XY. In practice, computational limitations do not allow the enu- meration of all possible assignments for long sentences, and smoothing is required for infre- quent events. This is described in more detail in the original publication (Church, 1988). Although more sophisticated algorithms for unsupervised learning - which can be trained on plain text instead on manually tagged corpora - are well established (see e.g. Merialdo, 1994), we decided not to use them. The main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which can lead to poor results without any indication to the user. More recently, in contrast to the statistical taggers, rule-based tagging algorithms have been suggested which were shown to reduce the error rate significantly (Samuelsson  Voutilainen, 1997). We consider this a promising approach and have started to develop such a system for German with the intention of later inclusion into Morphy. The tag set of Morphy's tagger is based on the feature system of the morphological analyzer. However, some features were discarded for tag- ging. For example, the tense of verbs is not con- sidered. This results in a set of about 1000 dif- ferent tags. A fragment of 20,000 words from the Frankfurter Rundschau Corpus, which we have been collecting since 1992, was tagged with this tag set by manually selecting the cor- rect choice from the set of possibilities gener- ated by the morphological analyzer. In the fol- lowing we refer to this corpus as the training corpus. Of all possible tags, only 456 actually occurred in the training corpus. The average ambiguity rate was 5.4 tags per word form. The performance of our tagger was evaluated by running it on a 5000-word test sample of the Frankfurter Rundschau-Corpus which was di- stinct from the training text. We also tagged the test sample manually and compared the results. 84.7 of the tags were correctly tagged. Al- though this result may seem poor at first glance, it should be noted that the large tag sets have many fine distinctions which lead to a high error rate. If a tag set does not have these distinctions, the accuracy improves significantly. In order to show this, in another experiment we mapped our large tag set to a smaller set of 51 tags, which is comparable to the tag set used in the Brown Corpus (Greene  Rubin, 1971). As a result, the average ambiguity rate per word decreased from 5.4 to 1.6, and the accuracy improved to 95.9, which is similar to the accuracy rates reported for statistical taggers with small tag sets in vari- ous other languages. Table 2 shows a tagging example for the large and the small tag set. Word large tag set small tag set Ich PRO PER NOM SIN 1PE PRO PER meine VER 1PE SIN VER meine POS AKK SIN FEM ATT POS ATT Frau SUB AKK FEM SIN SUB . SZE SZE Table 2: Tagging example for both tag sets. 3 The Lemmatizer For lemmatization (the reduction to base form), the integrated design of Morphy turned out to be advantageous. In the first step, the morphology- module delivers all possible lemmata for each word form. Secondly, the tagger determines the grammatical categories of the word forms. If, for any of the lemmata, the inflected form corre- sponding to the word form in the text does not agree with this grammatical category, the re- spective lemma is discarded. For example, in the sentence \"ich meine meine Frau\" (\"I mean my wife\"), the assignment of the two middle words to the verb meinen and the possessive pronoun mein is not clear to the morphology system. However, since the tagger assigns the tag se- quence \"pronoun verb pronoun noun\" to this sentence, it can be concluded that the first oc- currence of meine must refer to the verb meinen and the second to the pronoun mein. Unfortunately, this may not always work as well as in this example. One reason is that there may be semantic ambiguities which can not be resolved by syntactic considerations. Another is that the syntactic information delivered by the tagger may not be fine grained enough to resolve all syntactic ambiguities.2 Do we need the fine grained distinctions of the large tag set to re- solve ambiguities, or does the rough information from the small tag set suffice? To address these questions, we performed an evaluation using another test sample from the Frankfurter Rund- schau-Corpus. We found that - according to the Morphy lexi- con - of all 9,893 word forms in the sample, 9,198 (93.0) had an unambiguous lemma. Of the remaining 695 word forms, 667 had two possible lemmata and 28 were threefold ambi- guous (Table 3 gives some examples). Using the large tag set, 616 out of the 695 ambiguous word forms were correctly lemmatized (88.6). The corresponding figures for the small tag set were slightly better: 625 out of 695 ambiguities were resolved correctly (89.9). When the error-rate is related to the total number of word forms in the text, the accuracy is 99.2 for the large and 99.3 for the small tag set. The better performance when using the small tag set is somewhat surprising since there are a few cases of ambiguities in the test corpus which can only be resolved by the large tag set but not by the small tag set. For example, since the small tag set does not consider a noun's case, gender, and number, it can not decide whether Filmen is derived from der Film (the film) or from das Filmen (the filming). On the other hand, as shown in the previous section, the tag- ging accuracy is much better for the small tag set, which is an advantage in lemmatization and obviously compensates for the lack of detail. However, we believe that with future im- 2 For example the verb führen can be either a sub- junctive form of fahren (\"to drive\") or a regular form of führen (\"to lead\"). Since neither the large nor the small tag set consider mood, this ambiguity can not be resolved. provements in tagging accuracy lemmatization based on the large tag set will eventually be better. Nevertheless, the current implementation of the lemmatizer gives the user the choice of selecting between either tag set. Begriffen Begriff, begreifen Dank danken, dank (prep.), Dank Garten garen, Garten Trotz Trotz, trotzen, trotz Weise Weise, weise, weisen Wunder Wunder, wundern, wund Table 3: Word forms with several lemmata. Conclusions In this paper, a freely available integrated tool for German morphological analysis, part-of- speech tagging and context sensitive lemmatiza- tion was introduced. The morphological ana- lyzer is based on the standard Duden grammar and provides wide coverage due to a lexicon of 324,000 word forms and the ability to process compound nouns at runtime. It gives for each word form of a text all possible lemmata and morphological descriptions. The ambiguities of the morphological descriptions are resolved by the tagger, which provides about 85 accuracy for the large and 96 accuracy for the small tag set. The lemmatizer uses the output of the tagger to disambiguate word forms with more than one possible lemma. It achieves an overall accuracy of about 99.3. Acknowledgements The work described in this paper was conducted at the University of Paderborn and supported by the Heinz Nixdorf-Institute. The Frankfurter Rundschau Corpus was generously donated by the Druck- und Verlagshaus Frankfurt am Main. We thank Gisela Zunker for her help with the acquisition and preparation of the corpus. References Armstrong, S.; Russell, G.; Petitpierre, D.; Robert, G. (1995). An open architecture for multilingual text processing. In: Proceedings of the ACL SIGDAT Workshop. From Texts to Tags: Issues in Multilin- gual Language Analysis, Dublin. Church, K.W. (1988). A stochastic parts program and noun phrase parser for unrestricted text. Second Conference on Applied Natural Language Proc- essing, Austin, Texas, 136-143. Drosdowski, G. (ed.) (1984). Duden. Grammatik der deutschen Gegenwartssprache. Mannheim: Dudenverlag. Greene, B.B., Rubin, G.M. (1971). Automatic Grammatical Tagging of English. Internal Report, Brown University, Department of Linguistics: Providence, Rhode Island. Hausser, R. (ed.) (1996). Linguistische Verifikation. Dokumentation zur Ersten Morpholympics. Niemeyer: Tübingen. Karp, D.; Schabes, Y.; Zaidel, M.; Egedi, D. (1992). A freely available wide coverage mophological analyzer for English. In:. Proceedings of the 14th International Conference on Computational Lin- guistics. Nantes, France. Lezius, W. (1996). Morphologiesystem Morphy. In: R. Hausser (ed.): Linguistische Verifikation. Do- kumentation zur Ersten Morpholympics. Niemeyer: Tübingen. 25-35. Lezius, W.; Rapp, R.; Wettler, M. (1996). A mor- phology system and part-of-speech tagger for German. In: D. Gibbon (ed.): Natural Language Processing and Speech Technology. Results of the 3rd KONVENS Conference, Bielefeld. Berlin: Mouton de Gruyter. 369-378. Merialdo, B. (1994). Tagging English text with a probabilistic model. Computational Linguistics, 20(2), 155-171. Rapp, R. (1996). Die Berechnung von Assoziationen: ein korpuslinguistischer Ansatz. Hildesheim: Olms. Samuelsson, C., Voutilainen, A. (1997). Comparing a linguistic and a stochastic tagger. Proceedings of the 35th Annual Meeting of the ACL and 8th Con- ference of the European Chapter of the ACL. Wahrig, G. (1997). Deutsches Wörterbuch. Güters- loh: Bertelsmann. Appendix: Abbreviations AKK K accusative PLU plural ATT attributive usage POS possessive DAT dative PRÄ present tense FEM feminine PRO pronoun GEN genitive SIN singular IMP imperative SUB noun KJ1 subjunctive 1 SZE punctuation mark MAS masculine VER verb NOM nominative 1PE 1st person PER personal 3PE 3rd person",
  "9.pdf": "arXiv:cs9809106v1 [cs.CL] 25 Sep 1998 Processing Unknown Words in HPSG Petra Barg and Markus Walther Seminar fur Allgemeine Sprachwissenschaft Heinrich-Heine-Universitat Dusseldorf Universitatsstr. 1, D-40225 Dusseldorf, Germany {barg,walther}ling.uni-duesseldorf.de Abstract The lexical acquisition system presented in this pa- per incrementally updates linguistic properties of un- known words inferred from their surrounding con- text by parsing sentences with an HPSG grammar for German. We employ a gradual, information- based concept of unknownness providing a uni- form treatment for the range of completely known to maximally unknown lexical entries. Unknown in- formation is viewed as revisable information, which is either generalizable or specializable. Updating takes place after parsing, which only requires a mod- iﬁed lexical lookup. Revisable pieces of informa- tion are identiﬁed by grammar-speciﬁed declarations which provide access paths into the parse feature structure. The updating mechanism revises the cor- responding places in the lexical feature structures iff the context actually provides new information. For revising generalizable information, type union is re- quired. A worked-out example demonstrates the in- ferential capacity of our implemented system. 1 Introduction It is a remarkable fact that humans can often un- derstand sentences containing unknown words, in- fer their grammatical properties and incrementally reﬁne hypotheses about these words when encoun- tering later instances. In contrast, many current NLP systems still presuppose a complete lexicon. Notable exceptions include Zernik (1989), Erbach (1990), Hastings  Lytinen (1994). See Zernik for an intro- duction to the general issues involved. This paper describes an HPSG-based system which can incrementally learn and reﬁne proper- ties of unknown words after parsing individual sen- This work was carried out within the Sonderforschungs- bereich 282 Theorie des Lexikons (project B3), funded by the German Federal Research Agency DFG. We thank James Kil- bury and members of the B3 group for fruitful discussion. tences. It focusses on extracting linguistic proper- ties, as compared to e.g. general concept learning (Hahn, Klenner  Schnattinger 1996). Unlike Er- bach (1990), however, it is not conﬁned to sim- ple morpho-syntactic information but can also han- dle selectional restrictions, semantic types and argu- ment structure. Finally, while statistical approaches like Brent (1991) can gather e.g. valence informa- tion from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context. The following three goals serve to structure our model. It should i) incorporate a gradual, information-based conceptualization of unknown- ness. Words are not unknown as a whole, but may contain unknown, i.e. revisable pieces of infor- mation. Consequently, even known words can un- dergo revision to e.g. acquire new senses. This view replaces the binary distinction between open and closed class words. It should ii) maximally exploit the rich representations and modelling conventions of HPSG and associated formalisms, with essen- tially the same grammar and lexicon as compared to closed-lexicon approaches. This is important both to facilitate reuse of existing grammars and to en- able meaningful feedback for linguistic theorizing. Finally, it should iii) possess domain-independent in- ference and lexicon-updating capabilities. The gram- mar writer must be able to fully declare which pieces of information are open to revision. The system was implemented using MicroCUF, a simpliﬁed version of the CUF typed uniﬁcation formalism (Dorre  Dorna 1993) that we imple- mented in SICStus Prolog. It shares both the feature logic and the deﬁnite clause extensions with its big brother, but substitutes a closed-world type system for CUFs open-world regime. A feature of our type system implementation that will be signiﬁcant later on is that type information in internal feature struc- tures (FSs) can be easily updated. The HPSG grammar developed with MicroCUF models a fragment of German. Since our focus is on the lexicon, the range of syntactic variation treated is currently limited to simplex sentences with canon- ical word order. We have incorporated some recent developments of HPSG, esp. the revisions of Pol- lard  Sag (1994, ch. 9), Manning  Sag (1995)s proposal for an independent level of argument struc- ture and Bouma (1997)s use of argument structure to eliminate procedural lexical rules in favour of re- lational constraints. Our elaborate ontology of se- mantic types  useful for non-trivial acquisition of selectional restrictions and nominal sorts  was de- rived from a systematic corpus study of a biological domain (Knodel 1980, 154-188). The grammar also covers all valence classes encountered in the corpus. As for the lexicon format, we currently list full forms only. Clearly, a morphology component would sup- ply more contextual information from known afﬁxes but would still require the processing of unknown stems. 2 Incremental Lexical Acquisition When compared to a previous instance, a new sen- tential context can supply either identical, more spe- cial, more general, or even conﬂicting information along a given dimension. Example pairs illustrating the latter three relationships are given under (1)-(3) (words assumed to be unknown in bold face). (1) a. Im Axon tritt ein Ruhepotential auf. a rest potential occurs in the axon b. Das Potential wandert uber das Axon. the potential travels along the axon (2) a. Das Ohr reagiert auf akustische Reize. the ear reacts to acoustic stimuli b. Ein Sinnesorgan reagiert auf Reize. a sense organ reacts to stimuli (3) a. Die Nase ist fur Geruche sensibel. the nose is sensitive to smells b. Die sensible Nase reagiert auf Geruche. the sensitive nose reacts to smells In contrast to (1a), which provides the information that the gender of Axon is not feminine (via im), the context in (1b) is more specialized, assigning neuter gender (via das). Conversely, (2b) differs from (2a) in providing a more general selectional restriction for the subject of reagiert, since sense organs include ears as a subtype. Finally, the adjective sensibel is used predicatively in (3a), but attributively in (3b). The usage types must be formally disjoint, because some German adjectives allow for just one usage (ehemalig former, attr., schuld guilty, pred.). On the basis of contrasts like those in (1)-(3) it makes sense to statically assign revisable informa- tion to one of two classes, namely specializable or generalizable.1 Apart from the specializable kinds semantic type of nouns and gender, the inﬂec- tional class of nouns is another candidate (given a morphological component). Generalizable kinds of information include selectional restrictions of verbs and adjectives, predicative vs attributive usage of adjectives as well as case and form of PP argu- ments and valence class of verbs. Note that spe- cializable and generalizable information can cooccur in a given lexical entry. A particular kind of informa- tion may also ﬁgure in both classes, as e.g. seman- tic type of nouns and selectional restrictions of verbs are both drawn from the same semantic ontology. Yet the former must be invariantly specialized  indepen- dent of the order in which contexts are processed , whereas selectional restrictions on NP complements should only become more general with further con- texts. 2.1 Representation We require all revisable or updateable information to be expressible as formal types.2 As relational clauses can be deﬁned to map types to FSs, this is not much of a restriction in practice. Figure 1 shows a rele- vant fragment. Whereas the combination of special- ... gender prd pred attr fem u_g masc neut non_fem nom_sem sense_organ ear nose stimulus smell sound Figure 1: Excerpt from type hierarchy izable information translates into simple type uniﬁ- cation (e.g. non fem  neut  neut), combining 1The different behaviour underlying this classiﬁcation has previously been noted by e.g. Erbach (1990) and Hastings  Lytinen (1994) but received either no implementational status or no systematic association with arbitrary kinds of information. 2In HPSG types are sometimes also referred to as sorts. generalizable information requires type union (e.g. pred  attr  prd). The latter might pose problems for type systems requiring the explicit deﬁnition of all possible unions, corresponding to least common supertypes. However, type union is easy for (Mi- cro)CUF and similar systems which allow for arbi- trary boolean combinations of types. Generalizable information exhibits another peculiarity: we need a disjoint auxiliary type u g to correctly mark the ini- tial unknown information state. 3 This is because content types like prd, pred, attr are to be inter- preted as recording what contextual information was encountered in the past. Thus, using any of these to prespecify the initial value  either as the side-effect of a feature appropriateness declaration (e.g. prd) or through grammar-controlled speciﬁcation (e.g. pred, attr)  would be wrong (cf. prdinitial  attr  prd, but u ginitial  attr  u g  attr). Generalizable information evokes another ques- tion: can we simply have types like those in ﬁg. 1 within HPSG signs and do in-place type union, just like type uniﬁcation? The answer is no, for essen- tially two reasons. First, we still want to rule out ungrammatical constructions through (type) uniﬁca- tion failure of coindexed values, so that generalizable types cannot always be combined by nonfailing type union (e.g. der sensible Geruch the sensitive smell must be ruled out via sense organ  smell  ). We would ideally like to order all type uniﬁcations pertaining to a value before all unions, but this vi- olates the order independence of constraint solv- ing. Secondly, we already know that a given infor- mational token can simultaneously be generalizable and specializable, e.g. by being coindexed through HPSGs valence principle. However, simultaneous in-place union and uniﬁcation is contradictory. To avoid these problems and keep the declarative monotonic setting, we employ two independent fea- tures gen and ctxt. ctxt is the repository of contex- tually uniﬁed information, where conﬂicts result in ungrammaticality. gen holds generalizable informa- tion. Since all gen values contain u g as a type dis- junct, they are always uniﬁable and thus not restric- tive during the parse. To nevertheless get correct gen values we perform type union after parsing, i.e. dur- ing lexicon update. We will see below how this works out. 3Actually, the situation is more symmetrical, as we need a dual type u s to correctly mark unknown specializable infor- mation. This prevents incorrect updating of known information. However, u s is unnecessary for the examples presented below. The last representational issue is how to identify revisable information in (substructures of) the parse FS. For this purpose the grammar deﬁnes revisability clauses like the following: (4) a. generalizable( 1 , 2) :  synsem  loc  cat  head   adj prd gen 1 ctxt 2      b. specializable( 1 ) :  synsem  loc cat  head noun cont  ind  gend 1  2.2 Processing The ﬁrst step in processing sentences with unknown or revisable words consists of conventional parsing. Any HPSG-compatible parser may be used, subject to the obvious requirement that lexical lookup must not fail if a words phonology is unknown. A canon- ical entry for such unknown words is deﬁned as the disjunction of maximally underspeciﬁed generic lex- ical entries for nouns, adjectives and verbs. The actual updating of lexical entries consists of four major steps. Step 1 projects the parse FS derived from the whole sentence onto all participating word tokens. This results in word FSs which are contextu- ally enriched (as compared to their original lexicon state) and disambiguated (choosing the compatible disjunct per parse solution if the entry was disjunc- tive). It then ﬁlters the set of word FSs by uniﬁcation with the right-hand side of revisability clauses like in (4). The output of step 1 is a list of update candidates for those words which were uniﬁable. Step 2 determines concrete update values for each word: for each matching generalizable clause we take the type union of the gen value of the old, lexical state of the word (LexGen) with the ctxt value of its parse projection (Ctxt) : TU  LexGenCtxt. For each matching specializable(Spec) clause we take the parse value Spec. Step 3 checks whether updating would make a dif- ference w.r.t. the original lexical entry of each word. The condition to be met by generalizable information is that TU  LexGen, for specializable information we similarly require Spec  LexSpec. In step 4 the lexical entries of words surviving step 3 are actually modiﬁed. We retract the old lexical en- try, revise the entry and re-assert it. For words never encountered before, revision must obviously be pre- ceded by making a copy of the generic unknown en- try, but with the new words phonology. Revision it- self is the destructive modiﬁcation of type informa- tion according to the values determined in step 2, at the places in a word FS pointed to by the revis- ability clauses. This is easy in MicroCUF, as types are implemented via the attributed variable mecha- nism of SICStus Prolog, which allows us to substi- tute the type in-place. In comparison, general updat- ing of Prolog-encoded FSs would typically require the traversal of large structures and be dangerous if structure-sharing between substituted and unaffected parts existed. Also note that we currently assume DNF-expanded entries, so that updates work on the contextually selected disjunct. This can be motivated by the advantages of working with presolved struc- tures at run-time, avoiding description-level opera- tions and incremental grammar recompilation. 2.3 A Worked-Out Example We will illustrate how incremental lexical revision works by going through the examples under (5)-(7). (5) Die Nase ist ein Sinnesorgan. the nose is a sense organ (6) Das Ohr perzipiert. the ear perceives (7) Eine verschnupfte Nase perzipiert den Gestank. a bunged up nose perceives the stench The relevant substructures corresponding to the lex- ical FSs of the unknown noun and verb involved are depicted in ﬁg. 2. The leading feature paths synsemloccont for Nase and synsemloccatarg-st for perzipiert have been omitted. After parsing (5) the gender of the unknown noun Nase is instantiated to fem by agreement with the determiner die. As the specializable clause (4b) matches and the gend parse value differs from its lexical value gender, gender is updated to fem. Fur- thermore, the objects semantic type has percolated to the subject Nase. Since the objects sense organ type differs from generic initial nom sem, Nases ctxt value is updated as well. In place of the still nonex- isting entry for perzipiert, we have displayed the rel- evant part of the generic unknown verb entry. Having parsed (6) the system then knows that perzipiert can be used intransitively with a nomi- native subject referring to ears. Formally, an HPSG mapping principle was successful in mediating be- tween surface subject and complement lists and the argument list. Argument list instantiations are them- selves related to corresponding types by a further Nase perzipiert after (5)   gend fem gen u g ctxt sense organ   gen u g ctxt arg struc  after (6)   gend fem gen u g ctxt sense organ     gen u gnpnom ctxt arg struc args  loc  cont gen u gear ctxt nom sem      after (7)   gend fem gen u g ctxt nose     gen u gnpnomnpnom npacc ctxt arg struc args   loc  cont gen u gsense organ ctxt nom sem  ,  loc  cont gen u gsmell ctxt nom sem      Figure 2: Updates on lexical FSs mapping. On the basis of this type classiﬁcation of argument structure patterns, the parse derived the ctxt value npnom. Since gen values are generaliz- able, this new value is unioned with the old lexi- cal gen value. Note that ctxt is properly unaffected. The ﬁrst (subject) element on the args list itself is targeted by another revisability clause. This has the side-effect of further instantiating the underspeciﬁed lexical FS. Since selectional restrictions on nominal subjects must become more general with new con- textual evidence, the union of ear and the old value u g is indeed appropriate. Sentence (7) ﬁrst of all provides more speciﬁc evi- dence about the semantic type of partially known Nase by way of attributive modiﬁcation through ver- schnupfte. The system detects this through the differ- ence between lexical ctxt value sense organ and the parse value nose, so that the entry is specialized ac- cordingly. Since the subjects synsem value is coin- dexed with the ﬁrst args element, ctxt nosesimulta- neously appears in the FS of perzipiert. However, the revisability clause matching there is of class general- izable, so union takes place, yielding ear  nose  sense organ (w.r.t. the simpliﬁed ontology of ﬁg. 1 used in this paper). An analogous match with the second element of args identiﬁes the necessary up- date to be the unioning-in of smell, the semantic type of Gestank. Finally, the system has learned that an accusative NP object can cooccur with perzipiert, so the argument structure type of gen receives another update through union with npnom npacc. 3 Discussion The incremental lexical acquisition approach de- scribed above attains the goals stated earlier. It re- alizes a gradual, information-based conceptualiza- tion of unknownness by providing updateable formal types  classiﬁed as either generalizable or special- izable  together with grammar-deﬁned revisability clauses. It maximally exploits standard HPSG rep- resentations, requiring moderate rearrangements in grammars at best while keeping with the standard assumptions of typed uniﬁcation formalisms. One noteworthy demand, however, is the need for a type union operation. Parsing is conventional modulo a modiﬁed lexical lookup. The actual lexical revision is done in a domain-independent postprocessing step guided by the revisability clauses. Of course there are areas requiring further consid- eration. In contrast to humans, who seem to leap to conclusions based on incomplete evidence, our ap- proach employs a conservative form of generaliza- tion, taking the disjunction of actually observed val- ues only. While this has the advantage of not leading to overgeneralization, the requirement of having to encounter all subtypes in order to infer their com- mon supertype is not realistic (sparse-data problem). In (2) sense organ as the semantic type of the ﬁrst argument of perzipiert is only acquired because the simpliﬁed hierarchy in ﬁg. 1 has nose and ear as its only subtypes. Here the work of Li  Abe (1995) who use the MDL principle to generalize over the slots of observed case frames might prove fruitful. An important question is how to administrate alternative parses and their update hypotheses. In Das Aktionspotential erreicht den Dendriten the action potential reaches the dendrite(s), Dendriten is ambiguous between acc.sg. and dat.pl., giving rise to two valence hypotheses npnom npacc and npnom npdat for erreicht. Details remain to be worked out on how to delay the choice between such alternative hypotheses until further contexts provide enough information. Another topic concerns the treatment of cooc- currence restrictions. In ﬁg. 2 the system has in- dependently generalized over the selectional restric- tions for subject and object, yet there are clear cases where this overgenerates (e.g. Das Ohr perzipiert den Gestank the ear perceives the stench). An idea worth exploring is to have a partial, extensible list of type cooccurrences, which is traversed by a recursive principle at parse time. A more general issue is the apparent antagonism between the desire to have both sharp grammatical predictions and continuing openness to contextual revision. If after parsing (7) we transfer the fact that smells are acceptable objects to perzipiert into the re- stricting ctxt feature, a later usage with an object of type sound fails. The opposite case concerns newly acquired specializable values. If in a later context these are used to update a gen value, the result may be too general. It is a topic of future research when to consider information certain and when to make re- visable information restrictive. References Bouma, G. (1997). Valence Alternation without Lexi- cal Rules. In: Papers from the seventh CLIN Meet- ing 1996, Eindhoven, 2540. Brent, M. R. (1991). Automatic Acquisition of Subcat- egorization Frames From Untagged Text. In: Pro- ceedings of 29th ACL, Berkeley, 209214. Dorre, J.  M. Dorna (1993). CUF  A Formalism for Linguistic Knowledge Representation. In: J. Dorre (Ed.), Computational Aspects of Constraint-Based Linguistic Description. IMS, Universitat Stuttgart. Deliverable R1.2.A, DYANA-2  ESPRIT Project 6852. Erbach, G. (1990). Syntactic Processing of Un- known Words. IWBS Report 131, Institute for Knowledge-Based Systems (IWBS), IBM Stuttgart. Hahn, U., M. Klenner  K. Schnattinger (1996). Learning from Texts - A Terminological Meta- Reasoning Perspective. In: S. Wermter, E. Riloff  G. Scheler (Ed.), Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Lan- guage Processing, 453468. Berlin: Springer. Hastings, P. M.  S. L. Lytinen (1994). The Ups and Downs of Lexical Acquisition. In: Proceedings of AAAI94, 754759. Knodel, H. (1980). Linder Biologie  Lehrbuch fur die Oberstufe. Stuttgart: J.B. Metzlersche Verlags- buchhandlung. Li, H.  N. Abe (1995). Generalizing Case Frames Us- ing a Thesaurus and the MDL Principle. In: Pro- ceedings of Recent Advantages in Natural Lan- guage Processing, Velingrad, Bulgaria, 239248. Manning, C.  I. Sag (1995). Dissociations between argument structure and grammatical relations. Ms., Stanford University. Pollard, C.  I. Sag (1994). Head-Driven Phrase Structure Grammar. Chicago University Press. Zernik, U. (1989). Paradigms in Lexical Acquisition. In: U. Zernik (Ed.), Proceedings of the First Inter- national Lexical Acquisition Workshop, Detroit.",
  "10.pdf": "arXiv:cs9809107v1 [cs.CL] 25 Sep 1998 Computing Declarative Prosodic Morphology Markus Walther Seminar fur Allgemeine Sprachwissenschaft Heinrich-Heine-Universitat Dusseldorf Universitatsstr. 1, D-40225 Dusseldorf, Germany waltherling.uni-duesseldorf.de Abstract This paper describes a computational, declarative approach to prosodic morphology that uses invio- lable constraints to denote small ﬁnite candidate sets which are ﬁltered by a restrictive incremental opti- mization mechanism. The new approach is illustrated with an implemented fragment of Modern Hebrew verbs couched in MicroCUF, an expressive con- straint logic formalism. For generation and parsing of word forms, I propose a novel off-line technique to eliminate run-time optimization. It produces a ﬁnite- state oracle that efﬁciently restricts the constraint in- terpreters search space. As a byproduct, unknown words can be analyzed without special mechanisms. Unlike pure ﬁnite-state transducer approaches, this hybrid setup allows for more expressivity in con- straints to specify e.g. token identity for reduplica- tion or arithmetic constraints for phonetics. 1 Introduction Prosodic morphology (PM) circumscribes a number of phenomena ranging from nonconatenative root- and-pattern morphology over inﬁxation to various cases of reduplication, where the phonology strongly inﬂuences the shape of words by way of obedience to structural constraints deﬁning wellformed morae, syllables, feet etc. These phenomena have been difﬁ- cult to handle in earlier rule-based treatments (Sproat 1992, 159 ff.). Moreover, as early as Kisseberth (1970) authors have noted that derivational accounts of PM are bound to miss important linguistic gen- eralizations that are best expressed via constraints. Kisseberth showed that verb stems in Tonkawa, a Coahuiltecan language, display a complex V al- ternation pattern when various afﬁxes are added (ﬁg. 1). This leads to more and more complicated vowel deletion rules as the fragment is enlarged. In contrast, a straightforward constraint that bans three consecu- tive consonants offers a uniﬁed account of the condi- tions under which vowels must surface. Later devel- to cut to lick picn-ob netl-ob (3sg.obj.stem-3sg.subj.) we-pcen-ob we-ntal-ob (3pl.obj.-stem-3sg.subj.) picna-n-ob netle-n-ob (3sg.obj.stem-prog.-3sg.subj.) p(i)c(e)n(a) n(e)t(a)l(e) stems Figure 1: Tonkawa verb forms with V effects opments have reﬁned constraints such as CCC to re- fer to syllable structure instead: complex codas and onsets are disallowed. At least since Kahn (1976), Selkirk (1982), such segment-independent reference to syllable structure has been standardly assumed in the generative literature. Astonishing as it may be, even the latest computa- tional models of PM phenomena apparently eschew the incorporation of real prosodic representations, syllabiﬁcation and constraints. Kiraz (1996) uses multi-tape two-level morphology to analyze some Arabic data, but  despite the suggestive title  must simulate prosodic operations such as add a mora by their extensionalized rule counterparts, which re- fer to C or V segments instead of moras. There is no on-line syllabiﬁcation and the exclusive use of lexi- cally prespeciﬁed syllable-like symbols on a separate templatic pattern tape renders his approach vulnera- ble to postlexical resyllabiﬁcation effects. Similarly, Beesley (1996) seems content in employing a great number of CV templates in his large-scale ﬁnite-state model of Arabic morphology, which are intersected with lexical roots and then transformed to surface re- alizations by various epenthesis, deletion and assim- ilation rules. Beesley states that further application of his approach to e.g. Hebrew is foreseen. On the downside, however, again there is no real prosody in his model; the relationship between template form and prosody is not captured. Optimality Theory (OT, Prince  Smolensky 1993), as applied to PM (McCarthy  Prince 1993), does claim to capture this relationship, using a ranked set of violable prosodic constraints together with global violation minimization. However, to date there exist no sufﬁciently formalized analyses of nontrivial PM fragments that could be turned into testable computational models. The OT framework itself has been shown to be expressible with weighted ﬁnite-state automata, weighted intersection and best- path algorithms (Ellison 1994) if constraints and OTs GEN component  the function from under- lying forms to prosodiﬁed surface forms  are reg- ular sets. A recent proposal by Karttunen (1998) dispenses with the weights while still relying on the same regularity assumption. Published PM anal- yses, however, frequently make use of constraint parametrizations from the ALIGN family, which re- quires greater than regular power (Ellison 1995). Further developments of OT such as correspondence theory  extensively used in much newer work on PM  have not received a formal analysis so far. Fi- nally, although OT postulates that constraints are uni- versal, this metaconstraint has been violated from the outset, e.g. in presenting Tagalog -um- as a language- speciﬁc parameter to ALIGN in Prince  Smolen- sky (1993). Due to the convincing presentation of a number of other forceful arguments against con- straint universality in Ellison (to appear), the case for language-speciﬁc constraints must clearly be seen as reopened, and  as a corollary  the case for con- straint inviolability as well. Declarative Phonology (DP, Bird 1995, Scobbie 1991) is just such a constraint-based framework that dispenses with violability and requires a monostratal conception of phonological grammar, as compared to the multi-level approaches discussed above. Both abstract generalizations and concrete morphemes are expressed by constraints. DP requires analyses to be formally adequate, i.e. use a grammar descrip- tion language with formal syntax and semantics. As a consequence, Chomskys criteria for a generative grammar which must be perfectly explicit and not rely on the intelligence of the understanding reader (Chomsky 1965, 4) are automatically fulﬁlled. DP thus appears to be a good starting point for a restric- tive, surface-true theory of PM that is explicitly com- putational. The rest of this paper reviews in informal terms the theory of Walther (1997) (section 2), showing in for- mal detail in section 3 how to implement a concrete analysis of Modern Hebrew verbs. Section 4 explains a novel approach to both generation and parsing of word forms under the new theory. The paper con- cludes in section 5. 2 Declarative Prosodic Morphology Focussing on cases of nonconcatenative root-and- pattern morphology, Declarative Prosodic Morphol- ogy (DPM) starts with an intuition that is opposite to what the traditional idea of templates or ﬁxed phono- logical shapes (McCarthy 1979) suggests, namely that shape variance is actually quite common and should form the analytical basis for theoretical ac- counts of PM. Besides the Tonkawa case (ﬁg.1), shape variance is also at work in Modern Hebrew (MH) inﬂected verb forms (Glinert 1989), see ﬁg. 2.1 Here we see a systematic V alternation of both past future 1sg gamar-ti be-gmor 2sg.m gamar-ta ti-gmor 2sg.f gamar-t ti-gmer-i 3sg.m gamar ji-gmor 3sg.f gamr-a ti-gmor 1pl gamar-nu ni-gmor 2pl gamar-tem ti-gmer-u 3pl gamr-u ji-gmer-u Figure 2: Modern Hebrew  g.m.r ﬁnish (B1) stem vowels, depending on the afﬁxation pattern. This results in three stem shapes CVCVC, CVCC and CCVC. Any analysis that simply stipulates shape se- lection on the basis of speciﬁc inﬂectional categories or phonological context (e.g. 3sg.f  3pl or -V  CVCCstem  B1 past) misses the fact that the shapes, their alternating behaviour and their proper selec- tion are derivable. Derivational repairs by means of doubly open syllable syncope rules (ga.ma.r-a.  .gam.ra.) are similarly ad hoc.  A ﬁrst step in developing an alternative DPM analysis of MH verbs is to explicitly recognize al- ternation of an element X with zero  informally written (X)  as a serious formal device besides its function as a piece of merely descriptive no- tation (cf. Hudson 1986 for an earlier application to Arabic). In contrast to nonmonotonic deletion or epenthesis, (X) is a surface-true declarative ex- pression (Bird 1995, 93f.). The reader is reminded 1Regular MH verbs are traditionally divided into seven ver- bal classes or binyanim, B1-B7. Except for B4 and B6, which regularly act as passive counterparts of B3 and B4, the semantic contribution of each class is no longer transparent in the modern language. Also, in many cases the root (written  C1.C2.C3) is restricted to an idiosyncratic subset of the binyanim. An a-templatic treatment of MH prosodic morphology was ﬁrst proposed by Bat-El (1989, 40ff.) within an unformalized, non-surface-true, non-constraint-based setting. that DP sees grammar expressions as partial for- mal descriptions of sets of phonological objects. The former reside on a different ontological level from the latter, in contrast to traditional object-to-object transformations on the same level. Hence a prelim- inary grammar expression g(V1)m(V2)r for a He- brew stem (with abstract stem vowels) denotes the set {gmr, gV1mr, gmV2r, gV1mV2r}. Note that the (X) property as attributed to segmental positions is distinctive  in contrast to stem vowels root seg- ments do not normally alternate with zero, and nei- ther do afﬁx segments in an important asymmetry with stems. This point is reinforced by the exceptions that do exist: phonologically unpredictable C al- ternation occurs in some MH stems, e.g. natanlaka[ he gavetook vs ji-ten ji-ka[ he will givetake; by surface-true (nl) encoding we can avoid diacritical solutions here.  Step two uses concatenation to combine indi- vidual descriptions of stems and afﬁxes, besides con- necting segmental positions within these linguistic entities. Since, as we have just seen, a single de- scription can denote several objects of varying sur- face string length, concatenation (ˆ) at the description level is actually powerful enough to describe non- concatenative morphological phenomena. In DPM these do not receive independent ontological status (cf. Bird  Klein 1990 and Gafos 1995 for other for- mal and articulatory-phonological arguments leading to the same conclusion). A more detailed description of the 3pl.fut. inﬂected form of  g.m.r might there- fore be jˆiˆgˆ(V1)ˆmˆ(V2)rˆu. In order to allow for paradigmatic2 generalizations over independent enti- ties such as root and stem vowel pattern within con- catenated descriptions, a hierarchical lexicon con- ception based on multiple inheritance of named ab- stractions can be used (cf. Riehemann 1993).  Step three conjoins a word form description with declarative syllabiﬁcation and syllable structure constraints in order to impose prosodic well- formedness conditions. For Modern Hebrew (and Tonkawa), the syllable canon is basically CV(C). Expressed in prosodic terms, complex codas and onsets are banned, while an onset must precede each syllable nucleus. These syllable roles are established in the ﬁrst place by syllabiﬁcation constraints that exploit local sonority differences between successive segments (Walther 1993). Alltogether, the ensemble 2See Walther (1997) for a discussion of various ways to de- rive rather than stipulate the syntagmatic pattern of alternating and non-alternating segmental positions within stems. of prosodic constraints indeed succeeds in narrow- ing down the set for the 3sg.m past tense form to {.gmr., .gamr., .gmar., !.ga.mar.}  gamar. For 3pl. future tense B1, how- ever, an unresolved ambiguity remains: in {.jig.me.ru., .ji.gam.ru.}, only the ﬁrst ele- ment is grammatical.3 An important observation is that in general there can be no purely phonological constraint to disambiguate this type of situation. The reason lies in the existence of minimal pairs with different category. In our case, homophonous .ji.gam.ru. is grammatical as 3pl. fut. B2 they will be ﬁnished. We will return to the analysis of such cases after proposing a speciﬁc disambiguation mechanism in the next step.  Step four eliminates the remaining ambiguity by invoking an Incremental Optimization Princi- ple (IOP): For all (X) elements, prefer the zero al- ternant as early as possible. Early corresponds to traditional left-to-right directionality, but is meant to be understood w.r.t. the speech production time ar- row. As possible means that IOP application to a (X) position nevertheless realizes X if its omis- sion would lead to a constraint conﬂict. Hence, the IOP correctly rules out the second element of {.jig.me.ru., .ji.gam.ru.}. This is because .ji.gam.ru. represents a missed chance to leave out a, the earlier one of the two stem vowels. The reader may verify that the IOP as it stands also accounts for the Tonkawa data of ﬁg. 1. Tonkawa lends even clearer support to IOPs left-to-right nature due to the larger number of V vowels involved. As a limit- ing case, the IOP predicts the possibility of vowel- less surface stems, e.g. formed by two root conso- nants combined with vowel-ﬁnal preﬁx and sufﬁx. This prediction is strikingly conﬁrmed by MH forms like te-l[-i you (sg.f.) will go  (h).l.[, ti-kn-u youthey (pl.) will buy  k.n., ti-tn-i you (sg.f.) will give  (n).t.n; similar cases exist in Tigrinya. There can be no meaningful prosodic characteriza- tion of isolated CC stem shapes; only a wordform- based theory like the present one may explain why these forms exist. Note that, conceptually, IOP is piggybacked on au- tonomous DP-style constraint interaction. It merely ﬁlters the small ﬁnite set of objects described by the conjunction of all constraints. From another an- gle, IOP can be seen as a single context-free sub- 3Note that the prosodic view explains the pronounced inﬂu- ence of (C)V afﬁxes on the shape of the whole word: they pro- vide a nonalternating syllable nucleus which can host adjacent stem consonants. stitute for the various syncope rules employed in former transformational analyses. The claim is that ﬁxed-directionality-IOP is the only such mechanism needed to account for PM phenomena. A distinguishing feature of the IOP is its poten- tial for an economical procedural implementation in incremental production. If constraint contexts are sufﬁciently local, the principle can locally decide over (X) nonrealizations and there will be very lim- ited backtracking through delayed detection of con- straint violation. Because the IOP stops after ﬁnd- ing the ﬁrst (X) realization pattern that violates no constraints, it has less formal power than global opti- mization which must always consider all candidates. Moreover, the IOP supports economic communica- tion, as it leads to shortest surface forms wherever possible. Finally, at least for root-and-pattern mor- phologies it can be argued to aid in speech per- ception as well. This is because the closed class of stem vowel patterns is less informative than open- class root segments. Since IOP-guided vowel omis- sion causes root segments to (statistically) appear at an earlier point in time from the acoustic onset of the word, the IOP hypothesis actively prunes the size of the cohort of competing lexical candidates. As a result, unambigous recognition will generally be achieved more quickly during continous lexical access. In sum, the IOP hypothesis not only pos- sesses overall psycholinguistic plausibility but actu- ally gives some processing advantage to shape vari- ance. If future research provides the necessary ex- perimental conﬁrmation, we have yet another case of performance shaping competence.  Step ﬁve returns to the minimal pairs problem highlighted in step three: what to do with anti-IOP realizations such as that of a in .ji.gam.ru. for B2 fut.? The answer is (prosodic) prespeciﬁcation. A surface-true constraint demands that B2 future and inﬁnitive as well as all of B3, B4 must have an onset role for the ﬁrst stem element. Thus, the possibility of IOP eliminating the ﬁrst stem vowel is blocked by the constraint inconsistency that arises for the ﬁrst stem element: either syllabiﬁcation licenses an in- compatible coda or ﬁrst and second stem segment together form an illformed onset cluster. Note that if the constraint is lexicalized as part of the grammat- ical description of ﬁrst stem position, it will have a maximally local context, referring to just the position itself. In general, DPM analyses pay much attention to proper attachment sites of constraints in order to maximize their locality. The MH verbal sufﬁx -et (fem.sg.pres.) illustrates that sometimes another, segmental mode of prespec- iﬁcation is useful. This sufﬁx is always preceded by a syllable ending in e, although IOP application alone would e.g. prefer gom.ret over go.me.ret she ﬁnishes. The effect is morpheme-speciﬁc since other -VC sufﬁxes behave as expected here: gomr- imot they (masc.fem.) ﬁnish. One solution is to let part of the sufﬁx deﬁnition be a constraint statement which demands that the segment two positions to its left must be a front vowel. This move captures both the stability and the quality of this vowel at the same time. (Apophony constraints ensure that the second stem vowel is never i except in B5, which signiﬁ- cantly has a different sufﬁx -a in place of -et). Note that prespecifying the presufﬁxal segment to be in an onset position would not work. 3 On implementing analyses In the following I show how to implement a toy frag- ment of MH verbs using the MicroCUF formalism, a typed, feature-based constraint-logic programming language suitable for natural language modelling. MicroCUF implements a subset of CUF (Dorre  Dorna 1993), inheriting its formal semantics. It was initially delevoped by the author to overcome efﬁ- ciency problems with CUFs original type system. Additionally, its simpler implemenation provides an open platform for experimental modiﬁcations, as needed e.g. for parsing and generation with DPM. After brieﬂy introducing the essentials of MicroCUF ﬁrst, the MH analysis is developed and explained. 3.1 The MicroCUF constraint formalism This section assumes a basic knowledge of Prolog. Like in Prolog, MicroCUF variables start with upper- case letters or , whereas relational symbols, fea- tures and simplex types start in lowercase;  marks a comment (ﬁg. 3a). Relations like member are writ- ten in functional notation, with a notationally distin- guished result argument on the righthand side of : and the relation symbol plus its (optional) arguments on the lefthand side. Subgoals like member(Elem) can occur anywhere as subterms. Instead of Prologs ﬁxed-arity ﬁrst order terms, MicroCUF has typed feature terms as its basic data structures. As illus- trated in ﬁg. 3b, subterms are explicitly conjoined with  or disjunctively combined with ;, while only type terms may be preﬁxed by the negation oper- ator . Features like left, cat are separated from their righthand value terms by :. Terms may be tagged by conjunction with a variable (V1), allowing for the expression of structure sharing through mul- tiple occurences of the same variable. Feature appro- priateness declarations (::) ensure that both the term in which a feature occurs and its value are typed. For comparison, the result value of fs appears in HPSG- style notation under ﬁg. 3c. a.  MicroCUF member(Elem) : [Elem_]. member(Elem) : [_member(Elem)]. Prolog member(Elem,[Elem_]). member(Elem,[_Rest]) :- member(Elem,Rest). b. fs:cat:(((b2;b3;b5)past)V1)left:cat:V1. phonlist::[cat:categories]. c. fs    phonlist cat 1  ( (b2  b3  b5)  past ) left phonlist cat 1    Figure 3: MicroCUF vs Prolog and HPSG notation 3.2 Modern Hebrew verbs in MicroCUF Below I present a concrete MicroCUF grammar in successive pieces. It encodes a toy fragment of MH verbs and represents a simpliﬁed excerpt from a much larger computational grammar. For lack of space, the type hierarchy  specifying syl- lable roles, segments, morphological categories and word-peripheral position  and the deﬁnition of syllabify (formalized in Walther 1995) have been omitted. Let us start the explanation with a basic concatenation relation which adds a position Self in front of some string of Segments (1-6). 1 conc(Self, Segments) : 2 Self  3 right:(Segmentsleft:Selfcat:Cat)  4 cat:Cat  5 classify_position_in_word  6 constraints. 7 8 classify_position_in_word : 9 right:self:-ini  left:self:-fin. Here, the familiar recursive ﬁrst-rest encoding of lists translates into self-right features. This alone makes self and (arbitrarily long) right-context ref- erences possible. To support looking one or more segmental positions to the left  a frequent situation in phonological contexts  we supplement it with a new feature left to yield bidirectional lists. For this doubly-linked list encoding to be wellbehaved, a step right followed by a step left is constrained to return to the same position Self (3), thus yielding cyclic fea- ture structures. Next, the value of the feature cat at the current position is connected with its right neigh- bour (3-4). In the face of our recursively structured lists this makes morphological and other global cate- gorial information locally accessible at each segmen- tal position. Finally, relations to incrementally clas- sify each segmental position as word-initial, medial or wordﬁnal and to impose prosodic constraints are added in (5-6). Basic concatenation is used in (10-12) to deﬁne X positions. 10 x_0(_, Segments) : Segments. 11 x_0(X, Segments) : mark:marked  12 conc(X, Segments). 13 14 obl(X, Segments) : mark:unmarked  15 conc(X, Segments). 16 17 is(Segment) : self:seg:Segment. The ﬁrst clause of x 0 (10) realizes the zero alternant by equating in its second argument the Segments to follow with the result argument; the ﬁrst argument holding X is unused. It gets used in the second clause (11-12), however, where it is preﬁxed to the follow- ing Segments by ordinary concatenation. The value of an additional feature mark speciﬁes that realiz- ing an X position is marked w.r.t. the IOP, whereas no such value is prescribed in the ﬁrst clause. In- stead, the marking there will be supplied later by ad- jacent instances of either the second x 0 clause or obl (14-15). The latter is the version of concatena- tion used for specifying obligatory, i.e. nonalternat- ing positions, which consequently are speciﬁed as unmarked. Alltogether these means yield fully spec- iﬁed strings w.r.t. markedness information. We will see below how this simpliﬁes an implementation of the IOP. As can be seen in the accessor relation is (17), phonological segments are actually embedded un- der a further feature seg. This treatment enables structure-sharing of segments independent of their syllable roles. The syllable shape constraint (18-25) shows ﬁrst of all that syllable roles are modelled as types under self. 18 shape : 19 ( self:(nucleus  seg:vowel)  20 left:self:onset 21 ; self:(nucleus)  22 ( self:onset  left:self:(onset) 23 ; self:coda  left:self:(coda) 24 ) 25 ). 26 27 constraints : syllabify  shape. Lines (19-20) capture the fact that syllable nuclei in MH are always vowels and that every syllable nucleus is preceded by an onset. In (21-22) a non- nuclear position that is an onset may only license preceding non-onsets, thus disallowing complex on- sets; similarly for codas in (23). In (27) generic syllabify is intersected with shape, since seg- mental positions must be prosodiﬁed and conform to language-speciﬁc shape restrictions. The constraints under (28-30), included for com- pleteness, merely ensure proper termination of seg- mental strings at the word periphery. 28 word : self:(ini  prom:up  onset). 29 end : left:self:(fin  onset)  30 self:-fin. Prosodic prespeciﬁcation (31-36) faithfully models what was stated in prose in section 2. 31 prosodic_prespecification : 32 ( cat:((b2((past;pres)));b3;b4)  33 self:onset 34 ; 35 cat:(((b2 ((past;pres)));b3;b4)) 36 ). We proceed in (37-41) with a rudimentary deﬁnition of ﬁrst (v1) and second (v2) stem vowel which is sufﬁcient for our toy fragment. 37 v1 : is(low)  cat:(past  b1;b7). 38 v1 : is(round  -hi)  39 cat:(b1   past). 40 v2 : is(low). 41 v2 : is(front  -hi). The larger grammar mentioned above contains a full binary decision tree for each vowel. Still, even here one can see the use of type formulae like round  -hi to classify segments phonologically. Next come a number of exemplary inﬂectional af- ﬁxes (42-79), again simpliﬁed. The zero afﬁxes (42- 45, 47-54) are phonologically just like the zero alter- nant in (10) in taking up no segmental space. 42  initial \"0\" prefix 43 (More) : More  self:ini  44 cat:( fut   infinitive  45 ( b1 ; ( pres  (b3 ; b4)) )). 46 47  final \"0\" suffix 48 (More) : More  49 self:-ini  left:self:fin  50 ( cat:(sg  masc  third  past)  51 left:left:is(front) 52 ; cat:(sg  masc  third  pres)  53 left:left:is(front) 54 ). 55 56  overt prefix 57 ji(More) : self:ini  58 obl(is(i),obl(is(i), More))  59 cat:(fut  third  ((sgmasc) ; pl)  60 (b1 ; b2)). The segmental content of all other afﬁxes is spec- iﬁed via possibly repeated instances of obl, since afﬁxes are nonalternating. Apart from the respective categorial information, positional type information ini,fin ensures that preﬁxes and sufﬁxes are properly restricted to wordinitial and wordﬁnal position. Note that the glide-initial ji- preﬁx speciﬁes an initial i (58) which will be prosodiﬁed as onset by means of syllabify. This representational as- sumption is in line with other recent work in phono- logical theory which standardly analyzes glides as nonsyllabic high vowels. Hence, even in MH we have a case where segmental classes and prosodic roles dont align perfectly. To control second stem vowel apophony, some sufﬁxes demand (53,73) or forbid (51) front vowels two positions to their left. 61 u(More) : obl(is(u)self:fin,More) 62 left:left:is( (vowel  front))  63 cat:(pl  ( (past  third) 64 ; (fut   first) )). 65 66 a(More):obl(is(a)self:fin,More) 67 left:left:is( (vowel  front))  68 cat:((past  third  sg  fem) 69 ; (pres  sg  fem  b5)). 70 71 et(More) : 72 obl(is(e),obl(is(t)self:fin,More)) 73 left:left:is(front)  74 cat:(pres  sg  fem  b5). 75 76 im(More) : 77 obl(is(i),obl(is(m)self:fin,More)) 78 left:left:is( (vowel  front))  79 cat:(pres  pl  masc). Others posit the weaker demand vowel  front (62,67,78), thus not forbidding consonantal ﬁllings of the position adressed by left:left. The stem deﬁnition (80-82) for a regular triliteral is parametrized for the three root segments and the inﬂectional Suffixes to follow. 80 stem(C1, C2, C3, Suffixes) : 81 obl(is(C1),x_0(v1,obl(is(C2), 82 x_0(v2,obl(is(C3), Suffixes))))). 83 84 affixes(Stem, (end)) : (Stem). 85 affixes(Stem, a(end)) : (Stem). 86 affixes(Stem, et(end)) : (Stem). 87 affixes(Stem, im(end)) : (Stem). 88 affixes(Stem, u(end)) : yi(Stem). 89 90 verbform([C1  consonant,C2  consonant, 91 C3  consonant], Category) : 92 root_letter_tree([C1,C2,C3])  word  93 affixes( prosodic_prespecification  94 stem(C1,C2,C3, Suffixes), 95 Suffixes)  cat:Category. Given the informal description in section 2, the suc- cession of obligatory root and alternating stem vowel positions now looks familiar. It should be obvi- ous how to devise analogous stem deﬁnitions for quadriliterals (e.g. mi[Mev) and cluster verbs (e.g. ﬂirtet). A rather simple tabulation of affixes lists (a subset of) the allowable preﬁx-sufﬁx cooccur- rences in the MH verbal paradigm (84-88) be- fore everything is put together in the deﬁni- tion for verbform, parametrized for a list of root segments and Category (90-95). Note how prosodic prespecification is intersected with stem in (93-94), exploiting the power of the descrip- tion level to restrict stem realizations without diacrit- ical marking of stem vs afﬁx domains on the object level. The subgoal root letter tree (92) will be discussed below. When proving a goal like verbform([g,m,r], b1thirdplfut), the MicroCUF interpreter will enumerate the set of all candidate result feature structures, including one that describes the gram- matical surface string jigmeru. An implementation of the IOP, to be described next, must therefore complement the setup established sofar to exclude the suboptimal candidates. While the subtle inter- twining of zero alternant preference and constraint solving described above has its theoretical merits, a much simpler practical solution was devised. In a ﬁrst step, the small ﬁnite set of all candidate solu- tions for a goal is collected, together with numerical disharmony values representing each candidates degree of optimality. Disharmony is deﬁned as the binary number that results from application of the mapping {unmarked  012, marked  102} to the left-to-right markedness vector of a segmental string: e.g., j01i01g01a10m01r01u01 yields the dishar- mony value 010101100101012  552510  547710  010101011001012 from j01i01g01m01e10r01u01. Step two is a straightforward search for the candidate(s) with minimal disharmony. 4 Parsing and generation The preceding paragraph described how to com- pute surface forms given roots and categories. How- ever, this generation procedure amounts to an inefﬁ- cient generate-and-minimize mechanism which must compute otherwise useless suboptimal candidates as a byproduct of optimization. More importantly, due to the nonmonotonicity of optimization it is not obvi- ous how to invert the procedure for efﬁcient parsing in order to derive root and category given a surface form. A ﬁrst solution which comes to mind is to im- plement parsing as analysis-by-synthesis. A goal like ParseStringverbform(Root,Category) is submitted to a ﬁrst run of the MicroCUF con- straint solver, resulting in instantiations for Root and Category iff a proof consistent with the grammar was found. With these instantiations, a second run of MicroCUF uses the full generate- and-minimize mechanism to compute optimal strings OptString1, . . . ,OptStringN. The parse is accepted iff ParseString(OptString1; . . . ;OptStringN) is consistent. Note that for this solution to be feasible it is essential that constraints are inviolable, hence their evaluation in the ﬁrst run can disregard optimization. The main drawbacks of analysis-by-synthesis are that two runs are required and that the inefﬁciencies of generate-and-minimize are not avoided. The new solution recognizes the fact that bidi- rectional processing of DPM would be easy with- out optimization. We therefore seek to perform all optimization at compile time. The idea is this: ex- ploiting the ﬁniteness of natural language paradigms we compute  using generate-and-minimize  each paradigm cell of e.g. the verbal paradigm of MH for a suitable root. However, while doing so we record the proof sequence of relational clause invocations employed in the derivation of each optimal form, us- ing the fact that each clause has a unique index in internal representation. Such proof sequences have two noteworthy properties. By deﬁnition they ﬁrst of all record just clause applications, therefore nat- urally abstracting over all non-relational parameter ﬁllings of top-level goals. In particular, proving a goal like verbform([g,m,r], b1;b2) normally looses the information associated with the root and category parameters in the proof sequence represen- tation (although these parameters could indirectly in- ﬂuence the proof if relationally encoded choices in the grammar were dependent on it). Secondly, we can proﬁtably view each proof sequence as a linear ﬁnite state automaton (FSAcell). Since a paradigm is the union of all its cells, a complete abstract paradigm can therefore be represented by a unique minimal deterministic FSApara which is computed as the union of all FSAcell followed by determiniza- tion and minimization. At runtime we just need to run FSApara as a ﬁnite-state oracle in parallel with the MicroCUF constraint solver. This means that each proof step that uses a clause k must be sanc- tioned by a corresponding k-labelled FSA transition. With this technique we are now able to efﬁciently re- strict the search space to just the optimal proofs; the need for run-time optimization in DPM processing has been removed. However, a slight caveat is nec- essary: to apply the technique it must be possible to partition the data set into a ﬁnite number of equiv- alence classes. This condition is e.g. automatically fulﬁlled for all phenomena which exhibit a paradigm structure. What are the possible advantages of this hybrid FSA-guided constraint processing technique? First of all, it enables a particularly simple treatment of unknown words for root-and-pattern morphologies, surely a necessity in the face of ever-incomplete lexicons. If the grammar is set up properly to ab- stract from segmental detail of the Root segments as much as possible, then these details are also ab- sent in the proof sequences. Hence a single FSApara merging these sequences in effect represents an ab- stract paradigm which can be used for a large number of concrete instantiations. We thus have a principled way of parsing words that contain roots not listed in the lexicon. However, we want the system not to overgenerate, mistakenly analyzing known roots as unknown. Rather, the system should return the se- mantics of known roots and also respect their verbal class afﬁliations as well as other idiosyncratic prop- erties. This is the purpose of the root letter tree clauses in (96-123). 96 root_letter_tree([gRest]) : 97 root_letter_tree_g(Rest). 98 root_letter_tree([g_]) : 99 cat:sem:UNKNOWN. 100 101 root_letter_tree_g([mRest]) : 102 root_letter_tree_gm(Rest). 103 root_letter_tree_g([dRest]) : 104 root_letter_tree_gd(Rest). 105 root_letter_tree_g([md_]) : 106 cat:sem:UNKNOWN. 107 108 root_letter_tree_gm([r]) : 109 cat:(b1  sem:FINISH 110 ; b2  sem:BE FINISHED). 111 root_letter_tree_gm([r_]) : 112 cat:sem:UNKNOWN. 113 root_letter_tree_gd([r]) : 114 cat:( b1  sem:ENCLOSE 115 ; b2  sem:BE ENCLOSED 116 ; b3  sem:FENCE IN 117 ; b4  sem:BE FENCED IN 118 ; b5  sem:DEFINE 119 ; b6  sem:BE DEFINED 120 ; b7  sem:EXCEL 121 ). 122 root_letter_tree_gd([r_]) : 123 cat:sem:UNKNOWN. For each level in the letter tree a new terminal branch is added that covers the complement of all at- tested root segments at that level (99,106,112,123). This terminal branch is assigned an UNKNOWN se- mantics, whereas known terminal branches record a proper semantics and categorial restrictions. Dur- ing off-line creation of the proof sequences we now simply let the system backtrack over all choices in the root letter tree by feeding it a totally un- derspeciﬁed Root parameter. The resulting FSApara represents both the derivations of all known roots and of all possible unknown root types covered by the grammar. While this treatment results in a ho- mogenous grammar integrating lexical and gram- matical aspects, it considerably enlarges FSApara. It might therefore be worthwhile to separate lexical ac- cess from the grammar, running a separate proof of root letter tree(Root) to enforce root-speciﬁc restrictions after parsing with the abstract paradigm alone. It remains to be seen which approach is more promising w.r.t. overall space and time efﬁciency. A second advantage of separating FSA guidance from constraint processing, as compared to pure ﬁnite-state transducer approaches, is that we are free to build sufﬁcient expressivity into the constraint lan- guage. For example it seems that one needs token identity, i.e. structure sharing, in phonology to cover instances of antigemination, assimilation, dissimila- tion and reduplication in an insightful way. It is well- known that token identity is not ﬁnite-state repre- sentable and cumbersome to emulate in practice (cf. Antworth 1990, 157 on a FST attempt at reduplica- tion vs the DPM treatment of inﬁxal reduplication in Tigrinya verbs described in Walther 1997, 238- 247). Also, it would be fascinating to extend the constraint-based approach to phonetics. However, a pilot study reported in Walther  Kroger (1994) has found it necessary to use arithmetic constraints to do so, again transcending ﬁnite-state power. Finally, to the extent that sign-based approaches to grammar like HPSG are on the right track, the smooth integra- tion of phonology and morphology arguably is bet- ter achieved within a uniform formal basis such as MicroCUF which is expressive enough to cover the recursive aspects of syntax and semantics as well. In conclusion, some notes on the pilot implemen- tation. The MicroCUF system was modiﬁed to pro- duce two new incarnations of the MicroCUF inter- preter, one to record proof sequences, the other to perform FSA-guided proofs. FSApara was created with the help of ﬁnite-state tools from ATTs freely available fsm package (http:www. research. att. com sw tools fsm). I have measured speedups of more than 102 for the generation of MH forms ( 1 second with the new technique), although parse times in the range of 1 . . . 4 seconds on a Pen- tium 200 MHz PC with 64 MByte indicate that the current prototype is still too slow by a factor of more than 102. However, there is ample room for future improvements. Besides drawing from the wealth of optimizations found in the logic programming liter- ature to generally accelerate MicroCUF (e.g., term encoding of feature structures, memoization) we can also analyze the internal structure of FSApara to gain some speciﬁc advantages. This is due to the fact that each maximal linear sub-FSA of length k  1 corre- sponds to a deterministic proof subsequence whose clauses should be partially executable at compile time, subsequently saving k  1 proof steps at run- time. 5 Conclusion This paper has described a computational, declara- tive approach to prosodic morphology which uses inviolable constraints formulated in a sufﬁciently ex- pressive formalism (here: MicroCUF) together with a restrictive incremental optimization component. The approach has been illustrated by implementing an a-templatic analysis of a fragment of Modern He- brew verbs. The full grammar behind the illustrative fragment covers additional detail such as antigem- ination effects (noded-im, nodd-im they (masc.) wander), spirantization, B7 sibilant metathesis, etc. Also, the formalization of X presented here is actually a special case of the more powerful no- tion of resequencing, whose application to Tigrinya vowel coalescence and metathesis was demonstrated in Walther (1997). Despite the initial emphasis on incremental op- timization, a compilation technique was later pro- posed to remove the need for run-time optimiza- tion and guarantee fully bidirectional processing of prosodic morphology. Although the general idea of using a ﬁnite-state oracle to guide a parser has been previously proposed for context-free grammars (Johnson 1996), both the details of our implemen- tation of the idea and its speciﬁc application to prosodic morphology are believed to be novel. It was emphasized how the proposed technique aided in a simple treatment of unknown words. Note that un- known words are not normally integrated into ﬁnite- state transducer models of prosodic morphology, al- though the necessary extensions appear to be pos- sible (K. Beesley, p.c.). Finally, the fact that a hy- brid setup rather than a pure ﬁnite-state approach was chosen has been motivated inter alia by refer- ence to additional phenomena such as antigemina- tion and reduplication that require the richer notion of token identity. Future research will especially fo- cus on detailed analyses of reduplication phenomena to secure the relevance of the present approach to prosodic morphology at large. References Antworth, E. (1990). PC-KIMMO: A Two-Level Pro- cessor for Morphological Analysis. Dallas: SIL. Bat-El, O. (1989). Phonology and Word Structure in Modern Hebrew. Ph.D. thesis, UCLA. Beesley, K. R. (1996). Arabic Finite-State Morpholog- ical Analysis and Generation. In: Proceedings of COLING-96, vol. I, 8994. Bird, S. (1995). Computational Phonology. Cambridge University Press. Bird, S.  E. Klein (1990). Phonological events. Jour- nal of Linguistics 26, 3356. Chomsky, N. (1965). Aspects of the Theory of Syntax. Cambridge, MA: MIT Press. Dorre, J.  M. Dorna (1993). CUF  A Formalism for Linguistic Knowledge Representation. In: J. Dorre (Ed.), Computational Aspects of Constraint-Based Linguistic Description. IMS, Universitat Stuttgart. Deliverable R1.2.A, DYANA-2  ESPRIT Basic Research Project 6852. Ellison, T. M. (1994). Phonological Derivation in Op- timality Theory. In: Proceedings of COLING 94, vol. II, 10071013. (ROA-75, CMP-LG 9504021). Ellison, T. M. (1995). OT, Finite-State Representations and Procedurality. In: Proceedings of the Confer- ence on Formal Grammar, Barcelona. Ellison, T. M. (to appear). The Universal Constraint Set: Convention, not Fact. In: J. Dekkers, F. van der Leeuw  J. van de Weijer (Ed.), Conceptual Stud- ies in Optimality Theory. Oxford University Press. Gafos, A. (1995). On the Proper Characterization of Nonconcatenative Languages. Ms., Department of Cognitive Science, The Johns Hopkins Univer- sity, Baltimore. (ROA-106). Glinert, L. (1989). The grammar of Modern Hebrew. Cambridge University Press. Hudson, G. (1986). Arabic root and pattern morphol- ogy without tiers. Journal of Linguistics 22, 85 122. Johnson, M. (1996). Left Corner Transforms and Fi- nite State Approximations.Tech report MLTT-026, Rank Xerox Research Centre, Grenoble. Kahn, D. (1976). Syllable-Based Generalizations in English Phonology. Bloomington: Indiana Univer- sity Linguistics Club. ( MIT Ph.D. dissertation). Karttunen, L. (1998). The Proper Treatment of Opti- mality in Computational Phonology. In: Proceed- ings of FSMNLP98. International Workshop on Finite-State Methods in Natural Language Pro- cessing, Bilkent University, Ankara, 112. Kiraz, G. A. (1996). Computing Prosodic Morphology. In: Proceedings of COLING 96, vol. II, 664669. Kisseberth, C. (1970). On the functional unity of phonological rules. Linguistic Inquiry 1, 291306. McCarthy, J. (1979). Formal Problems in Semitic Phonology and Morphology. Ph.D. thesis, MIT. McCarthy, J.  A. Prince (1993). Prosodic Morphol- ogy I: Constraint Interaction and Satisfaction. Tech report RuCCS-TR-3, Rutgers University Center for Cognitive Science. Prince, A.  P. Smolensky (1993). Optimality The- ory. Constraint Interaction in Generative Gram- mar. Tech report RuCCS-TR-2, Rutgers University Center for Cognitive Science. Riehemann, S. (1993). Word Formation in Lexical Type Hierarchies: A Case Study of bar-Adjectives in German. Masters thesis, Universitat Tubingen. (also: SfS-Report-02-93, Seminar fur Sprachwis- senschaft, Universitat Tubingen). Scobbie, J. M. (1991). Towards Declarative Phonol- ogy. In: S. Bird (Ed.), Declarative Perspectives on Phonology, vol. 7 of Edinburgh Working Papers in Cognitive Science, 126. Centre for Cognitive Sci- ence, University of Edinburgh. Selkirk, E. (1982). The syllable. In: H. van der Hulst  N. Smith (Ed.), The structure of phonological representations, vol. II, 337383.Dordrecht: Foris. Sproat, R. (1992). Morphology and Computation. Cambridge, Mass.: MIT Press. Walther, M. (1993). Declarative Syllabiﬁcation with Applications to German. In: T. M. Ellison  J. M. Scobbie (Ed.), Computational Phonology, vol. 8 of Edinburgh Working Papers in Cognitive Science, 5579. Centre for Cognitive Science, University of Edinburgh. Walther, M. (1995). A Strictly Lexicalized Approach to Phonology. In: J. Kilbury  R. Wiese (Ed.), Pro- ceedings of DGfSCL95, 108113. Dusseldorf: Deutsche Gesellschaft fur Sprachwissenschaft, Sektion Computerlinguistik. Walther, M. (1997). Deklarative prosodische Mor- phologie  constraintbasierte Analysen und Com- putermodelle zum Finnischen und Tigrinya. Ph.D. thesis, Philosophische Fakultat der Heinrich- Heine-Universitat Dusseldorf. Walther, M.  B. Kroger (1994). Phonologie- Phonetik-Kopplung in einem constraintbasierten gesturalen Modell. In: H. Trost (Ed.), Tagungs- band KONVENS 94, Nummer 6 in Infor- matik Xpress, 387396. Wien: Osterreichische Gesellschaft fur Artiﬁcial Intelligence.",
  "11.pdf": "arXiv:cs9809112v1 [cs.CL] 28 Sep 1998 On the Evaluation and Comparison of Taggers: the Eﬀect of Noise in Testing Corpora. Lluıs Padro and Lluıs Marquez Dep. LSI. Technical University of Catalonia c Jordi Girona 1-3. 08034 Barcelona {padro,lluism}lsi.upc.es Abstract This paper addresses the issue of pos tagger evaluation. Such evaluation is usually per- formed by comparing the tagger output with a reference test corpus, which is assumed to be error-free. Currently used corpora contain noise which causes the obtained performance to be a distortion of the real value. We analyze to what extent this distortion may invalidate the com- parison between taggers or the measure of the improvement given by a new system. The main conclusion is that a more rigorous testing exper- imentation settingdesigning is needed to reli- ably evaluate and compare tagger accuracies. 1 Introduction and Motivation Part of Speech (pos) Tagging is a quite well deﬁned nlp problem, which consists of assign- ing to each word in a text the proper mor- phosyntactic tag for the given context. Al- though many words are ambiguous regarding their pos, in most cases they can be completely disambiguated taking into account an adequate context. Successful taggers have been built us- ing several approaches, such as statistical tech- niques, symbolic machine learning techniques, neural networks, etc. The accuracy reported by most current taggers ranges from 9697 to al- most 100 in the linguisticallymotivated Con- straint Grammar environment. Unfortunately, there have been very few di- rect comparisons of alternative taggers1 on iden- tical test data. However, in most current papers it is argued that the performance of some tag- gers is better than others as a result of some kind of indirect comparisons between them. We 1One of the exceptions is the work by (Samuelsson and Voutilainen, 1997), in which a very strict comparison between taggers is performed. think that there are a number of not enough controlledconsidered factors that make these conclusions dubious in most cases. In this direction, the present paper aims to point out some of the diﬃculties arising when evaluating and comparing tagger performances against a reference test corpus, and to make some criticism about common practices followed by the nlp researchers in this issue. The above mentioned factors can aﬀect ei- ther the evaluation or the comparison process. Factors aﬀecting the evaluation process are: (1) Training and test experiments are usually per- formed over noisy corpora which distorts the ob- tained results, (2) performance ﬁgures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable esti- mations of accuracy (Mooney, 1996), (3) testing experiments are usually done on corpora with the same characteristics as the training data usually a small fresh portion of the training corpus but no serious attempts have been done in order to determine the reliability of the re- sults when moving from one domain to another (Krovetz, 1997), and (4) no ﬁgures about com- putational eﬀort spacetime complexity are usually reported, even from an empirical per- spective. A factors aﬀecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multipletrial experiment with statistical tests of signiﬁcance. For these reasons, this paper calls for a dis- cussion on pos taggers evaluation, aiming to establish a more rigorous test experimentation settingdesigning, indispensable to extract reli- able conclusions. As a starting point, we will focus only on how the noise in the test corpus can aﬀect the obtained results. 2 Noise in the testing corpus From a machine learning perspective, the rele- vant noise in the corpus is that of non system- atically mistagged words (i.e. diﬀerent annota- tions for words appearing in the same syntac- ticsemantic contexts). Commonly used annotated corpora have noise. See, for instance, the following examples from the Wall Street Journal (wsj) corpus: Verb participle forms are sometimes tagged as such (vbn) and also as adjectives (jj) in other sentences with no structural diﬀerences: 1a) ... failing VBG to TO voluntarily RB submit VB the DT requested VBN information NN ... 1b) ... a DT large JJ sample NN of IN married JJ women NNS with IN at IN least JJS one CD child NN ... Another structure not coherently tagged are noun chains when the nouns (nn) are ambigu- ous and can be also adjectives (jj): 2a) ... Mr. NNP Hahn NNP , , the DT 62-year-old JJ chairman NN and CC chief NN executive JJ oﬃcer NN of IN Georgia-Pacific NNP Corp. NNP ... 2b) ... Burger NNP King NNP s POS chief JJ executive NN oﬃcer NN , , Barry NNP Gibbons NNP , , stars VBZ in IN ads NNS saying VBG ... The noise in the test set produces a wrong estimation of accuracy, since correct answers are computed as wrong and vice-versa. In following sections we will show how this uncertainty in the evaluation may be, in some cases, larger than the reported improvements from one system to another, so invalidating the conclusions of the comparison. 3 Model Setting To study the appropriateness of the choices made by a pos tagger, a reference tagging must be selected and assumed to be correct in or- der to compare it with the tagger output. This is usually done by assuming that the disam- biguated test corpora being used contains the right pos disambiguation. This approach is quite right when the tagger error rate is larger enough than the test corpus error rate, never- theless, the current pos taggers have reached a performance level that invalidates this choice, since the tagger error rate is getting too close to the error rate of the test corpus. Since we want to study the relationship be- tween the tagger error rate and the test corpus error rate, we have to establish an absolute ref- erence point. Although (Church, 1992) ques- tions the concept of correct analysis, (Samuels- son and Voutilainen, 1997) establish that there exists a statistically signiﬁcant absolute cor- rect disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. What we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference. The cases we can ﬁnd when evaluating the performance of a certain tagger are presented in table 1. okok stand for a rightwrong tag (respect to the absolute correct disambigua- tion). When both the tagger and the test cor- pus have the correct tag, the tag is correctly evaluated as right. When the test corpus has the correct tag and the tagger gets it wrong, the occurrence is correctly evaluated as wrong. But problems arise when the test corpus has a wrong tag: If the tagger gets it correctly, it is evaluated as wrong when it should be right (false negative). If the tagger gets it wrong, it will be rightly evaluated as wrong if the error commited by the tagger is other than the er- ror in the test corpus, but wrongly evaluated as right (false positive) if the error is the same. Table 1 shows the computation of the percent- corpus tagger eval: right eval: wrong okc okt (1C)t  okc okt  (1C)(1t) okc okt  Cu okc okt C(1u)p C(1u)(1p) Table 1: Possible cases when evaluating a tagger. ages of each case. The meanings of the used variables are: C: Test corpus error rate. Usually an estima- tion is supplied with the corpus. t: Tagger performance rate on words rightly tagged in the test corpus. It can be seen as P(oktokc). u: Tagger performance rate on words wrongly tagged in the test corpus. It can be seen as P(oktokc). p: Probability that the tagger makes the same error as the test corpus, given that both get a wrong tag. x: Real performance of the tagger, i.e. what would be obtained on an errorfree test set. K: Observed performance of the tagger, com- puted on the noisy test corpus. For simplicity, we will consider only perfor- mance on ambiguous words. Considering unam- biguous words will make the analysis more com- plex, since it should be taken into account that neither the behaviour of the tagger (given by u, t, p) nor the errors in the test corpus (given by c) are the same on ambiguous and unambiguous words. Nevertheless, this is an issue that must be further addressed. If we knew each one of the above proportions, we would be able to compute the real perfor- mance of our tagger (x) by adding up the okt rows from table 1, i.e. the cases in which the tagger got the right disambiguation indepen- dently from the tagging of the test set: x(1C)tCu (1) The equation of the observed performance can also be extracted from table 1, adding up what is evaluated as right: K (1C)tC(1u)p (2) The relationship between the real and the ob- served performance is derived from 1 and 2: xKC(1u)pCu Since only K and C are known (or approxi- mately estimated) we can not compute the real performance of the tagger. All we can do is to establish some reasonable bounds for t, u and p, and see in which range is x. Since all variables are probabilities, they are bounded in [0, 1]. We also can assume2 that K  C. We can use this constraints and the above equations to bound the values of all vari- ables. From 2, we obtain: u  1 Kt(1C) Cp ; p  Kt(1C) C(1u) ; t  KC(1u)p 1C Thus, u will be maximum when p and t are maximum (i.e. 1). This gives an upper bound 2In the cases we are interested in that is, current systems the tagger observed performance, K, is over 90, while the corpus error rate, C, is below 10. for u of (1K)C. When t  0, u will range in [, 1KC] depending on the value of p. Since we are assuming K C, the most informa- tive lower bound for u keeps being zero. Simi- larly, p is minimum when t1 and u0. When t  0 the value for p will range in [KC, ] depending on u. Since K  C, the most infor- mative upper bound for p is still 1. Finally, t will be maximum when u  1 and p  0, and minimum when u0 and p1. Summarizing: 0  u  min  1, 1K C  (3) max  0, KC1 C   p  1 (4) KC 1C  t  min  1, K 1C  (5) Since the values of the variables are mutually constrained, it is not possible that, for instance, u and t have simultaneously their upper bound values (if (1K)C 1 then K(1C)1 and viceversa). Any bound which is out of [0, 1] is not informative and the appropriate boundary, 0 or 1, is then used. Note that the lower bound for t will never be negative under the assump- tion K C. Once we have established these bounds, we can use equation 1 to compute the range for the real performance value of our tagger: x will be minimum when u and t are minimum, which produces the following bounds: xmin  KCp (6) xmax   KC if K  1C 1 KC1 p if K  1C (7) As an example, lets suppose we evaluate a tag- ger on a test corpus which is known to contain about 3 of errors (C 0.03), and obtain a re- ported performance of 933 (K 0.93). In this case, equations 6 and 7 yield a range for the real performance x that varies from [0.93, 0.96] when p0 to [0.90, 0.96] when p1. This results suggest that although we observe a performance of K, we can not be sure of how well is our tagger performing without taking into account the values of t, u and p. It is also obvious that the intervals in the above example are too wide, since they con- sider all the possible parameter values, even when they correspond to very unlikely param- 3This is a realistic case obtained by (Marquez and Padro , 1997) tagger. Note that 93 is the accuracy on ambiguous words (the equivalent overall accuracy was about 97). eter combinations4. In section 4 we will try to narrow those intervals, limiting the possibilities to reasonable cases. 4 Reasonable Bounds for the Basic Parameters In real cases, not all parameter combinations will be equally likely. In addition, the bounds for the values of t, u and p are closely related to the similarities between the training and test corpora. That is, if the training and test sets are extracted from the same corpus, they will prob- ably contain the same kind of errors in the same kind of situations. This may cause the training procedure to learn the errors especially if they are systematic and thus the resulting tagger will tend to make the same errors that appear in the test set. On the contrary, if the train- ing and test sets come from diﬀerent sources sharing only the tag set the behaviour of the resulting tagger will not depend on the right or wrong tagging of the test set. We can try to establish narrower bounds for the parameters than those obtained in section 3. First of all, the value of t is already con- strained enough, due to its high contribution (1  C) to the value of K, which forces t to take a value close to K. For instance, apply- ing the boundaries in equation 5 to the case C 0.03 and K 0.93, we obtain that t belongs to [0.928, 0.959]. The range for u can be slightly narrowed con- sidering the following: In the case of indepen- dent test and training corpora, u will tend to be equal to t. Otherwise, the more biased to- wards the corpus errors is the language model, the lower u will be. Note than ut would mean that the tagger disambiguates better the noisy cases than the correct ones. Concerning to the lower bound, only in the case that all the errors in the training and test corpus were systematic (and thus can be learned) could u reach zero. However, not only this is not a likely situation, but also requires a perfectlearning tagger. It seems more reasonable that, in normal cases, er- rors will be random, and the tagger will behave 4For instance, it is not reasonable that u  0, which would mean that the tagger never disambiguates cor- rectly a wrong word in the corpus, or p1, which would mean that it always makes the same error when both are wrong. randomly on the noisy occurrences. This yields a lower bound for u of 1a, being a the average ambiguity ratio for ambiguous words. The reasonable bounds for u are thus 1 a  u  min  t, 1K C  Finally, the value of p has similar constraints to those of u. If the test and training corpora are independent, the probability of making the same error, given that both are wrong, will be the random 1(a1). If the corpora are not independent, the errors that can be learned by the tagger will cause p to rise up to (potentially) 1. Again, only in the case that all errors where systematic, could p reach 1. Then, the reasonable bounds for p are: max  1 a1, KC1 C   p  1 5 On Comparing Tagger Performances As stated above, knowing which are the reason- able limits for the u, p and t parameters enables us to compute the range in which the real per- formance of the tagger can vary. So, given two diﬀerent taggers T1 and T2, and provided we know the values for the test corpus error rate and the observed performance of both cases (C1, C2, K1, K2), we can compare them by matching the reasonable intervals for the re- spective real performances x1 and x2. From a conservative position, we cannot strongly state than one of the taggers performs better than the other when the two intervals overlap, since this implies a chance that the real performances of both taggers are the same. The following real example has been ex- tracted from (Marquez and Padro , 1997): The tagger T1 uses only bigram information and has an observed performance on ambiguous words K1  0.9135 (96.86 overall). The tagger T2 uses trigrams and automatically acquired con- text constraints and has an accuracy of K2  0.9282 (97.39 overall). Both taggers have been evaluated on a corpus (wsj) with an estimated error rate5 C1 C2 0.03. The average ambigu- ity ratio of the ambiguous words in the corpus is a2.5 tagsword. 5The (wsj) corpus error rate is estimated over all words. We are assuming that the errors distribute uniformly among all words, although ambiguous words These data yield the following range of rea- sonable intervals for the real performance of the taggers. for pi (1a)0.4 for pi 1 x1  [91.35, 94.05] x1  [90.75, 93.99] x2  [92.82, 95.60] x2  [92.22, 95.55] The same information is included in ﬁgure 1 which presents the reasonable accuracy intervals for both taggers, for p ranging from 1a0.4 to 1 (the shadowed part corresponds to the over- lapping region between intervals). . . . . . . . . 90 91 92 93 94 95 1a0.4 1 (p) (x)  accuracy Overlap T T2 1 Figure 1: Reasonable intervals for both taggers The obtained intervals have a large overlap region which implies that there are reasonable parameter combinations that could cause the taggers to produce diﬀerent observed perfor- mances though their real accuracies were very similar. From this conservative approach, we would not be able to conclude that the tagger T2 is better than T1, even though the 95 con- ﬁdence intervals for the observed performances did allow us to do so. 6 Discussion The presented analysis of the eﬀects of noise in the test corpus on the evaluation of pos taggers leads us to conclude that when a tagger is eval- uated as better than another using noisy test corpus, there are reasonable chances that they are in fact very similar but one of them is just adapting better than the other to the noise in the corpus. probably have a higher error rate. Nevertheless, a higher value for C would cause the intervals to be wider and to overlap even more. We believe that the widespread practice of evaluating taggers against a noisy test corpus has reached its limit, since the performance of current taggers is getting too close to the error rate usually found in test corpora. An obvious solution and maybe not as costly as one might think, since small test sets properly used may yield enough statistical evidence is using only errorfree test corpora. Another pos- sibility is to further study the inﬂuence of noise in order to establish a criterion e.g. a thresh- old depending on the amount of overlapping be- tween intervals to decide whether a given tag- ger can be considered better than another. There is still much to be done in this direc- tion. This paper does not intend to establish a new evaluation method for pos tagging, but to point out that there are some issues such as the noise in test corpus that have been paid lit- tle attention and are more important than what they seem to be. Some of the issues that should be further con- sidered are: The eﬀect of noise on unambigu- ous words; the reasonable intervals for overall real performance; the probably diﬀerent val- ues of C, p, u and t for ambiguousunambiguous words; how to estimate the parameter values of the evaluated tagger in order to constrain as much as possible the intervals; the statistical signiﬁcance of the interval overlappings; a more informed (and less conservative) criterion to re- jectaccept the hypothesis that both taggers are diﬀerent, etc. References Church, K.W. 1992. Current Practice in Part of Speech Tagging and Suggestions for the Future. In Simmons (ed.), Sbornik praci: In Honor of Henry Kuˇcera. Michigan Slavic Studies. Krovetz, R. 1997. Homonymy and Polysemy in Information Retrieval. In Proceedings of joint EACL meeting. Marquez, L. and Padro, L. 1997. A Flexible POS Tagger Using an Automatically Acquired Lan- guage Model. In Proceedings of joint EACL meeting. Mooney, R.J. 1996. Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning. In Proceed- ings of EMNLP96 conference. Samuelsson, C. and Voutilainen, A. 1997. Compar- ing a Linguistic and a Stochastic Tagger. In Pro- ceedings of joint EACL meeting. Resum Aquest article versa sobre lavaluacio de desam- biguadors morfosintactics. Normalment, lava- luacio es fa comparant la sortida del desam- biguador amb un corpus de referencia, que se suposa lliure derrors. De tota manera, els cor- pus que susen habitualment contenen soroll que causa que el rendiment que sobte dels desam- biguadors sigui una distorsio del valor real. En aquest article analitzem ﬁns a quin punt aques- ta distorsio pot invalidar la comparacio entre desambiguadors o la mesura de la millora apor- tada per un nou sistema. La conclusio princi- pal es que cal establir procediments alternatius dexperimentacio mes rigorosos, per poder ava- luar i comparar ﬁablement les precisions dels desambiguadors morfosintactics. Laburtena Artikulu hau desanbiguatzaile morfosintak- tikoen ebaluazioaren inguruan datza. Nor- malean, ebaluazioa, desanbiguatzailearen irte- era eta ustez errorerik gabeko erreferentziako corpus bat konparatuz egiten da. Hala ere, maiz corpusetan erroreak egoten dira eta horrek de- sanbiguatzailearen emaitzaren benetako balioan eragina izaten du. Artikulu honetan, hain zuzen ere, horixe aztertuko dugu, alegia, zer neurritan distortsio horrek jar dezakeen auzitan desanbiguatzaileen arteko konparazioa edo sis- tema berri batek ekar dezakeen hobekuntza- maila. Konklusiorik nagusiena hauxe da: de- sanbiguatzaile morfosintaktikoak aztertzeko eta modu ziurrago batez konparatu ahal izateko, azterketa-bideak sakonagoak eta zehatzagoak izan beharko liratekeela.",
  "12.pdf": "arXiv:cs9809113v1 [cs.CL] 28 Sep 1998 Improving Tagging Accuracy by Using Voting Taggers Lluıs Marquez, Lluıs Padro and Horacio Rodrıguez Dep. LSI. Technical University of Catalonia c Jordi Girona 1-3. 08034 Barcelona {lluism,padro,horacio}lsi.upc.es Abstract We present a bootstrapping method to de- velop an annotated corpus, which is spe- cially useful for languages with few avail- able resources. The method is being ap- plied to develop a corpus of Spanish of over 5Mw. The method consists on tak- ing advantage of the collaboration of two diﬀerent POS taggers. The cases in which both taggers agree present a higher accu- racy and are used to retrain the taggers. Keywords: POS tagging, Corpus Anno- tation, Bootstrapping techniques 1 Introduction Usual automatic tagging algorithms involve a pro- cess of acquisition (or learning) of a statistical lan- guage model from a previously tagged training cor- pus (supervised learning). The statistical models contain lots of parameters that have to be reliably estimated from the corpus, so the sparseness of the training data is a severe problem. When a new annotated corpus for a language with a reduced amount of available linguistic resources is developed, this issue becomes even more important, since no training corpora are available and the man- ual tagging of a big enough training corpus is very expensive, both in time and human labour. If costly human labour is to be avoided, the accuracy of au- tomatic systems has to be as high as possible, even starting with relatively small manually tagged train- ing sets. In the case of English, existing resources are usu- ally enough, thus existing work on developing cor- pora does not rely much in bootstrapping, although reestimation procedures are widely used to improve tagger accuracies, specially when limited disam- biguated material is available (Church, 1988; Briscoe et al., 1994; Elworthy, 1994). We ﬁnd automatically tagged corpora which are hand corrected a posteri- ori (Marcus et al., 1993), and fully automatic disam- biguation procedures (Leech et al., 1994; Jarvinen, 1994) Bootstrapping is one of the methods that can be used to improve the performance of statistical taggers when only small training sets are available. The bootstrapping procedure starts by using a small hand-tagged portion of the corpus as an initial train- ing set. Then, the tagger is used to disambiguate fur- ther material, which is incorporated to the training set and used to retrain the tagger. Since the retrain- ing corpus can be much larger than the initial train- ing corpus we expect to better estimate (or learn) the statistical parameters of the tagging model and to obtain a more accurate tagger. Of course, this procedure can be iterated leading, hopefully, to pro- gressively better language models and more precise taggers. The procedure ends when no more improve- ment is achieved. As stated above, the bootstrapping reﬁning pro- cess is completely automatic. However each step of training corpus enlargement and enrichment could involve a certain amount of manual revision and correction. In this way the process would be semi automatic. The main problem of this approach is that the retraining material contains errors (because it has been tagged with a still poor tagger) and that this in- troduced noise could be very harmful for the learning procedure of the tagger. Depending on the amount of noise and on the robustness of the tagging algo- rithm, the reﬁning iteration could lead to no im- provement or even to a degradation of the perfor- mance of the initial tagger. Recent studies (Padro and Marquez, 1998) point that the noise in training and test corpora are crucial not only for the right evaluation of an NLP system, but also for its appropriate training to get an op- timal performance. So, keeping a low error rate in retraining material becomes an essential point if we want to guarantee the validity of the bootstrapping approach. In this paper we show that it is possible to take ad- vantage of the collaboration between two (or more) diﬀerent taggers in a bootstrapping process, by ob- serving that in the cases in which both taggers pro- pose the same tag present a much higher precision than any of them separately and that these coinci- dence cases represent a coverage of more than 95. Then, the corpus to retrain the taggers is built, at each step, on the basis of this intersection corpus, keeping fairly low error rates and leading to better language models and more precise taggers. In addition, it is clear that the combination of tag- gers can be used to get a high recall tagger, which proposes an unique tag for most words and two tags when both taggers disagree. Depending on the user needs, it might be worthwhile accepting a higher re- maining ambiguity in favour of a higher recall. The paper will be organized as follows: In sec- tion 2 we will propose a bootstrapping procedure that combines the information of two taggers. In section 3 we will describe the corpus used in the experiments, as well as the used analyzers, the ini- tial training set development and the initial results. Section 4 is devoted to describe the diﬀerent experi- ments performed to ﬁnd out the best way to combine the progressively obtained training corpora, and ﬁ- nally, in section 5, the best choice is presented and its results are reported. Preliminary work on extending the procedure to three voting taggers is discussed. 2 Bootstrapping algorithm The proposed bootstrapping algorithm is described in detail in ﬁgure 1. The meaning of the involved notation is described below:  Ci stands for the retraining corpus of i-th iter- ation. In particular, C0 stands for the initial handtagged training corpus.  T stands for a handtagged test corpus used to estimate the performance of the subsequent taggers.  N stands for the fresh part of the raw corpus used at each step to enlarge the training set. For simplicity we consider it independent of the speciﬁc iteration.  A1 and A2 stand for both taggers (including, indistinctly, the model acquisition and disam- biguation algorithms).  M j i stands for the j-th language model obtained by i-th tagger.  train(Ai, Cj) stands for the procedure of train- ing the i-th tagger with the j-th training corpus. The result is the language model M j i .  test(T , A1, M i 1, A2, M i 2) stands for a general procedure that returns the best accuracy ob- tained by any of the two taggers on the test set.  tag(N, Ai, M j i ) stands for the procedure of tag- ging the raw corpus N with the i-th tagger using the j-th language model, producing N i j .  combine(C0, N i 1  N i 2) is the general procedure of creation of (i1)-th training corpus. This is done by adding to the hand disambiguated corpus C0 the cases in N in which both taggers coincide in their predictions (noted N i 1  N i 2).  Train taggers using manual corpus M 0 1 : train(A1, C0); M 0 2 : train(A2, C0);  Compute achieved accuracy Acc-current : test(T , A1, M 0 1 , A2, M 0 2 ); Acc-previous : 0;  Initialize iteration counter i : 0; while (Acc-current signiﬁcantlybetter Acc-previous) do N : freshpartoftherawcorpus;  Tag the new data N i 1 : tag(N, A1, M i 1); N i 2 : tag(N, A2, M i 2);  Add the coincidence cases to  the manual training corpus Ci1 : combine(C0, N i 1  N i 2);  retrain the taggers M i1 1 : train(A1, Ci1); M i1 2 : train(A2, Ci1);  Prepare next iteration Acc-previous : Acc-current; Acc-current : test(T , A1, M i1 1 , A2, M i1 2 ) end-while Figure 1: Bootstrapping algorithm using two taggers In section 4 we study the proper tuning of the al- gorithm in our particular domain by exploring the right size of the retrain corpus (i.e: the size of N), the combination procedure (in particular we explore if a weighted combination is preferable to the sim- ple addition) and the number of iterations that are useful. Additionally, we have tested if the (relatively cheap) process of handcorrecting the disagreement cases between the two taggers at each step can give additional performance improvements. 3 Tagging the LexEsp Corpus The LexEsp Project is a multidisciplinary eﬀort impulsed by the Psychology Department from the University of Oviedo. It aims to create a large database of language usage in order to enable and encourage research activities in a wide range of ﬁelds, from linguistics to medicine, through psychol- ogy and artiﬁcial intelligence, among others. One of the main issues of that database of linguis- tic resources is the LexEsp corpus, which contains 5.5 Mw of written material, including general news, sports news, literature, scientiﬁc articles, etc. The corpus will be morphologically analyzed and disambiguated as well as syntactically parsed. The used tagset is PAROLE compliant, and consists of some 230 tags1 fully expanded (using all information about gender, number, person, tense, etc.) which can be reduced to 62 tags when only category and subcategory are considered. The corpus has been morphologically analyzed with the maco system, a fast, broadcoverage an- alyzer (Carmona et al., 1998). The percentage of ambiguous words is 39.26 and the average ambigu- ity ratio is 2.63 tagsword for the ambiguous words (1.64 overall). The output produced by maco, is used as the input for two diﬀerent POS taggers:  Relax (Padro, 1996). A relaxationlabelling based tagger which is able to incorporate infor- mation of diﬀerent sources in a common lan- guage of weighted context constraints.  TreeTagger (Marquez and Rodrıguez, 1997). A decisiontree based tagger that uses a machinelearning supervised algorithm for learning a base of statistical decision trees and an iterative disambiguation algorithm that ap- plies these trees and ﬁlters out low probability tags. Since both taggers require training data, 96 Kw were hand disambiguated2 to get an initial training set (C0) of 71 Kw and a test set (T ) of 25 Kw. The training set was used to extract bigram and trigram statistics and to learn decision trees with 1There are potentially many more possible tags, but they do not actually occur. 2A trained human annotator can reach a rate of 2000 words per hour, using a specially designed TclTk graph- ical interface. So, 100Kw can be annotated in about 50 man hours. TreeTagger. The taggers also require lexical probabilities, which were computed from the oc- currences in the training corpus applying smooth- ing (Laplace correction) to avoid zero probabilities. For the words not appearing in the training set, the probability distribution for their ambiguity class was used. For unseen ambiguity classes, unigram proba- bilities were used. Initial experiments consisted of evaluating the pre- cision of both taggers when trained on the above conditions. Table 1 shows the results produced by each tagger. The diﬀerent kinds of information used by the relaxation labelling tagger are coded as fol- lows: B stands for bigrams, T for trigrams and BT for the joint set. A baseline result produced by a mostfrequenttag tagger (Mft) is also reported. Tagger Model Ambiguous Overall Mft 88.9 95.8 TreeTagger 92.1 97.0 Relax (B) 92.9 97.3 Relax (T) 92.7 97.2 Relax (BT) 93.1 97.4 Table 1: Results of diﬀerent taggers using the C0 training set These results point out that a 71 Kw training set manually disambiguated provides enough evidence to allow the tagger to get quite good results. Nev- ertheless, it is interesting to notice that the trigram model has lower accuracy than the bigram model. This is caused by the size of the training corpus, too small to estimate a good trigram model. 4 Improving Tagging Accuracy by Combining Taggers In order to improve the model obtained from the initial hand tagged training corpus, we may try a reestimation procedure. The most straightforward and usual way of doing so is using a single tag- ger to disambiguate a fresh part of the corpus, and then use those data as a new training set. We will introduce the joint use of two taggers as a way to reduce the error rate introduced by the single tagger by selecting as retraining material only those cases in which both taggers coincide. Two properties must hold for this method to work: 1) the accuracy in the cases of coincidence should be higher than those of both taggers individually considered, and 2) the taggers should coincide in the majority of the cases (high coverage). For instance, using a ﬁrst set of 200Kw and given that both taggers agree in 97.5 of the cases and that 98.4 of those cases are correctly tagged, we get a new corpus of 195Kw with an error rate of 1.6. If we add the manually tagged 70Kw (assumed error free) from the initial training corpus we get a 265Kw corpus with an 1.2 error rate. 4.1 Size of the Retraining Corpus First of all, we need to establish which is the right size for the fresh part of the corpus to be used as re- training data. We have 5.4Mw of raw data available to do so, but note that the bigger the corpus is, the higher the error rate in the retraining corpus will be because of the increasing proportion of new noisy corpus with respect to the initial error free training corpus. So we will try to establish which is the corpus size at which further enlargements of the retraining cor- pus dont provide signiﬁcant improvements. Results for each tagger when retrained with diﬀerent corpus sizes are shown in ﬁgure 2 (accuracy ﬁgures are given over ambiguous words only). The size at which both taggers produce their best result is that of 800 Kw (namely C1 800), reaching 93.4 and 93.9 accuracy on ambiguous words. 92.5 93 93.5 94 94.5 0 200 400 600 800 1000 words (x1000) acc. TreeTagger Relax Figure 2: Results using retraining sets of increasing sizes The accuracies in ﬁgure 2 are computed retraining the taggers with the coincidence cases in the retrain corpus, as described in section 2. 4.2 Two taggers better than one Once we have chosen a size for the retraining corpus, we will check whether the joint use of two taggers to reduce the error in the training corpus is actually better than retraining only with a single tagger. Comparative results obtained for each of our tag- gers when using retraining material generated by a single tagger (the size of the fresh part of the cor- pus to be used as retrain data was also 800 Kw) and Tagger Model single C1 800 TreeTagger 93.0 93.4 Relax (BT) 93.7 93.9 Table 2: Comparative results when retraining with a new 800Kw corpus when using C1 800 are reported in table 2. Those re- sults point that the use of two taggers to generate the retraining corpus, slightly increases the accuracy of any tagger since it provides a less noisy model. The error rate in the retrain corpus when using the Relax-BT tagger alone is 2.4, while when using the coincidences of both taggers is reduced to 1.3. This improvement in the training corpus quality en- ables the taggers to learn better models and slightly improve their performance. Probably, the cause that the performance improvement is not larger must not be sought in the training corpus error rate, but in the learning abilities of the taggers. 4.3 Number of Iterations The bootstrapping algorithm must be stopped when no further improvements are obtained. This seems to happen after the ﬁrst iteration step. Using the 800Kw from the beginning yields similar results than progressively enlarging the corpus size at each step. Results are shown in table 3. Tagger Model C1 800 C2 800 TreeTagger 93.4 93.5 Relax (BT) 93.9 93.8 Table 3: Results when retraining with a 800Kw cor- pus in one and two steps Facts that support this conclusion are:  The variations with respect to the results for one reestimation iteration are not signiﬁcant.  TreeTagger gets a slight improvement while Relax decreases indicating that the re estimated model does not provide a clear improvement.  The intersection corpora used to retrain have the same accuracy (98.4) both in iteration one and two, and the diﬀerence in the number or coincidences (97.7 in iteration one vs. 98.3 in iteration two) is not large enough to provide extra information. 4.4 Use of Weighted Examples We have described so far how to combine the results of two POS taggers to obtain larger training cor- pora with low error rates. We have also combined the agreement cases of both taggers with the initial handdisambiguated corpus, in order to obtain a less noisy training set. Since the handdisambiguated corpus oﬀers a higher reliability than the tagger co- incidence set, we might want to establish a reliability degree for our corpus, by means of controlling the contribution of each part. This can be done through the estimation of the error rate of each corpus, and establishing a weighted combination which produces a new retraining corpus with the desired error rate. As mentioned above, if we put together a hand disambiguated (assumed error-free) 70Kw corpus and a 195Kw automatically tagged corpus with an estimated error rate of 1.6, we get a 265Kw cor- pus with a 1.2 error rate. But if we combine them with diﬀerent weights we can control the error rate of the corpus: e.g. taking the weight for the correct 70Kw twice the weight for the 195Kw part, we get a corpus of 335Kw3 with an error rate of 0.9. In that way we can adjust the weights to get a training corpus with the desired error rate. Figure 3 shows the relationship between the error rate and the relative weights between C0 and the retraining corpus. 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20 25 30 35 40 45 weigth  error rate Figure 3: Relationship between the error rate and the relative weights for training corpora. This weighted combination enables us to dim the undesired eﬀect of noise introduced in the automat- ically tagged part of the corpus. This combination works as a kind of back-oﬀ inter- polation between the correct examples of C0 and the slightly noisy corpus of coincidences added at each 3Obviously this occurrences are virtual since part of them are duplicated. step. By giving higher weights to the former, cases well represented in the C0 corpus are not seriously inﬂuenced by new erroneous instances, but cases not present in the C0 corpus are still incorporated to the model. So, the estimations of the statistical parame- ters for new cases will improve the tagging perfor- mance while statistical estimations of already well represented cases will be, at most, slightly poorer. We have performed an experiment to determine the performance obtained when the taggers are trained with corpus obtained combining C0 and the ﬁrst extension of 200,000 words (N 1 1  N 1 2 ) with dif- ferent relative weights4. The steps selected are the weights corresponding to error rates of 0.1, 0.2, 0.3, 0.4, 0.5, 0.75 and 1. It is obvious that too high weighting in favour of initial examples will produce a lower error rate (tending to zero, the same than the manual corpus), but it will also bias the taggers to behave like the initial tagger, and thus will not take advantage of the new cases. 92.6 92.8 93 93.2 93.4 93.6 93.8 94 94.2 94.4 0 5 10 15 20 25 30 35 40 45 50 relative weight acc. TreeTagger Relax Figure 4: Results using the C1 200 training set with dif- ferent weightings The results summarized in ﬁgure 4 show that there is a clear improvement in the tagger performance when combining two training corpora with a proper weighting adjustment. Obviously, there is a tradeoﬀ point where the performance starts to decrease due to an excessive weight for the initial data. Although the behaviour of both curves is similar, it is also clear that the diﬀerent tagging algorithms are not equally sensitive to the weighting values: In particular, TreeTagger achieves its highest per- formance for weights between 1 and 3, while Relax- BT needs a weight around 10. 4Weights were straightforwardly incorporated to the bigrams and trigrams statistics. The decision tree learn- ing algorithm had to be slightly modiﬁed to deal with weighted examples. 4.5 Handcorrecting Disagreement Cases Another possible way to reduce the error rate in the training corpus is hand correcting the disagreement cases between taggers. This reduces the error rate of the new training corpus at a low human labor cost, since the disagreement cases are only a small part of the total amount. For instance, In C1 200 corpus, there were 5,000 dis- agreement cases. Handcorrecting and adding them to the previous set we obtain a slightly larger corpus5 (270Kw) with a slightly lower error rate (1.17), which can be used to retrain the taggers. We call this corpus C1 M (M stands for manual revision). Results obtained with the corrected retraining corpus are shown in table 4, together with the re- sults obtained with fully automatic retraining corpus of 200 Kw (C1 200) and 800 Kw (C1 800). Tagger Model C1 200 C1 M C1 800 TreeTagger 93.2 93.8 93.4 Relax (BT) 93.3 93.8 93.9 Table 4: Comparative results using C1 200, C1 M and C1 800 training sets The ﬁrst conclusion in this case is that the hand correction of disagreement cases gives a signiﬁcant accuracy improvement in both cases. However, the gain obtained is the same order than that obtained with a larger retraining corpus automatically disam- biguated. Unfortunately we had neither more avail- able human resources nor time to hand-correct the remaining 15,000 disagreement words of C1 800 in or- der to test if some additional improvement can be achieved from the best automatic case. Without per- forming this experiment it is impossible to extract any reliable conclusion. However, we know that the price to pay for an uncertain accuracy gain is the ef- fort of manually tagging about 20,000 words. Even when that would mean an improvement, we suspect that it would be more productive to spend this eﬀort in constructing a larger initial training corpus. Thus, unless there is a very severe restriction on the size of the available retraining corpus, it seems to be cheaper and faster not to hand correct the disagreement cases. 5The increasing in number of training examples is specially noticeable in the case of decision trees (14,000). This is due to the fact that each example considers a context window of six items. After hand correction all sequences of six words are valid while be- fore correction it was quite probable to ﬁnd gaps (cases of disagreement) in the sequences of six words of the intersection corpus. 5 Best Tagger All the the above described combinations produce a wide range of possibilities to build a retraining corpus. We can use retraining corpus of diﬀerent sizes, perform several retraining steps, and weight the combination of the retraining parts. Although all possible combinations have not been explored, we have set the basis for a deeper analysis of the possibilities. A promising combination is using the more reli- able information obtained so far to build a C1 Best re- training corpus, consisting of C1 M (which includes C0) plus the coincidence cases from the C0 800 which were not included in C1 M. This combination has only been tested in its straightforward form, but we feel that the weighted combination of the constituents of C1 Best should produce better results than the reported so far. On the other hand, the above reported results were obtained using only either the TreeTagger with decision trees information or the Relax tag- ger using bigrams andor trigrams. Since the Re- lax tagger is able to combine diﬀerent kinds of con- straints, we can write the decision trees learned by TreeTagger in the form of constraints (C), and make Relax use them as in (Marquez and Padro, 1997). Table 5 shows the best results obtained with ev- ery combination of constraint kinds. The retraining corpora which yield each result are also reported. Tagger Model Amb. Overall Corpus TreeTagger 93.8 97.7 C1 M Relax (B) 93.3 97.5 C1 Best Relax (T) 93.7 97.6 C1 Best Relax (BT) 93.9 97.7 C1 800 C1 1000 Relax (C) 93.8 97.7 C1 Best Relax (BC) 94.1 97.8 C1 200 C1 M C1 Best Relax (TC) 94.2 97.8 C1 200 Relax (BTC) 94.2 97.8 C1 400 C1 Best Table 5: Best results for each tagger with all possible constraint combinations Further experiments must establish which is the most appropriate bootstrapping policy, and whether it depends on the used taggers. 6 Conclusions and Future Work We presented the collaboration between two diﬀer- ent POS taggers in a voting approach as a way to in- crease tagging accuracy. Since words in which both taggers make the same prediction present a higher accuracy ratio, tagger collaboration can also be used to develop large training sets with a low error rate, which is specially useful for languages with a reduced amount of available linguistic resources. The presented results show that:  The precision of the taggers taking into account only the cases in which they agree, is signiﬁ- cantly higher than overall cases. Although this is not useful to disambiguate a corpus, it may be used in new corpus development to reduce the amount of hand tagging while keeping the noise to a minimum. This has been used in a bootstrapping procedure to develop an anno- tated corpus of Spanish of over 5Mw, with an estimated accuracy of 97.8.  Obviously, the recall obtained joining the pro- posals from both taggers is higher than the results of any of them separately and a re- maining ambiguity is introduced, which causes a decrease in precision. Depending on the user needs, it might be worthwhile accepting a higher remaining ambiguity in favour of a higher recall. With the models acquired from the best training corpus, we get a tagger with a recall of 98.3 and a remaining ambiguity of 1.009 tagsword, that is, 99.1 of the words are fully disambiguated and the remaining 0.9 keep only two tags. This procedure can easily be extended to a larger number of taggers. We are currently studying the collaboration of three taggers, using a ECGI tagger (Pla and Prieto, 1998) in addition to the other two. Preliminary results point that the cases in which the three taggers coincide, present a higher accu- racy than when only two taggers are used (96.7 compared to 95.5 on ambiguous words) and that the coverage is still very high (96.2 compared to 97.7). Nevertheless, the diﬀerence is relatively small, and it must be further checked to establish whether it is worth using a larger number of taggers for build- ing low error rate training corpora. In addition, as pointed in (Padro and Marquez, 1998), the error in test corpora may introduce distortion in the eval- uation and invalidate small improvements and al- though we have used a manually disambiguated test corpus, it may contain human errors. For all this reasons, much work on improving the test corpus and on validating the so far obtained results is still to be done. 7 Acknowledgments This research has been partially funded by the Spanish Research Department (CICYTs ITEM project TIC961243C0302), by the EU Commis- sion (EuroWordNet LE4003) and by the Catalan Re- search Department (CIRITs quality research group 1995SGR 00566). References [Briscoe et al.1994] Briscoe, E.J.; Greﬀenstette, G.; Padro, L. and Serail, I.; 1994. Hybrid techniques for training Part-of-Speech taggers. ESPRIT BRA-7315 Acquilex II, Working Paper 45. [Carmona et al.1998] Carmona J.; Cervell S.; Marquez L.; Martı M.A.; Padro L.; Placer R.; Rodrıguez H.; Taule M. and Turmo J. 1998. An Environment for Morphosyntac- tic Processing of Unrestricted Spanish Text. In Pro- ceedings of 1st International Conference on Language Resources and Evaluation, LREC98. Granada, Spain. [Church1988] Church, K.W.; 1988. A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In Proceedings 2nd ANLP, Austin, Texas. [Elworthy1994] Elworthy, D.; 1994. Does Baum-Welch re-estimation help taggers?. In Proceedings 4th ANLP, Stuttgart, Germany. [Jarvinen1994] Jarvinen, T.; 1994. Annotating 200 Mil- lion Words: The Bank of English Project. In Pro- ceedings of 15th International Conference on Compu- tational Linguistics, COLING94, Kyoto, Japan. [Leech et al.1994] Leech, G.; Garside, R. and Bryant, M.; 1994. CLAWS4: The Tagging of the British National Corpus. In Proceedings of 15th International Confer- ence on Computational Linguistics, COLING94, Ky- oto, Japan. [Marcus et al.1993] Marcus, M.P.; Marcinkiewicz, M.A. and Santorini, B.; 1993. Building a Large Annotated Corpus of English: The Penn Treebank. In Computa- tional Linguistics, Vol.19, n.2. [Marquez and Padro1997] Marquez, L. and Padro, L. 1997. A Flexible POS Tagger Using an Automat- ically Acquired Language Model. In Proceedings of joint EACL meeting. Madrid, Spain. [Marquez and Rodrıguez1997] Marquez, L. and Rodrıguez, H. 1997. Automatically Acquiring a Lan- guage Model for POS Tagging Using Decision Trees. In Proceedings of the Second Conference on Recent Ad- vances in Natural Language Processing, RANLP 97. Tzigov Chark, Bulgaria. [Padro1996] Padro, L. 1996. POS Tagging Using Relax- ation Labelling. In Proceedings of 16th International Conference on Computational Linguistics, COLING 96. Copenhagen, Denmark. [Padro and Marquez1998] Padro, L. and Marquez, L. 1998. On the Evaluation and Comparison of Taggers: the Eﬀect of Noise in Testing Corpora. In Proccedings of the joint 17th International Conference on Compu- tational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics, COLING- ACL98. Montreal, Canada. [Pla and Prieto1998] Pla, F. and Prieto, N. 1998. Us- ing Grammatical Inference Methods for Automatic PartofSpeech Tagging. In Proceedings of 1st Inter- national Conference on Language Resources and Eval- uation, LREC98. Granada, Spain.",
  "13.pdf": "arXiv:cs9810014v1 [cs.CL] 13 Oct 1998 Resources for Evaluation of Summarization Techniques Judith L. Klavans and Kathleen R. McKeown and Min-Yen Kan and Susan Lee Center for Research on Information Access Department of Computer Science Computer Sciences Division Columbia University Columbia University University of California, Berkeley New York, NY 10027 New York, NY 10027 Berkeley, CA 94720 Abstract We report on two corpora to be used in the evaluation of component systems for the tasks of (1) linear segmentation of text and (2) summary-directed sentence extraction. We present characteristics of the corpora, methods used in the collection of user judgments, and an overview of the application of the corpora to evaluating the component system. Finally, we discuss the problems and issues with construction of the test set which apply broadly to the construction of evaluation resources for language technologies. 1. Application Context We report on two corpora to be used in the evaluation of component systems for the tasks of (1) linear segmentation of text and (2) summary-directed sentence extraction. Any development of a natural language processing (NLP) application requires systematic testing and evaluation. In the course of our ongoing development of a robust, domain- independentsummarization system at Columbia University, we have followed this procedure of incremental testing and evaluation1. However, we found that the resources that were necessary for the evaluation of our particular system components did not exist in the NLP community. Thus, we built a set of evaluation resources which we present in this paper. Our goal in this paper is to describe the resources and to discuss both theoretical and practical issues that arise in the development of such resources. All evaluation re- sources are publicly available, and we welcome collabora- tion on use and improvements. The two resources discussed in this paper were utilized in the initial evaluation of a text analysis module. In the larger context, the analysis module serves as the initial steps for a complete system for summarization by analysis and reformulation, rather than solely by sentence extraction. Anal- ysis components provide strategic conceptual information in the form of segments which are high in information con- tent, and in which similar or different; this information pro- vides input to subsequent processing, including reasoning about a single document or set of documents, followed by summary generation using language generation techniques (McKeown and Radev 1995, Radev and McKeown 1997). 1This research was supported by the NSF Award 9618797 in the Speech, Text, Image, and Multimedia in Language Technol- ogy (STIMULATE) program entitled STIMULATE: Generating Coherent Summaries of On-Line Documents: Combining Statisti- cal and Symbolic Techniques under the direction of Kathleen R. McKeown, Department of Computer Science and Judith L. Kla- vans, Center for Research on Information Access; Susan Lee was supported at Columbia University by a summer internship under the Computing Research Association (CRA) Distributed Mentor Project. 2. Description of Resources We detail these two evaluation corpora, both comprised of a corpus of human judgments, fashioned to accurately test the two technologies currently implemented in the text analysis module: namely, linear segmentation of text and sentence extraction. 2.1. Evaluation Resource for Segmentation The segmentation task is motivated by the observation that longer texts beneﬁt from automatic chunking of cohe- sive sections. Even though newspaper text appears to be segmented by paragraph and by headers, this segmentation is often driven by arbitrary page layout and length consid- erations rather than by discourse logic. For other kinds of text, such as transcripts, prior segmentation may not exist. Thus, our goal is to segment these texts by logical rhetorical considerations. In this section, we discuss the development of the eval- uation corpus for the task of segmentation. This task in- volves breaking input text into segments that represent some meaningful grouping of contiguous portions of the text. In our formulation of the segmentation task, we exam- ined the speciﬁcs of a linear multi-paragraph segmentation of the input text, linear in that we seek a sequential rela- tion between the chunks, as opposed to hierarchical seg- mentation (Marcu 1997). Multiple paragraph refers to the size of the units to be grouped, as opposed to sentences or words. We believe that this simple type of segmenta- tion yields useful information for summarization. Within the context of the text analysis module, segmentation is the ﬁrst step in the identiﬁcation of key areas of documents. Segmentation is followed by an identiﬁcation compo- nent to label segments according to function and impor- tance within the document. This labeling then permits rea- soning and ﬁltering over labeled and ranked segments. In the current implementation, segments are labeled according to centrality vis a vis the overall document. 2.1.1. Segmentation Corpus To evaluate our segmentation algorithms effectiveness, we needed to test our algorithm on a varied set of articles. We ﬁrst utilized the publicly available Wall Street Journal (WSJ) corpus provided by the Linguistic Data Consortium. Many of these articles are very short, i.e. 8 to 10 sen- tences, but segmentation is more meaningful in the context of longer articles; thus, we screened for articles as close as possible to 50 sentences. Additionally, we controlled our selection of articles for the absence of section headers within the article itself, to guarantee that articles were not written to ﬁt section headers. This is not to say that an eval- uation cannot be done with articles with headers, but rather that an initial evaluation was performed without this com- plicating factor. We arrived at a set of 15 newspaper articles from the WSJ corpus. We supplemented these by 5 articles from the on-line version of The Economist magazine, following the same restrictions, to protect against biasing our results to reﬂect WSJ style. Although WSJ articles were approxi- mately 50 sentences in length; the Economist articles were slightly longer, ranging from 50 to 75 sentences. Average paragraph length of the WSJ articles was 2 to 3 sentences, which is typical of newspaper paragraphing, and 3 to 4 for the Economist. Documents were domain independent but genre speciﬁc in general terms, i.e. current events (any topic) but journalistic writing, since this is the initial focus of our summarization project. 2.1.2. Task The goal of the task was to collect a set of user judg- ments on what is a meaningful segment with the hypothesis that what users perceive to be a meaningful unit will be use- ful data in evaluating the effectiveness of our system. The goal of our system is to identify segment boundaries and rank according to meaningfulness. The data could be used both to evaluate our algorithm, or in later stages, as part of training data for supervised learning. To construct the evaluation corpus, subjects were asked to divide an average of six selected articles into meaning- ful topical units at paragraph boundaries. The deﬁnition of segment was purposefully left vague in order to assess the users interpretation of the notion meaningful unit. Sub- jects were also encouraged to give subjective strengths of the segments, if they wanted to. Subjects were not told how the segments would be used for later processing, nor in- formed of the number of segment breaks to produce, and were given no further criteria for choice. Finally, subjects were not constrained by time restrictions; however, subjects were given the testers time estimate on task completion time of 10 minutes per article (for both reading the article and determining segment boundaries). In total, 13 volun- teers produced results, all graduate students or people with advanced degrees. A total of 19 articles were segmented by a minimum of four, and often ﬁve, subjects. All 13 subjects segmented the one remaining single article. 2.1.3. Analysis of Results of Human Segmentation The variation in segmentation style produced results rang- ing from very few segments (1-2 per document) to over 15 for the longer documents. As shown in Table 1, the num- ber of segments varied according to the length of the article and speciﬁc article in question. Most subjects completed the task within the time we had initially estimated. Subjects were found to be consistent in behavior: if they segmented one article with fewer segments than the aver- age, then the other articles segmented by the subject were often also segmented with fewer breaks. For example, Sub- ject 4 displays lumping behavior, whereas Subject 6 is a splitter. This points to an individuals notion of granu- larity, which is further discussed below in section 2.1.5. Article br7854 Article am3332 Article 0085 Article 0090 P  20 P  10 P  19 P  24 Subject 1 6 4 7 7 Subject 2 7 5 8 7 Subject 3 10 4 11 9 Article fn6703 Article mo0414 Article 0071 P  10 P  10 P  19 Subject 4 2 2 5 Subject 5 5 4 5 Subject 6 9 7 16 Table 1: Lumpers and Splitters problem on Segmentation Evaluation Corpora (where P  number of paragraph breaks in article) 2.1.4. Use To compile the gold standard we used majority opin- ion, as advocated by Gale et al, 1992, i.e. if the majority indicated a break at the same spot, then that location was deemed a segment boundary. We compiled the judgments into a database for use in optimal parameterization of a set of constraints for weighting groups of lexical and phrasal term occurrences. We calculated a high level of interjudge reliability using Cochrans Q, signiﬁcant to the 1 level for all but 2 articles which were signiﬁcant to the 5 level. See Kan et al, 1998 for further discussion of the use of data in evaluating the segmentation algorithm. 2.1.5. Issues The segmentation task is subject to interpretation, just like many natural language tasks which involve drawing subjective boundaries. Since the directions were open-ended, responses can be divided into the lumpers and the split- ters, to use the terminology applied to lexicographerswhen building dictionary deﬁnitions. In the case of dictionary construction, lumpers tend to write more terse, condensed deﬁnitions which consist of several possible uses in one deﬁnition, whereas splitters will divide deﬁnitions into a larger number of deﬁnitions, each of which may cover only one aspect or one usage of the word. For segmentation, the way this tendency expressed itself is that the lumpers tended to mark very few boundaries, whereas the splitters marked numerous boundaries. In fact, as mentioned above, some splitters marked over 15 segments for longer articles, which is over 85 of all possible paragraph breaks, on av- erage. For this reason, in determining what type of data to ex- tract from the evaluation corpus, we took only the majority segments for training and testing; the result is that lumpers end up determining the majority. 2.1.6. Future Work on the Segmentation Resource For future work, we would like to extend the resource to include a range of genres (such as journal articles, doc- umentation) as well as expand the range of sources to in- clude additional news articles (i.e. LDCs North American News Text Corpus). Also, we plan to extend our collection to other languages since there is little research on applica- bility of general techniques, such as segmentation based on terms and local maxima, across languages for multilingual analysis tasks. We are also considering analyzing articles with section headers, to see whether they follow the seg- ment boundaries and if so, how they can be utilized for ex- panding an evaluation resource. In addition to expanding the corpus by genre, we also plan to collect information for the segment labeling task. In this stage, segments are labeled for their function within the document. In addition, this resource will be useful in providing information on the function of the ﬁrst (or lead) segments. In journalistic prose, the lead segment can often be used as a summary due to stylized rules prescribing that the most important information must be ﬁrst. However, the lead can also be an anecdotal lead, i.e. material that grabs the readers attention and leads into the article. Thus, we plan to perform a formal analysis of how human subjects characterize anecdotal leads. 2.1.7. Availability The segmentation evaluation data is publicly available by request to the third author. Inquiries for the textual data that the evaluation corpus is based on should be directed to the respective owners of the materials. 2.2. Evaluation Resource for Sentence Extraction In this section, we describe the collection of judgments to create the evaluation resource used to test summary-directed sentence extraction. One method to produce a summary of a text is by performing sentence extraction. In this ap- proach a small set of informative sentences are chosen to represent the full text and presented to the user as a sum- mary. Although computationally appealing, this approach falls prey to at least two major disadvantages: (1) missing key information and (2) disﬂuencies in the extracted text. Our approach takes steps to handle both of these prob- lems and thus changes what we mean by the sentence ex- traction task. The majority of systems use sentence extrac- tion as a complete approach to summarization in that the sentences extracted from the text are, in fact, the summary presented to the user. In the context of our system, we use the sentence extraction component to choose a larger set of sentences than required for the intended summary length. All these sentences are then further analyzed for the generation component that will synthesize only the key information needed in a summary. The synthesis procedure will eliminate some clauses and possibly some whole sen- tences as well, resulting in a reformulated summary of the intended length. Thus, the goals of our sentence extrac- tion for generation task differ from sentence extraction as summarization in that we seek high recall of key infor- mation. 2.2.1. Extraction Corpus We used newswire text, available on the World-Wide Web from Reuters. In examining random articles available at the time of testing, we found that the number of sentences per article were short: 18, on average. Short paragraphs were also a characteristic of the corpus, similar to the cor- pus used for the segmentation evaluation: 1 to 3 sentences per paragraph on average. These shorter texts enabled us to analyze more articles than in the segmentation evaluation. As a result we were able to double the number of articles used for testing; we selected 40 articles, with titles, taken from this on-line version. 2.2.2. Task Naıve readers were asked to select sentences with high information content. Instructions were kept general, to let subjects form their own interpretation of informativeness, similar to the segmentation experiment. A minimum of one sentence was required, but no maximum number was set. All 15 subjects were volunteers, consisting of graduate stu- dents and professors from different ﬁelds. Subjects were grouped at random into 5 reading groups of 3 subjects each such that an evaluation based on majority opinion would possible. Each reading group analyzed 8 articles, which covered the entire 40 article set. Articles were provided in full with titles. 2.2.3. Analysis of Results of Human Sentence Extraction As expected with newswire and other journalistic text, many individuals chose the ﬁrst sentence. Although some subjects just took only the ﬁrst sentence for each article as a summary, the majority picked several sentences, usually including the ﬁrst sentence. Subjects implicitly followed the guidelines to pick whole sentences; no readers selected phrases or sentence fragments. Subjects indicated that this was not a difﬁcult task, unlike the segmentation task. 2.2.4. Use To establish the evaluation gold standard, we again ap- plied the majority method, which resulted in choosing all sentences that were selected by at least 2 of 3 judges as in- formative. The data was used for the automatic evaluation of an algorithm developed at Columbia, which exploits both symbolic and statistical techniques. The sentence extrac- tion algorithm we have developed uses ranked weighting for information from a number of well established statis- tical heuristics from the information retrieval community, such as TFIDF, combined with output from term identiﬁ- cation, segmentation, and segment function modules dis- cussed in the ﬁrst part of the paper. Additional weight is given to sentences containing title words. Furthermore, several experimental symbolic techniques were incorporated as factors in the sentence selection weighting process: such as looking for verbs of communication (Klavans and Kan, 1998, to appear). An informal examination of the data revealed high level of consistency among very important sentences, but a lower level of consistency when important detail was given. We suspect that the reason may be due to the equivalency and redundancy of certain sentences. 2.2.5. Issues Article 02 Article 18 Article 22 S  20 S  20 S  26 Subject 1 1 2 1 Subject 2 1 2 1 Subject 3 1 2 1 Article 03 Article 10 Article 11 S  26 S  15 S  17 Subject 4 4 4 4 Subject 5 3 2 2 Subject 6 1 1 1 Table 2: Verbose and Terse extracters phenomenon in Sen- tence Extraction Evaluation Corpora (where S  number of sentences in article) As mentioned in the ﬁrst section, the project which this resource was collected for consists of extraction of key sen- tences from text, and reformulation of a subset of these sen- tences into a coherent and concise summary. As such, our task is to extract more sentences than would be explicitly needed for a summary. The primary challenge in building this resource is anal- ogous to the lumpers versus splitters difference discussed in Section 2.1.5. For extraction, the issue is embodied in the verbose versus terse extractors, i.e. the number of sen- tences selected by subjects had a wide range. Some subjects consistently picked very few or just one sentence per arti- cle, whereas others consistently picked many more. This is shown in Table 2, where for example, subject 1 picked one or two sentences from each article over 20 sentences or more; whereas both subjects 2 and 3 picked an average of ﬁve sentences from the same article. Similarly, subject 6 consistently picked only one sentence, but subject 4 picked four sentences. This phenomenon, coupled with the use of a majority method evaluation biases results for high pre- cision rather than high recall. Thus, there is a mismatch between what we asked people to do and what the program was to produce. We believe that our compiled resource may be even better suited for an evaluation of a summarization approach based purely on sentence extraction, although it is still useful for our evaluation. 2.2.6. Future Work on the Extraction Resource We could compensate for the mismatch in task and al- gorithm above in two ways. One is in the way instructions are given; we could ask subjects to pick all of the sentences that could be considered of high information content, or we could give a number of sentences we would like them to pick for each article. For the very verbose, we could place an upper bound on the number of selected sentences. This could be done simply as some function of article length, logarithmic or linear. In the current collection, we found that some readers thought nearly every sentence was impor- tant, and this affected precision in the ﬁnal evaluation task. Some constraints would push our results towards the more verbose, and eliminate both the terse subject and the exces- sively verbose. Another approach is to relax the constraints for calculating the gold standard. As mentioned above, the majority method in conjunction with the lumpers versus splitters phenomenon biases results for high precision. In future work, we will investigate other methods for culling an evaluation corpus for correct answers, such as frac- tional recall and precision (Hatzivassiloglou and McKeown 93). 2.2.7. Availability The sentence extraction corpora is also publicly avail- able; send any requests to the ﬁrst author. Again, inquiries for the textual data that the evaluation corpus is based on should be directed to the respective owners of the materi- als. 3. Conclusion We have created two corpus resources to be used as a gold standard in the evaluation of two modules in the anal- ysis stage of a summarization system. We have discussed several fundamental issues that must be considered in the effective construction of evaluation resources. With an in- creasing number of publicly available evaluation resources such as these, we contribute to the goals of the collective sharing of resources and techniques to enable the NLP com- munity to improve the quality of our future work. 4. References Gale, William, Kenneth W. Church and David Yarowsky. (1992). Estimating upper and lower bounds on the per- formance of word-sense disambiguation programs. Pro- ceedings of the ACL. Newark, Delaware. pp. 249-256. Hatzivassiloglou, Vasileios and Kathleen R. McKeown. (1993). Towards the Automatic Identiﬁcation of Adjectival Scales: Clustering Adjectives According to Meaning. Proceed- ings of the 31st Annual Meeting of the Association for Computational Linguistics. Columbus, Ohio, USA. pp. 172-182. Kan, Min-Yen, Judith L. Klavans, Kathleen R. McKeown (1998) Linear Segmentation and Segment Relevance. Columbia University Computer Science Department Technical Re- port. Klavans, Judith L. and Min-Yen Kan. (1998, to appear). The Role of Verbs in Document Analysis. Proceedings of the 1998 COLING  ACL Conference. Montreal, Quebec, Canada. Marcu, Daniel. (1997) The Rhetorical Parsing of Unre- stricted Natural Language Texts. Proceedings of the 35th Annual Meeting of the Association Computational Lin- guistics and 8th Conference of the European Chapter of the Association for Computational Linguistics. Madrid, Spain. pp. 96-103. McKeown, Kathleen R. and Dragomir R. Radev. Generat- ing Summaries of Multiple News Articles. Proceedings, ACM Conference on Research and Development in In- formation Retrieval SIGIR95 (Seattle, WA, July 1995). Radev, Dragomir R. and Kathleen R. McKeown. (1997). Building a Generation Knowledge Source using Internet- Accessible Newswire. Proceedings of the 5th Confer- ence on Applied Natural Language Processing. Wash- ington, D.C.",
  "14.pdf": "arXiv:cs9810015v1 [cs.CL] 13 Oct 1998 Restrictions on Tree Adjoining Languages Giorgio Satta Dip. di Elettronica e Informatica Universita di Padova 35131 Padova, Italy sattadei.unipd.it William Schuler Computer and Information Science Dept. University of Pennsylvania Philadelphia, PA 19103 schulerlinc.cis.upenn.edu Abstract Several methods are known for parsing lan- guages generated by Tree Adjoining Grammars (TAGs) in O(n6) worst case running time. In this paper we investigate which restrictions on TAGs and TAG derivations are needed in order to lower this O(n6) time complexity, without in- troducing large runtime constants, and without losing any of the generative power needed to capture the syntactic constructions in natural language that can be handled by unrestricted TAGs. In particular, we describe an algorithm for parsing a strict subclass of TAG in O(n5), and attempt to show that this subclass retains enough generative power to make it useful in the general case. 1 Introduction Several methods are known that can parse lan- guages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n6), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Al- though asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of prac- tical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been ar- gued that methods for TAG parsing running in time asymptotically faster than O(n6) are un- likely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG pars- ing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More gen- erally, in this paper we investigate which re- strictions on TAGs are needed in order to lower the O(n6) time complexity, still retaining the generative power that is needed to capture the syntactic constructions of natural language that unrestricted TAGs can handle. The contribu- tion of this paper is twofold:  We deﬁne a strict subclass of TAG where adjunction of so-called wrapping trees at the spine is restricted to take place at no more than one distinct node. We show that in this case the parsing problem for TAG can be solved in worst case time O(n5).  We provide evidence that the proposed subclass still captures the vast majority of TAG analyses that have been currently proposed for the syntax of English and of several other languages. Several restrictions on the adjunction opera- tion for TAG have been proposed in the liter- ature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Diﬀerently from here, in all those works the main goal was one of characterizing, through the adjunction oper- ation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restric- tions mentioned above. 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and brieﬂy compare it with other proposals in the literature. A TAG is a tuple G  (N, Σ, I, A, S), where N, Σ are the ﬁnite sets of nonterminal and ter- minal symbols, respectively, I, A are the ﬁnite sets of initial and auxiliary trees, respectively, and S  N is the initial symbol. Trees in I  A are also called elementary trees. The reader is referred to (Joshi, 1985) for the deﬁnitions of tree adjunction, tree substitution, and language derived by a TAG. The spine of an auxiliary tree is the (unique) path that connects the root and the foot node. An auxiliary tree β is called a right (left) tree if (i) the leftmost (rightmost, resp.) leaf in β is the foot node; and (ii) the spine of β contains only the root and the foot nodes. An auxiliary tree which is neither left nor right is called a wrapping tree.1 The TAG restriction we propose is stated as followed: 1. At the spine of each wrapping tree, there is at most one node that can host adjunction of a wrapping tree. This node is called a wrapping node. 2. At the spine of each left (right) tree, no wrapping tree can be adjoined and no ad- junction constraints on right (left, resp.) auxiliary trees are found. The above restriction does not in any way con- strain adjunction at nodes that are not in the spine of an auxiliary tree. Similarly, there is no restriction on the adjunction of left or right trees at the spines of wrapping trees. Our restriction is fundamentally diﬀerent from those in (Schabes and Waters, 1993; Sch- abes and Waters, 1995) and (Rogers, 1994), in that we allow wrapping auxiliary trees to nest inside each other an unbounded number of times, so long as they only adjoin at one place in each others spines. Rogers, in contrast, restricts the nesting of wrapping auxiliaries to a number of times bounded by the size of the grammar, and Schabes and Waters forbid wrap- ping auxiliaries altogether, at any node in the grammar. We now focus on the recognition problem, and informally discuss the computational ad- vantages that arise in this task when a TAG obeys the above restriction. These ideas are formally developed in the next section. Most of 1The above names are also used in (Schabes and Wa- ters, 1995) for slightly diﬀerent kinds of trees. the tabular methods for TAG recognition rep- resent subtrees of derived trees, rooted at some node N and having the same span within the input string, by means of items of the form N, i, p, q, j. In this notation i, j are positions in the input spanned by N, and p, q are posi- tions spanned by the foot node, in case N be- longs to the spine, as we assume in the discus- sion below. i i j j p q N R Figure 1: O(n6) wrapping adjunction step. The most time expensive step in TAG recog- nition is the one that deals with adjunction. When we adjoin at N a derived auxiliary tree rooted at some node R, we have to combine to- gether two items R, i, i, j, j and N, i, p, q, j. This is shown in Figure 1. This step involves six diﬀerent indices that could range over any position in the input, and thus has a time cost of O(n6). Let us now consider adjunction of wrapping trees, and leave aside left and right trees for the moment. Assume that no adjunction has been performed in the portion of the spine below N. Then none of the trees adjoined below N will simultaneously aﬀect the por- tions of the tree yield to the left and to the right of the foot node. In this case we can safely split the tree yield and represent item N, i, p, q, j by means of two items of a new kind, Nleft, i, p and Nright, q, j. The adjunc- tion step can now be performed by means of two successive steps. The ﬁrst step combines R, i, i, j, j and Nleft, i, p, producing a new intermediate item I. The second step combines I and Nright, q, j, producing the desired result. In this way the time cost is reduced to O(n5). It is not diﬃcult to see that the above rea- soning also applies in cases where no adjunc- tion has been performed at the portion of the spine above N. This suggests that, when pro- N N N N N (a): (b): N N N N β Figure 2: O(n5) wrapping adjunction step. cessing a TAG that obeys the restriction intro- duced above, we can always split each wrap- ping tree into four parts at the wrapping node N, since N is the only site in the spine that can host adjunction (see Figure 2(a)). Adjunc- tion of a wrapping tree β at N can then be simulated by four steps, executed one after the other. Each step composes the item resulting from the application of the previous step with an item representing one of the four parts of the wrapping tree (see Figure 2(b)). We now consider adjunction involving left and right trees, and show that a similar split- ting along the spine can be performed. Assume that γ is a derived auxiliary tree, obtained by adjoining several left and right trees one at the spine of the other. Let x and y be the part of the yield of γ to the left and right, respectively, of the foot node. From the deﬁnition of left and right trees, we have that the nodes in the spine of γ have all the same nonterminal label. Also, from condition 2 in the above restriction we have that the left trees adjoined in γ do not constrain in any way the right trees adjoined in γ. Then the following derivation can always be performed. We adjoin all the left trees, each one at the spine of the other, in such a way that the resulting tree γleft has yield x. Similarly, we ad- joining all the right trees, one at the spine of the other, in such a way that the yield of the result- ing tree γright is y. Finally, we adjoin γright at the root of γleft, obtaining a derived tree having the same yield as γ. From the above observations it directly fol- lows that we can always recognize the yield of γ by independently recognizing γleft and γright. Most important, γleft and γright can be represented by means of items Rleft, i, p and Rright, q, j. As before, the adjunction of tree γ at some subtree represented by an item I can be recognized by means of two successive steps, one combining I with Rleft, i, p at its left, re- sulting in an intermediate item I, and the sec- ond combining I with Rright, q, j at its right, obtaining the desired result. 3 Recognition This section presents the main result of the pa- per. We provide an algorithm for the recogni- tion of languages generated by the subclass of TAGs introduced in the previous section, and show that the worst case running time is O(n5), where n is the length of the input string. To simplify the presentation, we assume the fol- lowing conditions throughout this section: ﬁrst, that elementary trees are binary (no more than two children at each node) and no leaf node is labeled by ε; and second, that there is always a wrapping node in each wrapping tree, and it diﬀers from the foot and the root node. This is without any loss of generality. 3.1 Grammar transformation Let G  (N, Σ, I, A) be a TAG obeying the re- strictions of Section 2. We ﬁrst transform A into a new set of auxiliary trees A that will be pro- cessed by our method. The root and foot nodes of a tree β are denoted Rβ and Fβ, respectively. The wrapping node (as deﬁned in Section 2) of β is denoted Wβ. Each left (right) tree β in A is inserted in A and is called βL (βR). Let β be a wrapping tree in A. We split β into four auxiliary trees, as informally described in Section 2. Let βD be the subtree of β rooted at Wβ. We call βU the tree obtained from β by removing every descendant of Wβ (and the corresponding arcs). We remove every node to the right (left) of the spine of βD and call βLD (βRD) the resulting tree. Similarly, we remove every node to the right (left) of the spine of βU and call βLU (βRU) the resulting tree. We set FβLD and FβRD equal to Fβ, and set FβLU and FβRU equal to Wβ. Trees βLU, βRU, βLD, and βRD are inserted in A for every wrapping tree β in A. Each tree in A inherits at its nodes the ad- junction constraints speciﬁed in G. In addition, we impose the following constraints:  only trees βL can be adjoined at the spine of trees βLD, βLU;  only trees βR can be adjoined at the spine of trees βRD, βRU;  no adjunction can be performed at nodes FβLU, FβRU . 3.2 The algorithm The algorithm below is a tabular method that works bottom up on derivation trees. Follow- ing (Shieber et al., 1995), we specify the algo- rithm using inference rules. (The speciﬁcation has been optimized for presentation simplicity, not for computational eﬃciency.) Symbols N, P, Q denote nodes of trees in A (including foot and root), α denotes initial trees and β denotes auxiliary trees. Symbol label(N) is the label of N and children(N) is a string denoting all children of N from left to right (children(N) is undeﬁned if N is a leaf). We write α  Sbst(N) if α can be substituted at N. We write β  Adj(N) if β can be adjoined at N, and nil  Adj(N) if adjunction at N is optional. We use two kind of items:  Item N X, i, j, X  {B, M, T}, denotes a subtree rooted at N and spanning the por- tion of the input from i to j. Note that two input positions are suﬃcient, since trees in A always have their foot node at the posi- tion of the leftmost or rightmost leaf. We have X  B if N has not yet been pro- cessed for adjunction, X  M if N has been processed only for adjunction of trees βL, and X  T if N has already been pro- cessed for adjunction.  Item β, i, p, q, j denotes a wrapping tree β (in A) with Rβ spanning the portion of the input from i to j and with Fβ spanning the portion of the input from p to q. In place of β we might use symbols [β, LD], [β, RD] and [β, RU] to denote the tempo- rary results of recognizing the adjunction of some wrapping tree at Wβ. Algorithm. Let G be a TAG with the re- strictions of Section 2, and let A be the asso- ciated set of auxiliary trees deﬁned as in sec- tion 3.1. Let a1a2    an, n  1, be an input string. The algorithm accepts the input iﬀ some item RT α, 0, n can be inferred for some α  I. Step 1 This step recognizes subtrees with root N from subtrees with roots in children(N). N T , i  1, i , label(N)  ai; F B β , i, i , β  A, 0  i  n; RT α, i, j N T , i, j , α  Sbst(N); P T , i, k QT , k, j N B, i, j , children(N)  PQ; P T , i, j N B, i, j , children(N)  P. Step 2 This step recognizes the adjunction of wrapping trees at wrapping nodes. We rec- ognize the tree hosting adjunction by compos- ing its four chunks, represented by auxiliary trees βLD, βRD, βRU and βLU in A, around the wrapped tree. RB βLD, k, p β, i, k, q, j [β, LD], i, p, q, j , β  Adj(Wβ), p  q; RB βRD, q, k [β, LD], i, p, k, j [β, RD], i, p, q, j , p  q; RT βRU , k, j [β, RD], i, p, q, k [β, RU], i, p, q, j ; RT βLU , i, k [β, RU], k, p, q, j β, i, p, q, j ; RT βLD, i, p RT βRD, q, j [β, RD], i, p, q, j , nil  Adj(Wβ), p  q. Step 3 This step recognizes all remaining cases of adjunction. RT βL, i, k N B, k, j N X, i, j , β Adj (N), X {M, T }; N X, i, k RT βR, k, j N T , i, j , β Adj (N), X {B, M}; N B, i, j N T , i, j , nil  Adj (N); N B, p, q β, i, p, q, j N T , i, j , β  Adj(N). Due to restrictions on space, we merely claim the correctness of the above algorithm. We now establish its worst case time complexity with re- spect to the input string length n. We need to consider the maximum number d of input posi- tions appearing in the antecedent of an inference rule. In fact, in the worst case we will have to execute a number of diﬀerent evaluations of each inference rule which is proportional to nd, and each evaluation can be carried out in an amount of time independent of n. It is easy to establish that Step 1 can be executed in time O(n3) and that Step 3 can be executed in time O(n4). Ad- junction at wrapping nodes performed at Step 2 is the most expensive operation, requiring an amount of time O(n5). This is also the time complexity of our algorithm. 4 Linguistic Relevance In this section we will attempt to show that the restricted formalism presented in Section 2 re- tains enough generative power to make it useful in the general case. 4.1 Athematic and Complement Trees We begin by introducing the distinction be- tween athematic auxiliary trees and comple- ment auxiliary trees (Kroch, 1989), which are meant to exhaustively characterize the auxiliary trees used in any natural language TAG gram- mar.2 An athematic auxiliary tree does not subcategorize for or assign a thematic role to its foot node, so the head of the foot node becomes the head of the phrase at the root. The struc- ture of an athematic auxiliary tree may thus be described as: Xn  Xn    (Y max)    , (1) where Xn is any projection of category X, Y max is the maximal projection of Y , and the order of the constituents is variable.3 A complement auxiliary tree, on the other hand, introduces a lexical head that subcategorizes for the trees foot node and assigns it a thematic role. The structure of a complement auxiliary tree may be described as: Xmax     Y 0    Xmax    , (2) where Xmax is the maximal projection of some category X, and Y 0 is the lexical projection 2The same linguistic distinction is used in the con- ception of modiﬁer and predicative trees (Schabes and Shieber, 1994), but Schabes and Shieber give the trees special properties in the calculation of derivation struc- tures, which we do not. 3The CFG-like notation is taken directly from (Kroch, 1989), where it is used to specify labels at the root and frontier nodes of a tree without placing constraints on the internal structure. of some category Y , whose maximal projection dominates Xmax. From this we make the following observations: 1. Because it does not assign a theta role to its foot node, an athematic auxiliary tree may adjoin at any projection of a category, which we take to designate any adjunction site in a host elementary tree. 2. Because it does assign a theta role to its foot node, a complement auxiliary tree may only adjoin at a certain complement ad- junction site in a host elementary tree, which must at least be a maximal projec- tion of a lexical category. 3. The foot node of an athematic auxiliary tree is dominated only by the root, with no intervening nodes, so it falls outside of the maximal projection of the head. 4. The foot node of a complement auxiliary tree is dominated by the maximal projec- tion of the head, which may also dominate other arguments on either side of the foot. To this we now add the assumption that each auxiliary tree can have only one complement ad- junction site projecting from Y 0, where Y 0 is the lexical category that projects Y max. This is justiﬁed in order to prevent projections of Y 0 from receiving more than one theta role from complement adjuncts, which would violate the underlying theta criterion in Government and Binding Theory (Chomsky, 1981).We also as- sume that an auxiliary tree can not have com- plement adjunction sites on its spine project- ing from lexical heads other than Y 0, in or- der to preserve the minimality of elementary trees (Kroch, 1989; Frank, 1992). Thus there can be no more than one complement adjunc- tion site on the spine of any complement auxil- iary tree, and no complement adjunction site on the spine of any athematic auxiliary tree, since the foot node of an athematic tree lies outside of the maximal projection of the head.4 4It is important to note that, in order to satisfy the theta criterion and minimality, we need only constrain the number of complement adjunctions  not the number of complement adjunction sites  on the spine of an aux- iliary tree. Although this would remain within the power of our formalism, we prefer to use constraints expressed in terms of adjunction sites, as we did in Section 2, be- Based on observations 3 and 4, we can further specify that only complement trees may wrap, because the foot node of an athematic tree lies outside of the maximal projection of the head, below which all of its subcategories must at- tach.5 In this manner, we can insure that only one wrapping tree (the complement auxiliary) can adjoin into the spine of a wrapping (com- plement) auxiliary, and only athematic auxil- iaries (which must be leftright trees) can ad- join elsewhere, fulﬁlling our TAG restriction in Section 2. 4.2 Possible Extensions We may want to weaken our deﬁnition to in- clude wrapping athematic auxiliaries, in order to account for modiﬁers with raised heads or complements as in Figure 3: They so revered him that they built a statue in his honor. This can be done within the above algorithm as long as the athematic trees do not wrap produc- tively (that is as long as they cannot be ad- joined one at the spine of the other) by splitting the athematic auxiliary tree down the spine and treating the two fragments as tree-local multi- components, which can be simulated with non- recursive features (Hockey and Srinivas, 1993). S NP VP W V revered NP β:  β S VP VP Adv so S C that β: Figure 3: Wrapping athematic tree. Since the added features are non-recursive, this extension would not alter the O(n5) result re- ported in Section 3. 4.3 Comparison of Coverage In contrast to the formalisms of Schabes and Waters (Schabes and Waters, 1993; Schabes and Waters, 1995), our restriction allows wrapping complement auxiliaries as in Figure 4 (Schabes and Waters, 1995). Although it is diﬃcult to ﬁnd examples in English which are excluded by cause it provides a restriction on elementary trees, rather than on derivations. 5 Except in the case of raising, discussed below. Rogers regular form restriction (Rogers, 1994), we can cite verb-raised complement auxiliary trees in Dutch as in Figure 5 (Kroch and San- torini, 1991). Trees with this structure may adjoin into each others internal spine nodes an unbounded number of times, in violation of Rogers deﬁnition of regular form adjunction, but within our criteria of wrapping adjunction at only one node on the spine. S NP VP β: Wβ V S PP NP P from discern Figure 4: Wrapping complement tree. S NP Wβ VP V ε S S V laten β: Figure 5: Verb-raising tree in Dutch. 5 Concluding remarks Our proposal is intended to contribute to the assessment of the computational complexity of syntactic processing. We have introduced a strict subclass of TAGs having the generative power that is needed to account for the syntac- tic constructions of natural language that unre- stricted TAGs can handle. We have speciﬁed a method that recognizes the generated languages in worst case time O(n5), where n is the length of the input string. In order to account for the dependency on the input grammar G, let us de- ﬁne G   N(1  Adj(N)), where N ranges over the set of all nodes of the elementary trees. It is not diﬃcult to see that the running time of our method is proportional to G. Our method works as a recognizer. As for many other tabular methods for TAG recogni- tion, we can devise simple procedures in order to obtain a derived tree associated with an ac- cepted string. To this end, we must be able to interleave adjunctions of left and right trees, that are always kept separate by our recognizer. The average case time complexity of our method should surpass its worst case time per- formance, as is the case for many other tabular algorithms for TAG recognition. In a more ap- plicative perspective, then, the question arises of whether there is any gain in using an algo- rithm that is unable to recognize more than one wrapping adjunction at each spine, as opposed to using an unrestricted TAG algorithm. As we have tried to argue in Section 4, it seems that standard syntactic constructions do not ex- ploit multiple wrapping adjunctions at a single spine. Nevertheless, the local ambiguity of nat- ural language, as well as cases of ill-formed in- put, could always produce cases in which such expensive analyses are attempted by an unre- stricted algorithm. In this perspective, then, we conjecture that having the single-wrapping- adjunction restriction embedded into the rec- ognizer would improve processing eﬃciency in the average case. Of course, more experimental work would be needed in order to evaluate such a conjecture, which we leave for future work. Acknowledgments Part of this research was done while the ﬁrst author was visiting the Institute for Research in Cognitive Science, University of Pennsylva- nia. The ﬁrst author was supported by NSF grant SBR8920230. The second author was sup- ported by U.S. Army Research Oﬃce Contract No. DAAH04-94G-0426. The authors would like to thank Christy Doran, Aravind Joshi, Anthony Kroch, Mark-Jan Nederhof, Marta Palmer, James Rogers and Anoop Sarkar for their help in this research. References Noam Chomsky. 1981. Lectures on government and binding. Foris, Dordercht. Robert Frank. 1992. Syntactic locality and tree ad- joining grammar: grammatical acquisition and processing perspectives. Ph.D. thesis, Computer Science Department, University of Pennsylvania. Beth Ann Hockey and Srinivas Bangalore. 1993. Feature-based TAG in place of multi-component adjunction: computational implications. In Pro- ceedings of the Natural Language Processing Pa- ciﬁc Rim Symposium (NLPRS), Fukuoka, Japan. Aravind K. Joshi. 1985. How much context sensitiv- ity is necessary for characterizing structural de- scriptions: Tree adjoining grammars. In L. Kart- tunen D. Dowty and A. Zwicky, editors, Natural language parsing: Psychological, computational and theoretical perspectives, pages 206250. Cam- bridge University Press, Cambridge, U.K. Anthony S. Kroch and Beatrice Santorini. 1991. The derived constituent structure of west ger- manic verb-raising construction. In Robert Frei- din, editor, Principles and Parameters in Com- parative Grammar, pages 269338. MIT Press. Anthony S. Kroch. 1989. Asymmetries in long dis- tance extraction in a TAG grammar. In M. Baltin and A. Kroch, editors, Alternative Conceptions of Phrase Structure, pages 6698. University of Chicago Press. Sanguthevar Rajasekaran and Shibu Yooseph. 1995. TAL recognition in O(M(n2)) time. In Proceed- ings of the 33rd Annual Meeting of the Associa- tion for Computational Linguistics (ACL 95). James Rogers. 1994. Capturing CFLs with tree adjoining grammars. In Proceedings of the 32nd Annual Meeting of the Association for Computa- tional Linguistics (ACL 94). Giorgio Satta. 1994. Tree adjoining grammar pars- ing and boolean matrix multiplication. Computa- tional Linguistics, 20(2):173192. Yves Schabes and Aravind K. Joshi. 1991. Pars- ing with lexicalized tree adjoining grammar. In M. Tomita, editor, Current Issues in Parsing Technologies. Kluwer Academic Publishers. Yves Schabes and Stuart M. Shieber. 1994. An al- ternative conception of tree-adjoining derivation. Computational Linguistics, 20(1):91124. Yves Schabes and Richard C. Waters. 1993. Lexi- calized context-free grammars. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL 93). Yves Schabes and Richard C. Waters. 1995. Tree insertion grammar: A cubic-time parsable formal- ism that lexicalizes context-free grammar without changing the trees produced. Computational Lin- guistics, 21(4):479515. Stuart M. Shieber, Yves Schabes, and Fer- nando C.N. Pereira. 1995. Principles and imple- mentation of deductive parsing. Journal of Logic Programming, 24:336.",
  "15.pdf": "arXiv:cs9811008v1 [cs.CL] 2 Nov 1998 In Proceedings Proceedings of the AMTASIG-IL Second Workshop on Interlinguas, Langhorne, PA, October 1998, pp. 23-30. Translating near-synonyms: Possibilities and preferences in the interlingua Philip Edmonds Department of Computer Science University of Toronto Toronto, Canada M5S 3G4 pedmondscs.toronto.edu 1 Introduction This paper argues that an interlingual representation must explicitly represent some parts of the meaning of a situation as possibilities (or preferences), not as neces- sary or deﬁnite components of meaning (or constraints). Possibilities enable the analysis and generation of nu- ance, something required for faithful translation. Fur- thermore, the representation of the meaning of words is crucial, because it speciﬁes which nuances words can convey in which contexts. In translation it is rare to ﬁnd the exact word that faithfully and directly translates a word of another lan- guage. Often, the target language will provide many near-synonyms for a source language word that differ (from the target word and among themselves) in nuances of meaning. For example, the French fournir could be translated as provide, supply, furnish, offer, volunteer, af- ford, bring, and so on, which differ in ﬁne-grained as- pects of denotation, emphasis, and style. (Figures 1 and 2 show some of the distinctions.) But none of these options may carry the right nuances to match those conveyed by fournir in the source text; unwanted extra nuances may be conveyed, or a desired nuance may be left out. Since an exact match is probably impossible in many situa- tions, faithful translation will require uncovering the nu- ances conveyed by a source word and then determining how the nuances can be conveyed in the target language by appropriate word choices in any particular context. The inevitable mismatches that occur are one type of translation mismatchdifferences of meaning, but not of form, in the source and target language (Kameyama et al., 1991).1 1A separate class of difference, translation divergence, involves dif- ferences in the form of the source and target texts and results from lexical gaps in the target language (in which no single word lexical- izes the meaning of a source word), and from syntactic and colloca- tional constraints imposed by the source language. Paraphrasing the source text in the target language is required in order to preserve the meaning as much as possible (Dorr, 1994; Stede, 1996; Elhadad et al., 1997). But even when paraphrasing, choices between near-synonyms will have to be made, so, clearly, translation mismatches and translation divergences are not independent phenomena. Just as standard semantic content can be incorporated or spread around in different ways, so can nuances of meaning. Provide may suggest foresight and stress the idea of making adequate preparation for something by stocking or ship- ping . . . Supply may stress the idea of replacing, of making up what is needed, or of satisfying a deﬁciency. Furnish may emphasize the idea of ﬁtting something or some- one with whatever is necessary, or sometimes, normal or desirable. Figure 1: An abridged entry from Websters New Dictio- nary of Synonyms (Gove, 1973). Offer and volunteer may both refer to a generous extending of aid, services, or a desired item. Those who volunteer agree by free choice rather than by submission to selec- tion or command. Figure 2: An abridged entry from Choose the Right Word (Hayakawa, 1994). 2 Near-synonyms across languages This section examines how near-synonyms can differ within and across languages. I will discuss some of the speciﬁc problems of lexical representation in an interlin- gual MT system using examples drawn from the French and English versions of the multi-lingual text provided for this workshop. To be as objective as possible, Ill rely on several dictionaries of synonym discrimination including, for English, Gove (1973) and Hayakawa (1994), and for French, Bailly (1970), Benac (1956), and Batchelor and Offord (1993). Unless otherwise stated, the information on differences below comes from one of these reference books. Notation: Below, english :: french indicates that the pair of words or expressions english and french corre- spond to one another in the multi-lingual text (i.e., they are apparent translations of each other). Fine-grained denotational mismatches If a word has near-synonyms, then they most likely differ in ﬁne-grained aspects of denotation. Consider the following pairs: 1. provides :: fournit 2. provided :: apportaient 3. provide :: offrir 4. brought :: fournissait 5. brought :: se chargeait These all share the basic meaning of giving or mak- ing available what is needed by another, but each adds its own nuances. And these are not the only words that the translator could have used: in English, furnish, supply, offer, and volunteer would have been possibil- ities; in French, approvisionner, munir, pourvoir, nan- tir, presenter, among others, could have been chosen. The differences are complex and often language-speciﬁc. Figures 1 and 2 discuss some of the differences between the English words, and ﬁgures 3 and 4 those between the French words. And this is the problem for transla- tion: none of the words match up exactly, and the nu- ances they carry when they are actually used are context- dependent. (Also notice that the usage notes are vague in many cases, using words like may and idee.) Consider this second example: 6a. began :: amorce b. began :: commenca c. started :: au debut Amorcer implies a beginning that prepares for something else; there is no English word that carries the same nu- ance, but begin appears to be the closest match. Com- mencer also translates as begin, although commencer is a general word in French, implying only that the thing begun has a duration. In English, begin differs from start in that the latter can imply a setting out from a certain point after inaction (in opposition to stop). More pairings that exhibit similar ﬁne-grained denota- tional differences include these: 7a. broaden :: elargir b. expand :: etendre c. increase :: accroˆıtre 8a. transformation :: passer b. transition :: transition 9. enable:: permettre 10. opportunities:: perspectives 11. assistance :: assistance There are two main problems in representing the meanings of these words. First, although some of the nuances could be represented by simple features, such as foresight or generous, most of them cannot because they are complex and have an internal structure. They are concepts that relate aspects of the situation. For ex- ample, for furnish, ﬁtting someone with what is nec- essary is not a simple feature; it involves a concept of Fourni a rapport a la quantite et ce dit de ce qui a sufﬁsamment ou en abondance le necessaire. Muni et arme sont relatifs a letat dune chose rendue forte ou capable, muni, plus generale, annoncant un secours pour faire quoi que ce soit. Pourvu comporte un idee de precaution et ce dit bien en par- lant des avantages naturels donnes par une sorte de ﬁnalite . . . Nanti, muni dun gage donne par un debiteur a son creancier, par ext. muni par precaution et, absolumment, assez en- richi pour ne pas craindre lavenir. Figure 3: An abridged entry from Benac (1956). Offrir, cest faire hommage dune chose a quelquun, en man- ifestant le desir quil laccepte, aﬁn que loffre devienne un don. Presenter, cest offrir une chose que lon tient a la main ou qui est la sous les yeux et dont la personne peut a linstant prendre possession. Figure 4: An abridged entry from Bailly (1970). ﬁtting, a patient (the same patient that the overall sit- uation has), a thing that is provided, and the idea of the necessity of that thing to someone. Thus, many nu- ances must be represented as fully-ﬂedged concepts (or instances thereof) in an interlingua. Second, many of the nuances are merely suggested or implied, if they are conveyed at all. That is, they are conveyed indirectlythe reader has the license to decide that such a nuance was unintendedand as such are not necessary conditions for the deﬁnition of the words. This has ramiﬁcations for both the analysis of the source text and the generation of the target text because one has to determine how strongly a certain nuance is intended, if at all (in the source), and then how it should be conveyed, if it can be, in the target language. One should seek to translate indirect expressions as such, and avoid making them direct. One must also avoid choosing a target word that might convey an unwanted implication. In any case, aspects of word meaning that are indirect must be repre- sented as such in the lexicon. Coarse-grained denotational mismatches Sometimes the translator chooses a target word that is se- mantically quite different from the source word, yet still conveys the same basic idea. Considering pair 5, above: bring seems to mean to carry as a contribution, and se charger to take responsibility for. Perhaps there are no good equivalents in the opposite languages for these terms, or alternatively, the words might have been cho- sen because of syntactic or collocational preferences they co-occur with leadership :: ladministration, which are not close translations either. In fact, the desire to use natural-sounding syntactic and collocational structures is probably responsible for many of these divergences. In another case, the pair fac- tors :: raisons occurs perhaps because the translator did not want to literally translate the expressions Many fac- tors contributed to :: Parmi les raisons de. Such mis- matches are outside the scope of this paper, because they fall more into the area of translation divergences. (See Smadja et al. (1996) for research on translating colloca- tions.) Stylistic mismatches Words can also differ on many stylistic dimensions, but formality is the most recognized dimension.2 Consider the following pairs: 12a. plans :: entend bien b. plan :: envisagent de While the French words differ in formality (entend bien is formal, and envisagent de is neutral), the same word was chosen in English. Note that the other French words that could have been chosen also differ in formality: se proposent de has intermediate formality, and comptent, avont lintention, and projetent de are all neutral. Similarly, in 6, above, amorcer is more formal than commencer. Considering the other near-synonyms: the English commence and initiate are quite formal, as is the French initier. Debuter and demarrer are informal, yet both are usually translated by begin, a neutral word in English. (Notice also that the French cognate of the for- mal English commence, commencer, is neutral.) Style, which can be conveyed by both the words and the structure of a text, is best represented as a global property in an interlingual representation. That way, it can inﬂuence all decisions that are made. (It is probably not always necessary to preserve the style of particular words across languages.) A separate issue of style in this text is its use of techni- cal or domain-speciﬁc vocabulary. Consider the follow- ing terms used to refer to the subject of the text: 13a. institution :: institution b. institution :: etablissement c. institution :: association d. joint venture :: association e. programme:: association f. bank :: etablissement g. bank :: banque In French, it appears that association must be used to re- fer to non-proﬁt companies and etablissement or banque for their regulated (for-proﬁt) counterparts. In English institution, among other terms, is used for both. Con- sider also the following pairs: 2Hovy (1988) suggests others including force and ﬂoridity, and Di- Marco et al. (1993) suggest concreteness or vividness. Actually, it seems that the French text is more vividif a text on banking can be considered vivid at allthan the English, using words such as baptisee, eclatant, contagieux, and demunis. 14a. seed capital :: capital initial b. working capital :: fonds de roulement c. equity capital:: capital social Attitudinal mismatches Words also differ in the attitude that they express. For ex- ample, of poor :: demunis, poor can express a derogatory attitude, but demunis (which can be translated as impov- erished) probably expresses a neutral attitude. Consider also people of indigenous background :: Indiens. Atti- tudes must be included in the interlingual representation of an expression, and they must refer to the speciﬁc par- ticipant(s) about whom the speaker is expressing an atti- tude. 3 Representing near-synonyms Before I discuss the requirements of the interlingual rep- resentation, I must ﬁrst discuss how the knowledge of near-synonyms ought to be modelled if we are to account for the complexities of word meaning in an interlingua. In the view taken here, the lexicon is given the central role as bridge between natural language and interlingua. The conventional model of lexical knowledge, used in many computational systems, is not suitable for rep- resenting the ﬁne-grained distinctions between near- synonyms (Hirst, 1995). In the conventional model, knowledge of the world is represented by ostensibly language-neutral concepts that are often organized as an ontology. The denotation of a lexical item is represented as a concept, or a conﬁguration of concepts, and amounts to a direct word-to-concept link. So except for polysemy and (absolute) synonymy, there is no logical difference between a lexical item and a concept. Therefore, words that are nearly synonymous have to be linked each to their own slightly different concepts. The problem comes in trying to represent these slightly different concepts and the relationships between them. Hirst (1995) shows that one ends up with an awkward proliferation of language- dependent concepts, contrary to the interlingual function of the ontology. And this assumes we can even build a representative taxonomy from a set of near-synonyms to begin with. Moreover, the denotation of a word is taken to em- body the necessary and sufﬁcient conditions for deﬁning the word. While this has been convenient for text anal- ysis and lexical choice, since a denotation can be used as an applicability condition of the word, the model is inadequate for representing the nuances of meaning that are conveyed indirectly, which, clearly, are not necessary conditions. An alternative representation is suggested by the prin- ciple behind Goves (1973) synonym usage notes. Words are grouped into a entry if they have the same essential meaning, i.e., that they can be deﬁned in the same terms up to a certain point (p. 25a) and differ only in terms of minor ideas involved in their meanings. We combine this principle with Saussures paradigmatic view that each liefern stellen furnish offer supply versehen offrir apporter versorgen provide pourvoir munir fournir approvisionner French English MAKING-AVAILABLE EVENT TRANSFERRING-POSSESSION GIVING-SOMETHING German Figure 5: The clustered model of lexical knowledge. of a set of synonyms ... has its particular value only be- cause they stand in contrast with one another (Saussure, 1983, p. 114) and envision a representation in which the meaning of a word arises out of a combination of its es- sential denotation (shared with other words) and a set of explicit differences to its near-synonyms. Thus, I propose a clustered model of lexical knowl- edge, depicted in ﬁgure 5. A cluster has two levels of representation: a core concept and peripheral concepts. The core concept is a denotation as in the conventional modela conﬁguration of concepts (that are deﬁned in the ontology) that functions as a necessary applicabil- ity condition (for choice)but it is shared by the near- synonyms in the cluster. In the ﬁgure, the ontological concepts are shown as rectangles; in this case all three clusters denote the concept of MAKING-AVAILABLE. All of the peripheral concepts that the words may differ in denoting, suggesting, or emphasizing are also repre- sented as conﬁgurations of concepts, but they are explic- itly distinguished from the core concept as indirect mean- ings that can be conveyed or not depending on the con- text. In the ﬁgure, the differences between words (in a single language) are shown as dashed lines; not all words need be differentiated. Stylistic, attitudinal, and colloca- tional factors are also encoded in the cluster. Each language has its own set of clusters. Corre- sponding clusters (across languages) need not have the same peripheral concepts since languages may differen- tiate their synonyms in entirely different terms. Differ- ences across languages are represented, for convenience, by dashed lines between clusters, though these would not be used in pure interlingual MT. Essentially, a cluster is a language-speciﬁc formal usage note, an idea originated by DiMarco et al. (1993) that Edmonds (forthcoming) is formalizing. 4 Interlingual representation Crucially, an interlingual representation should not be tied to any particular linguistic structure, whether lexi- cal or syntactic. Assuming that one has constructed an ontology or do- main model (of language-neutral concepts), an interlin- gual representation of a situation is, for us, an instan- tiation of part of the domain knowledge. Both Stede (1996) and Elhadad et al. (1997) have developed such formalisms for representing the input to natural language generation applications (the former to multilingual gen- eration), but they are applicable to interlingual MT as well. The formalisms allow their applications to para- phrase the same input in many ways including realiz- ing information at different syntactic ranks and cover- ingincorporating the input in different ways. For them, generation is a matter of satisfying two types of con- straints: (1) covering the whole input structure with a set of word denotations (thereby choosing the words), and (2) building a well-formed syntactic structure out of the words. But while their systems can provide many options to choose from, they lack the complementary ability to actually choose which is the most appropriate. Now, ﬁnding the most appropriate translation of a word involves a tradeoff between many possibly conﬂict- ing desires to express certain nuances in certain ways, to establish the right style, to observe collocational pref- erences, and to satisfy syntactic constraints. This sug- gests that lexical choice is not a matter of satisfying con- straints (i.e., of using the necessary applicability condi- tions of a word), but rather of attempting to meet a large set of preferences. Thus, a distinction must be made be- tween knowledge that should be treated as preferences as opposed to constraints in the interlingual representation. In the generation stage of MT, one attempts to choose the near-synonym from a cluster (activated because of the constraints) whose peripheral concepts best meet the most preferences. Turning to the analysis stage of MT, since many nu- ances are expressed indirectly and are inﬂuenced by the context, one cannot know for sure whether they have been expressed unless one performs a very thorough analysis. Indeed, it might not be possible for even a thor- ough analysis to decide whether a nuance was expressed, or how indirectly it was expressed, given the context- dependent nature of word meaning. Thus, on the basis of the knowledge of what words can express, stored in the clusters, the analysis stage would output an interlingual representation that includes possibilities of what was ex- pressed. The possibilities then become preferences dur- ing generation. 5 Examples Figures 69 give examples of interlingual representations for four segments of the text that involve some of the words discussed in section 2. Since my focus is on word meanings, I will not give complete representations of the expressions. Also note that while I use speciﬁc ontologi- cal concepts in these descriptions, this in no way implies that I claim these are the right concepts to representin fact, some are quite crude. A good ontology is crucial to MT, and I assume that such an ontology will in due course be constructed. I have used attribute-value structures, but any equiv- alent formalism would do. Square brackets enclose re- cursive structures of instantiations of ontological con- cepts. Names of instances are in lowercase; concepts are capitalized; relations between instances are in up- percase; and cross-reference is indicated by a digit in a square. A whole interlingual representation is sur- rounded by brace brackets and consists of exactly one speciﬁcation of the situation and any number of possi- bilities, attitudes, and stylistic preferences. The situa- tion encodes the information one might ﬁnd in a tradi- tional interlingual representationthe deﬁnite portion of meaning to be expressed. A possibility takes as a value a four-part structure of (1) frequency (never, sometimes, or always), which represents the degree of possibility; (2) strength (weak, medium, or strong), which represents how strongly the nuance is conveyed; (3) type (emphasis, suggestion, implication, or denotation), which represents how the nuance is conveyed; and (4) an instance of a con- cept. The style and attitude attributes should be self- explanatory. As for content, some of the meanings were discussed in section 2, and the rest are derived from the aforementioned dictionaries. Comments (labelled with ) are included to indicate which words gave rise to which possibilities. 6 Conclusion This paper has motivated the need to represent possibili- ties (or preferences) in addition to necessary components (or constraints) in the interlingual representation of a sit- uation. Possibilities are required because words can con- vey a myriad of sometimes indirect nuances of meaning depending on the context. Some examples of how one could represent possibilities were given. Acknowledgements For comments and advice, I thank Graeme Hirst. This work is ﬁnancially supported in part by the Natural Sci- ences and Engineering Research Council of Canada. References Rene Bailly. 1970. Dictionnaire des synonymes de la langue francaise. Paris Larousse. Ronald E. Batchelor and Malcolm H. Offord. 1993. Us- ing French Synonyms. Cambridge University Press. Henri Benac. 1956. Dictionnaire des synonymes. Paris Hachette. Chrysanne DiMarco, Graeme Hirst, and Manfred Stede. 1993. The semantic and stylistic differentiation of synonyms and near-synonyms. In AAAI Spring Sym- posium on Building Lexicons for Machine Translation, pages 114121, Stanford, CA, March. Bonnie J. Dorr. 1994. Machine translation divergences: A formal description and proposed solution. Compu- tational Linguistics, 20(4):597634. Philip Edmonds. forthcoming. Semantic Representa- tions of Near-Synonyms for Automatic Lexical Choice. Ph.D. thesis, Department of Computer Science, Uni- versity of Toronto. Michael Elhadad, Kathleen McKeown, and Jacques Robin. 1997. Floating constraints in lexical choice. Computational Linguistics, 2(23):195240. Philip B. Gove, editor. 1973. Websters New Dictionary of Synonyms. G.  C. Merriam Co.                                                                                                                                                  situation   provide1 instance-of MakingAvailable AGENT 1  accion-international instance-of NonProﬁtOrganization  OBJECT   assistance1 instance-of Helping ATTRIBUTE  technical1 instance-of Technical    RECIPIENT 2  network instance-of Network    possibility         frequency sometimes type suggestion concept   foresight1 instance-of Foreseeing AGENT 1            from the word provides possibility              frequency sometimes type emphasis concept   prepare1 instance-of Preparing AGENT 1 ATTRIBUTE  adequate instance-of Adequacy                  from provides possibility                frequency always type suggestion concept   subordinate-status instance-of Status DEGREE  subordinate instance-of Subordinate  ATTRIBUTE-OF 1 RELATIVE-TO 2                   from assistance                                                                                                                                                  ACCION International ... provides technical assistance to a network ... ACCION International ... fournit une assistance technique a un reseau ... Figure 6: Interlingual representation of the equivalent sentences shown above. Includes four possibilities of what is expressed. S. I. Hayakawa, editor. 1994. Choose the Right Word: A Contemporary Guide to Selecting the Precise Word for Every Situation. HarperCollins Publishers, New York. Graeme Hirst. 1995. Near-synonymy and the structure of lexical knowledge. In AAAI Symposium on Repre- sentation and Acquisition of Lexical Knowledge: Poly- semy, Ambiguity, and Generativity, pages 5156, Stan- ford, CA, March. Eduard Hovy. 1988. Generating Natural Language Un- der Pragmatic Constraints. Lawrence Erlbaum Asso- ciates. Megumi Kameyama, Ryo Ochitani, Stanley Peters, and Hidetoshi Sirai. 1991. Resolving translation mis- matches with information ﬂow. In Proceedings of the                                                                                                                                                        situation   provide2 instance-of MakingAvailable AGENT 1  prodem-venture instance-of NonProﬁtJointVenture  RECIPIENT 2  workers instance-of Worker  OBJECT   credit-and-training instance-of CreditAndTraining AGENT-OF 3   broaden instance-of Increasing PATIENT 4   opportunity instance-of Chance POSSESSED-BY 2 REGARDING 5  employment instance-of Employment          possibility         frequency sometimes type implication concept   scope instance-of Scope MANNER-OF 3            from the word broaden possibility         type implication concept   desire instance-of Desiring AGENT 2 PATIENT 5            from opportunities possibility             frequency sometimes strength weak type suggestion concept   provoke instance-of Provoking AGENT 4 PATIENT 2                from opportunities                                                                                                                                                        PRODEM ... provided credit and training to broaden employment opportunities ... PRODEM ... doffrir ... des possibilites de credit et de formation pour elargir leurs perspectives demploi Figure 7: Another interlingual representation with possibilities of what is expressed. 29th Annual Meeting of the Association for Computa- tional Linguistics, pages 193200. Ferdinand de Saussure. 1983. Course in General Lin- guistics. G. Duckworth, London. Translation by Roy Harris of Cours de linguistique generale, 1916. Frank Smadja, Kathleen McKeown, and Vasileios Hatzi- vassiloglou. 1996. Translating collocations for bilin- gual lexicons: A statistical approach. Computational Linguistics, 22(1):138. Manfred Stede. 1996. Lexical paraphrases in mul- tilingual sentence generation. Machine Translation, 11:75107.                                                      situation 1   begin instance-of Beginning OBJECT  transition instance-of StateChange  TIME  year-1989 instance-of Year    possibility       type implication concept   prepare2 instance-of Preparing AGENT 1          from amorcee style  formality  level high                                                       The transition ... began in 1989. La transition, amorcee en 1989 ... Figure 8: Interlingual representation with a stylistic preference (for high formality).                                        situation 1   workers instance-of Worker ATTRIBUTE   poor instance-of Poor DEGREE  high    ATTRIBUTE  self-employed instance-of EmploymentStatus    attitude  type neutral of 1                                         the very poor self-employed travailleurs independents les plus demunis Figure 9: Interlingual representation with an expressed attitude.",
  "16.pdf": "arXiv:cs9811009v1 [cs.CL] 2 Nov 1998 In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Madrid, 1997, pp. 507509. Choosing the Word Most Typical in Context Using a Lexical Co-occurrence Network Philip Edmonds Department of Computer Science, University of Toronto Toronto, Canada, M5S 3G4 pedmondscs.toronto.edu Abstract This paper presents a partial solution to a component of the problem of lexical choice: choosing the syn- onym most typical, or expected, in context. We apply a new statistical approach to representing the context of a word through lexical co-occurrence networks. The im- plementation was trained and evaluated on a large cor- pus, and results show that the inclusion of second-order co-occurrence relations improves the performance of our implemented lexical choice program. 1 Introduction Recent work views lexical choice as the process of map- ping from a set of concepts (in some representation of knowledge) to a word or phrase (Elhadad, 1992; Stede, 1996). When the same concept admits more than one lexicalization, it is often difﬁcult to choose which of these synonyms is the most appropriate for achieving the desired pragmatic goals; but this is necessary for high-quality machine translation and natural language generation. Knowledge-based approaches to representing the po- tentially subtle differences between synonyms have suf- fered from a serious lexical acquisition bottleneck (Di- Marco, Hirst, and Stede, 1993; Hirst, 1995). Statisti- cal approaches, which have sought to explicitly repre- sent differences between pairs of synonyms with respect to their occurrence with other speciﬁc words (Church et al., 1994), are inefﬁcient in time and space. This paper presents a new statistical approach to mod- eling context that provides a preliminary solution to an important sub-problem, that of determining the near- synonym that is most typical, or expected, if any, in a given context. Although weaker than full lexical choice, because it doesnt choose the best word, we believe that it is a necessary ﬁrst step, because it would allow one to determine the effects of choosing a non-typical word in place of the typical word. The approach relies on a gener- alization of lexical co-occurrence that allows for an im- plicit representation of the differences between two (or more) words with respect to any actual context. For example, our implemented lexical choice program selects mistake as most typical for the gap in sen- tence (1), and error in (2). (1) However, such a move also would run the risk of cutting deeply into U.S. economic growth, which is why some economists think it would be a big {error  mistake  oversight}. (2) The {error  mistake  oversight} was magniﬁed when the Army failed to charge the standard percentage rate for packing and handling. 2 Generalizing Lexical Co-occurrence 2.1 Evidence-based Models of Context Evidence-based models represent context as a set of fea- tures, say words, that are observed to co-occur with, and thereby predict, a word (Yarowsky, 1992; Golding and Schabes, 1996; Karow and Edelman, 1996; Ng and Lee, 1996). But, if we use just the context surrounding a word, we might not be able to build up a representation satis- factory to uncover the subtle differences between syn- onyms, because of the massive volume of text that would be required. Now, observe that even though a word might not co- occur signiﬁcantly with another given word, it might nevertheless predict the use of that word if the two words are mutually related to a third word. That is, we can treat lexical co-occurrence as though it were moderately transitive. For example, in (3), learn provides evidence for task because it co-occurs (in other contexts) with dif- ﬁcult, which in turn co-occurs with task (in other con- texts), even though learn is not seen to co-occur signiﬁ- cantly with task. (3) The teams most urgent task was to learn whether Chernobyl would suggest any safety ﬂaws at KWU-designed plants. So, by augmenting the contextual representation of a word with such second-order (and higher) co-occurrence relations, we stand to have greater predictive power, as- suming that we assign less weight to them in accordance with their lower information content. And as our results will show, this generalization of co-occurrence is neces- sary. We can represent these relations in a lexical co- occurrence network, as in ﬁgure 1, that connects lexi- cal items by just their ﬁrst-order co-occurrence relations. taskNN urgentJJ 1.41 difficultJJ 2.60 learnVB 0.41 easyJJ 1.37 calledVBD 1.30 forcesNNS 1.96 1.36 costlyJJ 1.33 findVB 3.03 safetyNN 1.35 flawsNNS 1.40 ToTO 2.19 suggestVB 1.36 1.89 2.54 practiceNN 1.25 teamNN 1.30 plantsNNS 1.24 Figure 1: A fragment of the lexical co-occurrence net- work for task. The dashed line is a second-order relation implied by the network. Second-order and higher relations are then implied by transitivity. 2.2 Building Co-occurrence Networks We build a lexical co-occurrence network as follows: Given a root word, connect it to all the words that sig- niﬁcantly co-occur with it in the training corpus;1 then, recursively connect these words to their signiﬁcant co- occurring words up to some speciﬁed depth. We use the intersection of two well-known measures of signiﬁcance, mutual information scores and t-scores (Church et al., 1994), to determine if a (ﬁrst-order) co- occurrence relation should be included in the network; however, we use just the t-scores in computing signif- icance scores for all the relations. Given two words, w0 and wd, in a co-occurrence relation of order d, and a shortest path P(w0,wd)  (w0,...,wd) between them, the signiﬁcance score is sig(w0,wd)  1 d3  wiP(w1,wd) t(wi1,wi) i This formula ensures that signiﬁcance is inversely pro- portional to the order of the relation. For example, in the network of ﬁgure 1, sig(task,learn)  [t(task,difﬁcult) 1 2t(difﬁcult,learn)]8  0.41. A single network can be quite large. For instance, the complete network for task (see ﬁgure 1) up to the third- order has 8998 nodes and 37,548 edges. 2.3 Choosing the Most Typical Word The amount of evidence that a given sentence provides for choosing a candidate word is the sum of the signiﬁ- cance scores of each co-occurrence of the candidate with a word in the sentence. So, given a gap in a sentence S, 1Our training corpus was the part-of-speech-tagged 1989 Wall Street Journal, which consists of N  2,709,659 tokens. No lemmati- zation or sense disambiguation was done. Stop words were numbers, symbols, proper nouns, and any token with a raw frequency greater than F  800. Set POS Synonyms (with training corpus frequency) 1 JJ difﬁcult (352), hard (348), tough (230) 2 NN error (64), mistake (61), oversight (37) 3 NN job (418), task (123), duty (48) 4 NN responsibility (142), commitment (122), obligation (96), burden (81) 5 NN material (177), stuff (79), substance (45) 6 VB give (624), provide (501), offer (302) 7 VB settle (126), resolve (79) Table 1: The sets of synonyms for our experiment. we ﬁnd the candidate c for the gap that maximizes M(c,S)   wS sig(c,w) For example, given S as sentence (3), above, and the net- work of ﬁgure 1, M(task,S)  4.40. However, job (using its own network) matches best with a score of 5.52; duty places third with a score of 2.21. 3 Results and Evaluation To evaluate the lexical choice program, we selected sev- eral sets of near-synonyms, shown in table 1, that have low polysemy in the corpus, and that occur with similar frequencies. This is to reduce the confounding effects of lexical ambiguity. For each set, we collected all sentences from the yet- unseen 1987 Wall Street Journal (part-of-speech-tagged) that contained any of the members of the set, ignoring word sense. We replaced each occurrence by a gap that the program then had to ﬁll. We compared the correct- ness of the choices made by our program to the baseline of always choosing the most frequent synonym accord- ing to the training corpus. But what are the correct responses? Ideally, they should be chosen by a credible human informant. But re- grettably, we are not in a position to undertake a study of how humans judge typical usage, so we will turn instead to a less ideal source: the authors of the Wall Street Jour- nal. The problem is, of course, that authors arent always typical. A particular word might occur in a pattern in which another synonym was seen more often, making it the typical choice. Thus, we cannot expect perfect accu- racy in this evaluation. Table 2 shows the results for all seven sets of syn- onyms under different versions of the program. We var- ied two parameters: (1) the window size used during the construction of the network: either narrow (4 words), medium ( 10 words), or wide ( 50 words); (2) the maximum order of co-occurrence relation allowed: 1, 2, or 3. The results show that at least second-order co- occurrences are necessary to achieve better than baseline accuracy in this task; regular co-occurrence relations are insufﬁcient. This justiﬁes our assumption that we need Set 1 2 3 4 5 6 7 Size 6665 1030 5402 3138 1828 10204 1568 Baseline 40.1 33.5 74.2 36.6 62.8 45.7 62.2 1 31.3 18.7 34.5 27.7 28.8 33.2 41.3 Narrow 2 47.2 44.5 66.2 43.9 61.9a 48.1 62.8a 3 47.9 48.9 68.9 44.3 64.6a 48.6 65.9 1 24.0 25.0 26.4 29.3 28.8 20.6 44.2 Medium 2 42.5 47.1 55.3 45.3 61.5a 44.3 63.6a 3 42.5 47.0 53.6     Wide 1 9.2 20.6 17.5 20.7 21.2 4.1 26.5 2 39.9a 46.2 47.1 43.2 52.7 37.7 58.6 aDifference from baseline not signiﬁcant. Table 2: Accuracy of several different versions of the lexical choice program. The best score for each set is in boldface. Size refers to the size of the sample collection. All differences from baseline are signiﬁcant at the 5 level according to Pearsons χ2 test, unless indicated. more than the surrounding context to build adequate con- textual representations. Also, the narrow window gives consistently higher ac- curacy than the other sizes. This can be explained, per- haps, by the fact that differences between near-synonyms often involve differences in short-distance collocations with neighboring words, e.g., face the task. There are two reasons why the approach doesnt do as well as an automatic approach ought to. First, as men- tioned above, our method of evaluation is not ideal; it may make our results just seem poor. Perhaps our results actually show the level of typical usage in the newspa- per. Second, lexical ambiguity is a major problem, af- fecting both evaluation and the construction of the co- occurrence network. For example, in sentence (3), above, it turns out that the program uses safety as evi- dence for choosing job (because job safety is a frequent collocation), but this is the wrong sense of job. Syntactic and collocational red herrings can add noise too. 4 Conclusion We introduced the problem of choosing the most typical synonym in context, and gave a solution that relies on a generalization of lexical co-occurrence. The results show that a narrow window of training context (4 words) works best for this task, and that at least second-order co-occurrence relations are necessary. We are planning to extend the model to account for more structure in the narrow window of context. Acknowledgements For comments and advice, I thank Graeme Hirst, Eduard Hovy, and Stephen Green. This work is ﬁnancially sup- ported by the Natural Sciences and Engineering Council of Canada. References Church, Kenneth Ward, William Gale, Patrick Hanks, Donald Hindle, and Rosamund Moon. 1994. Lexical substitutabil- ity. In B.T.S. Atkins and A. Zampolli, editors, Computa- tional Approaches to the Lexicon. Oxford University Press, pages 153177. DiMarco, Chrysanne, Graeme Hirst, and Manfred Stede. 1993. The semantic and stylistic differentiation of synonyms and near-synonyms. In AAAI Spring Symposium on Building Lexicons for Machine Translation, pages 114121, Stanford, CA, March. Elhadad, Michael. 1992. Using Argumentation to Control Lexical Choice: A Functional Uniﬁcation Implementation. Ph.D. thesis, Columbia University. Golding, Andrew R. and Yves Schabes. 1996. Combin- ing trigram-based and feature-based methods for context- sensitive spelling correction. In Proceedings of the 34th An- nual Meeting of the Association for Computational Linguis- tics. Hirst, Graeme. 1995. Near-synonymy and the structure of lexical knowledge. In AAAI Symposium on Representation and Acquisition of Lexical Knowledge: Polysemy, Ambigu- ity, and Generativity, pages 5156, Stanford, CA, March. Karow, Yael and Shimon Edelman. 1996. Learning similarity- based word sense disambiguation from sparse data. In Pro- ceedings of the Fourth Workshop on Very Large Corpora, Copenhagen, August. Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multi- ple sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics. Stede, Manfred. 1996. Lexical Semantics and Knowledge Rep- resentation in Multilingual Sentence Generation. Ph.D. the- sis, University of Toronto. Yarowsky, David. 1992. Word-sense disambiguation using sta- tistical models of Rogets categories trained on large cor- pora. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 454 460.",
  "17.pdf": "arXiv:cs9811016v1 [cs.CL] 11 Nov 1998 Comparing a statistical and a rule-based tagger for German (Published in Computers, Linguistics, and Phonetics between Language and Speech. Proc. of the 4th Conference on Natural Language Processing - KONVENS-98 Bonn, 1998. pp.125-137) Martin Volk and Gerold Schneider University of Zurich Department of Computer Science Computational Linguistics Group Winterthurerstr. 190, CH-8057 Zurich {volk,gschneid}ifi.unizh.ch Abstract In this paper we present the results of com- paring a statistical tagger for German based on decision trees and a rule-based Brill-Tagger for German. We used the same training cor- pus (and therefore the same tag-set) to train both taggers. We then applied the taggers to the same test corpus and compared their re- spective behavior and in particular their error rates. Both taggers perform similarly with an error rate of around 5. From the detailed er- ror analysis it can be seen that the rule-based tagger has more problems with unknown words than the statistical tagger. But the results are opposite for tokens that are many-ways ambigu- ous. If the unknown words are fed into the tag- gers with the help of an external lexicon (such as the Gertwol system) the error rate of the rule- based tagger drops to 4.7, and the respective rate of the statistical taggers drops to around 3.7. Combining the taggers by using the out- put of one tagger to help the other did not lead to any further improvement. In diesem Beitrag beschreiben wir die Re- sultate aus unserem Vergleich eines statistis- chen Taggers, der auf Entscheidungsbaumen basiert, und eines regel-basierten Brill-Taggers fur das Deutsche. Beim Vergleich benutzten wir dasselbe Trainingskorpus (und damit dasselbe Tagset), um beide Tagger zu trainieren. Danach wurden beide Tagger auf dasselbe Testkorpus angewendet und ihr jeweiliges Verhalten und ihre Fehlerraten verglichen. Beide Tagger liegen ungefahr bei 5 Fehlerrate. Bei der detail- lierten Fehleranalyse sieht man, dass der regel- basierte Tagger grossere Probleme bei unbekan- nten Wortformen hat als der statistische Tag- ger. Bei vielfach ambigen Wortformen ist das Ergebnis jedoch umgekehrt. Wenn man die un- bekannten Wortformen mit Hilfe eines externen Lexikons (z.B. mit dem Gertwol-System) re- duziert, sinkt die Fehlerrate des regel-basierten Taggers auf 4,7 und die entsprechende Rate des statistischen Taggers auf 3,7. Eine Kombi- nation der Tagger, der Output des einen als Hil- festellung fur den anderen, brachte keine weitere Verbesserung. 1 Introduction In recent years a number of part-of-speech tag- gers have been developed for German. (Lezius et al., 1996) list 6 taggers (all of which work with statistical methods) and provide compar- ison ﬁgures. They report that for a small tagset the accuracy of these 6 taggers varies from 92.8 to 97. But these ﬁgures do not tell us much about the comparative behavior of the taggers since the ﬁgures are based on diﬀerent tagsets, diﬀerent training corpora, and diﬀer- ent test corpora. A more rigorous approach to comparison is necessary to obtain valid results. Such an approach has been presented by (Teufel et al., 1996). They have developed an elabo- rate methodology for comparing taggers includ- ing tagger evaluation, tagset evaluation and text type evaluation. Tagger evaluation Tests allowing to assess the impact of diﬀerent tagging methods, by comparing the performance of diﬀerent taggers on the same training and test data, using the same tagset. Tagset evaluation Tests allowing to assess the impact of tagset modiﬁcations on the results, by using diﬀerent versions of a given tagset on the same texts. Text type evaluation Tests allowing to as- sess the impact of linguistic diﬀerences be- tween training texts and application texts, by using texts from diﬀerent text types in training and testing, tagsets and taggers being unchanged otherwise. In this paper we will focus on Tagger eval- uation for the most part, and only in section 5 will we brieﬂy sidestep to Text type evalua- tion. (Teufel et al., 1996) used their methodol- ogy only on two statistical taggers for German, the Xerox HMM tagger (Cutting et al., 1992) and the TreeTagger (Schmid, 1995; Schmid and Kempe, 1996). On contrast, we will compare one of these statistical taggers, the TreeTag- ger, to a rule-based tagger for German, the Brill-Tagger (Brill, 1992; Brill, 1994). Such a comparison is worthwhile since (Samuelsson and Voutilainen, 1997) have shown for English that their rule-based tagger, a constraint grammar tagger, outperforms any known statistical tag- ger. 2 Our Tagger Evaluation For our evaluation we used a manually tagged corpus of around 70000 tokens which we ob- tained from the University of Stuttgart.1 The texts in that corpus are taken from the Frank- furter Rundschau, a daily newspaper. We split the corpus into a 78 training corpus (60710 to- kens) and a 18 test corpus (8887 tokens) using a tool supplied by Eric Brill that divides a cor- pus sentence by sentence. The test corpus then contains sentences from many diﬀerent sections of the corpus. The average rate of ambiguity in the test corpus is 1.50. That means that on average for any token in the test corpus there is a choice of 1.5 tags in the lexicon, if the token is in the lexion. 1342 tokens from the test cor- pus are not present in the training corpus and are therefore not in the lexicon (these are called lexicon gaps by (Teufel et al., 1996)). The corpus is tagged with the STTS, the Stuttgart-Tubingen TagSet (Schiller et al., 1995; Thielen and Schiller, 1996). This tagset consists of 54 tags, including 3 tags for punc- tuation marks. We modiﬁed the tagset in one little aspect. The STTS contains one tag for both digit-sequence numbers (e.g. 2, 11, 100) 1Thanks to Uli Heid for making this corpus available to us. and letter-sequence numbers (two, eleven, hun- dred). The tag is called CARD since it stands for all cardinal numbers. We added a new tag, CARDNUM, for digit-sequence numbers and restricted the use of CARD to letter-sequence numbers. The assumption was that this move makes it easier for the taggers to recognize un- known numbers, most of which will be digit- sequence numbers. 2.1 Training the TreeTagger In a ﬁrst phase we trained the TreeTagger with its standard parameter settings as given by the author of the tagger.2 That is, it was trained with 1. Context length set to 2 (number of pre- ceding words forming the tagging context). Context length 2 corresponds to a trigram context. 2. Minimal decision tree gain set to 0.7. If the information gain at a leaf node of the decision tree is below this threshold, the node is deleted. 3. Equivalence class weight set to 0.15. This weight of the equivalence class is based on probability estimates. 4. Aﬃx tree gain set to 1.2. If the information gain at a leaf of an aﬃx tree is below this threshold, it is deleted. The training took less than 2 minutes and created an output ﬁle of 630 kByte. Using the tagger with this output ﬁle to tag the test cor- pus resulted in an error rate of 4.73. Table 1 gives an overview of the errors. Column 1 lists the ambiguity rates, i.e. the number of tags available to a token according to the lexicon. Note that the lexicon was built solely on the basis of the training corpus. From columns 1 and 2 we learn that 1342 tokens from the test corpus were not in lexicon, 5401 tokens in the test corpus have exactly one tag in the lexicon, 993 tokens have two tags in the lexicon and so on. Column 3, labelled correct, dis- plays the number of tokens correctly tagged by the TreeTagger. It is obvious that the correct assignment of tags is most diﬃcult for tokens 2These parameters are explained in the README ﬁle that comes with the tagger. ambiguity tokens in  correct in  LE in  DE in  0 1342 15.10 1128 84.05 214 15.95 0 0.00 1 5401 60.77 5330 98.69 71 1.31 0 0.00 2 993 11.17 929 93.55 3 0.30 61 6.14 3 795 8.95 757 95.22 0 0.00 38 4.78 4 260 2.93 240 92.31 0 0.00 20 7.69 5 96 1.08 83 86.46 0 0.00 13 13.54 total 8887 100.00 8467 95.27 288 3.24 132 1.49 Table 1: Error statistics of the TreeTagger that are not in the lexicon (84.05) and for to- kens that are many ways ambiguous (86.46 for tokens that are 5-ways ambiguous). The errors made by the tagger can be split into lexical errors (LE; column 4) and disam- biguation errors (DE; column 5). Lexical errors occur when the correct tag is not available in the lexicon. All errors for tokens not in the lex- icon are lexical errors (214). In addition there are a total of 74 lexical errors in the ambigu- ity rates 1 and 2 where the correct tag is not in the lexicon. On the contrary, disambiguation errors occur when the correct tag is available but the tagger picks the wrong one. Such errors can only occur if the tagger has a choice among at least two tags. Thus we get a rate of 3.24 lexical errors and 1.49 disambiguation errors adding up to the total error rate of 4.73. It should be noted that this error rate is higher than the error rate given for the Tree- Tagger in (Teufel et al., 1996). There, the TreeTagger had been trained over 62860 tokens and tested over 13416 tokens of a corpus very similar to ours (50000 words from the Frank- furter Rundschau plus 25000 words from the Stuttgarter Zeitung). (Teufel et al., 1996) re- port on an error rate of only 3.0 for the Tree- Tagger. It could be that they were using dif- ferent training parameters, these are not listed in the paper. But more likely they were using a more complete lexicon. They report on only 240 lexicon gaps among the 13416 test tokens. 2.2 Training the Brill-Tagger In parallel with the TreeTagger we trained the Brill-Tagger with our training corpus using the following parameter settings. Since we had some experience with training the Brill-Tagger we set the parameters slightly diﬀerent from the Brills suggestions.3 1. The threshold for the best found lexical rule was set to 2. The learner terminates when the score of the best found rule drops below this threshold. (Brill suggests 4 for a training corpus of 50K-100K words.) 2. The threshold for the best found contextual rule was set to 1. The learner terminates when the score of the best found rule drops below this threshold. (Brill suggests 3 for a training corpus of 50K-100K words.) 3. The bigram restriction value was set to 500. This tells the rule learner to only use bi- gram contexts where one of the two words is among the 500 most frequent words. A higher number will increase the accuracy at the cost of further increasing the training time. (Brill suggests 300.) Training this tagger takes much longer than training the TreeTagger. Our training step took around 30 hours (!!) on a Sun Ultra-Sparc work- station. It resulted in: 1. a fullform lexicon with 14147 entries (212 kByte) 2. a lexical-rules ﬁle with 378 rules (9 kByte) 3. a context-rules ﬁle with 329 rules (8 kByte) 4. a bigram list with 42279 entries (609 kByte) Using the tagger with this training output to tag the test corpus resulted in an error rate of 5.25. Table 2 gives an overview of the errors. 3The suggestions for the tagging parameters of the Brill-Tagger are given in the README ﬁle that is dis- tributed with the tagger. ambiguity tokens in  correct in  LE in  DE in  0 1342 15.10 1094 81.52 248 18.48 0 0.00 1 5401 60.77 5330 98.69 71 1.31 0 0.00 2 993 11.17 906 91.24 3 0.30 84 8.46 3 795 8.95 758 95.35 0 0.00 37 4.65 4 260 2.93 245 94.23 0 0.00 15 5.77 5 96 1.08 87 90.62 0 0.00 9 9.38 total 8887 100.00 8420 94.75 322 3.62 145 1.63 Table 2: Error statistics of the Brill-Tagger It is striking that the overall result is very similar to the TreeTagger. A closer look re- veals interesting diﬀerences. The TreeTagger is clearly better than the Brill-Tagger in dealing with unknown words (i.e. tokens not in the lex- icon). There, the TreeTagger reaches 84.05 correct assignments which is 2.5 better than the Brill-Tagger. On the opposite side of the ambiguity spectrum the Brill-Tagger is superior to the TreeTagger in disambiguating between highly ambiguous tokens. For 4-way ambiguous tokens it reaches 94.23 correct assignments (a plus of 1.9 over the TreeTagger) and even for 5-way ambiguous tokens it still reaches 90.62 correct tags which is 4.1 better than the Tree- Tagger. 2.3 Error comparison We then compared the types of errors made by both taggers. An error type is deﬁned by the tuple (correct tag, tagger tag), where correct tag is the manually assigned tag and tagger tag is the automatically assigned tag. Both taggers produce about the same number of error types (132 for the TreeTagger and 131 for the Brill-Tagger). Table 3 lists the most fre- quent error types for both taggers. The biggest problem for both taggers is the distinction be- tween proper nouns (NE) and common nouns (NN). This corresponds with the ﬁndings in (Teufel et al., 1996). The distribution of proper and common nouns is very similar in German and is therefore diﬃcult to distinguish by the taggers. er wollte auch WeberNN?NE? einstellen The second biggest problem results from the distinction between diﬀerent forms of full verbs: ﬁnite verbs (VVFIN), inﬁnite verbs (VVINF), and past participle verbs (VVPP). This prob- lem is caused by the limited window size of both taggers. The TreeTagger uses trigrams for its estimations, and the Brill-Tagger can base its decisions on up to three tokens to the right and to the left. This is rather limited if we consider the possible distance between the ﬁ- nite verb (in second position) and the rest of the verb group (in sentence ﬁnal position) in German main clauses. In addition, the taggers cannot distinguish between main and subordi- nate clause structure. ... weil wir die Probleme schon kennenVVFIN. Wir sollten die Probleme schon kennenVVINF. A third frequent error type arises between verb forms and adjectives (ADJA: adjective used as an attribute, inﬂected form; ADJD: ad- jective in predicative use, typically uninﬂected form). It might be surprising that the Brill- Tagger has so much diﬃculty to tell apart a ﬁ- nite verb and an inﬂected adjective (19 errors). But this can be explained by looking at the lex- ical rules learned by this tagger. These rules are used by the Brill-Tagger to guess a tag for un- known words (Brill, 1994). And the ﬁrst lexical rule learned from our training corpus says that a word form ending in the letter e should be treated as an adjective (ADJA). Of course this assignment can be overridden by other lexical rules or contextual rules, but these obviously miss some 19 cases. On the other hand it is surprising that the TreeTagger gets mixed up 8 times by past par- ticiple modal verbs (VMPP) which should be digit-sequence cardinal numbers (CARDNUM). There are 10 additional cases where a digit- sequence cardinal number was interpreted as some other tag by the TreeTagger. But there TreeTagger errors Brill-Tagger errors number correct tag tagger tag number correct tag tagger tag 48 NE NN 54 NE NN 21 VVINF VVFIN 31 NN NE 20 NN NE 19 VVFIN VVINF 17 VVFIN VVINF 19 VVFIN ADJA 10 VVPP VVFIN 17 VVINF VVFIN 10 VVFIN VVPP 15 VVPP VVFIN 8 CARDNUM VMPP 11 VVPP ADJD 7 ADJD VVFIN 11 ADJD VVFIN 7 ADJD ADV 8 VVINF ADJA Table 3: Most frequent error types are only 3 similar errors for the Brill-Tagger since its lexical rules are well suited to recog- nize unknown digit-sequence numbers. 3 Using an external lexicon Let us sum up the results of the above compar- ison and see if we can improve tagging accuracy by using an external lexicon. The above com- parison showed that: 1. The Brill-Tagger is better in recognizing special symbol items such as digit-sequence cardinal numbers, and it is better in dis- ambiguating tokens which are many-ways ambiguous in the lexicon. 2. The TreeTagger is better in dealing with unknown word forms. At ﬁrst sight it seems easiest to improve the Brill-Tagger by reducing its unknown word problem. We employed the Gertwol system (Oy, 1994) a wide-coverage morphological an- alyzer to ﬁll up the tagger lexicon before tag- ging starts. That means we extracted all un- known word forms4 from the test corpus and had Gertwol analyse them. From the 1342 un- known tokens we get 1309 types which we feed to Gertwol. Gertwol is able to analyse 1205 of these types. Gertwols output is mapped to the respective tags, and every word form with all possible tags is added temporarily to the tagger lexicon. In this way the tagger starts tagging the test corpus with an almost complete lexicon. The remaining lexicon gaps are the few words 4Unknown word forms in the test corpus are all tokens not seen in the training corpus. Gertwol cannot analyse. In our test corpus 109 tokens remain unanalysed. Our experiments showed a slight improve- ment in accuracy (about 0.5), but by far not as much as we had expected. The alternative of ﬁlling up the tagger lexicon by training over the whole corpus resulted in an improvement of around 3.5, an excellent tagger accuracy of more than 98. Note that we only used the lexicon ﬁlled in this way but the rules as learned from the training corpus alone. But, of course, it is an unrealistic scenario to know in advance (i.e. during tagger training) the text to be tagged. The diﬀerence between using a large exter- nal lexicon such as Gertwol and using the in- ternal vocabulary is due to two facts. First, Gertwol increases the average ambiguity of to- kens since it gives every possible tag for a word form. The internal vocabulary will only provide the tag occuring in the corpus. Second, in case of multiple tags for a word form the Brill-Tagger needs to know the most likely tag. This is very important for the Brill-Tagger algorithm. But Gertwol gives all possible tags in an arbitrary order. One solution is to sort Gertwols output according to overall tag probabilities. These can be computed from the frequencies of every tag in the training corpus irrespective of the word form. Using these rough probabilities improved the results in our experiments by about 0.2. This means that the best result for combining Gertwol with the Brill-Tagger is at 95.45 ac- curacy. In almost the same way we can use the ex- ternal lexicon with the TreeTagger. We add all types as analysed by Gertwol to the TreeTag- gers lexicon. Then, unlike the Brill-Tagger, the TreeTagger is retrained with the same parame- ters and input ﬁles as above except for the ex- tended lexicon. The Brill-Tagger loads its lex- icon for every tagging process, and the lexicon can therefore be extended without retraining the tagger. The TreeTagger, on the other hand, integrates the lexicon during training into its output ﬁle. It must therefore be retrained af- ter each lexicon extension. Extending the lexicon improves the TreeTag- gers accuracy by around 1 to 96.29. Table 4 gives the results for the TreeTagger with the extended lexicon. The recognition of the remaining unknown words is very low (66.06), but this does not inﬂuence the result much since only 1.23 of all tokens are left unknown. Also the rate of disam- biguation errors increases from 1.49 to 2.06. But at the same time the rate of lexical error drops from 3.24 to 1.65, which accounts for the noticeable increase in overall accuracy. 4 The best of both worlds? In the previous sections we observed that the statistical tagger and the rule-based tagger show complementary strengths. Therefore we exper- imented with combining the statistical and the rule-based tagger in order to ﬁnd out whether a combination would yield a result superior to any single tagger. First, we tried to employ the TreeTagger and the Brill-Tagger in this order. Tagging the test corpus now works in two steps. In step one, we tag the test corpus with the TreeTag- ger. We then add all unknown word forms to the Brill-Taggers lexicon with the tags assigned by the TreeTagger. In step two, we tag the test corpus with the Brill-Tagger. In this way we can increase the Brill-Taggers accuracy to 95.13. But the desired eﬀect of combining the strengths of both taggers in order to build one tagger that is better than either of the taggers alone was not achieved. The reason is that the wrong tags of the TreeTagger were carried over to the Brill-Tagger (together with the correct tags) and all of the new lexical entries were on the ambiguity level one or two, so that the Brill- Tagger could not show its strength in disam- biguation. In a second round we reduced the export of wrong tags from the TreeTagger to the Brill- Tagger. We made sure that on export all digit- sequence ordinal and cardinal numbers were as- signed the correct tags. We used a regular ex- pression to check each word form. In addition, we checked for all other unknown word forms if the tag assigned by the TreeTagger was permit- ted by Gertwol (i.e. if the TreeTagger tag was one of Gertwols tags). If so, the TreeTagger tag was exported. If the TreeTagger tag was not allowed by Gertwol, we checked how many tags Gertwol proposes. If Gertwol proposes ex- actly one tag this tag was exported, in all other cases no tag was exported. In this way we ex- ported 1171 types to the Brill-Taggers lexicon and we obtained a tagging accuracy of 95.90. The algorithm for selecting TreeTagger tags was further modiﬁed in one little respect. If Gertwol did not analyse a word form and the TreeTagger identiﬁed it as a proper noun (NE), then the tag was exported. We then export 1212 types and we obtain a tagging accuracy of 96.03, which is still slightly worse than the TreeTagger with the external lexicon. Second, we tried to employ the taggers in the reverse order: Brill-Tagger ﬁrst, and then the TreeTagger, using the Brill-Taggers output. In this test we extended the TreeTaggers lexicon with the tags assigned by the Brill-Tagger and we extended the training corpus with the test corpus tagged by the Brill-Tagger. We retrained the TreeTagger with the extended lexicon and the extended corpus. We then used the Tree- Tagger to tag the test corpus, which resulted in 95.05 accuracy. This means that the combi- nation of the taggers results in a worse result than the TreeTagger by itself (95.27). From these tests we conclude that it is not possible to improve the tagging result by sim- ply sequentialising the taggers. In order to ex- ploit their respective strengths a more elaborate intertwining of their tagging strategies will be necessary. 5 Text type evaluation So far, all our tests were performed over the same test corpus. We checked whether the gen- eral tendency will also carry over to other test corpora. Besides the corpus used for the above evaluation we have a second manually tagged ambiguity tokens in  correct in  LE in  DE in  0 109 1.23 72 66.06 37 33.94 0 0.00 1 6307 70.97 6209 98.45 98 1.55 0 0.00 2 1224 13.77 1119 91.42 10 0.82 95 7.76 3 852 9.59 805 94.48 2 0.23 45 5.28 4 296 3.33 266 89.86 0 0.00 30 10.14 5 99 1.11 86 86.87 0 0.00 13 13.13 total 8887 100.00 8557 96.29 147 1.65 183 2.06 Table 4: Error statistics of the TreeTagger with an extended lexicon corpus consisting of texts about the adminis- tration at the University of Zurich (the uni- versitys annual report; guidelines for student registration etc.). This corpus currently con- sists of 38007 tokens. We have applied the tag- gers, trained as above on 78 of the Frankfurter Rundschau corpus, to this corpus and com- pared the results. In this way we have a much larger test corpus but we have a higher rate of unknown words (10646 tokens, 28.01, are un- known). The TreeTagger resulted in an accu- racy rate of 92.37, whereas the Brill-Tagger showed an accuracy rate of 91.65. These re- sults correspond very well with the above ﬁnd- ings. The ﬁgures are close to each other with a small advantage for the TreeTagger. It should be noted that the much lower accuracy rates compared to the test corpus are in part due to inconsistencies in tagging decisions. E.g. the word Management was tagged as a regular noun (NN) in the training corpus but as for- eign material (FM) in the University of Zurich test corpus. 6 Conclusions We have compared a statistical and a rule-based tagger for German. It turned out that both taggers perform on the same general level, but the statistical tagger has an advantage of about 0.5 to 1. A detailed analysis shows that the statistical tagger is better in dealing with un- known words than the rule-based tagger. It is also more robust in using an external lexicon, which resulted in the top tagging accuracy of 96.29. The rule-based tagger is superior to the statistical tagger in disambiguating tokens that are many-ways ambiguous. But such tokens do not occur frequently enough to fully get equal with the statistical tagger. A sequential com- bination of both taggers in either order did not show any improvements in tagging accuracy. The statistical tagger is easier to handle in that its training time is 3 magnitudes shorter than the rule-based tagger (minutes vs. days). But it has to be retrained after lexicon ex- tension, which is not necessary with the rule- based tagger. The rule-based tagger has the additional advantage that rules (i.e. lexical and contextual rules) can be manually modiﬁed. As a side result our experiments show that a rule-based tagger that learns its rules like the Brill-Tagger does not match the results of the constraint grammar tagger (a manually built rule-based tagger) described in (Samuelsson and Voutilainen, 1997). That tagger is described as performing with an error rate of less than 2. Constraint grammar rules are much more pow- erful than the rules used in the Brill-Tagger. References Eric Brill. 1992. A simple rule-based part-of- speech tagger. In Proceedings of ANLP, pages 152155, TrentoItaly. ACL. Eric Brill. 1994. A report of recent progress in transformation-based error-driven learn- ing. In Proceedings of AAAI. D. Cutting, J. Kupiec, J. Pedersen, and P. Si- bun. 1992. A practical part-of-speech tag- ger. In Proc. of ANLP, pages 133140, TrentoItaly. W. Lezius, R. Rapp, and M. Wettler. 1996. A morphology-system and part-of-speech tag- ger for German. In D. Gibbon, editor, Natu- ral Language Processing and Speech Technol- ogy. Results of the 3rd KONVENS Confer- ence (Bielefeld), pages 369378, Berlin. Mou- ton de Gruyter. Lingsoft Oy. 1994. Gertwol. Questionnaire for Morpholympics 1994. LDV-Forum, 11(1):17 29. C. Samuelsson and A. Voutilainen. 1997. Com- paring a linguistic and a stochastic tagger. In Proc. of ACLEACL Joint Conference, pages 246253, Madrid. A. Schiller, S. Teufel, and C. Thielen. 1995. Guidelines fur das Tagging deutscher Textcorpora mit STTS (Draft). Technical report, Universitat Stuttgart. Institut fur maschinelle Sprachverarbeitung. H. Schmid and A. Kempe. 1996. Tagging von Korpora mit HMM, Entscheidungsbaumen und Neuronalen Netzen. In H. Feldweg and E.W. Hinrichs, editors, Wiederverwendbare Methoden und Ressourcen zur linguistischen Erschliessung des Deutschen, volume 73 of Lexicographica. Series Maior, pages 231244. Niemeyer Verlag, Tubingen. Helmut Schmid. 1995. Improvements in part- of-speech tagging with an application to Ger- man. Technical report, Universitat Stuttgart. Institut fur maschinelle Sprachverarbeitung. (Revised version of a paper presented at EACL SIGDAT, Dublin 1995). S. Teufel, H. Schmid, U. Heid, and A. Schiller. 1996. EAGLES validation (WP-4) task on tagset and tagger interaction. Technical re- port, IMS, Universitat Stuttgart, May. C. Thielen and A. Schiller. 1996. Ein kleines und erweitertes Tagset furs Deutsche. In H. Feldweg and E.W. Hinrichs, ed- itors, Wiederverwendbare Methoden und Ressourcen zur linguistischen Erschliessung des Deutschen, volume 73 of Lexicographica. Series Maior, pages 193203. Niemeyer Ver- lag, Tubingen.",
  "18.pdf": "arXiv:cs9811022v2 [cs.CL] 25 Jan 2000 Exploiting Syntactic Structure for Language Modeling Ciprian Chelba and Frederick Jelinek Center for Language and Speech Processing The Johns Hopkins University, Barton Hall 320 3400 N. Charles St., Baltimore, MD-21218, USA {chelba,jelinek}jhu.edu Abstract The paper presents a language model that devel- ops syntactic structure and uses it to extract mean- ingful information from the word history, thus en- abling the use of long distance dependencies. The model assigns probability to every joint sequence of wordsbinary-parse-structure with headword an- notation and operates in a left-to-right manner  therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved. 1 Introduction The main goal of the present work is to develop a lan- guage model that uses syntactic structure to model long-distance dependencies. During the summer96 DoD Workshop a similar attempt was made by the dependency modeling group. The model we present is closely related to the one investigated in (Chelba et al., 1997), however diﬀerent in a few important aspects:  our model operates in a left-to-right manner, al- lowing the decoding of word lattices, as opposed to the one referred to previously, where only whole sen- tences could be processed, thus reducing its applica- bility to n-best list re-scoring; the syntactic structure is developed as a model component;  our model is a factored version of the one in (Chelba et al., 1997), thus enabling the calculation of the joint probability of words and parse structure; this was not possible in the previous case due to the huge computational complexity of the model. Our model develops syntactic structure incremen- tally while traversing the sentence from left to right. This is the main diﬀerence between our approach and other approaches to statistical natural language parsing. Our parsing strategy is similar to the in- cremental syntax ones proposed relatively recently in the linguistic community (Philips, 1996). The probabilistic model, its parameterization and a few experiments that are meant to evaluate its potential for speech recognition are presented. the_DT contract_NN ended_VBD cents_NNS after cents_NP of_PP loss_NP loss_NP ended_VP with_PP contract_NP with_IN a_DT loss_NN of_IN 7_CD Figure 1: Partial parse 2 The Basic Idea and Terminology Consider predicting the word after in the sentence: the contract ended with a loss of 7 cents after trading as low as 89 cents. A 3-gram approach would predict after from (7, cents) whereas it is intuitively clear that the strongest predictor would be ended which is outside the reach of even 7-grams. Our assumption is that what enables humans to make a good prediction of after is the syntactic structure in the past. The linguistically correct partial parse of the word his- tory when predicting after is shown in Figure 1. The word ended is called the headword of the con- stituent (ended (with (...))) and ended is an ex- posed headword when predicting after  topmost headword in the largest constituent that contains it. The syntactic structure in the past ﬁlters out irrel- evant words and points to the important ones, thus enabling the use of long distance information when predicting the next word. Our model will attempt to build the syntactic structure incrementally while traversing the sen- tence left-to-right. The model will assign a probabil- ity P(W, T ) to every sentence W with every possible POStag assignment, binary branching parse, non- terminal label and headword annotation for every constituent of T . Let W be a sentence of length n words to which we have prepended s and appended s so that w0 s and wn1 s. Let Wk be the word k-preﬁx w0 . . . wk of the sentence and WkTk (s, SB) ....... (w_p, t_p) (w_{p1}, t_{p1}) ........ (w_k, t_k) w_{k1}.... s h_0  (h_0.word, h_0.tag) h_{-1} h_{-m}  (s, SB) Figure 2: A word-parse k-preﬁx (s, SB) (w_1, t_1) ..................... (w_n, t_n) (s, SE) (s, TOP) (s, TOP) Figure 3: Complete parse the word-parse k-preﬁx. To stress this point, a word-parse k-preﬁx contains  for a given parse  only those binary subtrees whose span is com- pletely included in the word k-preﬁx, excluding w0 s. Single words along with their POStag can be regarded as root-only trees. Figure 2 shows a word-parse k-preﬁx; h_0 .. h_{-m} are the ex- posed heads, each head being a pair(headword, non- terminal label), or (word, POStag) in the case of a root-only tree. A complete parse  Figure 3  is any binary parse of the (w1, t1) . . . (wn, tn) (s, SE) sequence with the restriction that (s, TOP) is the only allowed head. Note that ((w1, t1) . . . (wn, tn)) neednt be a constituent, but for the parses where it is, there is no restriction on which of its words is the headword or what is the non-terminal label that accompanies the headword. The model will operate by means of three mod- ules:  WORD-PREDICTOR predicts the next word wk1 given the word-parse k-preﬁx and then passes control to the TAGGER;  TAGGER predicts the POStag of the next word tk1 given the word-parse k-preﬁx and the newly predicted word and then passes control to the PARSER;  PARSER grows the already existing binary branching structure by repeatedly generating the transitions: (unary, NTlabel), (adjoin-left, NTlabel) or (adjoin-right, NTlabel) until it passes control to the PREDICTOR by taking a null transition. NTlabel is the non-terminal label assigned to the newly built constituent and {left,right} speciﬁes where the new headword is inherited from. The operations performed by the PARSER are illustrated in Figures 4-6 and they ensure that all possible binary branching parses with all possible s s T_{-m} h_{-1} h_0 h_{-2} ......... h_{-1} h_0 h_{-2} T_{-2} T_{-1} T_0 ......... Figure 4: Before an adjoin operation ............... T_0 T_{-1} T_0 s T_{-1}-T_{-2} h_{-1} h_0 h_{-1}  h_{-2} T_{-m1}-s h_0  (h_{-1}.word, NTlabel) Figure 5: Result of adjoin-left under NTlabel headword and non-terminal label assignments for the w1 . . . wk word sequence can be generated. The following algorithm formalizes the above description of the sequential generation of a sentence with a complete parse. Transition t;  a PARSER transition predict (s, SB); do{ WORD-PREDICTOR and TAGGER predict (next_word, POStag); PARSER do{ if(h_{-1}.word ! s){ if(h_0.word  s) t  (adjoin-right, TOP); else{ if(h_0.tag  NTlabel) t  [(adjoin-{left,right}, NTlabel), null]; else t  [(unary, NTlabel), (adjoin-{left,right}, NTlabel), null]; } } else{ if(h_0.tag  NTlabel) t  null; else t  [(unary, NTlabel), null]; } }while(t ! null) done PARSER }while(!(h_0.words  h_{-1}.words)) t  (adjoin-right, TOP); adjoin s_SB; DONE; The unary transition is allowed only when the most recent exposed head is a leaf of the tree  a regular word along with its POStag  hence it can be taken at most once at a given position in the ............... T_{-1}-T_{-2} T_0 h_0 h_{-1} s T_{-m1}-s h_{-1}h_{-2} T_{-1} h_0  (h_0.word, NTlabel) Figure 6: Result of adjoin-right under NTlabel input word string. The second subtree in Figure 2 provides an example of a unary transition followed by a null transition. It is easy to see that any given word sequence with a possible parse and headword annotation is generated by a unique sequence of model actions. This will prove very useful in initializing our model parameters from a treebank  see section 3.5. 3 Probabilistic Model The probability P(W, T ) of a word sequence W and a complete parse T can be broken into: P(W, T )  n1 k1[ P(wkWk1Tk1)  P(tkWk1Tk1, wk)  Nk  i1 P(pk i Wk1Tk1, wk, tk, pk 1 . . . pk i1)](1) where:  Wk1Tk1 is the word-parse (k  1)-preﬁx  wk is the word predicted by WORD-PREDICTOR  tk is the tag assigned to wk by the TAGGER  Nk  1 is the number of operations the PARSER executes before passing control to the WORD- PREDICTOR (the Nk-th operation at position k is the null transition); Nk is a function of T  pk i denotes the i-th PARSER operation carried out at position k in the word string; pk 1  {(unary, NTlabel), (adjoin-left, NTlabel), (adjoin-right, NTlabel), null}, pk i  { (adjoin-left, NTlabel), (adjoin-right, NTlabel)}, 1  i  Nk , pk i null, i  Nk Our model is based on three probabilities: P(wkWk1Tk1) (2) P(tkwk, Wk1Tk1) (3) P(pk i wk, tk, Wk1Tk1, pk 1 . . . pk i1) (4) As can be seen, (wk, tk, Wk1Tk1, pk 1 . . . pk i1) is one of the Nk word-parse k-preﬁxes WkTk at position k in the sentence, i  1, Nk. To ensure a proper probabilistic model (1) we have to make sure that (2), (3) and (4) are well de- ﬁned conditional probabilities and that the model halts with probability one. Consequently, certain PARSER and WORD-PREDICTOR probabilities must be given speciﬁc values:  P(nullWkTk)  1, if h_{-1}.word  s and h_{0}  (s, TOP)  that is, before predicting s  ensures that (s, SB) is adjoined in the last step of the parsing process;  P((adjoin-right, TOP)WkTk)  1, if h_0  (s, TOP) and h_{-1}.word  s and P((adjoin-right, TOP)WkTk)  1, if h_0  (s, TOP) and h_{-1}.word  s ensure that the parse generated by our model is con- sistent with the deﬁnition of a complete parse;  P((unary, NTlabel)WkTk)  0, if h_0.tag  POStag ensures correct treatment of unary produc- tions;  ǫ  0, Wk1Tk1, P(wksWk1Tk1)  ǫ ensures that the model halts with probability one. The word-predictor model (2) predicts the next word based on the preceding 2 exposed heads, thus making the following equivalence classiﬁcation: P(wkWk1Tk1)  P(wkh0, h1) After experimenting with several equivalence clas- siﬁcations of the word-parse preﬁx for the tagger model, the conditioning part of model (3) was re- duced to using the word to be tagged and the tags of the two most recent exposed heads: P(tkwk, Wk1Tk1)  P(tkwk, h0.tag, h1.tag) Model (4) assigns probability to diﬀerent parses of the word k-preﬁx by chaining the elementary oper- ations described above. The workings of the parser module are similar to those of Spatter (Jelinek et al., 1994). The equivalence classiﬁcation of the WkTk word-parse we used for the parser model (4) was the same as the one used in (Collins, 1996): P(pk i WkTk)  P(pk i h0, h1) It is worth noting that if the binary branching structure developed by the parser were always right- branching and we mapped the POStag and non- terminal label vocabularies to a single type then our model would be equivalent to a trigram language model. 3.1 Modeling Tools All model components  WORD-PREDICTOR, TAGGER, PARSER  are conditional probabilis- tic models of the type P(yx1, x2, . . . , xn) where y, x1, x2, . . . , xn belong to a mixed bag of words, POStags, non-terminal labels and parser operations (y only). For simplicity, the modeling method we chose was deleted interpolation among relative fre- quency estimates of diﬀerent orders fn() using a recursive mixing scheme: P(yx1, . . . , xn)  λ(x1, . . . , xn)  P(yx1, . . . , xn1)  (1  λ(x1, . . . , xn))  fn(yx1, . . . , xn), (5) f1(y)  uniform(vocabulary(y)) (6) As can be seen, the context mixing scheme dis- cards items in the context in right-to-left order. The λ coeﬃcients are tied based on the range of the count C(x1, . . . , xn). The approach is a standard one which doesnt require an extensive description given the literature available on it (Jelinek and Mer- cer, 1980). 3.2 Search Strategy Since the number of parses for a given word preﬁx Wk grows exponentially with k, {Tk}  O(2k), the state space of our model is huge even for relatively short sentences so we had to use a search strategy that prunes it. Our choice was a synchronous multi- stack search algorithm which is very similar to a beam search. Each stack contains hypotheses  partial parses  that have been constructed by the same number of predictor and the same number of parser operations. The hypotheses in each stack are ranked according to the ln(P(W, T )) score, highest on top. The width of the search is controlled by two parameters:  the maximum stack depth  the maximum num- ber of hypotheses the stack can contain at any given state;  log-probability threshold  the diﬀerence between the log-probability score of the top-most hypothesis and the bottom-most hypothesis at any given state of the stack cannot be larger than a given threshold. Figure 7 shows schematically the operations asso- ciated with the scanning of a new word wk1. The above pruning strategy proved to be insuﬃcient so we chose to also discard all hypotheses whose score is more than the log-probability threshold below the score of the topmost hypothesis. This additional pruning step is performed after all hypotheses in stage k have been extended with the null parser transition and thus prepared for scanning a new word. 3.3 Word Level Perplexity The conditional perplexity calculated by assigning to a whole sentence the probability: P(WT )  n  k0 P(wk1WkT  k ), (7) where T   argmaxT P(W, T ), is not valid because it is not causal: when predicting wk1 we use T  which was determined by looking at the entire sen- tence. To be able to compare the perplexity of our (k) (k) (k1) 0 parser 0 parser 0 parser p parser op op p parser op p parser op p1 parser p1 parser p1 parser P_k parser P_k parser P_k parser k1 predict. k1 predict. k1 predict. k1 predict. k1 predict. k1 predict. k1 predict. k1 predict. k1 predict. k1 predict. P_k1parser P_k1parser word predictor and tagger parser adjoinunary transitions null parser transitions op k predict. k predict. k predict. k predict. op Figure 7: One search extension cycle model with that resulting from the standard tri- gram approach, we need to factor in the entropy of guessing the correct parse T  k before predicting wk1, based solely on the word preﬁx Wk. The probability assignment for the word at posi- tion k  1 in the input sentence is made using: P(wk1Wk)   TkSk P(wk1WkTk)  ρ(Wk, Tk), (8) ρ(Wk, Tk)  P(WkTk)  TkSk P(WkTk) (9) which ensures a proper probability over strings W , where Sk is the set of all parses present in our stacks at the current stage k. Another possibility for evaluating the word level perplexity of our model is to approximate the prob- ability of a whole sentence: P(W)  N  k1 P(W, T (k)) (10) where T (k) is one of the N-best  in the sense deﬁned by our search  parses for W. This is a deﬁcient probability assignment, however useful for justifying the model parameter re-estimation. The two estimates (8) and (10) are both consistent in the sense that if the sums are carried over all possible parses we get the correct value for the word level perplexity of our model. 3.4 Parameter Re-estimation The major problem we face when trying to reesti- mate the model parameters is the huge state space of the model and the fact that dynamic programming techniques similar to those used in HMM parame- ter re-estimation cannot be used with our model. Our solution is inspired by an HMM re-estimation technique that works on pruned  N-best  trel- lises(Byrne et al., 1998). Let (W, T (k)), k  1 . . . N be the set of hypothe- ses that survived our pruning strategy until the end of the parsing process for sentence W. Each of them was produced by a sequence of model actions, chained together as described in section 2; let us call the sequence of model actions that produced a given (W, T ) the derivation(W, T ). Let an elementary event in the derivation(W, T ) be (y(ml) l , x(ml) l ) where:  l is the index of the current model action;  ml is the model component  WORD- PREDICTOR, TAGGER, PARSER  that takes action number l in the derivation(W, T );  y(ml) l is the action taken at position l in the deriva- tion: if ml  WORD-PREDICTOR, then y(ml) l is a word; if ml  TAGGER, then y(ml) l is a POStag; if ml  PARSER, then y(ml) l is a parser-action;  x(ml) l is the context in which the above action was taken: if ml  WORD-PREDICTOR or PARSER, then x(ml) l  (h0.tag, h0.word, h1.tag, h1.word); if ml  TAGGER, then x(ml) l  (word-to-tag, h0.tag, h1.tag). The probability associated with each model ac- tion is determined as described in section 3.1, based on counts C(m)(y(m), x(m)), one set for each model component. Assuming that the deleted interpolation coeﬃ- cients and the count ranges used for tying them stay ﬁxed, these counts are the only parameters to be re-estimated in an eventual re-estimation procedure; indeed, once a set of counts C(m)(y(m), x(m)) is spec- iﬁed for a given model m, we can easily calculate:  the relative frequency estimates f (m) n (y(m)x(m) n ) for all context orders n  0 . . .maximum-order(model(m));  the count C(m)(x(m) n ) used for determining the λ(x(m) n ) value to be used with the order-n context x(m) n . This is all we need for calculating the probability of an elementary event and then the probability of an entire derivation. One training iteration of the re-estimation proce- dure we propose is described by the following algo- rithm: N-best parse development data;  counts.Ei  prepare counts.E(i1) for each model component c{ gather_counts development model_c; } In the parsing stage we retain for each N-best hy- pothesis (W, T (k)), k  1 . . . N, only the quantity φ(W, T (k))  P(W, T (k)) N k1 P(W, T (k)) and its derivation(W, T (k)). We then scan all the derivations in the development set and, for each occurrence of the elementary event (y(m), x(m)) in derivation(W, T (k)) we accumulate the value φ(W, T (k)) in the C(m)(y(m), x(m)) counter to be used in the next iteration. The intuition behind this procedure is that φ(W, T (k)) is an approximation to the P(T (k)W) probability which places all its mass on the parses that survived the parsing process; the above proce- dure simply accumulates the expected values of the counts C(m)(y(m), x(m)) under the φ(W, T (k)) con- ditional distribution. As explained previously, the C(m)(y(m), x(m)) counts are the parameters deﬁning our model, making our procedure similar to a rigor- ous EM approach (Dempster et al., 1977). A particular  and very interesting  case is that of events which had count zero but get a non-zero count in the next iteration, caused by the N-best nature of the re-estimation process. Consider a given sentence in our development set. The N-best derivations for this sentence are trajectories through the state space of our model. They will change from one iteration to the other due to the smooth- ing involved in the probability estimation and the change of the parameters  event counts  deﬁn- ing our model, thus allowing new events to appear and discarding others through purging low probabil- ity events from the stacks. The higher the number of trajectories per sentence, the more dynamic this change is expected to be. The results we obtained are presented in the ex- periments section. All the perplexity evaluations were done using the left-to-right formula (8) (L2R- PPL) for which the perplexity on the development set is not guaranteed to decrease from one itera- tion to another. However, we believe that our re- estimation method should not increase the approxi- mation to perplexity based on (10) (SUM-PPL)  again, on the development set; we rely on the con- sistency property outlined at the end of section 3.3 to correlate the desired decrease in L2R-PPL with that in SUM-PPL. No claim can be made about the change in either L2R-PPL or SUM-PPL on test data. Z Z Z Z B Z Z Z Z A Y_1 Y_k Y_n Y_1 Y_k Y_n Figure 8: Binarization schemes 3.5 Initial Parameters Each model component  WORD-PREDICTOR, TAGGER, PARSER  is trained initially from a set of parsed sentences, after each parse tree (W, T ) undergoes:  headword percolation and binarization  see sec- tion 4;  decomposition into its derivation(W, T ). Then, separately for each m model component, we:  gather joint counts C(m)(y(m), x(m)) from the derivations that make up the development data using φ(W, T )  1;  estimate the deleted interpolation coeﬃcients on joint counts gathered from check data using the EM algorithm. These are the initial parameters used with the re- estimation procedure described in the previous sec- tion. 4 Headword Percolation and Binarization In order to get initial statistics for our model com- ponents we needed to binarize the UPenn Tree- bank (Marcus et al., 1995) parse trees and perco- late headwords. The procedure we used was to ﬁrst percolate headwords using a context-free (CF) rule- based approach and then binarize the parses by us- ing a rule-based approach again. The headword of a phrase is the word that best represents the phrase, all the other words in the phrase being modiﬁers of the headword. Statisti- cally speaking, we were satisﬁed with the output of an enhanced version of the procedure described in (Collins, 1996)  also known under the name Magerman  Black Headword Percolation Rules. Once the position of the headword within a con- stituent  equivalent with a CF production of the type Z  Y1 . . . Yn , where Z, Y1, . . . Yn are non- terminal labels or POStags (only for Yi)  is iden- tiﬁed to be k, we binarize the constituent as follows: depending on the Z identity, a ﬁxed rule is used to decide which of the two binarization schemes in Figure 8 to apply. The intermediate nodes created by the above binarization schemes receive the non- terminal label Z. 5 Experiments Due to the low speed of the parser  200 wdsmin for stack depth 10 and log-probability threshold 6.91 nats (11000)  we could carry out the re- estimation technique described in section 3.4 on only 1 Mwds of training data. For convenience we chose to work on the UPenn Treebank corpus. The vocab- ulary sizes were:  word vocabulary: 10k, open  all words outside the vocabulary are mapped to the unk token;  POS tag vocabulary: 40, closed;  non-terminal tag vocabulary: 52, closed;  parser operation vocabulary: 107, closed; The training data was split into development set  929,564wds (sections 00-20)  and check set  73,760wds (sections 21-22); the test set size was 82,430wds (sections 23-24). The check set has been used for estimating the interpolation weights and tuning the search parameters; the develop- ment set has been used for gatheringestimating counts; the test set has been used strictly for evalu- ating model performance. Table 1 shows the results of the re-estimation tech- nique presented in section 3.4. We achieved a reduc- tion in test-data perplexity bringing an improvement over a deleted interpolation trigram model whose perplexity was 167.14 on the same training-test data; the reduction is statistically signiﬁcant according to a sign test. iteration DEV set TEST set number L2R-PPL L2R-PPL E0 24.70 167.47 E1 22.34 160.76 E2 21.69 158.97 E3 21.26 158.28 3-gram 21.20 167.14 Table 1: Parameter re-estimation results Simple linear interpolation between our model and the trigram model: Q(wk1Wk)  λ  P(wk1wk1, wk)  (1  λ)  P(wk1Wk) yielded a further improvement in PPL, as shown in Table 2. The interpolation weight was estimated on check data to be λ  0.36. An overall relative reduction of 11 over the trigram model has been achieved. 6 Conclusions and Future Directions The large diﬀerence between the perplexity of our model calculated on the development set  used iteration TEST set TEST set number L2R-PPL 3-gram interpolated PPL E0 167.47 152.25 E3 158.28 148.90 3-gram 167.14 167.14 Table 2: Interpolation with trigram results for model parameter estimation  and test set  unseen data  shows that the initial point we choose for the parameter values has already captured a lot of information from the training data. The same problem is encountered in standard n-gram language modeling; however, our approach has more ﬂexibility in dealing with it due to the possibility of reestimat- ing the model parameters. We believe that the above experiments show the potential of our approach for improved language models. Our future plans include:  experiment with other parameterizations than the two most recent exposed heads in the word predictor model and parser;  estimate a separate word predictor for left-to- right language modeling. Note that the correspond- ing model predictor was obtained via re-estimation aimed at increasing the probability of the N-best parses of the entire sentence;  reduce vocabulary of parser operations; extreme case: no non-terminal labelsPOS tags, word only model; this will increase the speed of the parser thus rendering it usable on larger amounts of train- ing data and allowing the use of deeper stacks  resulting in more N-best derivations per sentence during re-estimation;  relax  ﬂatten  the initial statistics in the re- estimation of model parameters; this would allow the model parameters to converge to a diﬀerent point that might yield a lower word-level perplexity;  evaluate model performance on n-best sentences output by an automatic speech recognizer. 7 Acknowledgments This research has been funded by the NSF IRI-19618874 grant (STIMULATE). The authors would like to thank to Sanjeev Khu- danpur for his insightful suggestions. Also to Harry Printz, Eric Ristad, Andreas Stolcke, Dekai Wu and all the other members of the dependency model- ing group at the summer96 DoD Workshop for use- ful comments on the model, programming support and an extremely creative environment. Also thanks to Eric Brill, Sanjeev Khudanpur, David Yarowsky, Radu Florian, Lidia Mangu and Jun Wu for useful input during the meetings of the people working on our STIMULATE grant. References W. Byrne, A. Gunawardana, and S. Khudanpur. 1998. Information geometry and EM variants. Technical Report CLSP Research Note 17, De- partment of Electical and Computer Engineering, The Johns Hopkins University, Baltimore, MD. C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu- danpur, L. Mangu, H. Printz, E. S. Ristad, R. Rosenfeld, A. Stolcke, and D. Wu. 1997. Struc- ture and performance of a dependency language model. In Proceedings of Eurospeech, volume 5, pages 27752778. Rhodes, Greece. Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceed- ings of the 34th Annual Meeting of the Associ- ation for Computational Linguistics, pages 184 191. Santa Cruz, CA. A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. In Journal of the Royal Statistical Society, volume 39 of B, pages 138. Frederick Jelinek and Robert Mercer. 1980. Inter- polated estimation of markov source parameters from sparse data. In E. Gelsema and L. Kanal, ed- itors, Pattern Recognition in Practice, pages 381 397. F. Jelinek, J. Laﬀerty, D. M. Magerman, R. Mercer, A. Ratnaparkhi, and S. Roukos. 1994. Decision tree parsing using a hidden derivational model. In ARPA, editor, Proceedings of the Human Lan- guage Technology Workshop, pages 272277. M. Marcus, B. Santorini, and M. Marcinkiewicz. 1995. Building a large annotated corpus of En- glish: the Penn Treebank. Computational Lin- guistics, 19(2):313330. Colin Philips. 1996. Order and Structure. Ph.D. thesis, MIT. Distributed by MITWPL.",
  "19.pdf": "arXiv:cs9811025v2 [cs.CL] 25 Jan 2000 A Structured Language Model Ciprian Chelba The Johns Hopkins University CLSP, Barton Hall 320 3400 N. Charles Street, Baltimore, MD-21218 chelbajhu.edu Abstract The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model as- signs probability to every joint sequence of wordsbinary-parse-structure with head- word annotation. The model, its proba- bilistic parametrization, and a set of ex- periments meant to evaluate its predictive power are presented. 1 Introduction The main goal of the proposed project is to develop a language model(LM) that uses syntactic structure. The principles that guided this proposal were:  the model will develop syntactic knowledge as a built-in feature; it will assign a probability to every joint sequence of wordsbinary-parse-structure;  the model should operate in a left-to-right man- ner so that it would be possible to decode word lat- tices provided by an automatic speech recognizer. The model consists of two modules: a next word predictor which makes use of syntactic structure as developed by a parser. The operations of these two modules are intertwined. 2 The Basic Idea and Terminology Consider predicting the word barked in the sen- tence: the dog I heard yesterday barked again. A 3-gram approach would predict barked from (heard, yesterday) whereas it is clear that the predictor should use the word dog which is out- side the reach of even 4-grams. Our assumption is that what enables us to make a good predic- tion of barked is the syntactic structure in the the dog I heard yesterday barked heard dog dog Figure 1: Partial parse s w_1 ... w_p ........ w_q ... w_r w_{r1} ... w_k w_{k1} ..... w_n s T_{-m} sh_{-m} T_{-m1} T_0 T{-1} h_{-m1} h_{-1} h_0 Figure 2: A word-parse k-preﬁx past. The correct partial parse of the word his- tory when predicting barked is shown in Figure 1. The word dog is called the headword of the con- stituent ( the (dog (...))) and dog is an exposed headword when predicting barked  topmost head- word in the largest constituent that contains it. The syntactic structure in the past ﬁlters out irrelevant words and points to the important ones, thus en- abling the use of long distance information when predicting the next word. Our model will assign a probability P(W, T ) to every sentence W with ev- ery possible binary branching parse T and every possible headword annotation for every constituent of T . Let W be a sentence of length l words to which we have prepended s and appended s so that w0 s and wl1 s. Let Wk be the word k-preﬁx w0 . . . wk of the sentence and WkTk the word-parse k-preﬁx. To stress this point, a word-parse k-preﬁx contains only those binary trees whose span is completely included in the word k- preﬁx, excluding w0 s. Single words can be re- garded as root-only trees. Figure 2 shows a word- parse k-preﬁx; h_0 .. h_{-m} are the exposed head- words. A complete parse  Figure 3  is any bi- nary parse of the w1 . . . wl s sequence with the restriction that s is the only allowed headword. s s s w_1 ...... w_l s Figure 3: Complete parse Note that (w1 . . . wl) neednt be a constituent, but for the parses where it is, there is no restriction on which of its words is the headword. The model will operate by means of two modules:  PREDICTOR predicts the next word wk1 given the word-parse k-preﬁx and then passes control to the PARSER;  PARSER grows the already existing binary branching structure by repeatedly generating the transitions adjoin-left or adjoin-right until it passes control to the PREDICTOR by taking a null transition. The operations performed by the PARSER en- sure that all possible binary branching parses with all possible headword assignments for the w1 . . . wk word sequence can be generated. They are illus- trated by Figures 4-6. The following algorithm de- scribes how the model generates a word sequence with a complete parse (see Figures 3-6 for notation): Transition t;  a PARSER transition generate s; do{ predict next_word; PREDICTOR do{ PARSER if(T_{-1} ! s ) if(h_0  s) t  adjoin-right; else t  {adjoin-{left,right}, null}; else t  null; }while(t ! null) }while(!(h_0  s  T_{-1}  s)) t  adjoin-right;  adjoin s; DONE It is easy to see that any given word sequence with a possible parse and headword annotation is generated by a unique sequence of model actions. 3 Probabilistic Model The probability P(W, T ) can be broken into: P(W, T )  l1 k1[P(wkWk1Tk1) Nk i1 P(tk i wk, Wk1Tk1, tk 1 . . . tk i1)] where:  Wk1Tk1 is the word-parse (k  1)-preﬁx  wk is the word predicted by PREDICTOR  Nk  1 is the number of adjoin operations the PARSER executes before passing control to the PREDICTOR (the Nk-th operation at position k is the null transition); Nk is a function of T s s T_{-m} h_{-1} h_0 h_{-2} ......... h_{-1} h_0 h_{-2} T_{-2} T_{-1} T_0 ......... Figure 4: Before an adjoin operation ............... T_0 T_{-1} T_0 s T_{-1}-T_{-2} h_{-1} h_0 h_{-1}  h_{-2} h_0  h_{-1} T_{-m1}-s Figure 5: Result of adjoin-left ............... T_{-1}-T_{-2} T_0 h_0 h_{-1} s T_{-m1}-s h_{-1}h_{-2} T_{-1} h_0  h_0 Figure 6: Result of adjoin-right  tk i denotes the i-th PARSER operation carried out at position k in the word string; tk i  {adjoin-left,adjoin-right}, i  Nk , tk i null, i  Nk Our model is based on two probabilities: P(wkWk1Tk1) (1) P(tk i wk, Wk1Tk1, tk 1 . . . tk i1) (2) As can be seen (wk, Wk1Tk1, tk 1 . . . tk i1) is one of the Nk word-parse k-preﬁxes of WkTk, i  1, Nk at position k in the sentence. To ensure a proper probabilistic model we have to make sure that (1) and (2) are well deﬁned con- ditional probabilities and that the model halts with probability one. A few provisions need to be taken:  P(nullWkTk)  1, if T_{-1}  s ensures that s is adjoined in the last step of the parsing process;  P(adjoin-rightWkTk)  1, if h_0  s ensures that the headword of a complete parse is s; ǫ  0s.t. P(wksWk1Tk1)  ǫ, Wk1Tk1 ensures that the model halts with probability one. 3.1 The ﬁrst model The ﬁrst term (1) can be reduced to an n-gram LM, P(wkWk1Tk1)  P(wkwk1 . . . wkn1). A simple alternative to this degenerate approach would be to build a model which predicts the next word based on the preceding p-1 exposed headwords and n-1 words in the history, thus making the fol- lowing equivalence classiﬁcation: [WkTk]  {h_0 .. h_{-p2},wk1..wkn1}. The approach is similar to the trigger LM(Lau93), the diﬀerence being that in the present work triggers are identiﬁed using the syntactic structure. 3.2 The second model Model (2) assigns probability to diﬀerent binary parses of the word k-preﬁx by chaining the ele- mentary operations described above. The workings of the PARSER are very similar to those of Spat- ter (Jelinek94). It can be brought to the full power of Spatter by changing the action of the adjoin operation so that it takes into account the termi- nalnonterminal labels of the constituent proposed by adjoin and it also predicts the nonterminal la- bel of the newly created constituent; PREDICTOR will now predict the next word along with its POS tag. The best equivalence classiﬁcation of the WkTk word-parse k-preﬁx is yet to be determined. The Collins parser (Collins96) shows that dependency- grammarlike bigram constraints may be the most adequate, so the equivalence classiﬁcation [WkTk] should contain at least {h_0, h_{-1}}. 4 Preliminary Experiments Assuming that the correct partial parse is a func- tion of the word preﬁx, it makes sense to compare the word level perplexity(PP) of a standard n-gram LM with that of the P(wkWk1Tk1) model. We developed and evaluated four LMs:  2 bigram LMs P(wkWk1Tk1)  P(wkwk1) referred to as W and w, respectively; wk1 is the pre- vious (word, POStag) pair;  2 P(wkWk1Tk1)  P(wkh0) models, re- ferred to as H and h, respectively; h0 is the previous exposed (headword, POSnon-term tag) pair; the parses used in this model were those assigned man- ually in the Penn Treebank (Marcus95) after under- going headword percolation and binarization. All four LMs predict a word wk and they were implemented using the Maximum Entropy Model- ing Toolkit1 (Ristad97). The constraint templates in the {W,H} models were: 4  _ ?; 2  ?_ ?; 2  ?_? ?; 8  _? ?; and in the {w,h} models they were: 4  _ ?; 2  ?_ ?;  denotes a dont care position, ?_? a (word, tag) pair; for example, 4  ?_ ? will trig- ger on all ((word, any tag), predicted-word) pairs that occur more than 3 times in the training data. The sentence boundary is not included in the PP cal- culation. Table 1 shows the PP results along with 1ftp:ftp.cs.princeton.edupubpackagesmemt the number of parameters for each of the 4 models described . LM PP param LM PP param W 352 208487 w 419 103732 H 292 206540 h 410 102437 Table 1: Perplexity results 5 Acknowledgements The author thanks to all the members of the De- pendency Modeling Group (Chelba97):David Engle, Frederick Jelinek, Victor Jimenez, Sanjeev Khudan- pur, Lidia Mangu, Harry Printz, Eric Ristad, Roni Rosenfeld, Andreas Stolcke, Dekai Wu. References [Collins96] Michael John Collins. 1996. A new sta- tistical parser based on bigram lexical dependen- cies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, 184-191, Santa Cruz, CA. [Jelinek97] Frederick Jelinek. 1997. Information ex- traction from speech and text  course notes. The Johns Hopkins University, Baltimore, MD. [Jelinek94] Frederick Jelinek, John Laﬀerty, David M. Magerman, Robert Mercer, Adwait Ratna- parkhi, Salim Roukos. 1994. Decision Tree Parsing using a Hidden Derivational Model. In Proceed- ings of the Human Language Technology Work- shop, 272-277. ARPA. [Lau93] Raymond Lau, Ronald Rosenfeld, and Salim Roukos. 1993. Trigger-based language models: a maximum entropy approach. In Proceedings of the IEEE Conference on Acoustics, Speech, and Sig- nal Processing, volume 2, 45-48, Minneapolis. [Marcus95] Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz. 1995. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330. [Ristad97] Eric Sven Ristad. 1997. Maximum entropy modeling toolkit. Technical report, Department of Computer Science, Princeton University, Prince- ton, NJ, January 1997, v. 1.4 Beta. [Chelba97] Ciprian Chelba, David Engle, Freder- ick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry Printz, Eric Sven Ristad, Roni Rosenfeld, Andreas Stolcke, Dekai Wu. 1997. Structure and Performance of a Dependency Lan- guage Model. In Proceedings of Eurospeech97, Rhodes, Greece. To appear.",
  "20.pdf": "arXiv:cs9812001v3 [cs.CL] 3 Dec 1998 A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and Structural Disambiguation Hang LI A Dissertation Submitted to the Graduate School of Science of the University of Tokyo in Partial Fulﬁllment of the Requirements for the Degree of Doctor of Science in Information Science July 1998 c1998 by Hang LI All rights reserved. Abstract Structural disambiguation in sentence analysis is still a central problem in natural lan- guage processing. Past researches have veriﬁed that using lexical semantic knowledge can, to a quite large extent, cope with this problem. Although there have been many studies conducted in the past to address the lexical knowledge acquisition problem, further investigation, especially that based on a principled methodology is still needed, and this is, in fact, the problem I address in this thesis. The problem of acquiring and using lexical semantic knowledge, especially that of case frame patterns, can be formalized as follows. A learning module acquires case frame patterns on the basis of some case frame instances extracted from corpus data. A processing (disambiguation) module then refers to the acquired knowledge and judges the degrees of acceptability of some number of new case frames, including previously unseen ones. The approach I adopt has the following characteristics: (1) dividing the problem into three subproblems: case slot generalization, case dependency learning, and word clustering (thesaurus construction). (2) viewing each subproblem as that of statistical estimation and deﬁning probability models for each subproblem, (3) adopting the Min- imum Description Length (MDL) principle as learning strategy, (4) employing eﬃcient learning algorithms, and (5) viewing the disambiguation problem as that of statistical prediction. The need to divide the problem into subproblems is due to the complicatedness of this task, i.e., there are too many relevant factors simply to incorporate all of them into a single model. The use of MDL here leads us to a theoretically sound solution to the data sparseness problem, the main diﬃculty in a statistical approach to language processing. In Chapter 3, I deﬁne probability models for each subproblem: (1) the hard case slot model and the soft case slot model; (2) the word-based case frame model, the class-based case frame model, and the slot-based case frame model; and (3) the hard co-occurrence model and the soft co-occurrence model. These are respectively the probability models for (1) case slot generalization, (2) case dependency learning, and (3) word clustering. Here the term hard means that the model is characterized by a type of word clustering in which a word can only belong to a single class alone, while soft means that the model is characterized by a type of word clustering in which a word can belong to several diﬀerent classes. i ii In Chapter 4, I describe one method for learning the hard case slot model, i.e., generalizing case slots. I restrict the class of hard case slot models to that of tree cut models by using an existing thesaurus. In this way, the problem of generalizing the values of a case slot turns out to be that of estimating a model from the class of tree cut models for some ﬁxed thesaurus tree. I then employ an eﬃcient algorithm, which provably obtains the optimal tree cut model in terms of MDL. This method, in fact, conducts generalization in the following way. When the diﬀerences between the frequencies of the nouns in a class are not large enough (relative to the entire data size and the number of the nouns), it generalizes them into the class. When the diﬀerences are especially noticeable, on the other hand, it stops generalization at that level. In Chapter 5, I describe one method for learning the case frame model, i.e., learning dependencies between case slots. I restrict the class of case frame models to that of dependency forest models. Case frame patterns can then be represented as a depen- dency forest, whose nodes represent case slots and whose directed links represent the dependencies that exist between these case slots. I employ an eﬃcient algorithm to learn the optimal dependency forest model in terms of MDL. This method ﬁrst calcu- lates a statistic between all node pairs and sorts these node pairs in descending order with respect to the statistic. It then puts a link between the node pair highest in the order, provided that this value is larger than zero. It repeats this process until no node pair is left unprocessed, provided that adding that link will not create a loop in the current dependency graph. In Chapter 6, I describe one method for learning the hard co-occurrence model, i.e., automatically conducting word clustering. I employ an eﬃcient algorithm to repeatedly estimate a suboptimal MDL model from a class of hard co-occurrence models. The clustering method iteratively merges, for example, noun classes and verb classes in turn, in a bottom up fashion. For each merge it performs, it calculates the decrease in empirical mutual information resulting from merging any noun (or verb) class pair, and performs the merge having the least reduction in mutual information, provided that this reduction in mutual information is less than a threshold, which will vary depending on the data size and the number of classes in the current situation. In Chapter 7, I propose, for resolving ambiguities, a new method which combines the use of the hard co-occurrence model and that of the tree cut model. In the imple- mentation of this method, the learning module combines with the hard co-occurrence model to cluster words with respect to each case slot, and it combines with the tree cut model for generalizing the values of each case slot by means of a hand-made thesaurus. The disambiguation module ﬁrst calculates a likelihood value for each interpretation on the basis of hard co-occurrence models and outputs the interpretation with the largest likelihood value; if the likelihood values are equal (most particularly, if all of them are 0), it uses likelihood values calculated on the basis of tree cut models; if the likelihood values are still equal, it makes a default decision. The accuracy achieved by this method is 85.2, which is higher than that of state- of-the-art methods. Acknowledgements I would like to express my sincere appreciation to my supervisor, Prof. Junichi Tsujii of the University of Tokyo, for his continuous encouragement and guidance. It was Prof. Tsujii who guided me in the fundamentals of natural language processing when I was an undergraduate student at Kyoto University. His many helpful suggestions and comments have also been crucial to the completion of this thesis. I would also like to express my gratitude to the members of my dissertation commit- tee: Prof. Toshihisa Takagi and Prof. Hiroshi Imai of the University of Tokyo, Prof. Yuji Matsumoto of Nara Institute of Science and Technology (NAIST), and Prof. Kenji Kita of Tokushima University, who have been good enough to give this work a very serious review. Very special thanks are also due to Prof. Makoto Nagao of Kyoto University for his encouragement and guidance, particularly in his supervision of my master thesis when I was a graduate student at Kyoto University I learned a lot from him, especially in the skills of conducting research. He is one of the persons who continue to inﬂuence me strongly in my research carrer, even though the approach I am taking right now is quite diﬀerent from his own. I also would like to express my sincere gratitude to Prof. Yuji Matsumoto. He has given me much helpful advice with regard to the conduct of research, both when I was at Kyoto University and after I left there. The use of dependency graphs for representation of case frame patterns was inspired by one of his statements in a personal conversation. I would like to thank Prof. Junichi Nakamura of Kyoto University, Prof. Satoshi Sato of the Japan Advance Institute of Science and Technology, and other members of the Nagao Lab. for their advice and contributions to our discussions. The research reported in this dissertation was conducted at CC Media Research Laboratories, NEC Corporation and the Theory NEC Laboratory, Real World Comput- ing Partnership (RWCP). I would like to express my sincere appreciation to Mr. Kat- suhiro Nakamura, Mr. Tomoyuki Fujita, and Dr. Shun Doi of NEC. Without their continuous encouragement and support, I would not have been able to complete this work. Sincere appreciation also goes to Naoki Abe of NEC. Most of the research reported in this thesis was conducted jointly with him. Without his advice and proposals, I would not have been able to achieve the results represented here. Ideas for the tree cut iii iv model and the hard co-occurrence model came out in discussions with him, and the algorithm Find-MDL was devised on the basis of one of his ideas. I am deeply appreciative of the encouragement and advice given me by Kenji Ya- manishi of NEC, who introduced me to the MDL principle; this was to become the most important stimulus to the idea of conducting this research. He also introduced me to many useful machine learning techniques, that have broadened my outlook toward the ﬁeld. I also thank Jun-ichi Takeuchi, Atsuyoshi Nakamura, and Hiroshi Mamitsuka of NEC for their helpful advice and suggestions. Jun-ichi s introduction to me of the work of Joe Suzuki eventually leads to the development in this study of the case- dependency learning method. Special thanks are also due to Yuuko Yamaguchi and Takeshi Futagami of NIS who implemented the programs of Find-MDL, 2D-Clustering. In expressing my appreciation to Yasuharu Den of NAIST, David Carter of Speech Machines, and Takayoshi Ochiai of NIS, I would like them to know how much I had enjoyed the enlightening conversations I had with them. I am also grateful to Prof. Mark Petersen of Meiji University for what he has taught me about the technical writing of English. Prof. Petersen also helped correct the English of the text in this thesis, and without his help, it would be neither so readable nor so precise. Yasuharu Den, Kenji Yamanishi, David Carter, and Diana McCarthy of Sussex University read some or all of this thesis and made many helpful comments. Thanks also go to all of them, though the responsibility for ﬂaws and errors it contains remains entirely with me. I owe a great many thanks to many people who were kind enough to help me over the course of this work. I would like to express here my great appreciation to all of them. Finally, I also would like to express a deep debt of gratitude to my parents, who instilled in me a love for learning and thinking, and to my wife Hsiao-ya, for her constant encouragement and support. Contents Abstract i Acknowledgements iii 1 Introduction 1 1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Problem Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.4 Organization of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . 7 2 Related Work 9 2.1 Extraction of Case Frames . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.2 Case Slot Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2.1 Word-based approach and the data sparseness problem . . . . . 11 2.2.2 Similarity-based approach . . . . . . . . . . . . . . . . . . . . . 13 2.2.3 Class-based approach . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3 Word Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.4 Case Dependency Learning . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.5 Structural Disambiguation . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.5.1 The lexical approach . . . . . . . . . . . . . . . . . . . . . . . . 16 2.5.2 The combined approach . . . . . . . . . . . . . . . . . . . . . . 19 2.6 Word Sense Disambiguation . . . . . . . . . . . . . . . . . . . . . . . . 21 2.7 Introduction to MDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.7.1 Basics of Information Theory . . . . . . . . . . . . . . . . . . . 23 2.7.2 Two-stage code and MDL . . . . . . . . . . . . . . . . . . . . . 26 2.7.3 MDL as data compression criterion . . . . . . . . . . . . . . . . 30 2.7.4 MDL as estimation criterion . . . . . . . . . . . . . . . . . . . . 30 2.7.5 Employing MDL in NLP . . . . . . . . . . . . . . . . . . . . . . 34 3 Models for Lexical Knowledge Acquisition 37 3.1 Case Slot Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3.2 Case Frame Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.3 Co-occurrence Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 v vi CONTENTS 3.4 Relations between Models . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.5 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.6 Disambiguation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4 Case Slot Generalization 53 4.1 Tree Cut Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.2 MDL as Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 4.3 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 4.4 Advantages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 4.5 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.5.1 Experiment 1: qualitative evaluation . . . . . . . . . . . . . . . 62 4.5.2 Experiment 2: pp-attachment disambiguation . . . . . . . . . . 65 4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 5 Case Dependency Learning 73 5.1 Dependency Forest Model . . . . . . . . . . . . . . . . . . . . . . . . . 73 5.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 5.3 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5.3.1 Experiment 1: slot-based model . . . . . . . . . . . . . . . . . . 77 5.3.2 Experiment 2: slot-based disambiguation . . . . . . . . . . . . . 80 5.3.3 Experiment 3: class-based model . . . . . . . . . . . . . . . . . 81 5.3.4 Experiment 4: simulation . . . . . . . . . . . . . . . . . . . . . 82 5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 6 Word Clustering 91 6.1 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 6.2 MDL as Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 6.3 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 6.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 6.4.1 Experiment 1: qualitative evaluation . . . . . . . . . . . . . . . 96 6.4.2 Experiment 2: compound noun disambiguation . . . . . . . . . 97 6.4.3 Experiment 3: pp-attachment disambiguation . . . . . . . . . . 98 6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 7 Structural Disambiguation 101 7.1 Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 7.2 An Analysis System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 7.3 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 8 Conclusions 107 8.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 8.2 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 CONTENTS vii References 110 A 127 A.1 Derivation of Description Length: Two-stage Code . . . . . . . . . . . 127 A.2 Learning a Soft Case Slot Model . . . . . . . . . . . . . . . . . . . . . . 128 A.3 Number of Tree Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 A.4 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 A.5 Equivalent Dependency Tree Models . . . . . . . . . . . . . . . . . . . 131 A.6 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 Publication List 134 viii CONTENTS List of Tables 2.1 Example case frame data. . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2 Example case slot data. . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.3 Example case slot data. . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.4 Example input data as doubles. . . . . . . . . . . . . . . . . . . . . . . 17 2.5 Example input data as triples. . . . . . . . . . . . . . . . . . . . . . . . 18 2.6 Example input data as quadruples and labels. . . . . . . . . . . . . . . 18 3.1 Numbers of parameters in case slot models. . . . . . . . . . . . . . . . . 39 3.2 Example case frame data generated by a word-based model. . . . . . . 41 3.3 Example case frame data generated by a class-based model. . . . . . . 41 3.4 Example case frame data generated by a slot-based model. . . . . . . . 41 3.5 Numbers of parameters in case frame models. . . . . . . . . . . . . . . 42 3.6 Numbers of parameters in co-occurrence models. . . . . . . . . . . . . . 45 3.7 Summary of the formalization. . . . . . . . . . . . . . . . . . . . . . . . 46 4.1 Number of parameters and KL divergence for the ﬁve tree cut models. . 56 4.2 Calculating description length. . . . . . . . . . . . . . . . . . . . . . . . 58 4.3 Description lengths for the ﬁve tree cut models. . . . . . . . . . . . . . 58 4.4 Generalization result. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.5 Example input data (for the arg2 slot for eat). . . . . . . . . . . . . . 62 4.6 Examples of generalization results. . . . . . . . . . . . . . . . . . . . . 64 4.7 Required computation time and number of generalized levels. . . . . . . 65 4.8 Number of data items. . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.9 PP-attachment disambiguation results. . . . . . . . . . . . . . . . . . . 68 4.10 Example generalization results for SA and MDL. . . . . . . . . . . . . . 70 4.11 Some hard examples for LA. . . . . . . . . . . . . . . . . . . . . . . . . 71 5.1 Parameters labeled with each node. . . . . . . . . . . . . . . . . . . . . 74 5.2 The statistic θ for node pairs. . . . . . . . . . . . . . . . . . . . . . . . 76 5.3 Verbs appearing most frequently. . . . . . . . . . . . . . . . . . . . . . 77 5.4 Case slots considered. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 5.5 Verbs and their dependent case slots. . . . . . . . . . . . . . . . . . . . 79 5.6 Verbs and their dependent case slots. . . . . . . . . . . . . . . . . . . . 87 5.7 Verbs with signiﬁcant perplexity reduction. . . . . . . . . . . . . . . . . 88 ix x LIST OF TABLES 5.8 Randomly selected verbs and their perplexities. . . . . . . . . . . . . . 88 5.9 PP-attachment disambiguation results. . . . . . . . . . . . . . . . . . . 88 6.1 Compound noun disambiguation results. . . . . . . . . . . . . . . . . . 98 6.2 PP-attachment disambiguation results. . . . . . . . . . . . . . . . . . . 99 6.3 PP-attachment disambiguation results. . . . . . . . . . . . . . . . . . . 100 7.1 PP-attachment disambiguation results. . . . . . . . . . . . . . . . . . . 105 7.2 Results reported in previous work. . . . . . . . . . . . . . . . . . . . . . 105 8.1 Models proposed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 8.2 Algorithm employed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 List of Figures 1.1 Organization of this thesis. . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1 Frequency data for the subject slot for verb ﬂy. . . . . . . . . . . . . . 11 2.2 Word-based distribution estimated using MLE. . . . . . . . . . . . . . . 13 2.3 Example co-occurrence data. . . . . . . . . . . . . . . . . . . . . . . . . 15 3.1 An example hard co-occurrence model. . . . . . . . . . . . . . . . . . . 44 3.2 Relations between models. . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.1 An example thesaurus. . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.2 A tree cut model with [swallow, crow, eagle, bird, bug, bee, insect]. . . 54 4.3 A tree cut model with [BIRD, bug, bee, insect]. . . . . . . . . . . . . . 55 4.4 A tree cut model with [BIRD, INSECT]. . . . . . . . . . . . . . . . . . 55 4.5 The Find-MDL algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . 60 4.6 An example application of Find-MDL. . . . . . . . . . . . . . . . . . . 61 4.7 Example generalization result (for the arg2 slot for eat). . . . . . . . . 63 4.8 Accuracy-coverage plots for MDL, SA, and LA. . . . . . . . . . . . . . 67 5.1 Example dependency forests. . . . . . . . . . . . . . . . . . . . . . . . . 84 5.2 The learning algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.3 A dependency forest as case frame patterns. . . . . . . . . . . . . . . . 85 5.4 Case frame patterns (dependency forest model) for buy. . . . . . . . . 86 5.5 Number of links versus data size. . . . . . . . . . . . . . . . . . . . . . 86 5.6 KL divergence versus data size. . . . . . . . . . . . . . . . . . . . . . . 89 6.1 A part of a constructed thesaurus. . . . . . . . . . . . . . . . . . . . . . 96 6.2 Accuracy-coverage plots for 2D-Clustering, Brown, and Word-based. . . 98 7.1 The disambiguation procedure. . . . . . . . . . . . . . . . . . . . . . . 102 7.2 Outline of the natural language analysis system. . . . . . . . . . . . . . 104 A.1 An example one-way branching binary tree. . . . . . . . . . . . . . . . 130 A.2 Equivalent dependency tree models. . . . . . . . . . . . . . . . . . . . . 132 xi Chapter 1 Introduction ... to divide each of the diﬃculties under examina- tion into as many parts as possible, and as might be necessary for its adequate solution. - Rene Descartes 1.1 Motivation Structural (or syntactic) disambiguation in sentence analysis is still a central problem in natural language processing. To resolve ambiguities completely, we would need to construct a human language understanding system (Johnson-Laird, 1983; Tsujii, 1987; Altmann and Steedman, 1988). The construction of such a system would be extremely diﬃcult, however, if not impossible. For example, when analyzing the sentence I ate ice cream with a spoon, (1.1) a natural language processing system may obtain two interpretations: I ate ice cream using a spoon and I ate ice cream and a spoon. i.e., a pp-attachment ambiguity may arise, because the prepositional phrase with a spoon can syntactically be attached to both eat and ice cream. If a human speaker reads the same sentence, common sense will certainly lead him to assume the former interpretation over the latter, because he understands that: a spoon is a tool for eating food, a spoon is not edible, etc. Incorporating such world knowledge into a natural language processing system is highly diﬃcult, however, because of its sheer enormity. An alternative approach is to make use of only lexical semantic knowledge, specif- ically case frame patterns (Fillmore, 1968) (or their near equivalents: selectional pat- terns (Katz and Fodor, 1963), and subcategorization patterns (Pollard and Sag, 1987)). That is, to represent the content of a sentence or a phrase with a case frame having 1 2 CHAPTER 1. INTRODUCTION a head1 and multiple slots, and to incorporate into a natural language processing system the knowledge of which words can ﬁll into which slot of a case frame. For example, we can represent the sentence I ate ice cream as (eat (arg1 I) (arg2 ice-cream)), where the head is eat, the arg1 slot represents the subject and the arg2 slot represents the direct object. The values of the arg1 slot and the arg2 slot are I and ice cream, respectively. Furthermore, we can incorporate as the case frame patterns for the verb eat the knowledge that a member of the word class animal can be the value of the arg1 slot and a member of the word class food can be the value of the arg2 slot, etc. The case frames of the two interpretations obtained in the analysis of the above sentence (1.1), then, become (eat (arg1 I) (arg2 ice-cream) (with spoon)) (eat (arg1 I) (arg2 (ice-cream (with spoon)))). Referring to the case frame patterns indicating that spoon can be the value of the with slot when the head is eat, and spoon cannot be the value of the with slot when the head is ice cream, a natural language processing system naturally selects the former interpretation and thus resolves the ambiguity. Previous data analyses have indeed indicated that using lexical semantic knowledge can, to a quite large extent, cope with the structural disambiguation problem (Hobbs and Bear, 1990; Whittemore, Ferrara, and Brunner, 1990). The advantage of the use of lexical knowledge over that of world knowledge is the relative smallness of its amount. By restricting knowledge to that of relations between words, the construction of a natural language processing system becomes much easier. (Although the lexical knowledge is still unable to resolve the problem completely, past research suggests that it might be the most realistic path we can take right now.) As is made clear in the above example, case frame patterns mainly include gener- alized information, e.g., that a member of the word class animal can be the value of the arg2 slot for the verb eat. Classically, case frame patterns are represented by selectional restrictions (Katz and Fodor, 1963), i.e., discretely represented by semantic features, but it is better to represent them continuously, because a word can be the value of a slot to a certain probabilistic degree, as is suggested by the following list (Resnik, 1993b): (1) Mary drank some wine. (2) Mary drank some gasoline. (3) Mary drank some pencils. (4) Mary drank some sadness. 1I slightly abuse terminology here, as head is usually used for subcategorization patterns in the discipline of HPSG, but not in case frame theory. 1.2. PROBLEM SETTING 3 Furthermore, case frame patterns are not limited to reference to individual case slots. Dependencies between case slots need also be considered. The term dependency here refers to the relationship that may exist between case slots and that indicates strong co-occurrence between the values of those case slots. For example, consider the following sentences:2 (1) She ﬂies jets. (2) That airline company ﬂies jets. (3) She ﬂies Japan Airlines. (4) That airline company ﬂies Japan Airlines. (1.2) We see that an airline company can be the value of the arg1 slot, when the value of the arg2 slot is an airplane but not when it is an airline company. These sentences indicate that the possible values of case slots depend in general on those of others: dependencies between case slots exist.3 Another consensus on lexical semantic knowledge in recent studies is that it is prefer- able to learn lexical knowledge automatically from corpus data. Automatic acquisition of lexical knowledge has the merits of (1) saving the cost of deﬁning knowledge by hand, (2) doing away with the subjectivity inherent in human-deﬁned knowledge, and (3) making it easier to adapt a natural language processing system to a new domain. Although there have been many studies conducted in the past (described here in Chapter 2) to address the lexical knowledge acquisition problem, further investigation, especially that based on a principled methodology is still needed, and this is, in fact, the problem I address in this thesis. The search for a mathematical formalism for lexical knowledge acquisition is not only motivated by concern for logical niceties; I believe that it can help to better cope with practical problems (for example, the disambiguation problem). The ulti- mate outcome of the investigations in this thesis, therefore, should be a formalism of lexical knowledge acquisition and at the same time a high-performance disambiguation method. 1.2 Problem Setting The problem of acquiring and using lexical semantic knowledge, especially that of case frame patterns, can be formalized as follows. A learning module acquires case frame patterns on the basis of some case frame instances extracted from corpus data. A processing (disambiguation) module then refers to the acquired knowledge and judges 2 indicates an unacceptable natural language expression. 3One may argue that ﬂy has diﬀerent word senses in these sentences and for each of these word senses there is no dependency between the case slots. Word senses are in general diﬃcult to deﬁne precisely, however. I think that it is preferable not to resolve them until doing so is necessary in a particular application. That is to say that, in general, case dependencies do exist and the development of a method for learning them is needed. 4 CHAPTER 1. INTRODUCTION the degrees of acceptability of some new case frames, including previously unseen ones. The goals of learning are to represent more compactly the given case frames, and to judge more correctly the degrees of acceptability of new case frames. In this thesis, I propose a probabilistic approach to lexical knowledge acquisition and structural disambiguation. 1.3 Approach In general, a machine learning process consists of three elements: model, strategy (cri- terion), and algorithm. That is, when we conduct machine learning, we need consider (1) what kind of model we are to use to represent the problem, (2) what kind of strat- egy we should adopt to control the learning process, and (3) what kind of algorithm we should employ to perform the learning task. We need to consider each of these elements here. Division into subproblems The lexical semantic knowledge acquisition problem is a quite complicated task, and there are too many relevant factors (generalization of case slot values, dependencies between case slots, etc.) to simply incorporate all of them into a single model. As a ﬁrst step, I divide the problem into three subproblems: case slot generalization, case dependency learning, and word clustering (thesaurus construction). I deﬁne probability models (probability distributions) for each subproblem and view the learning task of each subproblem as that of estimating its corresponding probability models based on corpus data. Probability models We can assume that case slot data for a case slot for a verb are generated on the basis of a conditional probability distribution that speciﬁes the conditional probability of a noun given the verb and the case slot. I call such a distribution a case slot model. When the conditional probability of a noun is deﬁned as the conditional probability of the noun class to which the noun belongs, divided by the size of the noun class, I call the case slot model a hard case slot model. When the case slot model is deﬁned as a ﬁnite mixture model, namely a linear combination of the word probability distributions within individual noun classes, I call it a soft case slot model. Here the term hard means that the model is characterized by a type of word clustering in which a word can only belong to a single class alone, while soft means that the model is characterized by a type of word clustering in which a word can belong to several diﬀerent classes. I formalize the problem of generalizing the values of a case slot as that of estimating a hard (or soft) case slot model. The generalization problem, then, turns out to be 1.3. APPROACH 5 that of selecting a model, from a class of hard (or soft) case slot models, which is most likely to have given rise to the case slot data. We can assume that case frame data for a verb are generated according to a multi- dimensional joint probability distribution over random variables that represent the case slots. I call the distribution a case frame model. I further classify this case frame model into three types of probability models each reﬂecting the type of its random variables: the word-based case frame model, the class-based case frame model, and the slot-based case frame model. I formalize the problem of learning dependencies between case slots as that of estimating a case frame model. The dependencies between case slots are represented as probabilistic dependencies between random variables. We can assume that co-occurrence data for nouns and verbs with respect to a slot are generated based on a joint probability distribution that speciﬁes the co-occurrence probabilities of noun verb pairs. I call such a distribution a co-occurrence model. I call this co-occurrence model a hard co-occurrence model, when the joint probability of a noun verb pair is deﬁned as the product of the following three elements: (1) the joint probability of the noun class and the verb class to which the noun and the verb respectively belong, (2) the conditional probability of the noun given its noun class, and (3) the conditional probability of the verb given its verb class. When the co-occurrence model is deﬁned as a double mixture model, namely, a double linear combination of the word probability distributions within individual noun classes and those within individual verb classes, I call it a soft co-occurrence model. I formalize the problem of clustering words as that of estimating a hard (or soft) co-occurrence model. The clustering problem, then, turns out to be that of selecting a model from a class of hard (or soft) co-occurrence models, which is most likely to have given rise to the co-occurrence data. MDL as strategy For all subproblems, the learning task turns out to be that of selecting the best model from among a class of models. The question now is what the learning strategy (or criterion) is to be. I employ here the Minimum Description Length (MDL) principle. The MDL principle is a principle for both data compression and statistical estimation (described in Chapter 2). MDL provides a theoretically way to deal with the data sparseness problem, the main diﬃculty in a statistical approach to language processing. At the same time, MDL leads us to an information-theoretic solution to the lexical knowledge acquisition problem, in which case frames are viewed as structured data, and the learning process turns out to be that of data compression. 6 CHAPTER 1. INTRODUCTION Eﬃcient algorithms In general, there is a trade-oﬀ between model classes and algorithms. A complicated model class would be precise enough for representing a problem, but it might be diﬃcult to learn in terms of learning accuracy and computation time. In contrast, a simple model class might be easy to learn, but it would be too simplistic for representing a problem. In this thesis, I place emphasis on eﬃciency and restrict a model class when doing so is still reasonable for representing the problem at hand. For the case slot generalization problem, I make use of an existing thesaurus and restrict the class of hard case slot models to that of tree cut models. I also employ an eﬃcient algorithm, which provably obtains the optimal tree cut model in terms of MDL. For the case dependency learning problem, I restrict the class of case frame models to that of dependency forest models, and employ another eﬃcient algorithm to learn the optimal dependency forest model in terms of MDL. For the word clustering problem, I address the issue of estimating the hard co- occurrence model, and employ an eﬃcient algorithm to repeatedly estimate a subopti- mal MDL model from a class of hard co-occurrence models. Disambiguation methods I then view the structural disambiguation problem as that of statistical prediction. Speciﬁcally, the likelihood value of each interpretation (case frame) is calculated on the basis of the above models, and the interpretation with the largest likelihood value is output as the analysis result. I have devised several disambiguation methods along this line. One of them is especially useful when the data size for training is at the level of that currently available. In implementation of this method, the learning module combines with the hard co-occurrence model to cluster words with respect to each case slot, and it combines with the tree cut model to generalize the values of each case slot by means of a hand-made thesaurus. The disambiguation module ﬁrst calculates a likelihood value for each interpretation on the basis of hard co-occurrence models and outputs the interpretation with the largest likelihood value; if the likelihood values are equal (most particularly, if all of them are 0), it uses likelihood values calculated on the basis of tree cut models; if the likelihood values are still equal, it makes a default decision. The accuracy achieved by this method is 85.2, which is higher than that of state- of-the-art methods. 1.4. ORGANIZATION OF THE THESIS 7 1.4 Organization of the Thesis This thesis is organized as follows. In Chapter 2, I review previous work on lexical semantic knowledge acquisition and structural disambiguation. I also introduce the MDL principle. In Chapter 3, I deﬁne probability models for each subproblem of lexical semantic knowledge acquisition. In Chapter 4, I describe the method of using the tree cut model to generalize case slots. In Chapter 5, I describe the method of using the dependency forest model to learn dependencies between case slots. In Chapter 6, I describe the method of using the hard co-occurrence model to conduct word clustering. In Chapter 7, I describe the practical disambiguation method. In Chapter 8, I conclude the thesis with some remarks (see Figure 1.1). Chapter1 Introduction Chapter2 Related Work Chapter3 Basic Models Chapter4 Case Slot Generalization Chapter5 Case Dependency Learning Chapter6 Word Clustering Chapter7 Disambiguation Chapter8 Conclusions Figure 1.1: Organization of this thesis. 8 CHAPTER 1. INTRODUCTION Chapter 2 Related Work Continue to cherish old knowledge so as to con- tinue to discover new. - Confucius In this chapter, I review previous work on lexical knowledge acquisition and disam- biguation. I also introduce the MDL principle. 2.1 Extraction of Case Frames Extracting case frame instances automatically from corpus data is a diﬃcult task, because when conducting extraction, ambiguities may arise, and we need to exploit lexical semantic knowledge to resolve them. Since our goal of extraction is indeed to acquire such knowledge, we are faced with the problem of which is to come ﬁrst, the chicken or the egg. Although there have been many methods proposed to automatically extract case frames from corpus data, their accuracies do not seem completely satisfactory, and the problem still needs investigation. Manning (1992), for example, proposes extracting case frames by using a ﬁnite state parser. His method ﬁrst uses a statistical tagger (cf., (Church, 1988; Kupiec, 1992; Charniak et al., 1993; Merialdo, 1994; Nagata, 1994; Schutze and Singer, 1994; Brill, 1995; Samuelsson, 1995; Ratnaparkhi, 1996; Haruno and Matsumoto, 1997)) to assign a part of speech to each word in the sentences of a corpus. It then uses the ﬁnite state parser to parse the sentences and note case frames following verbs. Finally, it ﬁlters out statistically unreliable extracted results on the basis of hypothesis testing (see also (Brent, 1991; Brent, 1993; Smadja, 1993; Chen and Chen, 1994; Grefenstette, 1994)). Briscoe and Carroll (1997) extracted case frames by using a probabilistic LR parser. This parser ﬁrst parses sentences to obtain analyses with shallow phrase structures, and assigns a likelihood value to each analysis. An extractor then extracts case frames 9 10 CHAPTER 2. RELATED WORK from the most likely analyses (see also (Hindle and Rooth, 1991; Grishman and Sterling, 1992)). Utsuro, Matsumoto, and Nagao (1992) propose extracting case frames from a par- allel corpus in two diﬀerent languages. Exploiting the fact that a syntactic ambiguity found in one language may not exist at all in another language, they conduct pattern matching between case frames of translation pairs given in the corpus and choose the best matched case frames as extraction results (see also (Matsumoto, Ishimoto, and Utsuro, 1993)). An alternative to the automatic approach is to employ a semi-automatic method, which can provide much more reliable results. The disadvantage, however, is its re- quirement of having disambiguation decisions made by a human, and how to reduce the cost of human intervention becomes an important issue. Carter (1997) developed an interaction system for eﬀectively collecting case frames semi-automatically. This system ﬁrst presents a user with a range of properties that may help resolve ambiguities in a sentence. The user then designates the value of one of the properties, the system discards those interpretations which are inconsistent with the designation, and it re-displays only the properties which remain. After several such interactions, the system obtains a most likely correct case frame of a sentence (see also (Marcus, Santorini, and Marcinkiewicz, 1993)). Using any one of the methods, we can extract case frame instances for a verb, to obtain data like that shown in Table 2.1, although no method guarantees that the extracted results are completely correct. In this thesis, I refer to this type of data as case frame data. If we restrict our attention on a speciﬁc slot, we obtain data like that shown in Table 2.2. I refer to this type of data as case slot data. Table 2.1: Example case frame data. (ﬂy (arg1 girl)(arg2 jet)) (ﬂy (arg1 company)(arg2 jet)) (ﬂy (arg1 girl)(arg2 company)) Table 2.2: Example case slot data. Verb Slot name Slot value ﬂy arg1 girl ﬂy arg1 company ﬂy arg1 girl 2.2. CASE SLOT GENERALIZATION 11 2.2 Case Slot Generalization One case-frame-pattern acquisition problem is that of generalization of (values of) case slots; this has been intensively investigated in the past. 2.2.1 Word-based approach and the data sparseness problem Table 2.3 shows some example cast slot data for the arg1 slot for the verb ﬂy. By counting occurrences of each noun at the slot, we can obtain frequency data shown in Figure 2.1. Table 2.3: Example case slot data. Verb Slot name Slot value ﬂy arg1 bee ﬂy arg1 bird ﬂy arg1 bird ﬂy arg1 crow ﬂy arg1 bird ﬂy arg1 eagle ﬂy arg1 bee ﬂy arg1 eagle ﬂy arg1 bird ﬂy arg1 crow 0 1 2 3 4 swallow crow eagle bird bug bee insect \"Freq.\" Figure 2.1: Frequency data for the subject slot for verb ﬂy. The problem of learning case slot patterns for a slot for a verb can be viewed as the problem of estimating the underlying conditional probability distribution which 12 CHAPTER 2. RELATED WORK gives rise to the corresponding case slot data. The conditional distribution is deﬁned as P(nv, r), (2.1) where random variable n represents a value in the set of nouns N  {n1, n2,    , nN}, random variable v a value in the set of verbs V  {v1, v2,   , vV }, and random variable r a value in the set of slot names R  {r1, r2,    , rR}. Since random variables take on words as their values, this type of probability distribution is often referred to as a word-based model. The degree of noun ns being the value of slot r for verb v1 is represented by a conditional probability. Another way of learning case slot patterns for a slot for a verb is to calculate the association ratio measure, as proposed in (Church et al., 1989; Church and Hanks, 1989; Church et al., 1991). The association ratio is deﬁned as S(nv, r)  log P(nv, r) P(n) , (2.2) where n assumes a value from the set of nouns, v from the set of verbs and r from the set of slot names. The degree of noun n being the value of slot r for verb v is represented as the ratio between a conditional probability and a marginal probability. The two measures in fact represent two diﬀerent aspects of case slot patterns. The former indicates the relative frequency of a nouns being the slot value, while the latter indicates the strength of associativeness between a noun and the verb with respect to the slot. The advantage of the latter may be that it takes into account of the inﬂuence of the marginal probability P(n) on the conditional probability P(nv, r). The advantage of the former may be its ease of use in disambiguation as a likelihood value. Both the use of the conditional probability and that of the association ratio may suﬀer from the data sparseness problem, i.e., the number of parameters in the con- ditional distribution deﬁned in (2.1) is very large, and accurately estimating them is diﬃcult with the amount of data typically available. When we employ Maximum Likelihood Estimation (MLE) to estimate the param- eters, i.e., when we estimate the conditional probability P(nv, r) as2 ˆP(nv, r)  f(nv, r) f(v, r) , where f(nv, r) stands for the frequency of noun n being the value of slot r for verb v, f(v, r) the total frequency of r for v (Figure 2.2 shows the results for the data in Figure 2.1), we may obtain quite poor results. Most of the probabilities might be estimated as 0, for example, just because a possible value of the slot in question happens not to appear. 1Hereafter, I will sometimes use the same symbol to denote both a random variable and one of its values; it should be clear from the context, which it is denoting at any given time. 2Throughout this thesis, ˆθ denotes an estimator (or an estimate) of θ. 2.2. CASE SLOT GENERALIZATION 13 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 swallow crow eagle bird bug bee insect \"Prob.\" Figure 2.2: Word-based distribution estimated using MLE. To overcome this problem, we can smooth the probabilities by resorting to statistical techniques (Jelinek and Mercer, 1980; Katz, 1987; Gale and Church, 1990; Ristad and Thomas, 1995). We can, for example, employ an extended version of the Laplaces Law of Succession (cf., (Jeﬀreys, 1961; Krichevskii and Troﬁmov, 1981)) to estimate P(nv, r) as ˆP(nv, r)  f(nv, r)  0.5 f(v, r)  0.5  N where N denotes the size of the set of nouns.3 The results may still not be satisfactory, however. One possible way to cope better with the data sparseness problem is to exploit additional knowledge or data rather than make use of only related case slot data. Two such approaches have been proposed previously: one is called the similarity-based approach, the other the class-based approach. 2.2.2 Similarity-based approach Grishman and Sterling (1994) propose to estimate conditional probabilities by using other conditional probabilities under contexts of similar words, where the similar words themselves are collected on the basis of corpus data. Their method estimates the conditional probability P(nv, r) as ˆP(nv, r)   v λ(v, v)  ˆP(nv, r), where v represents a verb similar to verb v, and λ(v, v) the similarity between v and v. That is, it smoothes a conditional probability by taking the weighted average of 3This smoothing method can be justiﬁed from the viewpoint of Bayesian Estimation. The estimate is in fact the Bayesian estimate with Jeﬀreys Prior being the prior probability. 14 CHAPTER 2. RELATED WORK other conditional probabilities under contexts of similar words using similarities as the weights. Note that the equation  v λ(v, v)  1 must hold. The advantage of this approach is that it relies only on corpus data. (Cf., (Dagan, Marcus, and Makovitch, 1992; Dagan, Pereira, and Lee, 1994; Dagan, Pereira, and Lee, 1997).) 2.2.3 Class-based approach A number of researchers have proposed to employ class-based models, which use classes of words rather than individual words. An example of a class-based approach is Resniks method of learning case slot patterns by calculating the selectional association measure (Resnik, 1993a; Resnik, 1993b). The selectional association is deﬁned as: A(nv, r)  max Cn  P(Cv, r)  log P(Cv, r) P(C)  , (2.3) where n represents a value in the set of nouns, v a value in the set of verbs and r a value in the set of slot names, and C denotes a class of nouns present in a given thesaurus. (See also (Framis, 1994; Ribas, 1995).) This measure, however, is based on heuristics, and thus can be diﬃcult to justify theoretically. Other class-based methods for case slot generalization are also proposed (Almual- lim et al., 1994; Tanaka, 1994; Tanaka, 1996; Utsuro and Matsumoto, 1997; Miyata, Utsuro, and Matsumoto, 1997). 2.3 Word Clustering Automatically clustering words or constructing a thesaurus can also be considered to be a class-based approach, and it helps cope with the data sparseness problem not only in case frame pattern acquisition but also in other natural language learning tasks. If we focus our attention on one case slot, we can obtain co-occurrence data for verbs and nouns with respect to that slot. Figure 2.3, for example, shows such data, in this case, counts of co-occurrences of verbs and their arg2 slot values (direct ob- jects). We can classify words by using such co-occurrence data on the assumption that semantically similar words have similar co-occurrence patterns. A number of methods have been proposed for clustering words on the basis of co- occurrence data. Brown et al. (1992), for example, propose a method of clustering words on the basis of MLE in the context of n-gram estimation. They ﬁrst deﬁne an n-gram class model as P(wnwn1 1 )  P(wnCn)  P(CnCn1 1 ), 2.3. WORD CLUSTERING 15 make eat drink wine beer bread rice 0 3 1 0 5 1 4 0 2 4 0 0 Figure 2.3: Example co-occurrence data. where C represents a word class. They then view the clustering problem as that of partitioning the vocabulary (a set of words) into a designated number of word classes whose resulting 2-gram class model has the maximum likelihood value with respect to a given word sequence (i.e., co-occurrence data). Brown et al have also devised an eﬃcient algorithm for performing this task, which turns out to iteratively merge the word class pair having the least reduction in empirical mutual information until the number of classes created equals the designated number. The disadvantage of this method is that one has to designate in advance the number of classes to be created, with no guarantee at all that this number will be optimal. Pereira, Tishby, and Lee (1993) propose a method of clustering words based on co-occurrence data over two sets of words. Without loss of generality, suppose that the two sets are a set of nouns N and a set of verbs V, and that a sample of co-occurrence data is given as (ni, vi), ni  N , vi  V, i  1,    , s. They deﬁne P(n, v)   C P(C)  P(nC)  P(vC) as a model which can give rise to the co-occurrence data, where C represents a class of nouns. They then view the problem of clustering nouns as that of estimating such a model. The classes obtained in this way, which they call soft clustering, have the following properties: (1) a noun can belong to several diﬀerent classes, and (2) each class is characterized by a membership distribution. They devised an eﬃcient clustering algorithm based on deterministic annealing technique.4 Conducting soft clustering makes it possible to cope with structural and word sense ambiguity at the 4Deterministic annealing is a computation technique for ﬁnding the global optimum (minimum) value of a cost function (Rose, Gurewitz, and Fox, 1990; Ueda and Nakano, 1998). The basic idea is to conduct minimization by using a number of free energy functions parameterized by tempera- tures for which free energy functions with high temperatures loosely approximate a target function, 16 CHAPTER 2. RELATED WORK same time, but it also requires more training data and makes the learning process more computationally demanding. Tokunaga, Iwayama, and Tanaka (1995) point out that, for disambiguation pur- poses, it is necessary to construct one thesaurus for each case slot on the basis of co-occurrence data concerning to that slot. Their experimental results indicate that, for disambiguation, the use of thesauruses constructed from data speciﬁc to the target slot is preferable to the use of thesauruses constructed from data non-speciﬁc to the slot. Other methods for automatic word clustering have also been proposed (Hindle, 1990; Pereira and Tishby, 1992; McKeown and Hatzivassiloglou, 1993; Grefenstette, 1994; Stolcke and Omohundro, 1994; Abe, Li, and Nakamura, 1995; McMahon and Smith, 1996; Ushioda, 1996; Hogenhout and Matsumoto, 1997). 2.4 Case Dependency Learning There has been no method proposed to date, however, that learns dependencies between case slots. In past research, methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent (Hindle and Rooth, 1991; Sekine et al., 1992; Resnik, 1993a; Grishman and Sterling, 1994; Alshawi and Carter, 1994), or at most two case slots are dependent (Brill and Resnik, 1994; Ratnaparkhi, Reynar, and Roukos, 1994; Collins and Brooks, 1995). 2.5 Structural Disambiguation 2.5.1 The lexical approach There have been many probabilistic methods proposed in the literature to address the structural disambiguation problem. Some methods tackle the basic problem of resolving ambiguities in quadruples (v, n1, p, n2) (e.g., (eat, ice-cream, with, spoon)) by mainly using lexical knowledge. Such methods can be classiﬁed into the following three types: the double approach, the triple approach, and the quadruple approach. The ﬁrst two approaches employ what I call a generation model and the third approach employs what I call a decision model (cf., Chapter 3). while free energy functions with low temperatures precisely approximate it. A deterministic-annealing- based algorithm manages to ﬁnd the global minimum value of the target function by continuously ﬁnding the minimum values of the free energy functions while incrementally decreasing the tempera- tures. (Note that deterministic annealing is diﬀerent from the classical simulated annealing technique (Kirkpatrick, Gelatt, and Vecchi, 1983).) In Pereira et als case, deterministic annealing is used to ﬁnd the minimum of average distortion. They have proved that, in their problem setting, minimizing average distortion is equivalent to maximizing likelihood with respect to the given data (i.e., MLE). 2.5. STRUCTURAL DISAMBIGUATION 17 The double approach This approach takes doubles of the form (v, p) and (n1, p), like those in Table 2.4, as training data to acquire lexical knowledge and judges the attachment sites of (p, n2) in quadruples based on the acquired knowledge. Table 2.4: Example input data as doubles. eat in eat with ice-cream with candy with Hindle and Rooth (1991) propose the use of the so-called lexical association mea- sure calculated based on such doubles: P(pv), where random variable v represents a verb (in general a head), and random variable p a slot (preposition). They further propose viewing the disambiguation problem as that of hypothesis testing. More speciﬁcally, they calculate the t-score, which is a statistic on the diﬀerence between the two estimated probabilities ˆP(pv) and ˆP(pn1): t  ˆP(pv)  ˆP(pn1)  ˆσ2v Nv  ˆσ2n1 Nn1 , where ˆσv and ˆσn1 denote the standard deviations of ˆP(pv) and ˆP(pn1), respectively, and Nv and Nn1 denote the data sizes used to estimate these probabilities. If, for example, t  1.28, then (p, n2) is attached to v, t  1.28, (p, n2) is attached to n1, and otherwise no decision is made. (See also (Hindle and Rooth, 1993).) The triple approach This approach takes triples (v, p, n2) and (n1, p, n2), i.e., case slot data, like those in Table 2.5, as training data for acquiring lexical knowledge, and performs pp-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure (described in Section 2) calculated on the basis of such triples. The basic idea of his method is to compare A(n2v, p) and A(n2n1, p) deﬁned in (2.3), and make a disambiguation decision. Sekine et al. (1992) propose the use of joint probabilities P(v, p, n2) and P(n1, p, n2) in pp-attachment disambiguation. They devised a heuristic method for estimating the probabilities. (See also (Alshawi and Carter, 1994).) 18 CHAPTER 2. RELATED WORK Table 2.5: Example input data as triples. eat in park eat with spoon ice-cream with chocolate eat with chopstick candy with chocolate The quadruple approach This approach receives quadruples (v, n1, p, n2), as well as labels that indicate which way the pp-attachment goes, such as those in Table 2.6; and it learns disambiguation rules. Table 2.6: Example input data as quadruples and labels. eat ice-cream in park attv eat ice-cream with spoon attv eat candy with chocolate attn It in fact employs the conditional probability distribution  a decision model P(av, n1, p, n2), (2.4) where random variable a takes on attv and attn as its values, and random variables (v, n1, p, n2) take on quadruples as their values. Since the number of parameters in the distribution is very large, accurate estimation of the distribution would be impossible. In order to address this problem, Collins and Brooks (1995) devised a back-oﬀ method. It ﬁrst calculates the conditional probability P(av, n1, p, n2) by using the relative frequency f(a, v, n1, p, n2) f(v, n1, p, n2) , if the denominator is larger than 0; otherwise it successively uses lower order frequencies to heuristically calculate the probability. Ratnaparkhi, Reynar, and Roukos (1994) propose to learn the conditional probabil- ity distribution (2.4) with Maximum Entropy Estimation. They adopt the Maximum Entropy Principle (MEP) as the learning strategy, which advocates selecting the model having the maximum entropy from among the class of models that satisﬁes certain con- straints (see Section 2.7.4 for a discussion on the relation between MDL and MEP). The fact that a model must be one such that the expected value of a feature with 2.5. STRUCTURAL DISAMBIGUATION 19 respect to it equals that with respect to the empirical distribution is usually used as a constraint. Ratnaparkhi et als method deﬁnes, for example, a feature as follows fi   1 (p, n2) is attached to n1 in (-, ice-cream, with, chocolate) 0 otherwise. It then incrementally selects features, and eﬃciently estimates the conditional distribu- tion by using the Maximum Entropy Estimation technique (see (Jaynes, 1978; Darroch and Ratcliﬀ, 1972; Berger, Pietra, and Pietra, 1996)). Another method of the quadruple approach is to employ transformation-based error-driven learning (Brill, 1995), as proposed in (Brill and Resnik, 1994). This method learns and uses IF-THEN type rules, where the IF parts represent conditions like (p is with) and (v is see), and the THEN parts represent transformations from (attach to v) to (attach to n1), and vice-versa. The ﬁrst rule is always a default decision, and all the other rules indicate transformations (changes of attachment sites) subject to various IF conditions. 2.5.2 The combined approach Although the use of lexical knowledge can eﬀectively resolve ambiguities, it still has lim- itation. It is preferable, therefore, to utilize other kind of knowledge in disambiguation, especially when a decision cannot be made solely on the basis of lexical knowledge. The following two facts suggest that syntactic knowledge should also be used for the purposes of disambiguation. First, interpretations are obtained through syntactic parsing. Second, psycholinguistists observe that there are certain syntactic principles in humans language interpretation. For example, in English a phrase on the right tends to be attached to the nearest phrase on the left, - referred to as the right association principle (Kimball, 1973). (See also (Ford, Bresnan, and Kaplan, 1982; Frazier and Fodor, 1979; Hobbs and Bear, 1990; Whittemore, Ferrara, and Brunner, 1990)). We are thus led to the problem of how to deﬁne a probability model which combines the use of both lexical semantic knowledge and syntactic knowledge. One approach is to introduce probability models on the basis of syntactic parsing. Another approach is to introduce probability models on the basis of psycholinguistic principles (Li, 1996). Many methods belonging to the former approach have been proposed. A classical method is to employ the PCFG (Probabilistic Context Free Grammar) model (Fujisaki et al., 1989; Jelinek, Laﬀerty, and Mercer, 1990; Lari and Young, 1990), in which a CFG rule having the form of A  B1    Bm is associated with a conditional probability P(B1,   , BmA). (2.5) 20 CHAPTER 2. RELATED WORK In disambiguation the likelihood of an interpretation is deﬁned as the product of the conditional probabilities of the rules which are applied in the derivation of the inter- pretation. The use of PCFG, in fact, resorts more to syntactic knowledge rather than to lexical knowledge, and its performance seems to be only moderately good (Chitrao and Grishman, 1990). There are also many methods proposed which more eﬀectively make use of lexical knowledge. Collins (1997) proposes disambiguation through use of a generative probability model based on a lexicalized CFG (in fact, a restricted form of HPSG (Pollard and Sag, 1987)). (See also (Collins, 1996; Schabes, 1992; Hogenhout and Matsumoto, 1996; Den, 1996; Charniak, 1997).) In Collins model, each lexicalized CFG rule is deﬁned in the form of A  Ln    L1HR1    Rm, where a capitalized symbol denotes a category, with H being the head category on the right hand site. A category is deﬁned in the form of C(w, t), where C denotes the name of the category, w the head word associated with the category, and t the part-of-speech tag assigned to the head word. Furthermore, each rule is assigned a conditional probability P(Ln,   , L1, H, R1,    , RmA) (cf., (2.5)) that is assumed to satisfy P(Ln,   , L1, H, R1,    , RmA)  P(HA)  P(L1,   , LnA, H)  P(R1,    , RmA, H). In disambiguation, the likelihood of an interpretation is deﬁned as the product of the conditional probabilities of the rules which are applied in the derivation of the interpretation. While Collins has devised several heuristic methods for estimating the probability model, further investigation into learning methods for this model still appears necessary. Magerman (1995) proposes a new parsing approach based on probabilistic decision tree models (Quinlan and Rivest, 1989; Yamanishi, 1992a) to replace conventional context free parsing. His method uses decision tree models to construct parse trees in a bottom-up and left-to-right fashion. A decision might be made, for example, to create a new parse-tree-node, and conditions for making that decision might be, for example, the appearances of certain words and certain tags in a node currently being focussed upon and in its neighbor nodes. Magerman has also devised an eﬃcient algorithm for ﬁnding the parse tree (interpretation) with the highest likelihood value. The advantages of this method are its eﬀective use of contextual information and its non-use of a hand-made grammar. (See also (Magerman and Marcus, 1991; Magerman, 1994; Black et al., 1993; Ratnaparkhi, 1997; Haruno, Shirai, and Ooyama, 1998)) Su and Chang (1988) propose the use of a probabilistic score function for disam- biguation in generalized LR parsing (see also (Su et al., 1989; Chang, Luo, and Su, 1992; Chiang, Lin, and Su, 1995; Wright, 1990; Kita, 1992; Briscoe and Carroll, 1993; 2.6. WORD SENSE DISAMBIGUATION 21 Inui, Sornlertlamvanich, and Tanaka, 1998)). They ﬁrst introduce a conditional prob- ability of a category obtained after a reduction operation and in the context of the reduced categories and of the categories immediately left and right of those reduced categories. The score function, then, is deﬁned as the product of the conditional prob- abilities appearing in the derivation of the interpretation. The advantage of the use of this score function is its context-sensitivity, which can yield more accurate results in disambiguation. Alshawi and Carter (1994) propose for disambiguation purposes the use of a linear combination of various preference functions based on lexical and syntactic knowledge. They have devised a method for training the weights of a linear combination. Specif- ically, they employ the minimization of a squared-error cost function as a learning strategy and employ a hill-climbing algorithm to iteratively adjust weights on the basis of training data. Additionally, some non-probabilistic approaches to structural disambiguation have also been proposed (e.g., (Wilks, 1975; Wermter, 1989; Nagao, 1990; Kurohashi and Nagao, 1994)). 2.6 Word Sense Disambiguation Word sense disambiguation is an issue closely related to the structural disambiguation problem. For example, when analyzing the sentence Time ﬂies like an arrow, we obtain a number of ambiguous interpretations. Resolving the sense ambiguity of the word ﬂy (i.e., determining whether the word indicates an insect or the action of moving through the air), for example, helps resolve the structural ambiguity, and the converse is true as well. There have been many methods proposed to address the word sense disambiguation problem. (A number of tasks in natural language processing, in fact, fall into the category of word sense disambiguation (Yarowsky, 1993). These include homograph disambiguation in speech synthesis, word selection in machine translation, and spelling correction in document processing.) A simple approach to word sense disambiguation is to employ the conditional dis- tribution  a decision model P(DE1,   , En), where random variable D assumes word senses as its values, and random variables Ei(i  1,   , n) represent pieces of evidence for disambiguation. For example, D can be the insect sense or the action sense of the word ﬂy, Ei can be the presence or absence of the word time in the context. Word sense disambiguation, then, can be realized as the process of ﬁnding the sense d whose conditional probability P(de1,   , en) is the largest, where e1,    , en are the values of the random variables E1,    , En in the current context. Since the conditional distribution has a large number of parameters, however, it is 22 CHAPTER 2. RELATED WORK diﬃcult to estimate them. One solution to this diﬃculty is to estimate the conditional probabilities by using Bayes rule and by assuming that the pieces of evidence for disambiguation are mutually independent (Yarowsky, 1992). Speciﬁcally, we select a sense d satisfying: arg maxdD P(de1,    , en)  arg maxdD{ P (d)P (e1,,end) P (e1,,en) },  arg maxdD{P(d)  P(e1,    , end)},  arg maxdD{P(d)  n i1 P(eid)}, Another way of estimating the conditional probability distribution is to represent it in the form of a probabilistic decision list5, as is proposed in (Yarowsky, 1994). Since a decision list is a sequence of IF-THEN type rules, the use of it in disambiguation turns out to utilize only the strongest pieces of evidence. Yarowsky has also devised a heuristic method for eﬃcient learning of a probabilistic decision list. The merits of this method are ease of implementation, eﬃciency in processing, and clarity. Another approach to word sense disambiguation is the use of weighted majority learning (Littlestone, 1988; Littlestone and Warmuth, 1994). Suppose, for the sake of simplicity, that the disambiguation decision is binary, i.e., it can be represented as 1 or 0. We can ﬁrst deﬁne a linear threshold function: n  i1 wi  xi where feature xi(i  1,   , n) takes on 1 and 0 as its values, representing the pres- ence and absence of a piece of evidence, respectively, and wi(i  1,   , n) denotes a non-negative real-valued weight. In disambiguation, if the function exceeds a prede- termined threshold θ, we choose 1, otherwise 0. We can further employ a learning algorithm called winnow that updates the weights in an on-line (or incremental) fash- ion.6 This algorithm has the advantage of being able to handle a large set of features, and at the same time not ordinarily be aﬀected by features that are irrelevant to the disambiguation decision. (See (Golding and Roth, 1996).) For word sense disambiguation methods, see also (Black, 1988; Brown et al., 1991; Guthrie et al., 1991; Gale, Church, and Yarowsky, 1992; McRoy, 1992; Leacock, Towell, and Voorhees, 1993; Yarowsky, 1993; Bruce and Wiebe, 1994; Niwa and Nitta, 1994; Voorhees, Leacock, and Towell, 1995; Yarowsky, 1995; Golding and Schabes, 1996; Ng and Lee, 1996; Fujii et al., 1996; Schutze, 1997; Schutze, 1998). 5A probabilistic decision list (Yamanishi, 1992a) is a kind of conditional distribution and diﬀerent from a deterministic decision list (Rivest, 1987), which is a kind of Boolean function. 6Winnow is similar to the well-known classical perceptron algorithm, but the former uses a mul- tiplicative weight update scheme while the latter uses an additive weight update scheme. Littlestone (1988) has shown that winnow performs much better than perceptron when many attributes are irrelevant. 2.7. INTRODUCTION TO MDL 23 2.7 Introduction to MDL The Minimum Description Length principle is a strategy (criterion) for data com- pression and statistical estimation, proposed by Rissanen (1978; 1983; 1984; 1986; 1989; 1996; 1997). Related strategies were also proposed and studied independently in (Solomonoﬀ, 1964; Wallace and Boulton, 1968; Schwarz, 1978). A number of im- portant properties of MDL have been demonstrated by Barron and Cover (1991) and Yamanishi (1992a). MDL states that, for both data compression and statistical estimation, the best probability model with respect to given data is that which requires the shortest code length in bits for encoding the model itself and the data observed through it.7 In this section, we will consider the basic concept of MDL and, in particular how to calculate description length. Interested readers are referred to (Quinlan and Rivest, 1989; Yamanishi, 1992a; Yamanishi, 1992b; Han and Kobayashi, 1994) for an intro- duction to MDL. 2.7.1 Basics of Information Theory IID process Suppose that a data sequence (or a sequence of symbols) xn  x1x2    xn is independently generated according to a discrete probability distribution P(X), (2.6) where random variable (information source) X takes on values from a set of symbols: {1, 2,   , s}. Such a data generation process is generally referred to as i.i.d (independently and identically distributed). In order to transmit or compress the data sequence, we need to deﬁne a code for encoding the information source X, i.e., to assign to each value of X a codeword, namely a bit string. In order for the decoder to be able to decode a codeword as soon as it comes to the end of that codeword, the code must be one in which no codeword is a preﬁx of any other codeword. Such a code is called a preﬁx code. 7In this thesis, I describe MDL as a criterion for both data compression and statistical estimation. Strictly speaking, however, it is only referred to as the MDL principle when used as a criterion for statistical estimation. 24 CHAPTER 2. RELATED WORK Theorem 1 The suﬃcient and necessary condition for a code to be a preﬁx code is as follows, s  i1 2l(i)  1, where l(i) denotes the code length of the codeword assigned to symbol i. This is known as Krafts inequality. We deﬁne the expected (average) code length of a code for encoding the information source X as L(X)  s  i1 P(i)  l(i). Moreover, we deﬁne the entropy of (the distribution of) X as8 H(X)   s  i1 P(i)  log P(i). Theorem 2 The expected code length of a preﬁx code for encoding the information source X is greater than or equal to the entropy of X, namely L(X)  H(X). We can deﬁne a preﬁx code in which symbol i is assigned a codeword with code length l(i)   log P(i) (i  1,   , s), according to Theorem 1, since s  i1 2log P (i)  1. Such a code is on average the most eﬃcient preﬁx code, according to Theorem 2. Here- after, we refer to this type of code as a  non-redundant code. (In real communication, a code length must be a truncated integer:  log P(i),9 but we use here  log P(i) for ease of mathematical manipulation. This is not harmful and on average the error due to it is negligible.) When the distribution P(X) is a uniform distribution, i.e., P(i)  1 s (i  1,    , s), the code length for encoding each symbol i turns out to be l(i)   log P(i)   log 1 s  log s (i  1,   , s). 8Throughout this thesis, log denotes logarithm to the base 2. 9x denotes the least integer not less than x. 2.7. INTRODUCTION TO MDL 25 General case We next consider a more general case. We assume that the data sequence xn  x1x2    xn is generated according to a probability distribution P(Xn) where random variable Xi(i  1,    , n) takes on values from {1, 2,   , s}. The data generation process needs neither be i.i.d. nor even stationary (for the deﬁnition of a stationary process, see, for example, (Cover and Thomas, 1991)). Again, our goal is to transmit or compress the data sequence. We deﬁne the expected code length for encoding a sequence of n symbols as L(Xn)   xn P(xn)  l(xn), where P(xn) represents the probability of observing the data sequence xn and l(xn) the code length for encoding xn. We further deﬁne the entropy of (the distribution of) Xn as H(Xn)    xn P(xn) log P(xn). We have the following theorem, widely known as Shannons ﬁrst theorem (cf., (Cover and Thomas, 1991)). Theorem 3 The expected code length of a preﬁx code for encoding a sequence of n symbols Xn is greater than or equal to the entropy of Xn, namely L(Xn)  H(Xn). As in the i.i.d. case, we can deﬁne a non-redundant code in which the code length for encoding the data sequence xn is l(xn)   log P(xn). (2.7) The expected code length of the code for encoding a sequence of n symbols then becomes L(Xn)  H(Xn). (2.8) Here we assume that we know in advance the distribution P(X) (in general P(Xn)). In practice, however, we usually do not know what kind of distribution it is. We have to estimate it by using the same data sequence xn and transmit ﬁrst the estimated model and then the data sequence, which leads us to the notion of two-stage coding. 26 CHAPTER 2. RELATED WORK 2.7.2 Two-stage code and MDL In two-stage coding, we ﬁrst introduce a class of models which includes all of the possible models which can give rise to the data. We then choose a preﬁx code and encode each model in the class. The decoder is informed in advance as to which class has been introduced and which code has been chosen, and thus no matter which model is transmitted, the decoder will be able to identify it. We next calculate the total code length for encoding each model and the data through the model, and select the model with the shortest total code length. In actual transmission, we transmit ﬁrst the selected model and then the data through the model. The decoder then can restore the data perfectly. Model class We ﬁrst introduce a class of models, of which each consists of a discrete model (an expression) and a parameter vector (a number of parameters). When a discrete model is speciﬁed, the number of parameters is also determined. For example, the tree cut models within a thesaurus tree, to be deﬁned in Chapter 4, form a model class. A discrete model in this case corresponds to a cut in the thesaurus tree. The number of free parameters equals the number of nodes in the cut minus one. The class of linear regression models is also an example model class. A discrete model is a0  a1  x1      ak  xk  ǫ, where xi(i  1,   , k) denotes a random variable, ai(i  0, 1,   , k) a parameter, and ǫ a random variable based on the standard normal distribution N(0, 1). The number of parameters in this model equals (k  1). A class of models can be denoted as M  {Pθ(X) : θ  Θ(m), m  M}, where m stands for a discrete model, M a set of discrete models, θ a parameter vector, and Θ(m) a parameter space associated with m. Usually we assume that the model class we introduced contains the true model which has given rise to the data, but it does not matter if it does not. In such case, the best model selected from the class can be considered an approximation of the true model. The model class we introduce reﬂects our prior knowledge on the problem. Total description length We next consider how to calculate total description length. Total description length equals the sum total of the code length for encoding a dis- crete model (model description length l(m)), the code length for encoding parameters given the discrete model (parameter description length l(θm)), and the code length 2.7. INTRODUCTION TO MDL 27 for encoding the data given the discrete model and the parameters (data description length l(xnm, θ)). Note that we also sometimes refer to the model description length as l(m)  l(θm). Our goal is to ﬁnd the minimum description length of the data (in number of bits) with respect to the model class, namely, Lmin(xn : M)  min mM min θΘ(m)  l(m)  l(θm)  l(xnm, θ)  . Model description length Let us ﬁrst consider how to calculate model description length l(m). The choice of a code for encoding discrete models is subjective; it depends on our prior knowledge on the model class. If the set of discrete models M is ﬁnite and the probability distribution over it is a uniform distribution, i.e., P(m)  1 M, m  M, then we need l(m)  log M to encode each discrete model m using a non-redundant code. If M is a countable set, i.e., each of its members can be assigned a positive integer, then the Elias code, which is usually used for encoding integers, can be employed (Rissanen, 1989). Letting i be the integer assigned to a discrete model m, we need l(m)  log c  log i  log log i     to encode m. Here the sum includes all the positive iterates and c denotes a constant of about 2.865. Parameter description length and data description length When a discrete model m is ﬁxed, a parameter space will be uniquely determined. The model class turns out to be Mm  {Pθ(X) : θ  Θ}, where θ denotes a parameter vector, and Θ the parameter space. Suppose that the dimension of the parameter space is k, then θ is a vector with k real-valued components: θ  (θ1,    , θk)T, where XT denotes a transpose of X. 28 CHAPTER 2. RELATED WORK We next consider a way of calculating the sum of the parameter description length and the data description length through its minimization: min θΘ(l(θm)  l(xnm, θ)). Since the parameter space Θ is usually a subspace of the k-dimensional Euclidean space and has an inﬁnite number of points (parameter vectors), straightforwardly en- coding each point in the space takes the code length to inﬁnity, and thus is intractable. (Recall the fact that before transmitting an element in a set, we need encode each element in the set.) One possible way to deal with this diﬃculty is to discretize the parameter space; the process can be deﬁned as a mapping from the parameter space to a discretized space, depending on the data size n: n : Θ  Θn. A discretized parameter space consists of a ﬁnite number of elements (cells). We can designate one point in each cell as its representative and use only the representatives for encoding parameters. The minimization then turns out to be min θΘn(l(θm)  l(xnm, θ)), where l(θm) denotes the code length for encoding a representative θ and l(xnm, θ) denotes the code length for encoding the data xn through that representative. A simple way of conducting discretization is to deﬁne a cell as a micro k-dimensional rectangular solid having length δi on the axis of θi. If the volume of the parameter space is V , then we have V(δ1    δk) number of cells. If the distribution over the cells is uniform, then we need l(θm)  log V δ1   δk to encode each representative θ using a non-redundant code. On the other hand, since the number of parameters is ﬁxed and the data is given, we can estimate the parameters by employing Maximum Likelihood Estimation (MLE), obtaining ˆθ  (ˆθ1,    , ˆθk)T. We may expect that the representative of the cell into which the maximum likelihood estimate falls is the nearest to the true parameter vector among all representatives. And thus, instead of conducting minimization over all representatives, we need only consider minimization with respect to the representative of the cell which the max- imum likelihood estimate belongs to. This representative is denoted here as θ. We approximate the diﬀerence between ˆθ and θ as θ  ˆθ  δ δ  (δ1,   , δk)T. 2.7. INTRODUCTION TO MDL 29 Data description length using θ then becomes l(xnm, θ)   log Pθ(xn). Now, we need consider only minδ(l(xnm, θ)  l(θm))  minδ  log V δ1δk  log Pθ(xn)  . There is a trade-oﬀ relationship between the ﬁrst term and the second term. If δ is large, then the ﬁrst term will be small, while on the other hand the second term will be large, and vice-versa. That means that if we discretize the parameter space loosely, we will need less code length for encoding the parameters, but more code length for encoding the data. On the other hand, if we discretize the parameter space precisely, then we need less code length for encoding the data, but more code length for encoding the parameters. In this way of calculation (see Appendix A.1 for a derivation), we have l(θm)  l(xnm, θ)   log Pˆθ(xn)  k 2  log n  O(1), (2.9) where O(1) indicates limn O(1)  c, a constant. The ﬁrst term corresponds to the data description length and has the same form as that in (2.7). The second term corresponds to the parameter description length. An intuitive explanation of it is that the standard deviation of the maximum likelihood estimator of one of the parameters is of order O( 1 n),10 and hence encoding the parameters using more than k  ( log 1 n)  k 2  log n bits would be wasteful for the given data size. In this way, the sum of the two kinds of description length l(θm)  l(xnm, θ) is obtained for a ﬁxed discrete model m (and a ﬁxed dimension k). For a diﬀerent m, the sum can also be calculated. Selecting a model with minimum total description length Finally, the minimum total description length becomes, for example, Lmin(xn : M)  min m   log Pˆθ(xn)  k 2  log n  log M  . We select the model with the minimum total description length for transmission (data compression). 10It is well known that under certain suitable conditions, when the data size increases, the distri- bution of the maximum likelihood estimator ˆθ will asymptotically become the normal distribution N(θ, 1 nI ) where θ denotes the true parameter vector, I the Fisher information matrix, and n the data size (Fisher, 1956). 30 CHAPTER 2. RELATED WORK 2.7.3 MDL as data compression criterion Rissanen has proved that MDL is an optimal criterion for data compression. Theorem 4 (Rissanen, 1984) Under certain suitable conditions, the expected code length of the two-stage code described above (with code length (2.9) for encoding the data sequence xn) satisﬁes L(Xn)  H(Xn)  k 2  log n  O(1), where H(Xn) denotes the entropy of Xn. This theorem indicates that when we do not know the true distribution P(Xn) in communication, we have to waste on average about k 2 log n bits of code length (cf., (2.8)). Theorem 5 (Rissanen, 1984) Under certain suitable conditions, for any preﬁx code, for some ǫn  0 such that limn ǫn  0 and Ωn  Θ such that the volume of it vol(Ωn) satisﬁes limn vol(Ωn)  0, for any model with parameter θ  (Θ  Ωn), the expected code length L(Xn) is bounded from below by L(Xn)  H(Xn)  k 2  ǫn   log n. This theorem indicates that in general, i.e., excluding some special cases, we cannot make the average code length of a preﬁx code more eﬃcient than the quantity H(Xn) k 2  log n. The introduction of Ωn eliminates the case in which we happen to select the true model and achieve on average a very short code length: H(Xn). Theorem 5 can be considered as an extension of Shannons Theorem (Theorem 3). Theorems 4 and 5 suggest that using the two-stage code above is nearly optimal in terms of expected code length. We can, therefore, say that encoding a data sequence xn in the way described in (2.9) is the most eﬃcient approach not only to encoding the data sequence, but also, on average, to encoding a sequence of n symbols. 2.7.4 MDL as estimation criterion The MDL principle stipulates that selecting a model having the minimum descrip- tion length is also optimal for conducting statistical estimation that includes model selection. Deﬁnition of MDL The MDL principle can be described more formally as follows (Rissanen, 1989; Barron and Cover, 1991). For a data sequence xn and for a model class M  {Pθ(X) : θ  2.7. INTRODUCTION TO MDL 31 Θ(m), m  M}, the minimum description length of the data with respect to the class is deﬁned as Lmin(xn : M)  min mM inf n min θΘn   log Pθ(xn)  l(θm)  l(m)  , (2.10) where n : Θ(m)  Θn denotes a discretization of Θ(m) and where l(θm) is the code length for encoding θ  Θn, satisfying  θΘn 2l(θm)  1. Note that infn stead of minn is used here because there are an inﬁnite number of points which can serve as a representative for a cell. Furthermore, l(m) is the code length for encoding m  M, satisfying  mM 2l(m)  1. For both data compression and statistical estimation, the best probability model with respect to the given data is that which achieves the minimum description length given in (2.10). The minimum description length deﬁned in (2.10) is also referred to as the stochas- tic complexity of the data relative to the model class. Advantages MDL oﬀers many advantages as a criterion for statistical estimation, the most impor- tant perhaps being its optimal convergency rate. Consistency The models estimated by MDL converge with probability one to the true model when data size increases  a property referred to as consistency (Barron and Cover, 1991). That means that not only the parameters themselves but also the number of them converge to those of the true model. Rate of convergence Consistency, however, is a characteristic to be considered only when data size is large; in practice, when data size can generally be expected to be small, rate of convergence is a more important guide to the performance of an estimator. Barron and Cover (1991) have veriﬁed that MDL as an estimation strategy is near optimal in terms of the rate of convergence of its estimated models to the true model as the data size increases. When the true model is included in the class of models considered, the models selected by MDL converge in probability to the true model at 32 CHAPTER 2. RELATED WORK the rate of O( klog n 2n ), where k is the number of parameters in the true model, and n the data size. This is nearly optimal. Yamanishi (1992a) has derived an upper bound on the data size necessary for learn- ing probably approximately correctly (PAC) a model from among a class of conditional distributions, which he calls stochastic rules with ﬁnite partitioning. This upper bound is of order O( k ǫ log k ǫ  l(m) ǫ ), where k denotes the number of parameters of the true model, and ǫ(0  ǫ  1) the accuracy parameter for the stochastic PAC learning. For MLE, the corresponding upper bound is of order O( kmax ǫ log kmax ǫ  l(m) ǫ ), where kmax denotes the maximum of the number of parameters of a model in the model class. These upper bounds indicate that MDL requires less data than MLE to achieve the same accuracy in statistical learning, provided that kmax  k (note that, in general, kmax  k). MDL and MLE When the number of parameters in a probability model is ﬁxed, and the estimation problem involves only the estimation of parameters, MLE is known to be satisfactory (Fisher, 1956). Furthermore, for such a ﬁxed model, it is known that MLE is equivalent to MDL: given the data xn  x1   xn, the maximum likelihood estimator ˆθ is deﬁned as one that maximizes likelihood with respect to the data, that is, ˆθ  arg max θ P(xn). (2.11) It is easy to see that ˆθ also satisﬁes ˆθ  arg min θ  log P(xn). This is, in fact, no more than the MDL estimator in this case, since  log Pˆθ(xn) is the data description length. When the estimation problem involves model selection, MDLs behavior signiﬁ- cantly deviates from that of MLE. This is because MDL insists on minimizing the sum total of the data description length and the model description length, while MLE is still equivalent to minimizing the data description length alone. We can, therefore, say that MLE is a special case of MDL. Note that in (2.9), the ﬁrst term is of order O(n) and the second term is of order O(log n), and thus the ﬁrst term will dominate that formula when the data size in- creases. That means that when data size is suﬃciently large, the MDL estimate will turn out to be the MLE estimate; otherwise the MDL estimate will be diﬀerent from the MLE estimate. MDL and Bayesian Estimation In an interpretation of MDL from the viewpoint of Bayesian Estimation, MDL is essentially equivalent to the MAP estimation in Bayesian terminology. Given data 2.7. INTRODUCTION TO MDL 33 D and a number of models, the Bayesian (MAP) estimator ˆ M is deﬁned as one that maximizes posterior probability, i.e., ˆ M  arg maxM(P(MD))  arg maxM( P (M)P (DM) P (D) )  arg maxM(P(M)  P(DM)), (2.12) where P(M) denotes the prior probability of model M and P(DM) the probability of observing data D through M. In the same way, ˆ M satisﬁes ˆ M  arg min M ( log P(M)  log P(DM)). This is equivalent to the MDL estimator if we take  log P(M) to be the model de- scription length. Interpreting  log P(M) as the model description length translates, in Bayesian Estimation, to assigning larger prior probabilities to simpler models, since it is equivalent to assuming that P(M)  ( 1 2)l(M), where l(M) is the code length of model M. (Note that if we assign uniform prior probability to all models, then (2.12) becomes equivalent to (2.11), giving the MLE estimator.) MDL and MEP The use of the Maximum Entropy Principle (MEP) has been proposed in statistical language processing (Ratnaparkhi, Reynar, and Roukos, 1994; Ratnaparkhi, Reynar, and Roukos, 1994; Ratnaparkhi, 1997; Berger, Pietra, and Pietra, 1996; Rosenfeld, 1996)). Like MDL, MEP is also a learning criterion, one which stipulates that from among the class of models that satisﬁes certain constraints, the model which has the maximum entropy should be selected. Selecting a model with maximum entropy is, in fact, equivalent to selecting a model with minimum description length (Rissanen, 1983). Thus, MDL provides an information-theoretic justiﬁcation of MEP. MDL and stochastic complexity The sum of parameter description length and data description length given in (2.9) is still a loose approximation. Recently, Rissanen has derived this more precise formula: l(θm)  l(xnm, θ)   log Pˆθ(xn)  k 2  log n 2π  log   I(θ)dθ  o(1), (2.13) where I(θ) denotes the Fisher information matrix, A the determinant of matrix A, and π the circular constant, and o(1) indicates limn o(1)  0. It is thus preferable to use this formula in practice. This formula can be obtained not only on the basis of the complete two-stage code, but also on that of quantized maximum likelihood code, and has been proposed as the new deﬁnition of stochastic complexity (Rissanen, 1996). (See also (Clarke and Barron, 1990).) 34 CHAPTER 2. RELATED WORK When the data generation process is i.i.d. and the distribution is a discrete proba- bility distribution like that in (2.6), the sum of parameter description length and data description length turns out to be (Rissanen, 1997) l(θ)  l(xnm, θ)   n i1 log Pˆθ(xi)  k 2  log n 2π  log π(k1)2 Γ( (k1) 2 )  o(1), (2.14) where Γ denotes the Gamma function11. This is because in this case, the determinant of the Fisher information matrix becomes 1 k1 i1 P (i), and the integral of its square root can be calculated by the Dirichlets integral as π(k1)2 Γ( (k1) 2 ). 2.7.5 Employing MDL in NLP Recently MDL and related techniques have become popular in natural language pro- cessing and related ﬁelds; a number of learning methods based on MDL have been proposed for various applications (Ellison, 1991; Ellison, 1992; Cartwright and Brent, 1994; Stolcke and Omohundro, 1994; Brent, Murthy, and Lundberg, 1995; Ristad and Thomas, 1995; Brent and Cartwright, 1996; Grunwald, 1996). Coping with the data sparseness problem MDL is a powerful tool for coping with the data sparseness problem, an inherent diﬃculty in statistical language processing. In general, a complicated model might be suitable for representing a problem, but it might be diﬃcult to learn due to the sparseness of training data. On the other hand, a simple model might be easy to learn, but it might be not rich enough for representing the problem. One possible way to cope with this diﬃculty is to introduce a class of models with various complexities and to employ MDL to select the model having the most appropriate level of complexity. An especially desirable property of MDL is that it takes data size into consideration. Classical statistics actually assume implicitly that the data for estimation are always suﬃcient. This, however, is patently untrue in natural language. Thus, the use of MDL might yield more reliable results in many NLP applications. Employing eﬃcient algorithms In practice, the process of ﬁnding the optimal model in terms of MDL is very likely to be intractable because a model class usually contains too many models to calculate a description length for each of them. Thus, when we have modelized a natural language acquisition problem on the basis of a class of probability models and want to employ MDL to select the best model, what is necessary to consider next is how to perform the task eﬃciently, in other words, how to develop an eﬃcient algorithm. 11Eulers Gamma function is deﬁned as Γ(x)    0 tx1  etdt. 2.7. INTRODUCTION TO MDL 35 When the model class under consideration is restricted to one related to a tree structure, for instance, the dynamic programming technique is often applicable and the optimal model can be eﬃciently found. Rissanen (1997), for example, has devised such an algorithm for learning a decision tree. Another approach is to calculate approximately the description lengths for the probability models, by using a computational-statistic technique, e.g., the Markov chain Monte-Carlo method, as is proposed in (Yamanishi, 1996). In this thesis, I take the approach of restricting a model class to a simpler one (i.e., reducing the number of models to consider) when doing so is still reasonable for tackling the problem at hand. 36 CHAPTER 2. RELATED WORK Chapter 3 Models for Lexical Knowledge Acquisition The world as we know it is our interpretation of the observable facts in the light of theories that we ourselves invent. - Immanuel Kant (paraphrase) In this chapter, I deﬁne probability models for each subproblem of the lexical se- mantic knowledge acquisition problem: (1) the hard case slot model and the soft case slot model; (2) the word-based case frame model, the class-based case frame model, and the slot-based case frame model; and (3) the hard co-occurrence model and the soft co-occurrence model. These are respectively the probability models for (1) case slot generalization, (2) case dependency learning, and (3) word clustering. 3.1 Case Slot Model Hard case slot model We can assume that case slot data for a case slot for a verb like that shown in Ta- ble 2.2 are generated according to a conditional probability distribution, which speciﬁes the conditional probability of a noun given the verb and the case slot. I call such a distribution a case slot model. When the conditional probability of a noun is deﬁned as that of the noun class to which the noun belongs, divided by the size of the noun class, I call the case slot model a hard-clustering-based case slot model, or simply a hard case slot model. Suppose that N is the set of nouns, V is the set of verbs, and R is the set of slot names. A partition Π of N is deﬁned as a set satisfying Π  2N,1 CΠC  N and 12A denotes the power set of a set A; if, for example, A  {a, b}, then 2A  {{}, {a}, {b}, {a, b}}. 37 38 CHAPTER 3. MODELS FOR LEXICAL KNOWLEDGE ACQUISITION Ci, Cj  Π, Ci  Cj  , (i  j). An element C in Π is referred to as a class. A hard case slot model with respect to a partition Π is deﬁned as a conditional probability distribution: P(nv, r)  1 C  P(Cv, r) n  C, (3.1) where random variable n assumes a value from N , random variable v from V, and random variable r from R, and where C  Γ is satisﬁed. 2 We can formalize the case slot generalization problem as that of estimating a hard case slot model. The problem, then, turns out to be that of selecting a model, from a class of hard case slot models, which is most likely to have given rise to the case slot data. This formalization of case slot generalization will make it possible to deal with the data sparseness problem, an inherent diﬃculty in a statistical approach to natural lan- guage processing. Since many words in natural language are synonymous, it is natural to classify them into the same word class and employ class-based probability models. A class-based model usually has far fewer parameters than a word-based model, and thus the use of it can help handle the data sparseness problem. An important characteristic of the approach taken here is that it automatically conducts the optimization of word clustering by means of statistical model selection. That is to say, neither the number of word classes nor the way of word classiﬁcation are determined in advance, but are determined automatically on the basis of the input data. The uniform distribution assumption in the hard case slot model seems to be nec- essary for dealing with the data sparseness problem. If we were to assume that the distribution of words (nouns) within a class is a word-based distribution, then the number of parameters would not be reduced and the data sparseness problem would still prevail. Under the uniform distribution assumption, generalization turns out to be the pro- cess of ﬁnding the best conﬁguration of classes such that the words in each class are equally likely to be the value of the slot in question. (Words belonging to a single word class should be similar in terms of likelihood; they do not necessarily have to be syn- onyms.) Conversely, if we take the generalization to be such a process, then viewing it as statistical estimation of a hard case slot model seems to be quite appropriate, because the class of hard case slot models contains all of the possible models for the purposes of generalization. The word-based case slot model (i.e., one in which each word forms its own word class) is a (discrete) hard case slot model, and any grouping of words (nouns) leads to one (discrete) hard case slot model. 2Rigorously, a hard case slot model with respect to a noun partition Π should be represented as PΠ(nv, r)   CΠ P(nC)  P(Cv, r) P(nC)   1 C n  C 0 otherwise. 3.1. CASE SLOT MODEL 39 Soft case slot model Note that in the hard case slot model a word (noun) is assumed to belong to a single class. In practice, however, many words have sense ambiguities and a word can belong to several diﬀerent classes, e.g., bird is a member of both animal and meat. It is also possible to extend the hard case slot model so that each word probabilistically belongs to several diﬀerent classes, which would allow us to resolve both syntactic and word sense ambiguities at the same time. Such a model can be deﬁned in the form of a ﬁnite mixture model, which is a linear combination of the word probability distribu- tions within individual word (noun) classes. I call such a model a soft-clustering-based case slot model, or simply a soft case slot model. First, a covering Γ of the noun set N is deﬁned as a set satisfying Γ  2N, CΓC  N . An element C in Γ is referred to as a class. A soft case slot model with respect to a covering Γ is deﬁned as a conditional probability distribution: P(nv, r)   CΓ P(Cv, r)  P(nC) (3.2) where random variable n denotes a noun, random variable v a verb, and random variable r a slot name. We can also formalize the case slot generalization problem as that of estimating a soft case slot model. If we assume, in a soft case slot model, that a word can only belong to a single class alone and that the distribution within a class is a uniform distribution, then the soft case slot model will become a hard case slot model. Numbers of parameters Table 3.1 shows the numbers of parameters in a word-based case slot model (2.1), a hard case slot model (3.1), and a soft case slot model (3.2). Here N denotes the size of the set of nouns, Π the partition in the hard case slot model, and Γ the covering in the soft case slot model. Table 3.1: Numbers of parameters in case slot models. word-based model O(N) hard case slot model O(Π) soft case slot model O(Γ   CΓ C) The number of parameters in a hard case slot models is generally smaller than that in a soft case slot model. Furthermore, the number of parameters in a soft case slot model is generally smaller than that in a word-based case slot model (note that the parameters P(nC) is common to each soft case slot model). As a result, hard case slot models require less data for parameter estimation than soft case slot models, and 40 CHAPTER 3. MODELS FOR LEXICAL KNOWLEDGE ACQUISITION soft case slot models less data than word-based case slot models. That is to say, hard and soft case slot models are more useful than word-based models, given the fact that usually the size of data for training is small. Unfortunately, currently available data sizes are still insuﬃcient for the accurate estimating of a soft case slot model. (Appendix A.2 shows a method for learning a soft case slot model.) (See (Li and Yamanishi, 1997) for a method of using a ﬁnite mixture model in document classiﬁcation, for which more data are generally available.) In this thesis, I address only the issue of estimating a hard case slot model. With regard to the word-sense ambiguity problem, one can employ an existing word-sense disambiguation technique (cf., Chapter2) in pre-processing, and use the disambiguated word senses as virtual words in the subsequent learning process. 3.2 Case Frame Model We can assume that case frame data like that in Table 2.1 are generated according to a multi-dimensional discrete joint probability distribution in which random variables represent case slots. I call such a distribution a case frame model. We can formalize the case dependency learning problem as that of estimating a case frame model. The dependencies between case slots are represented as probabilistic dependencies between random variables. (Recall that random variables X1,    , Xn are mutually independent, if for any k  n, and any 1  i1      ik  n, P(Xi1,    , Xik)  P(Xi1)   P(Xik); otherwise, they are mutually dependent.) The case frame model is the joint probability distribution of type, PY (X1, X2,   , Xn), where index Y stands for the verb, and each of the random variables Xi, i  1, 2,   , n, represents a case slot. In this thesis, case slots refers to surface case slots, but they can also be deep case slots. Furthermore, obligatory cases and optional cases are uniformly treated. The possible case slots can vary from verb to verb. They can also be a predetermined set for all of the verbs, with most of the slots corresponding to (English) prepositions. The case frame model can be further classiﬁed into three types of probability models according to the type of value each random variable Xi assumes. When Xi assumes a word or a special symbol 0 as its value, the corresponding model is referred to as a word-based case frame model. Here 0 indicates the absence of the case slot in question. When Xi assumes a word-class (such as person or company) or 0 as its value, the corresponding model is referred to as a class-based case frame model. When Xi takes on 1 or 0 as its value, the model is called a slot-based case frame model. Here 1 indicates the presence of the case slot in question, and 0 the absence of it. For example, the data in Table 3.2 could have been generated by a word-based model, the data in Table 3.3 by a class-based model, where    denotes a word class, and the 3.2. CASE FRAME MODEL 41 Table 3.2: Example case frame data generated by a word-based model. Case frame Frequency (ﬂy (arg1 girl)(arg2 jet)) 2 (ﬂy (arg1 boy)(arg2 helicopter)) 1 (ﬂy (arg1 company)(arg2 jet)) 2 (ﬂy (arg1 girl)(arg2 company)) 1 (ﬂy (arg1 boy)(to Tokyo)) 1 (ﬂy (arg1 girl)(from Tokyo) (to New York)) 1 (ﬂy (arg1 JAL)(from Tokyo) (to Bejing)) 1 Table 3.3: Example case frame data generated by a class-based model. Case frame Frequency (ﬂy (arg1 person)(arg2 airplane)) 3 (ﬂy (arg1 company)(arg2 airplane)) 2 (ﬂy (arg1 person)(arg2 company)) 1 (ﬂy (arg1 person)(to place)) 1 (ﬂy (arg1 person)(from place)(to place)) 1 (ﬂy (arg1 company)(from place)(to place)) 1 data in Table 3.4 by a slot-based model. Suppose, for simplicity, that there are only 4 possible case slots corresponding, respectively, to subject, direct object, from phrase, and to phrase. Then, Pﬂy(Xarg1  girl, Xarg2  jet, Xfrom  0, Xto  0) is speciﬁed by a word-based case frame model. In contrast, Pﬂy(Xarg1  person, Xarg2  airplane, Xfrom  0, Xto  0) is speciﬁed by a class-based case frame model, where person and airplane denote Table 3.4: Example case frame data generated by a slot-based model. Case frame Frequency (ﬂy (arg1 1)(arg2 1)) 6 (ﬂy (arg1 1)(to 1)) 1 (ﬂy (arg1 1)(from 1)(to 1)) 2 42 CHAPTER 3. MODELS FOR LEXICAL KNOWLEDGE ACQUISITION word classes. Finally, Pﬂy(Xarg1  1, Xarg2  1, Xfrom  0, Xto  0) is speciﬁed by a slot-based case frame model. One can also deﬁne a combined model in which, for example, some random variables assume word classes and 0 as their values while others assume 1 and 0. Note that since in general Pﬂy(Xarg1  1, Xarg2  1, Xfrom  0, Xto  0)  Pﬂy(Xarg1  1, Xarg2  1), one should not use here the joint probability Pﬂy(Xarg1  1, Xarg2  1) as the proba- bility of the case frame (ﬂy (arg1 1)(arg2 1)). In learning and using of the case frame models, it is also assumed that word sense ambiguities have been resolved in pre-processing. One may argue that when the ambiguities of a verb are resolved, there would not exist case dependencies at all (cf., ﬂy in sentences of (1.2)). Sense ambiguities, however, are generally diﬃcult to deﬁne precisely. I think that it is preferable not to resolve them until doing so is necessary in a particular application. That is to say, I think that, in general, case dependencies do exist and the development of a method for learning them is needed. Numbers of parameters Table 3.5 shows the numbers of parameters in a word-based case frame model, a class- based case frame model, and a slot-based case frame model, where n denotes the number of random variables, N the size of the set of nouns, and kmax the maximum number of classes in any slot. Table 3.5: Numbers of parameters in case frame models. word-based case frame model O(Nn) class-based case frame model O(kn max) slot-based case frame model O(2n) 3.3 Co-occurrence Model Hard co-occurrence model We can assume that co-occurrence data over a set of nouns and a set of verbs like that in Figure 2.3 are generated according to a joint probability distribution that speciﬁes 3.3. CO-OCCURRENCE MODEL 43 the co-occurrence probabilities of noun verb pairs. I call such a distribution a co- occurrence model. I call the co-occurrence model a hard-clustering-based co-occurrence model, or simply a hard co-occurrence model, when the joint probability of a noun verb pair can be deﬁned as the product of the joint probability of the noun class and the verb class to which the noun and the verb respectively belong, the conditional probability of the noun given its noun class, and the conditional probability of the verb given its verb class. Suppose that N is the set of nouns, and V is the set of verbs. A partition Πn of N is deﬁned as a set which satisﬁes Πn  2N, CnΠnCn  N and Ci, Cj  Πn, CiCj  , (i  j). A partition Πv of V is deﬁned as a set which satisﬁes Πv  2V, CvΠvCv  V and Ci, Cj  Πv, Ci  Cj  , (i  j). Each element in a partition forms a class of words. I deﬁne a hard co-occurrence model with respect to a noun partition Πn and a verb partition Πv as a joint probability distribution of type: P(n, v)  P(Cn, Cv)  P(nCn)  P(vCv) n  Cn, v  Cv, (3.3) where random variable n denotes a noun and random variable v a verb and where Cn  Πn and Cv  Πv are satisﬁed. 3 Figure 3.1 shows a hard co-occurrence model, one that can give rise to the co-occurrence data in Figure 2.3. Estimating a hard co-occurrence model means selecting, from the class of such models, one that is most likely to have given rise to the co-occurrence data. The selected model will contain a hard clustering of words. We can therefore formalize the problem of word clustering as that of estimating a hard co-occurrence model. We can restrict the hard co-occurrence model by assuming that words within a same class are generated with an equal probability (Li and Abe, 1996; Li and Abe, 1997), obtaining P(n, v)  P(Cn, Cv)  1 Cn  1 Cv n  Cn, v  Cv. Employing such a restricted model in word clustering, however, has an undesirable tendency to result in classifying into diﬀerent classes those words that have similar co-occurrence patterns but have diﬀerent absolute frequencies. 3 Rigorously, a hard co-occurrence model with respect to a noun partition Πn and a verb partition Πv should be represented as PΠnΠv(n, v)   CnΠn,CvΠv P(Cn, Cv)  P(nCn)  P(vCv) P(xCx)   Q(xCx) x  Cx 0 otherwise (x  n, v) Cx,  xCx Q(xCx)  1, (x  n, v). 44 CHAPTER 3. MODELS FOR LEXICAL KNOWLEDGE ACQUISITION make eat drink wine beer bread rice 0.4 0.4 0.1 0.1 1 0.5 0.5 0.4 0.6 0.6 0.4 P(vCv) P(nCn) P(Cn,Cv) Figure 3.1: An example hard co-occurrence model. The hard co-occurrence model in (3.3) can also be considered an extension of that proposed in (Brown et al., 1992). First, dividing the equation by P(v), we obtain P(n, v) P(v)  P(CnCv)  P(nCn)  P(Cv)  P(vCv) P(v)  n  Cn, v  Cv. Since P (Cv)P (vCv) P (v)  1 holds, we have P(nv)  P(CnCv)  P(nCn) n  Cn, v  Cv. We can rewrite the model for word sequence predication as P(w2w1)  P(C2C1)  P(w2C2) w1  C1, w2  C2, (3.4) where random variables w1 and w2 take on words as their values. In this way, the hard co-occurrence model turns out to be a bigram class model and is similar to that proposed in (Brown et al., 1992) (cf., Chapter 2).4 The diﬀerence is that the model in (3.4) assumes that the conﬁguration of word groups for C2 and the conﬁguration of word groups for C1 can be diﬀerent, while Brown et als model assumes that the conﬁgurations for the two are always the same. 4Strictly speaking, the bigram class model proposed by (Brown et al., 1992) and the hard case slot model deﬁned here are diﬀerent types of probability models; the former is a conditional distribution, while the latter is a joint distribution. 3.3. CO-OCCURRENCE MODEL 45 Soft co-occurrence model The co-occurrence model can also be deﬁned as a double mixture model, which is a double linear combination of the word probability distributions within individual noun classes and those within individual verb classes. I call such a model a soft-clustering- based co-occurrence model, or simply soft co-occurrence model. First, a covering Γn of the noun set N is deﬁned as a set which satisﬁes Γn  2N, CnΓnCn  N . A covering Γv of the verb set V is deﬁned as a set which satisﬁes Γv  2V, CvΓvCv  V. Each element in a covering is referred to as a class. I deﬁne a soft co-occurrence model with respect to a noun covering Γn and a verb covering Γv as a joint probability distribution of type: P(n, v)   CnΓn  CvΓv P(Cn, Cv)  P(nCn)  P(vCv), where random variable n denotes a noun and random variable v a verb. Obviously, the soft co-occurrence model includes the hard co-occurrence model as a special case. If we assume that a verb class consists of a single verb alone, i.e., Γv  {{v}v  V}, then the soft co-occurrence model turns out to be P(n, v)   CnΓn P(Cn, v)  P(nCn), which is equivalent to that proposed in (Pereira, Tishby, and Lee, 1993). Estimating a soft co-occurrence model, thus, means selecting, from the class of such models, one that is most likely to have given rise to the co-occurrence data. The selected model will contain a soft clustering of words. We can formalize the word clustering problem as that of estimating a soft co-occurrence model. Numbers of parameters Table 3.6 shows the numbers of parameters in a hard co-occurrence model and in a soft co-occurrence model. Here N denotes the size of the set of nouns, V the size of the set of verbs, Πn and Πv are the partitions in the hard co-occurrence model, and Γn and Γv are the coverings in the soft co-occurrence model. Table 3.6: Numbers of parameters in co-occurrence models. hard co-occurrence model O(Πn  Πv  V  N) soft co-occurrence model O(Γn  Γv   CnΓn Cn   CvΓv Cv) In this thesis, I address only the issue of estimating a hard co-occurrence model. 46 CHAPTER 3. MODELS FOR LEXICAL KNOWLEDGE ACQUISITION 3.4 Relations between Models Table 3.7 summarizes the formalization I have made above. Table 3.7: Summary of the formalization. Input Output Side eﬀect case slot data hardsoft case slot model case slot generalization case frame data wordclassslot-based case frame model case dependency learning co-occurrence data hardsoft co-occurrence model word clustering The models described above are closely related. The soft case slot model includes the hard case slot model, and the soft co-occurrence model includes the hard co- occurrence model. The slot-based case frame model will become the class-based case frame model when we granulate slot-based case slot values into class-based slot values. The class-based case frame model will become the word-based case frame model when we perform further granulation. The relation between the hard case slot model and the case frame models, that between the hard case slot model and the hard co-occurrence model, and that between the soft case slot model and the soft co-occurrence model are described below. Hard case slot model and case frame models The relationship between the hard case slot model and the case frame models may be expressed by transforming the notation of the conditional probability P(Cv, r) in the hard case slot model to P(Cv, r)  Pv(Xr  CXr  1)  Pv(Xr  C) Pv(Xr  1) , (3.5) which is the ratio between a marginal probability in the class-based case frame model and a marginal probability in the slot-based case frame model. This relation (3.5) implies that we can generalize case slots by using the hard case slot model and then acquire class-based case frame patterns by using the class-based case frame model. Hard case slot model and hard co-occurrence model If we assume that the verb set consists of a single verb alone, then the hard co- occurrence model with respect to slot r becomes Pr(n, v)  Pr(Cn, v)  Pr(nCn) n  Cn. 3.4. RELATIONS BETWEEN MODELS 47 Slot-based case frame model Class-based case frame model Word-based case frame model Granulation Hard case slot model Soft case slot model Hard clustering model Soft clustering model Inclusion Inclusion Granulation (3.5) (3.6) (3.7) Figure 3.2: Relations between models. If we further assume that nouns within a same noun class have an equal probability, then we have Pr(n, v) Pr(v)  Pr(Cnv)  1 Cn n  Cn. (3.6) This is no more than the hard case slot model, which has a diﬀerent notation. Soft case slot model and soft co-occurrence model If we assume that the verb set consists of a single verb alone, then the soft co-occurrence model with respect to slot r becomes Pr(n, v)   CnΓn Pr(Cn, v)  Pr(nCn). Suppose that Pr(nCn) is common to each slot r, then we can denote it as P(nCn) and have Pr(n, v) Pr(v)   CnΓn Pr(Cnv)  P(nCn). (3.7) This is equivalent to the soft case slot model. 48 CHAPTER 3. MODELS FOR LEXICAL KNOWLEDGE ACQUISITION 3.5 Discussions Generation models v.s. decision models The models deﬁned above are what I call generation models. A case frame generation model is a probability distribution that gives rise to a case frame with a certain prob- ability. In disambiguation, a generation model predicts the likelihood of the occurrence of each case frame. Alternatively, we can deﬁne what I call decision models to perform the disam- biguation task. A decision model is a conditional distribution which represents the conditional probabilities of disambiguation (or parsing) decisions. For instance, the decision tree model and the decision list model are example decision models (cf., Chap- ter 2). In disambiguation, a decision model predicts the likelihood of the correctness of each decision. A generation model can generally be represented as a joint distribution P(X) (or a conditional distribution P(X.)), where random variables X denote linguistic (syn- tactical andor lexical) features. A decision model can generally be represented by a conditional distribution P(Y X) where random variables X denote linguistic features and random variable Y denotes usually a small number of decisions. Estimating a generation model requires merely positive examples. On the other hand, estimating a decision model requires both positive and negative examples. A case frame generation model can be used for purposes other than structural disambiguation. A decision model, on the other hand, is deﬁned speciﬁcally for the purpose of disambiguation. In this thesis, I investigate generation models because of their important generality. The case slot models are, in fact, one-dimensional lexical generation models, the co-occurrence models are two-dimensional lexical generation models, and the case frame models are multi-dimensional lexical generation models. Note that the case frame models are not simply straightforward extensions of the case slot models and the co-occurrence models; one can easily deﬁne diﬀerent multi-dimensional models as extensions of the case slot models and the co-occurrence models (from one or two dimensions to multi-dimensions). Linguistic models The models I have so far deﬁned can also be considered to be linguistic models in the sense that they straightforwardly represent case frame patterns (or selectional patterns, subcategorization patterns) proposed in the linguistic theories of (Fillmore, 1968; Katz and Fodor, 1963; Pollard and Sag, 1987). In other words, they are generally intelligible to humans, because they contain descriptions of language usage. 3.6. DISAMBIGUATION METHODS 49 Probability distributions v.s. probabilistic measures An alternative to deﬁning probability distributions for lexical knowledge acquisition, and consequently for disambiguation, is to deﬁne probabilistic measures (e.g., the as- sociation ratio, the selectional association measure). Calculating these measures in a theoretically sound way can be diﬃcult, however, and needs further investigation. The methods commonly employed to calculate the association ratio measure (cf., Chapter 2) are based on heuristics. For example, it is calculated as ˆS(nv, r)  log ˆP(nv, r) ˆP(n) , where ˆP(nv, r) and ˆP(n) denote, respectively, the Laplace estimates of the probabil- ities P(nv, r) and P(n). Here, each of the two estimates can only be calculated with a certain degree of precision which depends on the size of training data. Any small inaccuracies in the two may be greatly magniﬁed when they are calculated as a ratio, and this will lead to an extremely unreliable estimate of S(nv, r) (note that associ- ation ratio is an unbounded measure). Since training data is always insuﬃcient, this phenomenon may occur very frequently. Unfortunately, a theoretically sound method of calculation has yet to be developed. Similarly, a theoretically sound method for calculating the selectional association measure also has yet to be developed. (See (Abe and Li, 1996) for a heuristic method for learning a similar measure on the basis of the MDL principle.) In this thesis I employ probability distributions rather than probabilistic measures. 3.6 Disambiguation Methods The models proposed above can be independently used for disambiguation purposes, they can also be combined into a single natural language analysis system. In this section, I ﬁrst describe how they can be independently used and then how they can be combined. Using case frame models Suppose for example that in the analysis of the sentence The girl will ﬂy a jet from Tokyo, the following alternative interpretations are obtained. (ﬂy (arg1 girl) (arg2 (jet)) (from Tokyo)) (ﬂy (arg1 girl) (arg2 (jet (from Tokyo)))). 50 CHAPTER 3. MODELS FOR LEXICAL KNOWLEDGE ACQUISITION We wish to select the more appropriate of the two interpretations. Suppose for simplic- ity that there are four possible case slots for the verb ﬂy, and there is only one possible case slot for the noun jet. A disambiguation method based on word-based case frame models would calculate the following likelihood values and select the interpretation with higher likelihood value: Pﬂy(Xarg1  girl, Xarg2  jet, Xfrom  Tokyo, Xto  0)  Pjet(Xfrom  0) and Pﬂy(Xarg1  girl, Xarg2  jet, Xfrom  0, Xto  0)  Pjet(Xfrom  Tokyo). If the former is larger than the latter, we select the former interpretation, otherwise we select the latter interpretation. If we assume here that case slots are independent, then we need only compare Pﬂy(Xfrom  Tokyo)  Pjet(Xfrom  0) and Pﬂy(Xfrom  0)  Pjet(Xfrom  Tokyo). Similarly, when the models are slot-based and the case slots are assumed to be independent, we need only compare Pﬂy(Xfrom  1)  Pjet(Xfrom  0)  Pﬂy(Xfrom  1)   1  Pjet(Xfrom  1)  and Pﬂy(Xfrom  0)  Pjet(Xfrom  1)   1  Pﬂy(Xfrom  1)   Pjet(Xfrom  1). That is to say, we need noly compare Pﬂy(Xfrom  1) and Pjet(Xfrom  1). The method proposed by Hindle and Rooth (1991) in fact compares the same proba- bilities; they do it by means of statistical hypothesis testing. 3.6. DISAMBIGUATION METHODS 51 Using hard case slot models Another way of conducting disambiguation under the assumption that case slots are independent is to employ the hard case slot model. Speciﬁcally we compare P(Tokyoﬂy, from) and P(Tokyojet, from). If the former is larger than the latter, we select the former interpretation, otherwise we select the latter interpretation. Using hard co-occurrence models We can also use the hard co-occurrence model to perform the disambiguation task, under the assumption that case slots are independent. Speciﬁcally, we compare Pfrom(Tokyoﬂy)  Pfrom(Tokyo, ﬂy)  nN Pfrom(n, ﬂy) and Pfrom(Tokyojet)  Pfrom(Tokyo, jet)  nN Pfrom(n, jet). Here, Pfrom(Tokyo, ﬂy) is calculated on the basis of a hard co-occurrence model over the set of nouns and the set of verbs with respect to the from slot, and Pfrom(Tokyo, jet) on the basis of a hard co-occurrence model over the set of nouns with respect to the from slot. Since the joint probabilities above are all estimated on the basis of class-based models, the conditional probabilities are in fact calculated on the basis of not only the co-occurrences of the related words but also of those of similar words. That means that this disambiguation method is similar to the similarity-based approach (cf., Chapter 2). The diﬀerence is that the method described here is based on a probability model, while the similarity-based approach usually is based on heuristics. A combined method Let us next consider a method based on combination of the above models. We ﬁrst employ the hard co-occurrence model to construct a thesaurus for each case slot (we can, however, construct only thesauruses for which there are enough co- occurrence data with respect to the corresponding case slots). We next employ the hard case slot model to generalize values of case slots into word classes (word classes used in a hard case slot model can be either from a hand-made thesaurus or from an automatically constructed thesaurus; cf., Chapter 4). Finally, we employ the class- based case frame model to learn class-based case frame patterns. 52 CHAPTER 3. MODELS FOR LEXICAL KNOWLEDGE ACQUISITION In disambiguation, we refer to the case frame patterns, calculate likelihood values for the ambiguous case frames, and select the most likely case frame as output. With regard to the above example, we can calculate and compare the following likelihood values: L(1)  Pﬂy(Xarg1  person, Xarg2  airplane, Xfrom  place)  Pjet(Xfrom  0) and L(2)  Pﬂy(Xarg1  person, Xarg2  airplane, Xfrom  0)  Pjet(Xfrom  place), assuming that there are only three case slots: arg1, arg2 and from for the verb ﬂy, and there is one case slot: from for the noun jet. Here    denotes a word class. We make the pp-attachment decision as follows: if L(1)  L(2), we attach the phrase from Tokyo to ﬂy; if L(1)  L(2), we attach it to jet; otherwise we make no decision. Unfortunately, it is still diﬃcult to attain high performance with this method at the current stage of statistical language processing, since the corpus data currently available is far less than that necessary to estimate accurately the class-based case frame models. 3.7 Summary I have proposed the softhard case slot model for case slot generalization, the word- basedclass-basedslot-based case frame model for case dependency learning, and the softhard co-occurrence model for word clustering. In Chapter 4, I will describe a method for learning the hard case slot model, i.e., generalizing case slots; in Chapter 5, a method for learning the case frame model, i.e., learning case dependencies; and in Chapter 6, a method for learning the hard co-occurrence model, i.e., conducting word clustering. In Chapter 7, I will describe a disambiguation method, which is based on the learning methods proposed in Chapters 4 and 6. (See Figure 1.1.) Chapter 4 Case Slot Generalization Make everything as simple as possible - but not simpler. - Albert Einstein In this chapter, I describe one method for learning the hard case slot model, i.e., generalizing case slots. 4.1 Tree Cut Model As described in Chapter 3, we can formalize the case slot generalization problem into that of estimating a conditional probability distribution referred to as a hard case slot model. The problem thus turns to be that of selecting the best model from among all possible hard case slot models. Since the number of partitions for a set of nouns is very large, the number of such models is very large, too. The problem of estimating a hard case slot model, therefore, is most likely intractable. (The number of partitions for a set of nouns is N i1 i j1 (1)ijjN (ij)!j! , where N is the size of the set of nouns (cf., (Knuth, 1973)), and is roughly of order O(NN).) ANIMAL BIRD INSECT swallow crow eagle bird bug bee insect Figure 4.1: An example thesaurus. 53 54 CHAPTER 4. CASE SLOT GENERALIZATION To deal with this diﬃculty, I take the approach of restricting the class of case slot models. I reduce the number of partitions necessary for consideration by using a thesaurus, following a similar proposal given in (Resnik, 1992). Speciﬁcally, I restrict attention to those partitions that exist within the thesaurus in the form of a cut. Here by thesaurus is meant a rooted tree in which each leaf node stands for a noun, while each internal node represents a noun class, and a directed link represents set inclusion (cf., Figure 4.1). A cut in a tree is any set of nodes in the tree that can represent a partition of the given set of nouns. For example, in the thesaurus of Figure 4.1, there are ﬁve cuts: [ANIMAL],[BIRD, INSECT], [BIRD, bug, bee, insect], [swallow, crow, eagle, bird, INSECT], and [swallow, crow, eagle, bird, bug, bee, insect]. The class of tree cut models with respect to a ﬁxed thesaurus tree is then obtained by restricting the partitions in the deﬁnition of a hard case slot model to be those that are present as a cut in that thesaurus tree. The number of models, then, is drastically reduced, and is of order Θ(2 N b ) when the thesaurus tree is a complete b-ary tree, because the number of cuts in a complete b-ary tree is of that order (see Appendix A.3). Here, N denotes the number of leaf nodes, i.e., the size of the set of nouns. A tree cut model M can be represented by a pair consisting of a tree cut Γ (i.e., a discrete model), and a probability parameter vector θ of the same length, that is, M  (Γ, θ), where Γ and θ are Γ  [C1, C2,   , Ck1], θ  [P(C1), P(C2),   , P(Ck1)], where C1, C2,   , Ck1 forms a cut in the thesaurus tree and where k1 i1 P(Ci)  1 is satisﬁed. Hereafter, for simplicity I sometimes write P(Ci) for P(Civ, r), where i  1,   , (k  1). 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 swallow crow eagle bird bug bee insect \"Prob.\" Figure 4.2: A tree cut model with [swallow, crow, eagle, bird, bug, bee, insect]. 4.1. TREE CUT MODEL 55 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 swallow crow eagle bird bug bee insect \"Prob.\" Figure 4.3: A tree cut model with [BIRD, bug, bee, insect]. 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 swallow crow eagle bird bug bee insect \"Prob.\" Figure 4.4: A tree cut model with [BIRD, INSECT]. If we employ MLE for parameter estimation, we can obtain ﬁve tree cut models from the case slot data in Figure 2.1; Figures 4.2-4.4 show three of these. For example, ˆ M  ([BIRD, bug, bee, insect], [0.8, 0, 0.2, 0]) shown in Figure 4.3 is one such tree cut model. Recall that ˆ M deﬁnes a conditional probability distribution P ˆ M(nv, r) in the following way: for any noun that is in the tree cut, such as bee, the probability is given as explicitly speciﬁed by the model, i.e., P ˆ M(beeﬂy, arg1)  0.2; for any class in the tree cut, the probability is distributed uniformly to all nouns included in it. For example, since there are four nouns that fall under the class BIRD, and swallow is one of them, the probability of swallow is thus given by P ˆ M(swallowﬂy, arg1)  0.84  0.2. Note that the probabilities assigned to the nouns under BIRD are smoothed, even if the nouns have diﬀerent observed frequencies. In this way, the problem of generalizing the values of a case slot has been formal- ized into that of estimating a model from the class of tree cut models for some ﬁxed thesaurus tree. 56 CHAPTER 4. CASE SLOT GENERALIZATION 4.2 MDL as Strategy The question now becomes what strategy (criterion) we should employ to select the best tree cut model. I propose to adopt the MDL principle. Table 4.1: Number of parameters and KL divergence for the ﬁve tree cut models. Γ Number of parameters KL divergence [ANIMAL] 0 1.4 [BIRD, INSECT] 1 0.72 [BIRD, bug, bee, insect] 3 0.4 [swallow, crow, eagle, bird, INSECT] 4 0.32 [swallow, crow, eagle, bird, bug, bee, insect] 6 0 In our current problem, a model nearer the root of the thesaurus tree, such as that of Figure 4.4, generally tends to be simpler (in terms of the number of parameters), but also tends to have a poorer ﬁt to the data. By way of contrast, a model nearer the leaves of the thesaurus tree, such as that in Figure 4.2, tends to be more complex, but also tends to have a better ﬁt to the data. Table 4.1 shows the number of free parameters and the KL divergence between the empirical distribution (namely, the word-based distribution estimated by MLE) of the data shown in Figure 2.2 and each of the ﬁve tree cut models.1 In the table, we can see that there is a trade-oﬀ between the simplicity of a model and the goodness of its ﬁt to the data. The use of MDL can balance the trade-oﬀ relationship. Let us consider how to calculate description length for the current problem, where the notations are slightly diﬀerent from those in Chapter 2. Suppose that S denotes a sample (or data), which is a multi-set of examples, each of which is an occurrence of a noun at a slot r for a verb v (i.e., duplication is allowed). Further suppose that S denotes the size of S, and n  S indicates the inclusion of n in S. For example, the column labeled slot value in Table 2.3 represents a sample S for the arg1 slot for ﬂy, and in this case S  10. Given a sample S and a tree cut Γ, we can employ MLE to estimate the param- eters of the corresponding tree cut model ˆ M  (Γ, ˆθ), where ˆθ denotes the estimated parameters. The total description length l( ˆ M, S) of the tree cut model ˆ M and the data S observed through ˆ M may be computed as the sum of model description length l(Γ), 1The KL divergence (also known as relative entropy) is a measure of the distance between two probability distributions, and is deﬁned as D(PQ)   i pi  log pi qi where pi and qi represent, respectively, probabilities in discrete distributions P and Q (Cover and Thomas, 1991). 4.2. MDL AS STRATEGY 57 parameter description length l(ˆθΓ), and data description length l(SΓ, ˆθ), i.e., l( ˆ M, S)  l((Γ, ˆθ), S)  l(Γ)  l(ˆθΓ)  l(SΓ, ˆθ). Model description length l(Γ), here, may be calculated as2 l(Γ)  log G, where G denotes the set of all cuts in the thesaurus tree T. From the viewpoint of Bayesian Estimation, this corresponds to assuming that each tree cut model to be equally likely a priori. Parameter description length l(ˆθΓ) may be calculated by l(ˆθΓ)  k 2  log S, where S denotes the sample size and k denotes the number of free parameters in the tree cut model, i.e., k equals the number of nodes in Γ minus one. Finally, data description length l(SΓ, ˆθ) may be calculated as l(SΓ, ˆθ)    nS log ˆP(n), where for simplicity I write ˆP(n) for P ˆ M(nv, r). Recall that ˆP(n) is obtained by MLE, i.e., ˆP(n)  1 C  ˆP(C) for each n  C, where for each C  Γ ˆP(C)  f(C) S , where f(C) denotes the frequency of nouns in class C in data S. With the description length deﬁned in the above manner, we wish to select a model with the minimum description length, and then output it as the result of generalization. Since every tree cut has an equal l(Γ), technically we need only calculate and compare L( ˆ M, S)  l(ˆθΓ)  l(SΓ, ˆθ). In the discussion which follows, I sometimes use L(Γ) for L( ˆ M, S), where Γ is the tree cut of ˆ M, for the sake of simplicity. The description lengths of the data in Figure 2.1 for the tree cut models with respect to the thesaurus tree in Figure 4.1 are shown in Table 4.3. (Table 4.2 shows how the description length is calculated for the model with tree cut [BIRD, bug, bee, insect].) These ﬁgures indicate that according to MDL, the model in Figure 4.4 is the best model. Thus, given the data in Table 2.3 as input, we are able to obtain the generalization result shown in Table 4.4. 2Throughout this thesis, log denotes the logarithm to base 2. 58 CHAPTER 4. CASE SLOT GENERALIZATION Table 4.2: Calculating description length. C BIRD bug bee insect f(C) 8 0 2 0 C 4 1 1 1 ˆP(C) 0.8 0.0 0.2 0.0 ˆP(n) 0.2 0.0 0.2 0.0 Γ [BIRD, bug, bee, insect] l(ˆθΓ) (41) 2  log 10  4.98 l(SΓ, ˆθ) (2  4  2  2)  log 0.2  23.22 Table 4.3: Description lengths for the ﬁve tree cut models. Γ l(ˆθΓ) l(SΓ, ˆθ) L(Γ) [ANIMAL] 0 28.07 28.07 [BIRD, INSECT] 1.66 26.39 28.05 [BIRD, bug, bee, insect] 4.98 23.22 28.20 [swallow, crow, eagle, bird, INSECT] 6.64 22.39 29.03 [swallow, crow, eagle, bird, bug, bee, insect] 9.97 19.22 29.19 Let us next consider some justiﬁcations for calculating description lengths in the above ways. For the model description length l(Γ), I assumed the length to be equal for all the discrete tree cut models. We could, alternatively, have assigned larger code lengths to models nearer the root node and smaller code lengths to models nearer the leaf nodes. I chose not to do so for the following reasons: (1) in general, when we have no information about a class of models, it is optimal to assume, on the basis of the minmax strategy in Bayesian Estimation, that each model has equal prior probability (i.e., to assume equal ignorance); (2) when the data size is large enough, the model description length, which is only of order O(1), will be negligible compared to the parameter description length, which is of order O(log S); (3) this way of calculating the model description length is Table 4.4: Generalization result. Verb Slot name Slot value Probability ﬂy arg1 BIRD 0.8 ﬂy arg1 INSECT 0.2 4.3. ALGORITHM 59 compatible with the dynamic-programming-based learning algorithm described below. With regard to the calculation of parameter description length l(ˆθΓ), we should note that the use of the looser form (2.9) rather than the more precise form (2.14) is done out of similar consideration of compatibility with the dynamic programming technique. 4.3 Algorithm In generalizing the values of a case slot using MDL, if computation time were of no concern, one could in principle calculate the description length for every possible tree cut model and output a model with the minimum description length as a generalization result, But since the number of cuts in a thesaurus tree is usually exponential (cf., Appendix A.3), it is impractical to do so. Nonetheless, we were able to devise a simple and eﬃcient algorithm, based on dynamic programming, which is guaranteed to ﬁnd a model with the minimum description length. The algorithm, which we call Find-MDL, recursively ﬁnds the optimal submodel for each child subtree of a given (sub)tree and follows one of two possible courses of action: (1) it either combines these optimal submodels and returns this combination as output, or (2) it collapses all these optimal submodels into the (sub)model containing the root node of the given (sub)tree. Find-MDL simply chooses the course of action which will result in the shorter description length (cf., Figure 4.5). Note that for simplicity I describe Find-MDL as outputting a tree cut, rather than a tree cut model. Note in the above algorithm that the parameter description length is calculated as k1 2  log S, where k  1 is the number of nodes in the current cut, both when t is the entire tree and when it is a proper subtree. This contrasts with the fact that the number of free parameters is k for the former, while it is k  1 for the latter. For the purpose of ﬁnding a tree cut model with the minimum description length, however, this distinction can be ignored (cf., Appendix A.4). Figure 4.6 illustrates how the algorithm works. In the recursive application of Find-MDL on the subtree rooted at AIRPLANE, the if-clause on line 9 is true since L([AIRPLANE])  32.20, L([jet, helicopter, airplane])  32.72, and hence [AIRPLANE] is returned. Similarly, in the application of Find-MDL on the subtree rooted at ARTIFACT, the same if-clause is false since L([VEHICLE, AIRPLANE])  40.83, L([ARTIFACT])  40.95, and hence [VEHICLE, AIRPLANE] is returned. Concerning the above algorithm, the following proposition holds: Proposition 1 The algorithm Find-MDL terminates in time O(N), where N denotes the number of leaf nodes in the thesaurus tree T, and it outputs a tree cut model of T with the minimum description length (with respect to the coding scheme described in Section 4.2). See Appendix A.4 for a proof of the proposition. 60 CHAPTER 4. CASE SLOT GENERALIZATION Let t denote a thesaurus (sub)tree, while root(t) denotes the root of t. Let c denote a tree cut in t. Initially t is set to the entire tree. algorithm Find-MDL(t): c 1. if 2. t is a leaf node 3. then 4. return([t]); 5. else 6. For each child subtree ti of t ci :Find-MDL(ti); 7. c: append(ci); 8. if 9. L([root(t)])  L(c) 10. then 11. return([root(t)]); 12. else 13. return(c). Figure 4.5: The Find-MDL algorithm. 4.4 Advantages Coping with the data sparseness problem Using the MDL-based method described above, we can generalize the values of a case slot. The probability of a noun being the value of a slot can then be represented as a conditional probability estimated (smoothed) from a class-based model on the basis of the MDL principle. The advantage of this method over the word-based method described in Chapter 2 lies in its ability to cope with the data sparseness problem. Formalizing this problem as a statistical estimation problem that includes model selection enables us to select models with various complexities, while employing MDL enables us to select, on the basis of training data, a model with the most appropriate level of complexity. Generalization The case slot generalization problem can also be restricted to that of generalizing individual nouns present in case slot data into classes of nouns present in a given thesaurus. For example, given the thesaurus in Figure 4.1 and frequency data in Figure 2.1, we would like our system to judge that the class BIRD and the noun bee can be the value of the arg1 slot for the verb ﬂy. The problem of deciding whether to stop generalizing at BIRD and bee or to continue generalizing further to ANIMAL 4.4. ADVANTAGES 61 ANIMAL BIRD INSECT swallow crow eagle bird bug bee insect ARTIFACT VEHICLE AIRPLANE car jet airplane helicopter bike ENTITY 0.51 0 0.23 0 0.03 0.23 f(swallow)4,f(crow)4,f(eagle)4,f(bird)6,f(bee)8,f(car)1,f(jet)4,f(airplane)4 L([jet,helicopter,airplane])32.72 L([AIRPLANE])32.27 L([ARTIFACT])41.09 L([VEHICLE,AIRPLANE])40.97 Figure 4.6: An example application of Find-MDL. has been addressed by a number of researchers (cf., (Webster and Marcus, 1989; Velardi, Pazienza, and Fasolo, 1991; Nomiyama, 1992)). The MDL-based method described above provides a disciplined way to realize this on the basis of data compression and statistical estimation. The MDL-based method, in fact, conducts generalization in the following way. When the diﬀerences between the frequencies of the words in a class are not large enough (relative to the entire data size and the number of the words), it generalizes them into the class. When the diﬀerences are especially noticeable (relative to the entire data size and the number of the words), on the other hand, it stops generalization at that level. As described in Chapter 3, the class of hard case slot models contains all of the possible models for generalization, if we view the generalization process as that of ﬁnding the best conﬁguration of words such that the words in each class are equally likely to the value of a case slot. And thus if we could estimate the best model from the class of hard case slot models on the basis of MDL, we would be able to obtain the most appropriate generalization result. When we make use of a thesaurus (hand-made or automatically constructed) to restrict the model class, the generalization result will inevitablely be aﬀected by the thesaurus used, and the tree cut model selected may be a loose approximation of the best model. Because MDL achieves a balanced trade-oﬀ between model simplicity and data ﬁt, we may expect that the model it selects will represent a reasonable compromise. Coping with extraction noise Avoiding the inﬂuence of noise in case slot data is another problem that needs con- sideration in case slot generalization. For example, suppose that the case slot data 62 CHAPTER 4. CASE SLOT GENERALIZATION on the noun car in Figure 4.6 is noise. In such case, the MDL-based method tends to generalize a noun to a class at quite high a level, since the diﬀerences between the frequency of the noun and those of its neighbors are not high (e.g., f(car)  1 and f(bike)  0). The probabilities of the generalized classes will, however, be small. If we discard those classes in the obtained tree cut that have small probabilities, we will still acquire reliable generalization results. That is to say, the proposed method is robust against noise. 4.5 Experimental Results 4.5.1 Experiment 1: qualitative evaluation I have applied the MDL-based generalization method to a data corpus and inspected the obtained tree cut models to see if they agree with human intuition. In the experiments, I used existing techniques (cf., (Manning, 1992; Smadja, 1993)) to extract case slot data from the tagged texts of the Wall Street Journal corpus (ACLDCI CD-ROM1) consisting of 126,084 sentences. I then applied the method to generalize the slot values. Table 4.5 shows some example case slot data for the arg2 slot for the verb eat. There were some extraction errors present in the data, but I chose not to remove them because extraction errors are such a generally common occurrence that a realistic evaluation should include them. Table 4.5: Example input data (for the arg2 slot for eat). eat arg2 food 3 eat arg2 lobster 1 eat arg2 seed 1 eat arg2 heart 2 eat arg2 liver 1 eat arg2 plant 1 eat arg2 sandwich 2 eat arg2 crab 1 eat arg2 elephant 1 eat arg2 meal 2 eat arg2 rope 1 eat arg2 seafood 1 eat arg2 amount 2 eat arg2 horse 1 eat arg2 mushroom 1 eat arg2 night 2 eat arg2 bug 1 eat arg2 ketchup 1 eat arg2 lunch 2 eat arg2 bowl 1 eat arg2 sawdust 1 eat arg2 snack 2 eat arg2 month 1 eat arg2 egg 1 eat arg2 jam 2 eat arg2 eﬀect 1 eat arg2 sprout 1 eat arg2 diet 1 eat arg2 debt 1 eat arg2 nail 1 eat arg2 pizza 1 eat arg2 oyster 1 When generalizing, I used the noun taxonomy of WordNet (version1.4) (Miller, 1995) as the thesaurus. The noun taxonomy of WordNet is structured as a directed acyclic graph (DAG), and each of its nodes stands for a word sense (a concept), often containing several words having the same word sense. WordNet thus deviates from the notion of a thesaurus as deﬁned in Section 4.1  a tree in which each leaf node stands 4.5. EXPERIMENTAL RESULTS 63 for a noun, and each internal node stands for a class of nouns; we need to take a few measures to deal with this. First, each subgraph having multiple parents is copied so that the WordNet is transformed into a tree structure 3 and the algorithm Find-MDL can be applied. Next, the issue of word sense ambiguity is heuristically addressed by equally dividing the observed frequency of a noun between all the nodes containing that noun. Finally, the highest nodes actually containing the values of the slot are used to form the staring cut from which to begin generalization and the frequencies of all the nodes below to a node in the starting cut are added to that node. Since word senses of nouns that occur in natural language tend to concentrate in the middle of a taxonomy,4 a starting cut given by this method usually falls around the middle of the thesaurus. TOP entity abstraction life_form... animal... plant... lobster lobster pizza food solid fluid substance object space time measure,quantity,amount... artifact... rope horse mushroom 0.11 0.10 0.08 0.39 starting cut resulting cut Figure 4.7: Example generalization result (for the arg2 slot for eat). Figure 4.7 indicates the starting cut and the resulting cut in WordNet for the arg2 slot for eat with respect to the data in Table 4.5, where    denotes a node in Word- Net. The starting cut consists of those nodes plant,   ,food,etc. which are the high- est nodes containing the values of the arg2 slot for eat. Since food has signiﬁcantly more frequencies than its neighbors solid and ﬂuid, MDL has the generalization stop there. By way of contrast, because the nodes under life form,    have relatively small diﬀerences in their frequencies, they are generalized to the node life form,   . 3In fact, there are only few nodes in WordNet, which have multiple parent nodes, i.e., the structure of WordNet approximates that of a tree. 4Cognitive scientists have observed that concepts in the middle of a taxonomy tend to be more important with respect to learning, recognition, and memory, and their linguistic expressions occur more frequently in natural language  a phenomenon known as basic level primacy. (cf., (Lakoﬀ, 1987)) 64 CHAPTER 4. CASE SLOT GENERALIZATION The same is true of the nodes under artifact,   . Since   , amount,    has a much higher frequency than its neighbors time and space, generalization does not proceed any higher. All of these results seem to agree with human intuition, indicating that the method results in an appropriate level of generalization. Table 4.6 shows generalization results for the arg2 slot for eat and three other arbitrarily selected verbs, where classes are sorted in descending order with respect to probability values. (Classes with probabilities less than 0.05 have been discarded due to space limitations.) Despite the fact that the employed extraction method is not noise-free, and word sense ambiguities remain after extraction, the generalization results seem to agree with intuition to a satisfactory degree. (With regard to noise, at least, this is not too surprising since the noisy portion usually has a small probability and thus tends to be discarded.) Table 4.6: Examples of generalization results. Class Probability Example words arg2 slot of eat food, nutrient 0.39 pizza, egg life form, organism,    0.11 lobster, horse measure, quantity,    0.10 amount of artifact, article,    0.08 as if eat rope arg2 slot of buy object,    0.30 computer, painting asset 0.10 stock, share group, grouping 0.07 company, bank legal document,     0.05 security, ticket arg2 slot of ﬂy entity 0.35 airplane, ﬂag, executive linear measure,     0.28 mile group, grouping 0.08 delegation arg2 slot of operate group, grouping 0.13 company, ﬂeet act, human action,     0.13 ﬂight, operation structure,     0.12 center abstraction 0.11 service, unit possession 0.06 proﬁt, earnings Table 4.7 shows the computation time required (on a SPARC Ultra 1 work station, not including that for loading WordNet) to obtain the results shown in Table 4.6. Even though the noun taxonomy of WordNet is a large thesaurus containing approximately 50,000 nodes, the MDL-based method still manages to generalize case slots eﬃciently 4.5. EXPERIMENTAL RESULTS 65 Table 4.7: Required computation time and number of generalized levels. Verb CPU time (second) Average number of generalized levels eat 1.00 5.2 buy 0.66 4.6 ﬂy 1.11 6.0 operate 0.90 5.0 Average 0.92 5.2 with it. The table also shows the average number of levels generalized for each slot, i.e., the average number of links between a node in the starting cut and its ancestor node in the resulting cut. (For example, the number of levels generalized for plant,    is one in Figure 4.7.) One can see that a signiﬁcant amount of generalization is performed by the method  the resulting tree cut is on average about 5 levels higher than the starting cut. 4.5.2 Experiment 2: pp-attachment disambiguation Case slot patterns obtained by the method can be used in various tasks in natural language processing. Here, I test the eﬀectiveness of the use of the patterns in pp- attachment disambiguation. In the experiments described below, I compare the performance of the proposed method, referred to as MDL, against the methods proposed by (Hindle and Rooth, 1991), (Resnik, 1993b), and (Brill and Resnik, 1994), referred to respectively as LA, SA, and TEL. Data set As a data set, I used the bracketed data of the Wall Street Journal corpus (Penn Tree Bank 1) (Marcus, Santorini, and Marcinkiewicz, 1993). First I randomly selected one of the 26 directories of the WSJ ﬁles as test data and what remained as training data. I repeated this process ten times and obtained ten sets of data consisting of diﬀerent training and test data. I used these ten data sets to conduct cross validation, as described below. From the test data in each data set, I extracted (v, n1, p, n2) quadruples using the extraction tool provided by the Penn Tree Bank called tgrep. At the same time, I obtained the answer for the pp-attachment for each quadruple. I did not double- check to conﬁrm whether or not the answers were actually correct. From the training data of each data set, I then extracted (v, p) and (n1, p) doubles, and (v, p, n2) and (n1, p, n2) triples using tools I developed. I also extracted quadruples from the training data as before. I then applied 12 heuristic rules to further preprocess the data; this 66 CHAPTER 4. CASE SLOT GENERALIZATION processing included (1) changing the inﬂected form of a word to its stem form, (2) replacing numerals with the word number, (3) replacing integers between 1900 and 2999 with the word year, (4) replacing co., ltd. with the word company, (5) etc. After preprocessing some minor errors still remained, but I did not attempt to remove them because of lacking a good method to do so automatically. Table 4.8 shows the number of diﬀerent types of data obtained in the above process. Table 4.8: Number of data items. Training data average number of doubles per data set 91218.1 average number of triples per data set 91218.1 average number of quadruples per data set 21656.6 Test data average number of quadruples per data set 820.4 Experimental procedure I ﬁrst compared the accuracy and coverage for MDL, SA and LA. For MDL, n2 is generalized on the basis of two sets of triples (v, p, n2) and (n1, p, n2) that are given as training data for each data set, with WordNet being used as the thesaurus in the same manner as it was in Experiment 1. When disambiguating, rather than comparing ˆP(n2v, p) and ˆP(n2n1, p) I compare ˆP(C1v, p) and ˆP(C2n1, p), where C1 and C2 are classes in the output tree cut models dominating n25; because I empirically found that to do so gives a slightly better result. For SA, I employ a basic application (also using WordNet) in which n2 is generalized given (v, p, n2) and (n1, p, n2) triples. For disambiguation I compare ˆA(n2v, p) and ˆA(n2n1, p) (deﬁned in (2.2) in Chapter 2)). For LA, I estimate ˆP(pv) and ˆP(pn1) from the training data of each data set and compare them for disambiguation. I then evaluated the results achieved by the three methods in terms of accuracy and coverage. Here coverage refers to the percentage of test data by which a disambigua- tion method can reach a decision, and accuracy refers to the proportion of correct decisions among all decisions made. Figure 4.8 shows the accuracy-coverage curves for the three methods. In plotting these curves, I ﬁrst compare the respective values for the two possible attachments. If the diﬀerence between the two values exceeds a certain threshold, I make the de- cision to attach at the higher-value site. The threshold here was set successively to 0,0.01,0.02,0.05,0.1,0.2,0.5,and 0.75 for each of the three methods. When the diﬀerence 5Recall that a node in WordNet represents a word sense and not a word, n2 can belong to several dif- ferent classes in the thesaurus. In fact, I compared maxCin2( ˆP(Civ, p)) and maxCjn2( ˆP(Cjn1, p)). 4.5. EXPERIMENTAL RESULTS 67 between the two values is less than the threshold, no decision is made. These curves were obtained by averaging over the ten data sets. Figure 4.8 shows that, with respect to accuracy-coverage curves, MDL outperforms both SA and LA throughout, while SA is better than LA. 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 1 0 0.2 0.4 0.6 0.8 1 accuracy coverage \"MDL\" \"SA\" \"LA\" \"LA.t\" Figure 4.8: Accuracy-coverage plots for MDL, SA, and LA. I also implemented the method proposed by (Hindle and Rooth, 1991) which makes disambiguation judgements using t-scores (cf., Chapter 2). Figure 4.8 shows the result as LA.t, where the threshold for the t-score is 1.28 (at a signiﬁcance level of 90 percent.) Next, I tested the method of applying a default rule after applying each method. That is, attaching (p, n2) to v for the part of the test data for which no decision was made by the method in question. (Interestingly, over the data set as a whole it is more favorable to attach (p, n2) to n1, but for what remains after applying LA, SA, and MDL, it turns out to be more favorable to attach (p, n2) to v.) I refer to these combined methods as MDLDefault, SADefault, LADefault, and LA.tDefault. Table 4.9 shows the results, again averaged over the ten data sets. Finally, I used transformation-based error-driven learning (TEL) to acquire trans- formation rules for each data set and applied the obtained rules to disambiguate the test data (cf., Chapter 2). The average number of obtained rules for a data set was 2752.3. Table 4.9 shows disambiguation results averaged over the ten data sets. From Table 4.9, we see that TEL performs the best, edging out the second place MDLDefault by a tiny margin, and followed by LADefault, and SADefault. I discuss these results below. 68 CHAPTER 4. CASE SLOT GENERALIZATION Table 4.9: PP-attachment disambiguation results. Method Coverage() Accuracy() Default 100 56.2 MDL  Default 100 82.2 SA  Default 100 76.7 LA  Default 100 80.7 LA.t  Default 100 78.1 TEL 100 82.4 MDL and SA Experimental results show that the accuracy and coverage of MDL appear to be some- what better than those of SA. Table 4.10 shows example generalization results for MDL (with classes with probability less than 0.05 discarded) and SA. Note that MDL tends to select a tree cut model closer to the root of the thesaurus. This is probably the key reason that MDL has a wider coverage than SA for the same degree of accuracy. One may be concerned that MDL may be over-generalizing here, but as shown in Figure 4.8, this does not seem to degrade its disambiguation accuracy. Another problem which must be dealt with concerning SA is how to increase the reliability of estimation. Since SA actually uses the ratio between two probability estimates, namely ˆP(Cv,r) ˆP (C) , when one of the estimates is unreliably estimated, the ratio may be lead astray. For instance, the high estimated value shown in Table 4.10 for drop,bead,pearl at protect against is rather odd, and arises because the estimate of ˆP(C) is unreliable (very small). This problem apparently costs SA a non-negligible drop in the disambiguation accuracy. MDL and LA LA makes its disambiguation decision completely ignoring n2. As (Resnik, 1993b) pointed out, if we hope to improve disambiguation performance with increasing training data, we need a richer model, such as those used in MDL and SA. I found that 8.8 of the quadruples in the entire test data were such that they shared the same (v, p, n1) but had diﬀerent n2, and their pp-attachment sites went both ways in the same data, i.e., both to v and to n1. Clearly, for these examples, the pp-attachment site cannot be reliably determined without knowing n2. Table 4.11 shows some of these examples. (I have adopted the attachment sites given in the Penn Tree Bank, without correcting apparently wrong judgements.) 4.6. SUMMARY 69 MDL and TEL TEL seems to perform slightly better than MDL. We can, however, develop a more sophisticated MDL method which outperforms TEL, as may be seen in Chapter 7. 4.6 Summary I have proposed a method for generalizing case slots. The method has the following merits: (1) it is theoretically sound; (2) it is computationally eﬃcient; (3) it is ro- bust against noise. One of the disadvantages of the method is that its performance depends on the structure of the particular thesaurus used. This, however, is a prob- lem commonly shared by any generalization method which uses a thesaurus as prior knowledge. The approach of applying MDL to estimate a tree cut model in an existing thesaurus is not limited to just the problem of generalizing values of a case slot. It is potentially useful in other natural language processing tasks, such as estimating n-gram models (cf., (Brown et al., 1992; Stolcke and Segal, 1994; Pereira and Singer, 1995; Rosenfeld, 1996; Ristad and Thomas, 1995; Saul and Pereira, 1997)) or semantic tagging (cf., (Cucchiarelli and Velardi, 1997)). 70 CHAPTER 4. CASE SLOT GENERALIZATION Table 4.10: Example generalization results for SA and MDL. Input Verb Preposition Noun Frequency protect against accusation 1 protect against damage 1 protect against decline 1 protect against drop 1 protect against loss 1 protect against resistance 1 protect against squall 1 protect against vagary 1 Generalization result of MDL Verb Preposition Noun class Probability protect against act, human action, human activity 0.212 protect against phenomenon 0.170 protect against psychological feature 0.099 protect against event 0.097 protect against abstraction 0.093 Generalization result of SA Verb Preposition Noun class SA protect against caprice, impulse, vagary, whim 1.528 protect against phenomenon 0.899 protect against happening, occurrence, natural event 0.339 protect against deterioration, worsening, decline, declination 0.285 protect against act, human action, human activity 0.260 protect against drop, bead, pearl 0.202 protect against drop 0.202 protect against descent, declivity, fall, decline, downslope 0.188 protect against resistor, resistance 0.130 protect against underground, resistance 0.130 protect against immunity, resistance 0.124 protect against resistance, opposition 0.111 protect against loss, deprivation 0.105 protect against loss 0.096 protect against cost, price, terms, damage 0.052 4.6. SUMMARY 71 Table 4.11: Some hard examples for LA. Attached to v Attached to n1 acquire interest in year acquire interest in ﬁrm buy stock in trade buy stock in index ease restriction on export ease restriction on type forecast sale for year forecast sale for venture make payment on million make payment on debt meet standard for resistance meet standard for car reach agreement in august reach agreement in principle show interest in session show interest in stock win verdict in winter win verdict in case 72 CHAPTER 4. CASE SLOT GENERALIZATION Chapter 5 Case Dependency Learning The concept of the mutual independence of events is the most essential sprout in the development of probability theory. - Andrei Kolmogorov In this chapter, I describe one method for learning the case frame model, i.e., learning dependencies between case frame slots. 5.1 Dependency Forest Model As described in Chapter 3, we can view the problem of learning dependencies between case slots for a given verb as that of learning a multi-dimensional discrete joint prob- ability distribution referred to as a case frame model. The number of parameters in a joint distribution will be exponential, however, if we allow interdependencies among all of the variables (even the slot-based case frame model has O(2n) parameters, where n is the number of random variables ), and thus their accurate estimation may not be feasible in practice. It is often assumed implicitly in natural language processing that case slots (random variables) are mutually independent. Although assuming that random variables are mutually independent would drasti- cally reduce the number of parameters (e.g., under the independence assumption, the number of parameters in a slot-based model becomes O(n)). As illustrated in (1.2) in Chapter 1, this assumption is not necessarily valid in practice. What seems to be true in practice is that some case slots are in fact dependent on one another, but that the overwhelming majority of them are mutually independent, due partly to the fact that usually only a few case slots are obligatory; the others are optional. (Optional case slots are not necessarily independent, but if two optional case slots are randomly selected, it is very likely that they are independent of one another.) Thus the target joint distribution is likely to be approximatable as the product of 73 74 CHAPTER 5. CASE DEPENDENCY LEARNING lower order component distributions, and thus has in fact a reasonably small number of parameters. We are thus lead to the approach of approximating the target joint distribution by a simpliﬁed distribution based on corpus data. In general, any n-dimensional discrete joint distribution can be written as P(X1, X2,    , Xn)  n  i1 P(XmiXm1,   , Xmi1) for a permutation (m1, m2,   , mn) of (1, 2,   , n), letting P(Xm1Xm0) denote P(Xm1). A plausible assumption regarding the dependencies between random variables is that each variable directly depends on at most one other variable. This is one of the simplest assumptions that can be made to relax the independence assumption. For example, if the joint distribution P(X1, X2, X3) over 3 random variables X1, X2, X3 can be written (approximated) as follows, it (approximately) satisﬁes such an assumption: P(X1, X2, X3)  ()P(X1)  P(X2X1)  P(X3X2). (5.1) I call such a distribution a dependency forest model. A dependency forest model can be represented by a dependency forest (i.e., a set of dependency trees), whose nodes represent random variables (each labeled with a number of parameters), and whose directed links represent the dependencies that exist between these random variables. A dependency forest model is thus a restricted form of a Bayesian network (Pearl, 1988). Graph (5) in Figure 5.1 represents the dependency forest model deﬁned in (5.1). Table 5.1 shows the parameters associated with each node in the graph, assuming that the dependency forest model is slot-based. When a distribution can be represented by a single dependency tree, I call it a dependency tree model. Table 5.1: Parameters labeled with each node. Node Parameters X1 P(X1  1), P(X1  0) X2 P(X2  1X1  1), P(X2  0X1  1), P(X2  1X1  0), P(X2  0X1  0) X3 P(X3  1X2  1), P(X3  0X2  1), P(X3  1X2  0), P(X3  0X2  0) It is not diﬃcult to see that disregarding the actual values of the probability pa- rameters, we will have 16 and only 16 dependency forest models (i.e., 16 dependency forests) as approximations of the joint distribution P(X1, X2, X3), Since some of them are equivalent with each other, they can be further reduced into 7 equivalent classes of dependency forest models. Figure 5.1 shows the 7 equivalent classes and their mem- bers. (It is easy to verify that the dependency tree models based on a labeled free 5.2. ALGORITHM 75 tree are equivalent to one another (cf., Appendix A.5). Here a labeled free tree refers to a tree in which each node is uniquely associated with a label and in which any node can be the root (Knuth, 1973).) 5.2 Algorithm Now we turn to the problem of how to select the best dependency forest model from among all possible ones to approximate a target joint distribution based on the data. This problem has already been investigated in the area of machine learning and re- lated ﬁelds. One classical method is Chow  Lius algorithm for estimating a multi- dimensional discrete joint distribution as a dependency tree model, in a way which is both eﬃcient and theoretically sound (Chow and Liu, 1968).1 More recently, Suzuki has extended their algorithm, on the basis of the MDL principle, so that it estimates the target joint distribution as a dependency forest model (Suzuki, 1993), and Suzukis is the algorithm I employ here. Suzukis algorithm ﬁrst calculates the statistic θ between all node pairs. The statis- tic θ(Xi, Xj) between node Xi and Xj is deﬁned as θ(Xi, Xj)  ˆI(Xi, Xj)  (ki  1)  (kj  1) 2  log N, where ˆI(Xi, Xj) denotes the empirical mutual information between random variables Xi and Xj; ki and kj denote, respectively, the number of possible values assumed by Xi and Xj; and N the input data size. The empirical mutual information between random variables Xi and Xj is deﬁned as ˆI(Xi, Xj)  ˆH(Xi)  ˆH(XiXj) ˆH(Xi)    xiXi ˆP(xi)  log ˆP(xi) ˆH(XiXj)    xiXi  xjXj ˆP(xi, xj)  log ˆP(xixj), where ˆP(.) denotes the maximum likelihood estimate of probability P(.). Furthermore, 0  log 0  0 is assumed to be satisﬁed. The algorithm then sorts the node pairs in descending order with respect to θ. It then puts a link between the node pair with the largest θ value, provided that this value is larger than zero. It repeats this process until no node pair is left unprocessed, provided that adding that link will not create a loop in the current dependency graph. Figure 5.2 shows the algorithm. Note that the dependency forest that is output by the algorithm may not be uniquely determined. Concerning the above algorithm, the following proposition holds: 1In general, learning a Baysian network is an intractable task (Cooper and Herskovits, 1992). 76 CHAPTER 5. CASE DEPENDENCY LEARNING Proposition 2 The algorithm outputs a dependency forest model with the minimum description length. See Appendix A.6 for a proof of the proposition. It is easy to see that the number of parameters in a dependency forest model is of the order O(n  k2), where k is the maximum of all ki, and n is the number of random variables. If we employ the quick sort algorithm to perform line 4, average case time complexity of the algorithm will be only of the order O(n2  (k2  log n)), and worst case time complexity will be only of the order O(n2  (k2  n2)). Let us now consider an example of how the algorithm works. Suppose that the input data is as given in Table 3.3 and there are 4 nodes (random variables) Xarg1, Xarg2, Xfrom, and Xto. Table 5.2 shows the statistic θ for all node pairs. The dependency forest shown in Figure 5.3 has been constructed on the basis of the values given in Table 5.2. The dependency forest indicates that there is dependency between the to slot and the arg2 slot, and between the to slot and the from slot. Table 5.2: The statistic θ for node pairs. θ Xarg1 Xarg2 Xfrom Xto Xarg1 0.28 0.16 0.18 Xarg2 0.11 0.57 Xfrom 0.28 Xto As previously noted, the algorithm is based on the MDL principle. In the current problem, a simple model means a model with fewer dependencies, and thus MDL pro- vides a theoretically sound way to learn only those dependencies that are statistically signiﬁcant in the given data. As mentioned in Chapter 2, an especially interesting fea- ture of MDL is that it incorporates the input data size in its model selection criterion. This is reﬂected, in this case, in the derivation of the threshold θ. Note that when we do not have enough data (i.e., N is too small), the thresholds will be large and few nodes will be linked, resulting in a simpler model in which most of the random variables are judged to be mutually independent. This is reasonable since with a small data size most random variables cannot be determined to be dependent with any signiﬁcance. Since the number of dependency forest models for a ﬁxed number of random vari- ables n is of order O(2n1  nn2) (the number of dependency tree models is of order Θ(nn2) (Knuth, 1973)), it would be impossible to calculate description length straight- forwardly for all of them. Suzukis algorithm eﬀectively utilizes the tree structures of the models and eﬃciently calculates description lengths by doing it locally (as does Chow  Lius algorithm). 5.3. EXPERIMENTAL RESULTS 77 5.3 Experimental Results I have experimentally tested the performance of the proposed method of learning de- pendencies between case slots. Most speciﬁcally, I have tested to see how eﬀective the dependencies acquired by the proposed method are when used in disambiguation experiments. In this section, I describe the procedures and the results of those experi- ments. 5.3.1 Experiment 1: slot-based model In the ﬁrst experiment, I tried to learn slot-based dependencies. As training data, I used the entire bracketed data of the Wall Street Journal corpus (Penn Tree Bank). I extracted case frame data from the corpus using heuristic rules. There were 354 verbs for which more than 50 case frame instances were extracted from the corpus. Table 5.3 shows the most frequent verbs and the corresponding numbers of case frames. In the experiment, I only considered the 12 most frequently occurring case slots (shown in Table 5.4) and ignored others. Table 5.3: Verbs appearing most frequently. Verb Number of case frames be 17713 say 9840 have 4030 make 1770 take 1245 expect 1201 sell 1147 rise 1125 get 1070 go 1042 do 982 buy 965 fall 862 add 740 come 733 include 707 give 703 pay 700 see 680 report 674 78 CHAPTER 5. CASE DEPENDENCY LEARNING Table 5.4: Case slots considered. arg1 arg2 on in for at by from to as with against Example case frame patterns I acquired slot-based case frame patterns for the 354 verbs. There were on average 484354  1.4 dependency links acquired for each of these 354 verbs. As an example, Figure 5.4 shows the case frame patterns (dependency forest model) obtained for the verb buy. There are four dependencies in this model; one indicates that, for example, the arg2 slot is dependent on the arg1 slot. I found that there were some verbs whose arg2 slot is dependent on a preposition (hereafter, p for short) slot. Table 5.5 shows the 40 verbs having the largest values of P(Xarg2  1, Xp  1), sorted in descending order of these values. The dependencies found by the method seem to agree with human intuition. Furthermore, I found that there were some verbs having preposition slots that depend on each other (I refer to these as p1 and p2 for short). Table 5.6 shows the 40 verbs having the largest values of P(Xp1  1, Xp2  1), sorted in descending order. Again, the dependencies found by the method seem to agree with human intuition. Perplexity reduction I also evaluated the acquired case frame patterns (slot-based models) for all of the 354 verbs in terms of reduction of the test data perplexity.2 I conducted the evaluation through a ten-fold cross validation. That is, to acquire case frame patterns for the verb, I used nine tenths of the case frames for each verb as training data, saving what remained for use as test data, and then calculated the test data perplexity. I repeated this process ten times and calculated average perplexity. I also calculated average perplexity for independent models which were acquired based on the assumption that each case slot is independent. Experimental results indicate that for some verbs the use of the dependency forest model results in less perplexity than does use of the independent model. For 30 of the 354 (8) verbs, perplexity reduction exceeded 10, while average perplexity reduction overall was 1. Table 5.7 shows the 10 verbs having the largest perplexity reductions. Table 5.8 shows perplexity reductions for 10 randomly selected verbs. There were a 2The test data perplexity is a measure of testing how well an estimated probability model predicts future data, and is deﬁned as 2H(PT ,PM), H(PT , PM)    x PT (x)log PM(x), where PM(x) denotes the estimated model, PT (x) the empirical distribution of the test data (cf., (Bahl, Jelinek, and Mercer, 1983)). It is roughly the case that the smaller perplexity a model has, the closer to the true model it is. 5.3. EXPERIMENTAL RESULTS 79 Table 5.5: Verbs and their dependent case slots. Verb Dependent slots Example base arg2 on base pay on education advance arg2 to advance 4 to 40 gain arg2 to gain 10 to 100 compare arg2 with compare proﬁt with estimate invest arg2 in invest share in fund acquire arg2 for acquire share for billion estimate arg2 at estimate price at million convert arg2 to convert share to cash add arg2 to add 1 to 3 engage arg2 in enage group in talk ﬁle arg2 against ﬁle suit against company aim arg2 at aim it at transaction sell arg2 to sell facility to ﬁrm lose arg2 to lose million to 10 pay arg2 for pay million for service leave arg2 with leave himself with share charge arg2 with charge them with fraud provide arg2 for provide engine for plane withdraw arg2 from withdraw application from oﬃce prepare arg2 for prepare case for trial succeed arg2 as succeed Taylor as chairman discover arg2 in discover mile in ocean move arg2 to move employee to New York concentrate arg2 on concentrate business on steel negotiate arg2 with negotiate rate with advertiser open arg2 to open market to investor protect arg2 against protect investor against loss keep arg2 on keep eye on indicator describe arg2 in describe item in inch see arg2 as see shopping as symptom boost arg2 by boost value by 2 pay arg2 to pay commission to agent contribute arg2 to contribute million to leader bid arg2 for bid million for right threaten arg2 against threaten sanction against lawyer ﬁle arg2 for ﬁle lawsuit for dismissal know arg2 as know him as father sell arg2 at sell stock at time settle arg2 at settle session at 99 see arg2 in see growth in quarter 80 CHAPTER 5. CASE DEPENDENCY LEARNING small number of verbs showing perplexity increases with the worst case being 5. It seems safe to say that the dependency forest model is more suitable for representing the true model of case frames than the independent model, at least for 8 of the 354 verbs. 5.3.2 Experiment 2: slot-based disambiguation To evaluate the eﬀectiveness of the use of dependency knowledge in natural language processing, I conducted a pp-attachment disambiguation experiment. Such disam- biguation would be, for example, to determine which word, ﬂy or jet, the phrase from Tokyo should be attached to in the sentence She will ﬂy a jet from Tokyo. A straightforward way of disambiguation would be to compare the following likelihood values, based on slot-based models, Pﬂy(Xarg2  1, Xfrom  1)  Pjet(Xfrom  0) and Pﬂy(Xarg2  1, Xfrom  0)  Pjet(Xfrom  1), assuming that there are only two case slots: arg2 and from for the verb ﬂy, and there is one case slot: from for the noun jet. In fact, we need only compare Pﬂy(Xfrom  1Xarg2  1)  (1  Pjet(Xfrom  1)) and (1  Pﬂy(Xfrom  1Xarg2  1))  Pjet(Xfrom  1), or equivalently, Pﬂy(Xfrom  1Xarg2  1) and Pjet(Xfrom  1). Obviously, if we assume that the case slots are independent, then we need only compare Pﬂy(Xfrom  1) and Pjet(Xfrom  1). This is equivalent to the method proposed by (Hindle and Rooth, 1991). Their method actually compares the two probabilities by means of hypothesis testing. It is here that we ﬁrst employ the proposed dependency learning method to judge if slots Xarg2 and Xfrom with respect to verb ﬂy are mutually dependent; if they are de- pendent, we make a disambiguation decision based on the t-score between Pﬂy(Xfrom  1Xfrom  1) and Pjet(Xfrom  1); otherwise, we consider the two slots independent and make a decision based on the t-score between Pﬂy(Xfrom  1) and Pjet(Xfrom  1). I refer to this method as DepenLA. In the experiment, I ﬁrst randomly selected the ﬁles under one directory for a por- tion of the WSJ corpus, a portion containing roughly one 26th of the entire bracketed 5.3. EXPERIMENTAL RESULTS 81 corpus data, and extracted (v, n1, p, n2) quadruples (e.g., (ﬂy, jet, from, Tokyo)) as test data. I then extracted case frames from the remaining bracketed corpus data as I did in Experiment 1 and used them as training data. I repeated this process ten times and obtained ten data sets consisting of diﬀerent training data and test data. In each training data set, there were roughly 128, 000 case frames on average for verbs and roughly 59, 000 case frames for nouns. On average, there were 820 quadruples in each test data set. I used these ten data sets to conduct disambiguation through cross validation. I used the training data to acquire dependency forest models, which I then used to perform disambiguation on the test data on the basis of DepenLA. I also tested the method of LA. I set the threshold for the t-score to 1.28. For both LA and DepenLA, there were still some quadruples remaining whose attachment sites could not be determined. In such cases, I made a default decision, i.e., forcibly attached (p, n2) to v, because I empirically found that, at least for our data set for what remained after applying LA and DepenLA, it is more likely for (p, n2) to go with v. Tab. 5.9 summarizes the results, which are evaluated in terms of disambiguation accuracy, averaged over the ten trials. I found that as a whole DepenLADefault only slightly improves LADefault. I further found, however, that for about 11 of the data in which the dependencies are strong (i.e., P(Xp  1Xarg2  1)  0.2 or P(Xp  1Xarg2  1)  0.002), DepenLADefault signiﬁcantly improves LADefault. That is to say that when sig- niﬁcant dependencies between case slots are found, the disambiguation results can be improved by using dependency knowledge. These results to some extent agree with the perplexity reduction results obtained in Experiment 1. 5.3.3 Experiment 3: class-based model I also used the 354 verbs in Experiment 1 to acquire case frame patterns as class-based dependency forest models. Again, I considered only the 12 slots listed in Table 5.4. I generalized the values of the case slots within these case frames using the method proposed in Chapter 4 to obtain class-based case frame data like those presented in Table 3.3.3 I used these data as input to the learning algorithm. On average, there was only a 64354  0.2 dependency link found in the patterns for a verb. That is, very few case slots were determined to be dependent in the case frame patterns. This is because the number of parameters in a class based model was larger than the size of the data we had available. The experimental results indicate that it is often valid in practice to assume that class-based case slots (and also word-based case slots) are mutually independent, when the data size available is at the level of what is provided by Penn Tree Bank. For this 3Since a node in WordNet represents a word sense and not a word, a word can belong to several diﬀerent classes (nodes) in an output tree cut model. I have heuristically replaced a word n with the word class C such that maxCn(P(Cv, r)) is satisﬁed. 82 CHAPTER 5. CASE DEPENDENCY LEARNING reason, I did not conduct disambiguation experiments using the class-based dependency forest models. I believe that the proposed method provides a theoretically sound and eﬀective tool for detecting whether there exists a statistically signiﬁcant dependency between case slots in given data; this decision has up to now been based simply on human intuition. 5.3.4 Experiment 4: simulation In order to test how large a data size is required to estimate a dependency forest model, I conducted the following experiment. I deﬁned an artiﬁcial model in the form of a dependency forest model and generated data on the basis of its distribution. I then used the obtained data to estimate a model, and evaluated the estimated model by measuring the KL divergence between the estimated model and the true model. I also checked the number of dependency links in the obtained model. I repeatedly generated data and observed the learning curve, namely the relationship between the data size used in estimation and the number of links in the estimated model, and the relationship between the data size and the KL divergence separating the estimated and the true model. I deﬁned two other artiﬁcial models and conducted the same experiments. Figures 5.5 and 5.6 show the results of these experiments for the three artiﬁcial models averaged over 10 trials. The number of parameters in Model 1, Model 2, and Model 3 are 18, 30, and 44 respectively, and the number of links in them 1, 3, and 5. Note that the KL divergences between the estimated models and the true models converge to 0, as expected. Also note that the numbers of links in the estimated models converge to the correct value (1, 3, and 5) in each of the three examples. These simulation results verify the consistency property of MDL (i.e., the numbers of parameters in the selected models converge in probability to that of the true model as the data size increases), which is crucial for the goal of learning dependencies. Thus we can be conﬁdent that the dependencies between case slots can be accurately learned when there are enough data, as long as the true model exists as a dependency forest model. We also see that to estimate a model accurately the data size required is as large as 5 to 10 times the number of parameters. For example, for the KL divergence to go to below 0.1, we need more than 200 examples, which is roughly 5 to 10 times the number of parameters. Note that in Experiment 3, I considered 12 slots, and for each slot there were roughly 10 classes as its values; thus a class-based model tended to have about 120 parameters. The corpus data available to us was insuﬃcient for accurate learning of the dependencies between case slots for most verbs (cf., Table 5.3). 5.4. SUMMARY 83 5.4 Summary I conclude this chapter with the following remarks. 1. The primary contribution of the research reported in this chapter is the proposed method of learning dependencies between case slots, which is theoretically sound and eﬃcient. 2. For slot-based models, some case slots are found to be dependent. Experimental results demonstrate that by using the knowledge of dependency, when depen- dency does exist, we can signiﬁcantly improve pp-attachment disambiguation results. 3. For class-based models, most case slots are judged independent with the data size currently available in the Penn Tree Bank. This empirical ﬁnding indicates that it is often valid to assume that case slots in a class-based model are mutually independent. The method of using a dependency forest model is not limited to just the problem of learning dependencies between case slots. It is potentially useful in other natural language processing tasks, such as word sense disambiguation (cf., ((Bruce and Wiebe, 1994))). 84 CHAPTER 5. CASE DEPENDENCY LEARNING X1 X2 X3 P(X1, X2, X3)  P(X1)P(X2X1)P(X3X2)  P(X2)P(X1X2)P(X3X2)  P(X3)P(X2X3)P(X1X2)       (5) X1 X2 X3 P(X1, X2, X3)  P(X1)P(X3X1)P(X2X1)  P(X2)P(X1X2)P(X3X1)  P(X3)P(X1X3)P(X2X1)           (6) X1 X2 X3 P(X1, X2, X3)  P(X1)P(X3X1)P(X2X3)  P(X3)P(X1X3)P(X2X3)  P(X2)P(X3X2)P(X1X3)       (7) X1 X2 X3 P(X1, X2, X3)  P(X1)P(X2)P(X3X2)  P(X1)P(X3)P(X2X3)  (2) X1 X2 X3 P(X1, X2, X3)  P(X1)P(X2X1)P(X3)  P(X2)P(X1X2)P(X3)      (3) X1 X2 X3 P(X1, X2, X3)  P(X1)P(X3X1)P(X2)  P(X3)P(X1X3)P(X2)      (4) X1 X2 X3 P(X1, X2, X3)  P(X1)P(X2)P(X3) (1) Figure 5.1: Example dependency forests. 5.4. SUMMARY 85 Algorithm: 1. Let T : ; 2. Let V  {{Xi}, i  1, 2,   , n}; 3. Calculate θ(Xi, Xj) for all node pairs (Xi, Xj) ; 4. Sort the node pairs in descending order of θ, and store them into queue Q; 5. while 6. max(Xi,Xj)Q θ(Xi, Xj)  0 7. do 8. Remove arg max(Xi,Xj)Q θ(Xi, Xj) from Q; 9. if 10. Xi and Xj belong to diﬀerent sets W1,W2 in V 11. then 12. Replace W1 and W2 in V with W1  W2, and add edge (Xi, Xj) to T; 13. Output T as the set of edges of the dependency forest. Figure 5.2: The learning algorithm. Xarg1 Xarg2 Xfrom Xto       P(Xarg1, Xarg2, Xfrom, Xto)  P(Xarg1)P(Xarg2)P(XtoXarg2)P(XfromXto) Figure 5.3: A dependency forest as case frame patterns. 86 CHAPTER 5. CASE DEPENDENCY LEARNING buy: [arg1]: [P(arg10)0.004 P(arg11)0.996] [arg2]: [P(arg20arg10)0.100,P(arg21arg10)0.900, P(arg20arg11)0.136,P(arg21arg11)0.864] [for]: [P(for0arg10)0.300,P(for1arg10)0.700, P(for0arg11)0.885,P(for1arg11)0.115] [at]: [P(at0for0)0.911,P(at1for0)0.089, P(at0for1)0.979,P(at1for1)0.021] [in]: [P(in0at0)0.927,P(in0at0)0.073, P(in0at1)0.994,P(in1at1)0.006] [on]: [P(on0)0.975,P(on1)0.025] [from]: [P(from0)0.937,P(from1)0.063] [to]: [P(to0)0.997,P(on1)0.003] [by]: [P(by0)0.995,P(by1)0.005] [with]: [P(with0)0.993,P(with1)0.007] [as]: [P(as0)0.991,P(as1)0.009] [against]: [P(against0)0.999,P(against1)0.001] Figure 5.4: Case frame patterns (dependency forest model) for buy. 0 1 2 3 4 5 6 10 100 1000 10000 \"model1\" \"model2\" \"model3\" Figure 5.5: Number of links versus data size. 5.4. SUMMARY 87 Table 5.6: Verbs and their dependent case slots. Head Dependent slots Example range from to range from 100 to 200 climb from to climb from million to million rise from to rise from billion to billion shift from to shift from stock to bond soar from to soar from 10 to 20 plunge from to plunge from 20 to 2 fall from to fall from million to million surge from to surge from 100 to 200 increase from to increase from million to million jump from to jump from yen to yen yield from to yield from 1 to 5 climb from in climb from million in period apply to for apply to commission for permission grow from to grow from million to million draw from in draw from thrift in bonus boost from to boost from 1 to 2 convert from to convert from form to form raise from to raise from 5 to 10 retire on as retire on 2 as oﬃcer move from to move from New York to Atlanta cut from to cut from 700 to 200 sell to for sell to bakery for amount open for at open for trading at yen lower from to lower from 10 to 2 rise to in rise to 5 in month trade for in trade for use in amount supply with by supply with meter by 1990 elect to in elect to congress in 1978 point to as point to contract as example drive to in drive to clinic in car vote on at vote on proposal at meeting acquire from for acquire from corp. for million end at on end at 95 on Friday apply to in apply to congress in 1980 gain to on gain to 3 on share die on at die on Sunday at age bid on with bid on project with Mitsubishi ﬁle with in ﬁle with ministry in week slow from to slow from pound to pound improve from to improve from 10 to 50 88 CHAPTER 5. CASE DEPENDENCY LEARNING Table 5.7: Verbs with signiﬁcant perplexity reduction. Verb Independent Dependency forest (reduction in percentage) base 5.6 3.6(36) lead 7.3 4.9(33) ﬁle 16.4 11.7(29) result 3.9 2.8(29) stem 4.1 3.0(28) range 5.1 3.7(28) yield 5.4 3.9(27) beneﬁt 5.6 4.2(26) rate 3.5 2.6(26) negotiate 7.2 5.6(23) Table 5.8: Randomly selected verbs and their perplexities. Verb Independent Dependency forest (reduction in percentage) add 4.2 3.7(9) buy 1.3 1.3(0) ﬁnd 3.2 3.2(0) open 13.7 12.3(10) protect 4.5 4.7(4) provide 4.5 4.3(4) represent 1.5 1.5(0) send 3.8 3.9(2) succeed 3.7 3.6(4) tell 1.7 1.7(0) Table 5.9: PP-attachment disambiguation results. Method Accuracy() Default 56.2 LADefault 78.1 DepenLADefault 78.4 LADefault(11 of data) 93.8 DepenLADefault(11 of data) 97.5 5.4. SUMMARY 89 0 0.5 1 1.5 2 2.5 10 100 1000 10000 \"model1\" \"model2\" \"model3\" Figure 5.6: KL divergence versus data size. 90 CHAPTER 5. CASE DEPENDENCY LEARNING Chapter 6 Word Clustering We may add that objects can be classiﬁed, and can become similar or dissimilar, only in this way - by being related to needs and interests. - Karl Popper In this chapter, I describe one method for learning the hard co-occurrence model, i.e., clustering of words on the basis of co-occurrence data. This method is a natural extension of that proposed by Brown et al (cf., Chapter 2), and it overcomes the drawbacks of their method while retaining its merits. 6.1 Parameter Estimation As described in Chapter 3, we can view the problem of clustering words (constructing a thesaurus) on the basis of co-occurrence data as that of estimating a hard co-occurrence model. The ﬁxing of partitions determines a discrete hard co-occurrence model and the number of parameters. We can estimate the values of the parameters on the basis of co-occurrence data by employing Maximum Likelihood Estimation (MLE). For given co-occurrence data S  {(n1, v1), (n2, v2),   , (nm, vm)}, where ni (i  1,   , m) denotes a noun, and vi (i  1,    , m) a verb. The maximum likelihood estimates of the parameters are deﬁned as the values that maximize the following likelihood function with respect to the data: m  i1 P(ni, vi)  m  i1 (P(niCni)  P(viCvi)  P(Cni, Cvi)). 91 92 CHAPTER 6. WORD CLUSTERING It is easy to verify that we can estimate the parameters as ˆP(Cn, Cv)  f(Cn, Cv) m ˆP(nCn)  f(n) f(Cn) ˆP(vCv)  f(v) f(Cv), so as to maximize the likelihood function, under the conditions that the sum of the joint probabilities over noun classes and verb classes equals one, and that the sum of the conditional probabilities over words in each class equals one. Here, m denotes the entire data size, f(Cn, Cv) the frequency of word pairs in class pair (Cn, Cv), f(n) the frequency of noun n, f(v) that of v, f(Cn) the frequency of words in class Cn, and f(Cv) that in Cv. 6.2 MDL as Strategy I again adopt the MDL principle as a strategy for statistical estimation. Data description length may be calculated as L(SM)    (n,v)S log ˆP(n, v). Model description length may be calculated, here, as L(M)  k 2  log m, where k denotes the number of free parameters in the model, and m the data size. We in fact implicitly assume here that the description length for encoding the discrete model is equal for all models and view only the description length for encoding the parameters as the model description length. Note that there are alternative ways of calculating the model description length. Here, for eﬃciency in clustering, I use the simplest formulation. If computation time were of no concern, we could in principle calculate the total description length for each model and select the optimal model in terms of MDL. However, since the number of hard co-occurrence models is of order O(NN  V V ) (cf., Chapter 4), where N and V denote the sizes of the set of nouns and the set of verbs respectively, it would be infeasible to do so. We therefore need to devise an eﬃcient algorithm that will heuristically perform this task. 6.3. ALGORITHM 93 6.3 Algorithm The algorithm that we have devised, denoted here as 2D-Clustering, iteratively selects a suboptimal MDL model from among a class of hard co-occurrence models. These models include the current model and those which can be obtained from the current model by merging a noun (or verb) class pair. The minimum description length cri- terion can be reformalized in terms of (empirical) mutual information. The algorithm can be formulated as one which calculates, in each iteration, the reduction of mutual information which would result from merging any noun (or verb) class pair. It would perform the merge having the least mutual information reduction, provided that the least mutual information reduction is below a threshold, which will vary depending on the data size and the number of classes in the current situation. 2D-Clustering(S) S is input co-occurrence data. bn and bv are positive integers. 1. Initialize the set of noun classes Πn and the set of verb classes Πv as: Πn  {{n}n  N } Πv  {{v}v  V} N and V denote the set of nouns and the set of verbs, respectively. 2. Repeat the following procedure: (a) execute Merge(S, Πn, Πv, bn) to update Πn, (b) execute Merge(S, Πv, Πn, bv) to update Πv, (c) if Πn and Πv are unchanged, go to Step 3. 3. Construct and output a thesaurus of nouns based on the history of Πn, and one for verbs based on the history of Πv. For the sake of simplicity, let us next consider only the procedure for Merge as it is applied to the set of noun classes while the set of verb classes is ﬁxed. Merge(S, Tn, Tv, bn) 1. For each class pair in Tn, calculate the reduction in mutual information which would result from merging them. (Details of such a calculation are given below.) Discard those class pairs whose mutual information reduction is not less than the threshold of (kB  kA)  log m 2  m , (6.1) 94 CHAPTER 6. WORD CLUSTERING where m denotes total data size, kB the number of free parameters in the model before the merge, and kA the number of free parameters in the model after the merge. Sort the remaining class pairs in ascending order with respect to mutual information reduction. 2. Merge the ﬁrst bn class pairs in the sorted list. 3. Output current Tn. For improved eﬃciency, the algorithm performs a maximum of bn merges at step 2, which will result in the output of an at most bn-ary tree. Note that, strictly speaking, once we perform one merge, the model will change and there will no longer be any guarantee that the remaining merges continue to be justiﬁable from the viewpoint of MDL. Next, let us consider why the criterion formalized in terms of description length can be reformalized in terms of mutual information. Let MB refer to the pre-merge model, MA to the post-merge model. According to MDL, MA should be that model which has the least increase in data description length δLdat  L(SMA)  L(SMB)  0 and that at the same time satisﬁes δLdat  (kB  kA)  log m 2 , since the decrease in model description length equals L(MB)  L(MA)  (kB  kA)  log m 2  0, and the decrease in model description length is common to each merge. In addition, suppose that MA is obtained by merging two noun classes Ci and Cj in MB to a noun class Cij. We in fact need only calculate the diﬀerence in description lengths with respect to these classes, i.e., δLdat    CvΠv  nCij,vCv log ˆP(n, v)   CvΠv  nCi,vCv log ˆP(n, v)   CvΠv  nCj,vCv log ˆP(n, v). Since P(n, v)  P(n) P(Cn)  P(v) P(Cv)  P(Cn, Cv)  P(Cn, Cv) P(Cn)P(Cv)  P(n)  P(v) holds, we also have ˆP(n)  f(n) m , 6.3. ALGORITHM 95 ˆP(v)  f(v) m , ˆP(Cn)  f(Cn) m , and ˆP(Cv)  f(Cv) m . Hence, δLdat    CvΠv f(Cij, Cv)  log ˆP(Cij,Cv) ˆP(Cij) ˆP (Cv)   CvΠv f(Ci, Cv)  log ˆP (Ci,Cv) ˆP (Ci) ˆP (Cv)   CvΠv f(Cj, Cv)  log ˆP (Cj,Cv) ˆP (Cj) ˆP (Cv). (6.2) The quantity δLdat is equivalent to the data size times the empirical mutual information reduction. We can, therefore, say that in the current context a clustering with the least data description length increase is equivalent to that with the least mutual information decrease. Note further that in (6.2), since ˆP(Cv) is unchanged before and after the merge, it can be canceled out. Replacing the probabilities with their maximum likelihood estimates, we obtain 1 m  δLdat  1 m     CvΠv(f(Ci, Cv)  f(Cj, Cv))  log f(Ci,Cv)f(Cj,Cv) f(Ci)f(Cj)   CvΠv f(Ci, Cv)  log f(Ci,Cv) f(Ci)   CvΠv f(Cj, Cv)  log f(Cj,Cv) f(Cj)  . We need calculate only this quantity for each possible merge at Step 1 in Merge. In an implementation of the algorithm, we ﬁrst load the co-occurrence data into a matrix, with nouns corresponding to rows, verbs to columns. When merging a noun class in row i and that in row j (i  j), for each Cv, we add f(Ci, Cv) and f(Cj, Cv), obtaining f(Cij, Cv); then write f(Cij, Cv) on row i; move f(Clast, Cv) to row j. This reduces the matrix by one row. With the above implementation, the worst case time complexity of the algorithm turns out to be O(N3  V  V 3  N), where N denotes the size of the set of nouns, and V that of verbs. If we can merge bn and bv classes at each step, the algorithm will become slightly more eﬃcient, with a time complexity of O( N3 bn  V  V 3 bv  N). The method proposed in this chapter is an extension of that proposed by Brown et al. Their method iteratively merges the word class pair having the least reduction in mutual information until the number of word classes created equals a certain designated number. This method is based on MLE, but it only employs MLE locally. In general, MLE is not able to select the best model from a class of models having diﬀerent numbers of parameters because MLE will always suggest selecting the model having the largest number of parameters, which would have a better ﬁt to the given data. In Brown et als case, MLE is used to iteratively select the model with the 96 CHAPTER 6. WORD CLUSTERING maximum likelihood from a class of models that have the same number of parameters. Such a model class is repeatedly obtained by merging any word class pair in the current situation. The number of word classes within the models in the ﬁnal model class, therefore, has to be designated in advance. There is, however, no guarantee at all the designated number will be optimal. The method proposed here resolves this problem by employing MDL. This is re- ﬂected in use of the threshold (6.1) in clustering, which will result in automatic selection of the optimal number of word classes to be created. 6.4 Experimental Results 6.4.1 Experiment 1: qualitative evaluation In this experiment, I used heuristic rules to extract verbs and their arg2 slot values (direct objects) from the tagged texts of the WSJ corpus (ACLDCI CD-ROM1) which consists of 126,084 sentences. share, asset, data stock, bond, security inc. ,corp. ,co. house, home bank, group, firm price, tax money, cash car, vehicle profit, risk software, network pressure, power Figure 6.1: A part of a constructed thesaurus. I then constructed a number of thesauruses based on these data, using the method proposed in this chapter. Figure 6.1 shows a part of a thesaurus for 100 randomly se- lected nouns, that serve as direct objects of 20 randomly selected verbs. The thesaurus seems to agree with human intuition to some degree. The words stock, security, and bond are classiﬁed together, for example, despite the fact that their absolute frequen- cies are quite diﬀerent (272, 59, and 79, respectively). The results seem to demonstrate 6.4. EXPERIMENTAL RESULTS 97 one desirable feature of the proposed method: it classiﬁes words solely on the basis of the similarities in co-occurrence data and is not aﬀected by the absolute frequencies of the words. 6.4.2 Experiment 2: compound noun disambiguation I tested the eﬀectiveness of the clustering method by using the acquired word classes in compound noun disambiguation. This would determine, for example, the word base or system to which data should be attached in the compound noun triple (data, base, system). To conduct compound noun disambiguation, we can use here the probabilities ˆP(database), (6.3) ˆP(datasystem). (6.4) If the former is larger, we attach data to base; if the latter is larger we attach it to system; otherwise, we make no decision. I ﬁrst randomly selected 1000 nouns from the corpus, and extracted from the corpus compound noun doubles (e.g., (data, base)) containing the nouns as training data and compound noun triples containing the nouns as test data. There were 8604 training data and 299 test data. I also labeled the test data with disambiguation answers. I conducted clustering on the nouns in the left position in the training data, and also on the nouns in the right position, by using, respectively, both the method proposed in this chapter, denoted as 2D-Clustering, and Brown et als, denoted as Brown. I actually implemented an extended version of their method, which separately conducts clustering for nouns on the left and those on the right (which should only improve the performance). I conducted structural disambiguation on the test data, using the probabilities like those in (6.3) and (6.4), estimated on the basis of 2D-Clustering and Brown, respectively. I also tested the method of using probabilities estimated based on word occurrences, denoted here as Word-based. Figure 6.2 shows the results in terms of accuracy and coverage, where coverage refers to the percentage of test data for which the disambiguation method was able to make a decision. Since for Brown the number of word classes ﬁnally created has to be designed in advance, I tried a number of alternatives and obtained results for them (for 2D-Clustering, the optimal number of word classes is automatically selected). We see that, for Brown, when the number of word classes ﬁnally to be created is small, though the coverage will be large, the accuracy will deteriorate dramatically, indicating that in word clustering it is preferable to introduce a mechanism to automatically determine the ﬁnal number of word classes. Table 6.1 shows ﬁnal results for the above methods combined with Default in which we attach the ﬁrst noun to the neighboring noun when a decision cannot be made by an individual method. 98 CHAPTER 6. WORD CLUSTERING 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.55 0.6 0.65 0.7 0.75 0.8 0.85 Accuracy Coverage \"Word-based\" \"Brown\" \"2D-Clustering\" Figure 6.2: Accuracy-coverage plots for 2D-Clustering, Brown, and Word-based. Table 6.1: Compound noun disambiguation results. Method Accuracy() Default 59.2 Word-based  Default 73.9 Brown  Default 77.3 2D-Clustering  Default 78.3 We can see here that 2D-Clustering performs the best. These results demonstrate one desirable aspect of 2D-Clustering: its ability to automatically select the most ap- propriate level of clustering, i.e., it results in neither over-generalization nor under- generalization. (The ﬁnal result of 2D-Clustering is still not completely satisfactory, however. I think that this is partly due to insuﬃcient training data.) 6.4.3 Experiment 3: pp-attachment disambiguation I tested the eﬀectiveness of the proposed method by using the acquired classes in pp-attachment disambiguation involving quadruples (v, n1, p, n2). As described in Chapter 3, in disambiguation of (eat, ice-cream, with, spoon), we can perform disambiguation by comparing the probabilities ˆPwith(spooneat), (6.5) ˆPwith(spoonice cream). (6.6) 6.4. EXPERIMENTAL RESULTS 99 If the former is larger, we attach with spoon to eat; if the latter is larger we attach it to ice-cream; otherwise, we make no decision. I used the ten sets used in Experiment 2 in Chapter 4, and conducted experiments through ten-fold cross validation, i.e., all of the experimental results reported below were obtained from averages taken over ten trials. Table 6.2: PP-attachment disambiguation results. Method Coverage() Accuracy() Default 100 56.2 Word-based 32.3 95.6 Brown 51.3 98.3 2D-Clustering 51.3 98.3 WordNet 74.3 94.5 1D-Thesaurus 42.6 97.1 I conducted word clustering by using the method proposed in this chapter, de- noted as 2D-Clustering, and the method proposed in (Brown et al., 1992), denoted as Brown. In accord with the proposal oﬀered by (Tokunaga, Iwayama, and Tanaka, 1995), for both methods, I separately conducted clustering with respect to each of the 10 most frequently occurring prepositions (e.g., for, with, etc). I did not cluster words with respect to rarely occurring prepositions. I then performed disambiguation by using probabilities estimated based on 2D-Clustering and Brown. I also tested the method of using the probabilities estimated based on word co-occurrences, denoted here as Word-based. Next, rather than using the co-occurrence probabilities estimated by 2D-Clustering, I only used the noun thesauruses constructed by 2D-Clustering, and applied the method of estimating the best tree cut models within the thesauruses in order to estimate conditional probabilities like those in (6.5) and (6.6). I call this method 1D-Thesaurus. Table 6.2 shows the results for all these methods in terms of coverage and accuracy. It also shows the results obtained in the experiment described in Chapter2, denoted here as WordNet. I then enhanced each of these methods by using a default decision of attaching (p, n2) to n1 when a decision cannot be made. This is indicated as Default. Table 6.3 shows the results of these experiments. We can make a number of observations from these results. (1) 2D-Clustering achieves broader coverage than does 1D-Thesaurus. This is because, in order to esti- mate the probabilities for disambiguation, the former exploits more information than the latter. (2) For Brown, I show here only its best result, which happens to be the same as the result for 2D-Clustering, but in order to obtain this result I had to take the trouble of conducting a number of tests to ﬁnd the best level of clustering. For 100 CHAPTER 6. WORD CLUSTERING Table 6.3: PP-attachment disambiguation results. Method Accuracy() Word-based  Default 69.5 Brown  Default 76.2 2D-Clustering  Default 76.2 WordNet  Default 82.2 1D-Thesaurus  Default 73.8 2D-Clustering, this needed to be done only once and could be done automatically. (3) 2D-Clustering outperforms WordNet in term of accuracy, but not in terms of coverage. This seems reasonable, since an automatically constructed thesaurus is more domain dependent and therefore captures the domain dependent features better, thus helping achieve higher accuracy. On the other hand, with the relatively small size of training data we had available, its coverage is smaller than that of a general purpose hand-made thesaurus. The result indicates that it makes sense to combine the use of automatically constructed thesauruses with that of a hand-made thesaurus. I will describe such a method and the experimental results with regard to it in Chapter 7. 6.5 Summary I have described in this chapter a method of clustering words. That is a natural extension of Brown et als method. Experimental results indicate that it is superior to theirs. The proposed clustering algorithm, 2D-Clustering, can be used in practice so long as the data size is at the level of the current Penn Tree Bank. It is still relatively com- putationally demanding, however, and the important work of improving its eﬃciency remains to be performed. The method proposed in this chapter is not limited to word clustering; it can be applied to other tasks in natural language processing and related ﬁelds, such as, document classiﬁcation (cf., (Iwayama and Tokunaga, 1995)). Chapter 7 Structural Disambiguation To have good fruit you must have a healthy tree; if you have a poor tree you will have bad fruit. - The Gospel according to Matthew In this chapter, I propose a practical method for pp-attachment disambiguation. This method combines the use of the hard co-occurrence model with that of the tree cut model. 7.1 Procedure Let us consider here the problem of structural disambiguation, in particular, the prob- lem of resolving pp-attachment ambiguities involving quadruples (v, n1, p, n2), such as (eat, ice-cream, with, spoon). As described in Chapter 6, we can resolve such an ambiguity by using probabilities estimated on the basis of hard co-occurrence models. I denote them as ˆPhcm(spooneat, with), ˆPhcm(spoonice cream, with). Further, as described in Chapter 4, we can also resolve the ambiguity by using proba- bilities estimated on the basis of tree cut models with respect to a hand-made thesaurus, denoted as ˆPtcm(spooneat, with), ˆPtcm(spoonice cream, with). Both methods are a class-based approach to disambiguation, and thus can help to handle the data sparseness problem. The former method is based on corpus data and thus can capture domain speciﬁc features and achieve higher accuracy. At the same time, since corpus data is never suﬃciently large, coverage is bound to be less 101 102 CHAPTER 7. STRUCTURAL DISAMBIGUATION than satisfactory. By way of contrast, the latter method is based on human-deﬁned knowledge and thus can bring about broader coverage. At the same time, since the knowledge used is not domain-speciﬁc, accuracy might be expected to be less than satisfactory. Since both methods have pros and cons, it would seem be better to combine the two, and I propose here a back-oﬀ method to do so. In disambiguation, we ﬁrst use probabilities estimated based on hard co-occurrence models; if the probabilities are equal (particularly both of them are 0), we use prob- abilities estimated based on tree cut models with respect to a hand-made thesaurus; if the probabilities are still equal, we make a default decision. Figure 7.1 shows the procedure of this method. Procedure: 1. Take (v, n1, p, n2) as input; 2. if 3. ˆPhcm(n2v, p)  ˆPhcm(n2n1, p) 4. then 5. attach (p, n2) to v; 6. else if 7. ˆPhcm(n2v, p)  ˆPhcm(n2n1, p) 8. then 9. attach (p, n2) to n1; 10. else 11. if 12. ˆPtcm(n2v, p)  ˆPtcm(n2n1, p) 13. then 14. attach (p, n2) to v; 15. else if 16. ˆPtcm(n2v, p)  ˆPtcm(n2n1, p) 17. then 18. attach (p, n2) to n1; 19. else 20. make a default decision. Figure 7.1: The disambiguation procedure. 7.2. AN ANALYSIS SYSTEM 103 7.2 An Analysis System Let us consider this disambiguation method in more general terms. The natural lan- guage analysis system that implements the method operates on the basis of two pro- cesses: a learning process and an analysis process. During the learning process, the system takes natural language sentences as input and acquires lexical semantic knowledge. First, the POS (part-of-speech) tagging mod- ule uses a probabilistic tagger (cf., Chapter 2) to assign the most likely POS tag to each word in the input sentences. The word sense disambiguation module then employs a probabilistic model (cf., Chapter 2) to resolve word sense ambiguities. Next, the case frame extracting module employs a heuristic method (cf., Chapter 2) to extract case frame instances. Finally, the learning module acquires lexical semantic knowledge (case frame patterns) on the basis of the case frame instances. During the analysis process, the system takes a sentence as input and outputs a most likely interpretation (or several most likely interpretations). The POS tagging module assigns the most likely tag to each word in the input sentence, as is in the case of learning. The word sense disambiguation module then resolves word sense ambiguities, as is in the case of learning. The parsing module then analyzes the sentence. When ambiguity arises, the structural disambiguation module refers to the acquired knowl- edge, calculates the likelihood values of the ambiguous interpretations (case frames) and selects the most likely interpretation as the analysis result. Figure 7.2 shows an outline of the system. Note that while for simplicity the parsing process and the disambiguation process are separated into two modules, they can (and usually should) be uniﬁed into one module. Furthermore, for simplicity some other knowledge necessary for natural language analysis, e.g., a grammar, has also been omitted from the ﬁgure. The learning module consists of two submodules: a thesaurus construction submod- ule, and a case slot generalization submodule. The thesaurus construction submodule employs the hard co-occurrence model to calculate probabilities. The case slot gener- alization submodule then employs the tree cut model to calculate probabilities. The structural disambiguation module refers to the probabilities, and calculates likelihood for each interpretation. The likelihood values based on the hard co-occurrence model for the two interpretations of the sentence (1.1) are calculated as follows Lhcm(1)  ˆPhcm(Ieat, arg1)  ˆPhcm(ice creameat, arg2)  ˆPhcm(spooneat, with) Lhcm(2)  ˆPhcm(Ieat, arg1)  ˆPhcm(ice creameat, arg2)  ˆPhcm(spoongirl, with). The likelihood values based on the tree cut model can be calculated analogously. Fi- nally, the disambiguation module selects the most likely interpretation on the basis of a back-oﬀ procedure like that described in Section 1. Note that in its current state of development, the disambiguation module is still unable to exploit syntactic knowledge. As described in Chapter 2, disambiguation 104 CHAPTER 7. STRUCTURAL DISAMBIGUATION POS Tagging Module Word Sense Disambiguation Module Parsing Module Structural Disambiguation Module POS Tagging Module Word Sense Disambiguation Module Case Frame Extracting Module Learning Module Lexical Semantic Knowledge Sentence Most likely interpretation Sentences Figure 7.2: Outline of the natural language analysis system. decisions may not be made solely on the basis of lexical knowledge; it is necessary to utilize syntactic knowledge as well. Further study is needed to determine how to deﬁne a uniﬁed model which combines both lexical knowledge and syntactic knowledge. In terms of syntactic factors, we need to consider psycholinguistic principles, e.g., the right association principle. I have found in my study that using a probability model embodying these principles helps improve disambiguation results (Li, 1996). Another syntactic factor we need to take into consideration is the likelihood of the phrase structure of an interpretation (cf., (Charniak, 1997; Collins, 1997; Shirai et al., 1998)). 7.3 Experimental Results I tested the proposed disambiguation method by using the data used in Chapters 4 and 6. Table 7.1 shows the results; here the method is denoted as 2D-ClusteringWordNetDefault. Table 7.1 also shows the results of WordNetDefault and TEL which were described in Chapter 4, and the result of 2D-ClusteringDefault which was described in Chapter 6. We see that the disambiguation method proposed in this chapter performs the best of four. Table 7.2 shows the disambiguation results reported in other studies. Since the 7.3. EXPERIMENTAL RESULTS 105 Table 7.1: PP-attachment disambiguation results. TEL 82.4 2D-Clustering  Default 76.2 WordNet  Default 82.2 2D-Clustering  WordNet  Default 85.2 data sets used in the respective studies were diﬀerent, a straightforward comparison of the various results would have little signiﬁcance, we may say that the method proposed in this chapter appears to perform relatively well with respect to other state-of-the-art methods. Table 7.2: Results reported in previous work. Method Data Accuracy () (Hindle and Rooth, 1991) AP News 78.3 (Resnik, 1993a) WSJ 82.2 (Brill and Resnik, 1994) WSJ 81.8 (Ratnaparkhi, Reynar, and Roukos, 1994) WSJ 81.6 (Collins and Brooks, 1995) WSJ 84.5 106 CHAPTER 7. STRUCTURAL DISAMBIGUATION Chapter 8 Conclusions If all I know is a fraction, then my only fear is of losing the thread. - Lao Tzu 8.1 Summary The problem of acquiring lexical semantic knowledge is an important issue in natural language processing, especially with regard to structural disambiguation. The approach I have adopted here to this problem has the following characteristics: (1) dividing the problem into three subproblems: case slot generalization, case dependency learning, and word clustering, (2) viewing each subproblem as that of statistical estimation and deﬁning probability models (probability distributions) for each subproblem, (3) adopting MDL as a learning strategy, (4) employing eﬃcient learning algorithms, and (5) viewing the disambiguation problem as that of statistical prediction. Major contributions of this thesis include: (1) formalization of the lexical knowl- edge acquisition problem, (2) development of a number of learning methods for lexi- cal knowledge acquisition, and (3) development of a high-performance disambiguation method. Table 8.1 shows the models I have proposed, and Table 8.2 shows the algorithms I have employed. The overall accuracy achieved by the pp-attachment disambiguation method is 85.2, which is better than that of state-of-the-art methods. 8.2 Open Problems Lexical semantic knowledge acquisition and structural disambiguation are diﬃcult tasks. Although I think that the investigations reported in this thesis represent some signiﬁcant progress, further research on this problem is clearly still needed. 107 108 CHAPTER 8. CONCLUSIONS Table 8.1: Models proposed. Purpose Basic model Restricted model case slot generalization case slot model tree cut model (hard, soft) case dependency learning case frame model dependency forest model (word-based, class-based, slot-based) word clustering co-occurrence model hard co-occurrence model (hard, soft) Table 8.2: Algorithm employed. Purpose Algorithm Time complexity case slot generalization Find-MDL O(N) case dependency learning Suzukis algorithm O(n2(k2  n2)) word clustering 2D-Clustering O(N3  V  V 3  N) Other issues not investigated in this thesis and some possible solutions include: More complicated models In the discussions so far, I have restricted the class of hard case slot models to that of tree cut models for an existing thesaurus tree. Under this restriction, we can employ an eﬃcient dynamic-programming-based learning algorithm which can provablely ﬁnd the optimal MDL model. In prac- tice, however, the structure of a thesaurus may be a directed acyclic graph (DAG) and straightforwardly extending the algorithm to a DAG may no longer guaran- tee that the optimal model will be found. The question now is whether there exist sub-optimal algorithms for more complicated model classes. The same problem arises in case dependency learning, for which I have restricted the class of case frame models to that of dependency forest models. It would be more appropriate, however, to restrict the class to, for example, the class of normal Bayesian Net- works. How to learn such a complicated model, then, needs further investigation. Uniﬁed model I have divided the problem of learning lexical knowledge into three subproblems for easy examination. It would be more appropriate to deﬁne a single uniﬁed model. How to deﬁne such a model, as well as how to learn it, are issues for future research. (See (Miyata, Utsuro, and Matsumoto, 1997; Utsuro and Matsumoto, 1997) for some recent progress on this issue; see also discussions in Chapter 3.) 8.2. OPEN PROBLEMS 109 Combination with extraction We have seen that the amount of data currently available is generally far less than that necessary for accurate learning, and the problem of how to collect suﬃcient data may be expected to continue to be a crucial issue. One solution might be to employ bootstrapping, i.e., to conduct ex- traction and generalization, iteratively. How to combine the two processes needs further examination. Combination with word sense disambiguation I have not addressed the word sense ambiguity problem in this thesis, simply proposing to conduct word sense disambiguation in pre-processing. (See (McCarthy, 1997) for her proposal on word sense disambiguation.) In order to improve the disambiguation results, however, it would be better to employ the soft case slot model to perform struc- tural and word sense disambiguation at the same time. How to eﬀectively learn such a model requires further work. Soft clustering I have formalized the problem of constructing a thesaurus into that of learning a double mixture model. How to eﬃciently learn such a model is still an open problem. Parsing model The use of lexical knowledge alone in disambiguation might result in the resolving of most of the ambiguities in sentence parsing, but not all of them. As has been described, one solution to the problem might be to deﬁne a uniﬁed model combining both lexical knowledge and syntactic knowledge. The problem still requires further work. 110 References References [Abe and Li1996] Abe, Naoki and Hang Li. 1996. Learning word association norms using tree cut pair models. Proceedings of the 13th International Conference on Machine Learning, pages 311. [Abe, Li, and Nakamura1995] Abe, Naoki, Hang Li, and Atsuyoshi Nakamura. 1995. On-line learning of binary lexical relations using two-dimensional weighted majority algorithms. Proceedings of the 12th International Conference on Machine Learning, pages 311. [Almuallim et al.1994] Almuallim, Hussein, Yasuhiro Akiba, Takefumi Yamazaki, Akio Yokoo, and Shigeo Kaneda. 1994. Two methods for ALT-JE translation rules from examples and a semantic hierarchy. Proceedings of the 15th International Conference on Computational Linguistics, pages 5763. [Alshawi and Carter1994] Alshawi, Hiyan and David Carter. 1994. Training and scaling preference functions for disambiguation. Computational Linguistics, 20(4):635648. [Altmann and Steedman1988] Altmann, Gerry and Mark Steedman. 1988. Interaction with context during human sentence processing. Cognition, 30:191238. [Bahl, Jelinek, and Mercer1983] Bahl, Lalit R., Frederick Jelinek, and Robert Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transaction on Pattern Analysis and Machine Intelligence, 5(2):179190. [Barron and Cover1991] Barron, Andrew R. and Thomas M. Cover. 1991. Mini- mum complexity density estimation. IEEE Transaction on Information Theory, 37(4):10341054. [Berger, Pietra, and Pietra1996] Berger, Adam L., Stephen J. Della Pietra, and Vin- cent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):3971. [Black1988] Black, Ezra. 1988. An experiment in computational discrimination of En- glish word senses. IBM Journal of Research and Development, 32(2):185193. [Black et al.1993] Black, Ezra, Fred Jelinek, John Laﬀerty, and David M. Magerman. 1993. Towards history-based grammars: Using richer models for probabilistic pars- ing. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 3137. [Brent1991] Brent, Michael R. 1991. Automatic acquisition of subcategorization frames from untagged text. Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 209214. References 111 [Brent1993] Brent, Michael R. 1993. From grammar to lexicon: Unsupervised learning of lexical syntax. Computational Linguistics, 19(2):243262. [Brent and Cartwright1996] Brent, Michael R. and Timothy A. Cartwright. 1996. Dis- tributional regularity and phonotactic constraints are useful for segmentation. Cog- nition, 61:93125. [Brent, Murthy, and Lundberg1995] Brent, Michael R., Sreerama K. Murthy, and An- drew Lundberg. 1995. Discovering morphemic suﬃxes: A case study in minimum description length induction. Proceedings of the 5th International Workshop on Artiﬁcial Intelligence and Statistics. [Brill1995] Brill, Eric. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Lin- guistics, 21(4):543565. [Brill and Resnik1994] Brill, Eric and Philip Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. Proceedings of the 15th Interna- tional Conference on Computational Linguistics, pages 11981204. [Briscoe and Carroll1993] Briscoe, Ted and John Carroll. 1993. Generalized proba- bilistic LR parsing of natural language (corpora) with uniﬁcation-based grammars. Computational Linguistics, 19(1):2559. [Briscoe and Carroll1997] Briscoe, Ted and John Carroll. 1997. Automatic extraction of subcategorization from corpora. Proceedings of the 5th Conference on Applied Natural Language Processing. [Brown et al.1991] Brown, Peter, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1991. Word sense disambiguation using statistical methods. Pro- ceedings of Annual the 29th Meeting of the Association for Computational Linguis- tics, pages 264270. [Brown et al.1992] Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):283298. [Bruce and Wiebe1994] Bruce, Rebecca and Janyce Wiebe. 1994. Word-sense disam- biguation using decomposable models. Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 139145. [Carter1997] Carter, David. 1997. The treebanker: a tool for supervised training of parsed corpora. Proceedings of the Computational Environments for Grammar De- velopment. 112 References [Cartwright and Brent1994] Cartwright, Timothy A. and Michael R. Brent. 1994. Seg- menting speech without a lexicon: The roles of phonotactics and speech source. Proceedings of the 1st Meeting of the ACL Special Interest Group in Computational Phonology, pages 8390. [Chang, Luo, and Su1992] Chang, Jing-Shin, Yih-Fen Luo, and Keh-Yih Su. 1992. GPSM: A generalized probabilistic semantic model for ambiguity resolution. Pro- ceedings of the 30th Annual Meeting of the Association for Computational Linguis- tics, pages 177184. [Charniak1997] Charniak, Eugene. 1997. Statistical parsing with a context-free gram- mar and word statistics. Proceedings of the 15th National Conference on Artiﬁcial Intelligency. [Charniak et al.1993] Charniak, Eugene, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz. 1993. Equations for part-of-speech tagging. Proceedings of the 11th National Conference on Artiﬁcial Intelligency, pages 784789. [Chen and Chen1994] Chen, Kuang-hua and Hisn-Hsi Chen. 1994. Extracting noun phrases from large-scale texts: A hybrid approach and its automatic evaluation. Proceedings of the 32nd Annual Meeting of the Association for Computational Lin- guistics, pages 234241. [Chiang, Lin, and Su1995] Chiang, Tung-Hui, Yi-Chung Lin, and Keh-Yih Su. 1995. Robust learing, smoothing, and parameter tying on syntactic ambiguity resolution. Computational Linguistics, 21(3):321349. [Chitrao and Grishman1990] Chitrao, Mahesh V. and Ralph Grishman. 1990. Statis- tical parsing of messages. Proceedings of DARPA Speech and Natural Language Workshop, pages 263266. [Chow and Liu1968] Chow, C.K. and C.N. Liu. 1968. Approximating discrete prob- ability distributions with dependence trees. IEEE Transactions on Information Theory, 14(3):462467. [Church et al.1989] Church, Kenneth, William Gale, Patrick Hanks, and Donald Hin- dle. 1989. Parsing, word associations and typical predicate-argument relations. Proceedings of the International Workshop on Parsing Technology 89, pages 389 398. [Church et al.1991] Church, Kenneth, William Gale, Patrick Hanks, and Donald Hindle. 1991. Using statistics in lexical analysis. In Uri Zernik, editor, Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon. Lawrence ErlBaum Associates, Hillsdale, pages 115164. References 113 [Church and Hanks1989] Church, Kenneth and Patrick Hanks. 1989. Word associa- tion norms, mutual information, and lexicography. Proceedings of the 27th Annual Meeting of Association for Computational Linguistics, pages 7683. [Church1988] Church, Kenneth. W. 1988. A stochastic parts program and noun phrase parser for unrestricted text. Proceedings of the 2nd Conference on Applied Natural Language Processing, pages 136143. [Clarke and Barron1990] Clarke, Bertrand S. and Andrew R. Barron. 1990. Information-theoretic asymptotics of Bayes methods. IEEE Transaction on In- formation Theory, 36(3):453471. [Collins1996] Collins, Michael. 1996. A new statistical parser based on bigram lexi- cal dependencies. Proceedings of the 34th Annual Meeting of the Association for Computation al Linguistics, pages 184191. [Collins1997] Collins, Michael. 1997. Three generative lexicalized models for statistical parsing. Proceedings of the 35th Annual Meeting of the Association for Computation al Linguistics, pages 1623. [Collins and Brooks1995] Collins, Michael and James Brooks. 1995. Prepositional phrase attachment through a backed-oﬀ model. Proceedings of the 3rd Workshop on Very Large Corpora. [Cooper and Herskovits1992] Cooper, Gregory F. and Edward Herskovits. 1992. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9:309347. [Cover and Thomas1991] Cover, Thomas M. and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley  Sons Inc., New York. [Cucchiarelli and Velardi1997] Cucchiarelli, Alessandro and Paola Velardi. 1997. Auto- matic selection of class labels from a thesaurus for an eﬀective semantic tagging of corpora. Proceedings of the 5th Conference on Applied Natural Language Process- ing, pages 380387. [Dagan, Marcus, and Makovitch1992] Dagan, Ido, Shaul Marcus, and Shaul Makovitch. 1992. Contextual word similarity and estimation from sparse data. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 164171. [Dagan, Pereira, and Lee1994] Dagan, Ido, Fernando Pereira, and Lillian Lee. 1994. Similarity-based estimation of word cooccurrence probabilities. Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 272 278. 114 References [Dagan, Pereira, and Lee1997] Dagan, Ido, Fernando Pereira, and Lillian Lee. 1997. Similarity-based methods for word sense disambiguation. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 5663. [Darroch and Ratcliﬀ1972] Darroch, J. N. and D. Ratcliﬀ. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43(5):17401480. [Dempster, Laird, and Rubin1977] Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):138. [Den1996] Den, Yasuharu. 1996. A Uniform Approach ot Spoken Language Analysis. Ph.D. Thesis, Kyoto Univ. [Ellison1991] Ellison, T. Mark. 1991. Discovering planar segregations. Proceedings of AAAI Spring Symposium on Machine Learning of Natural Language and Ontology, pages 4247. [Ellison1992] Ellison, T. Mark. 1992. Discovering vowel harmony. In Walter Daelmans and David Powers, editors, Background and Experiments in Machine Learning of Natural Language. pages 205207. [Fillmore1968] Fillmore, C. 1968. The case for case. In E. Bach and R. Harms, editors, Universals in Linguistics theory. Holt, RineHart, and Winston, New York. [Fisher1956] Fisher, R.A. 1956. Statistical Methods and Scientiﬁc Inference. Olyver and Boyd. [Ford, Bresnan, and Kaplan1982] Ford, Marylyn, Joan Bresnan, and Ronald Kaplan. 1982. A competence based theory of syntactic closure. The Mental Representation of Grammatical Relations. [Framis1994] Framis, Francesc Ribas. 1994. An experiment on learning appropriate selectional restrictions from a parsed corpus. Proceedings of the 15th International Conference on Computational Linguistics, pages 769774. [Frazier and Fodor1979] Frazier, Lyn and Janet Fodor. 1979. The sausage machine: A new two-stage parsing model. Cognition, 6:291325. [Fujii et al.1996] Fujii, Atsushi, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1996. To what extent does case contribute to verb sense disambiguation. Proceedings of the 16th International Conference on Computational Linguistics, pages 5964. [Fujisaki et al.1989] Fujisaki, T., F. Jelinek, J. Cocke, E. Black, and T. Nishino. 1989. A probabilistic parsing method for sentence disambiguation. Proceedings of the International Workshop on Parsing Technology 89, pages 8594. References 115 [Gale, Church, and Yarowsky1992] Gale, William, Kenneth Ward Church, and David Yarowsky. 1992. Estimating upper and lower bounds on the performance of word- sense disambiguation programs. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 249256. [Gale and Church1990] Gale, Williams A. and Kenneth W. Church. 1990. Poor es- timates of context are worse than none. Proceedings of the DARPA Speech and Natural Language Workshop, pages 283287. [Gelfand and Smith1990] Gelfand, Alan E. and Adrian F.M. Smith. 1990. Sampling- based approach to calculating marginal densities. Journal of the American Statis- tical Association, 85(40):398409. [Geman and Geman1984] Geman, Stuart and Donald Geman. 1984. Stochastic relax- ation, Gibbs distributions and the Bayes restoration of images. IEEE Tran. on Pattern Analysis and Machine Intelligence, PAMI-6(6):721741. [Golding and Roth1996] Golding, Andrew R. and Dan Roth. 1996. Applying winnow to context-sensitive spelling correction. Proceedings of the 13th International Con- ference on Machine Learning, pages 182190. [Golding and Schabes1996] Golding, Andrew R. and Yves Schabes. 1996. Combining trigram-based and feature-based methods for context-sensitive spelling correction. Proceedings of the 34th Annual Meeting of the Association for Computational Lin- guistics, pages 7178. [Grefenstette1994] Grefenstette, Gregory. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston. [Grishman and Sterling1992] Grishman, Ralph and John Sterling. 1992. Acquisition of selectional patterns. Proceedings of the 14th International Conference on Compu- tational Linguistics, pages 658664. [Grishman and Sterling1994] Grishman, Ralph and John Sterling. 1994. Generalizing automatically generated selectional patterns. Proceedings of the 15th International Conference on Computational Linguistics, pages 742747. [Grunwald1996] Grunwald, Peter. 1996. A minimum description length approach to grammar inference. In S. Wemter, E. Riloﬀ, and G. Scheler, editors, Symbolic, Connectionist and Statistical Approaches to Learning for Natural Language Pro- cessing, Lecture Note in AI. Springer Verlag, pages 203216. [Guthrie et al.1991] Guthrie, Joe A., Louise Guthrie, Yorick Wilks, and Homa Aidine- jad. 1991. Subject-dependent co-occurrence and word sense disambiguation. Pro- ceedings of the 29th Annual Meeting of the Association for Computational Linguis- tics, pages 146152. 116 References [Han and Kobayashi1994] Han, Te Sun and Kingo Kobayashi. 1994. Mathematics of Information and Coding. Iwanami Shoten Publishers, Tokyo. [Haruno and Matsumoto1997] Haruno, Masahiko and Yuji Matsumoto. 1997. Mistake- driven mixture of hierarchical tag context trees. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 230237. [Haruno, Shirai, and Ooyama1998] Haruno, Masahiko, Satoshi Shirai, and Yoshifumi Ooyama. 1998. Using decision trees to construct a practical parser. Machine Learning (to appear). [Hastings1970] Hastings, W.K. 1970. Monte Carlo sampling method using Markov chains and their applications. Biometrika, 57:97109. [Helmbold et al.1995] Helmbold, David P., Robert E. Schapire, Yoram Singer, and Man- fred K. Warmuth. 1995. A comparison of new and old algorithm for a mixture estimation problem. Proceedings of the 8th Annual Conference on Computational Learning Theory, pages 6978. [Hindle1990] Hindle, Donald. 1990. Noun classiﬁcation from predicate-argument struc- tures. Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 268275. [Hindle and Rooth1991] Hindle, Donald and Mats Rooth. 1991. Structural ambiguity and lexical relations. Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 229236. [Hindle and Rooth1993] Hindle, Donald and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103120. [Hobbs and Bear1990] Hobbs, Jerry R. and John Bear. 1990. Two principles of parse preference. Proceedings of the 13th International Conference on Computational Linguistics, pages 162167. [Hogenhout and Matsumoto1996] Hogenhout, Wide R. and Yuji Matsumoto. 1996. Training stochastical grammars on semantical categories. In Stefan Wermter, Ellen Riloﬀ, and Gabriele Scheler, editors, Connectionist, Statistical and Symbolic Ap- proaches to Learning for Natural Language Processing. Springer. [Hogenhout and Matsumoto1997] Hogenhout, Wide R. and Yuji Matsumoto. 1997. A preliminary study of word clustering based on syntactic behavior. CoNLL97:Computational Natural Language Learning, Proceedings of the 1997 Meeting of the ACL Special Interest Group in Natural Language Learning, pages 1624. References 117 [Inui, Sornlertlamvanich, and Tanaka1998] Inui, Kentaro, Virach Sornlertlamvanich, and Hozumi Tanaka. 1998. Probabilistic GLR parsing: A new formalization and its impact on parsing performance. Journal of Natural Language Processing, 5(3):33 52. [Iwayama and Tokunaga1995] Iwayama, Makoto and Takenobu Tokunaga. 1995. Cluster-based text categorization: A comparison of category search strategy. Pro- ceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 273280. [Jaynes1978] Jaynes, E. T. 1978. Where do we stand on maximum entropy? In R. D. Levine and M. Tribus, editors, The Maximum Entropy Formalism. MIT Press. [Jeﬀreys1961] Jeﬀreys, H. 1961. Theory of Probability. Oxford Univ. Press. [Jelinek, Laﬀerty, and Mercer1990] Jelinek, F., J. D. Laﬀerty, and R. L. Mercer. 1990. Basic methods of probabilistic context free grammars. IBM Research Report, RC 16374. [Jelinek and Mercer1980] Jelinek, F. and R. I. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In E.S. Gelseman and L.N. Kanal, editors, Pattern Recognition in Practice. North-Holland Publishing Company, pages 381397. [Johnson-Laird1983] Johnson-Laird, P. N. 1983. Mental Model: Towards a Cognitive Science of Language, Inference, and Consciousness. Harvard Univ. Press. [Katz and Fodor1963] Katz, J. J. and J. A. Fodor. 1963. The structure of semantic theory. Language, 39:170210. [Katz1987] Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, ASSP-35(3):400401. [Kimball1973] Kimball, John. 1973. Seven principles of surface structure parsing in natural language. Cognition, 2(1):1547. [Kirkpatrick, Gelatt, and Vecchi1983] Kirkpatrick, S., C. D. Gelatt, and M.P. Vecchi. 1983. Optimization by simulated annealing. Science, 220:671680. [Kita1992] Kita, Kenji. 1992. A Study on Language Modeling for Speech Recognition. Ph.D Thesis, Waseda Univ. [Knuth1973] Knuth, Donald E. 1973. The Art of Computer Programming (Second Edition). Addison-Wesley Publishing Company. 118 References [Krichevskii and Troﬁmov1981] Krichevskii, R. E. and V. K. Troﬁmov. 1981. The per- formance of universal coding. IEEE Transaction on Information Theory, 27(2):199 207. [Kupiec1992] Kupiec, Julian M. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6:225242. [Kurohashi and Nagao1994] Kurohashi, Sadao and Makoto Nagao. 1994. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20(4):507534. [Lakoﬀ1987] Lakoﬀ, George. 1987. Women, Fire, and Dangerous Things: What Cate- gories Reveal about the Mind. The University of Chicago Press. [Lari and Young1990] Lari, K. and S.J. Young. 1990. The estimation of stochastic context free grammars using the inside-outside algorithm. Computer Speech and Language, 4:3556. [Leacock, Towell, and Voorhees1993] Leacock, Claudia, Geoﬀrey Towell, and Ellen Voorhees. 1993. Corpus-based statistical sense resolution. Proceedings of ARPA Workshop on Human Language Technology. [Li1996] Li, Hang. 1996. A probabilistic disambiguation method based on psycholin- guistic principles. Proceedings of the 4th Workshop on Very Large Corpora, pages 141154. [Li and Abe1996] Li, Hang and Naoki Abe. 1996. Clustering words with the MDL principle. Proceedings of the 16th International Conference on Computational Lin- guistics, pages 49. [Li and Abe1997] Li, Hang and Naoki Abe. 1997. Clustering words with the MDL principle. Journal of Natural Language Processing, 4(2):7188. [Li and Yamanishi1997] Li, Hang and Kenji Yamanishi. 1997. Document classiﬁcation using a ﬁnite mixture model. Proceedings of the 35th Annual Meeting of Association for Computational Linguistics, pages 7188. [Littlestone1988] Littlestone, Nick. 1988. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285318. [Littlestone and Warmuth1994] Littlestone, Nick and Manfred K. Warmuth. 1994. The weighted majority algorithm. Information and Computation, 108:212261. [Magerman1994] Magerman, David M. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. Thesis, Stanford Univ. References 119 [Magerman1995] Magerman, David M. 1995. Statistical decision-tree models for pars- ing. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 276283. [Magerman and Marcus1991] Magerman, David M. and Mitchell P. Marcus. 1991. Pearl: A probabilistic chart parser. Proceedings of the International Workshop on Parsing Technology 91, pages 193199. [Manning1992] Manning, Christopher D. 1992. Automatic acquisition of a large sub- categorization dictionary from corpora. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 235242. [Marcus, Santorini, and Marcinkiewicz1993] Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of En- glish: The Penn Treebank. Computational Linguistics, 19(1):313330. [Matsumoto, Ishimoto, and Utsuro1993] Matsumoto, Yuji, Hiroyuki Ishimoto, and Takehito Utsuro. 1993. Structural matching of parallel texts. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 2330. [McCarthy1997] McCarthy, Diana. 1997. Word sense disambiguation for acquisition of selectional preferences. Proceedings of ACLEACL Workshop on Automatic Infor- mation Extraction and Building of Lexical Semantic Resources for NLP Applica- tions, pages 5260. [McKeown and Hatzivassiloglou1993] McKeown, Kathleen and Vasileios Hatzivas- siloglou. 1993. Augmenting lexicons automatically: Clustering semantically related adjectives. Proceedings of ARPA Workshop on Human Language Technology. [McMahon and Smith1996] McMahon, John C. and Francis J. Smith. 1996. Improving statistical language model performance with automatically generated word hierar- chies. Computational Linguistics, 22(2):217247. [McRoy1992] McRoy, Susan W. 1992. Using multiple knowledge sources for word sense discrimination. Computational Linguistics, 18(1):130. [Merialdo1994] Merialdo, Bernard. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155171. [Miller1995] Miller, George A. 1995. WordNet: A lexical database for English. Com- munications of the ACM, pages 3941. [Miyata, Utsuro, and Matsumoto1997] Miyata, Takashi, Takehito Utsuro, and Yuji Matsumoto. 1997. Bayesian network models of subcategorization and their MDL- based learning from corpus. Proceedings of the 4th Natural Language Processing Paciﬁc Rim Symposium, pages 321326. 120 References [Nagao1990] Nagao, Katashi. 1990. Dependency analyzer: A knowledge-based ap- proach to structural disambiguation. Proceedings of the 13th International Confer- ence on Computational Linguistics, 2:282287. [Nagata1994] Nagata, Masaaki. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A n-best search algorithm. Proceedings of the 15th International Conference on Computational Linguistics, pages 201207. [Ng and Lee1996] Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multiple knowl- edge sources to disambiguate word sense: An examplar-based approach. Proceed- ings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 4047. [Niwa and Nitta1994] Niwa, Yoshiki and Yoshihiko Nitta. 1994. Co-occurrence vec- tors from corpora vs. distance vectors from dictionaries. Proceedings of the 14th International Conference on Computational Linguistics, pages 304309. [Nomiyama1992] Nomiyama, Hiroshi. 1992. Machine translation by case generalization. Proceedings of the 14th International Conference on Computational Linguistics, pages 714720. [Pearl1988] Pearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Net- works of Plausible Inference. Morgan Kaufmann Publishers Inc., San Mateo, Cali- fornia. [Pereira and Singer1995] Pereira, Fernando and Yoram Singer. 1995. Beyond word n-grams. Proceedings of the 3rd Workshop on Very Large Corpora, pages 183190. [Pereira and Tishby1992] Pereira, Fernando and Naftali Tishby. 1992. Distributional clustering, phase transitions and hierarchical clustering. Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 108112. [Pereira, Tishby, and Lee1993] Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183190. [Pollard and Sag1987] Pollard, C. and I. A. Sag. 1987. Information-Based Syntax and Semantics. Volume 1: Syntax. CSLI Lecture Notes 13. Chicago Univ. Press. [Quinlan and Rivest1989] Quinlan, J. Ross and Ronald L. Rivest. 1989. Inferring deci- sion trees using the minimum description length principle. Information and Com- putation, 80:227248. [Ratnaparkhi1996] Ratnaparkhi, Adwait. 1996. A maximum entroy part-of-speech tag- ger. Proceedings of 1st Conference on Empirical Methods in Natural Language Processing. References 121 [Ratnaparkhi1997] Ratnaparkhi, Adwait. 1997. A linear observed time statistical parser based on maximum models. Proceedings of 2nd Conference on Empirical Methods in Natural Language Processing. [Ratnaparkhi, Reynar, and Roukos1994] Ratnaparkhi, Adwait, Jeﬀ Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. Proceedings of ARPA Workshop on Human Language Technology, pages 250255. [Resnik1992] Resnik, Philip. 1992. WordNet and distributional analysis: A class-based approach to lexical discovery. Proceedings of AAAI Workshop on Statistically-based NLP Techniques. [Resnik1993a] Resnik, Philip. 1993a. Selection and Information: A Class-based Ap- proach to Lexical Relationships. Ph.D. Thesis, Univ. of Pennsylvania. [Resnik1993b] Resnik, Philip. 1993b. Semantic classes and syntactic ambiguity. Pro- ceedings of ARPA Workshop on Human Language Technology. [Ribas1995] Ribas, Francesc. 1995. On learning more appropriate selectional restric- tions. Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics. [Rissanen1983] Rissanen, Jorma. 1983. A universal prior for integers and estimation by minimum description length. The Annals of Statistics, 11(2):416431. [Rissanen1984] Rissanen, Jorma. 1984. Universal coding, information, predication and estimation. IEEE Transaction on Information Theory, 30(4):629636. [Rissanen1989] Rissanen, Jorma. 1989. Stochastic Complexity in Statistical Inquiry. World Scientiﬁc Publishing Co., Singapore. [Rissanen1996] Rissanen, Jorma. 1996. Fisher information and stochastic complexity. IEEE Transaction on Information Theory, 42(1):4047. [Rissanen1997] Rissanen, Jorma. 1997. Stochastic complexity in learning. Journal of Computer and System Sciences, 55:8995. [Ristad and Thomas1995] Ristad, Eric Sven and Robert G. Thomas. 1995. New tech- niques for context modeling. Proceedings of the 33rd Annual Meeting of the Asso- ciation for Computational Linguistics. [Rivest1987] Rivest, Ronald L. 1987. Learning decision lists. Machine Learning, pages 229246. [Rose, Gurewitz, and Fox1990] Rose, Kenneth, Eitan Gurewitz, and Geoﬀrey C. Fox. 1990. Statistical mechanics and phase transitions in clustering. Physical Review Letters, 65(8):945948. 122 References [Rosenfeld1996] Rosenfeld, Ronald. 1996. A maximum entropy approach to adaptive statistical language modeling. Computational Linguistics, 22(1):3971. [Samuelsson1995] Samuelsson, Christer. 1995. A novel framework for reductionistic statistical parsing. Proceedings of International Workshop on Parsing Technology 95, pages 208215. [Saul and Pereira1997] Saul, Lawrence and Fernando Pereira. 1997. Aggregate and mixed-order Markov models for statistical language processing. Proceedings of 2nd Conference on Empirical Methods in Natural Language Processing. [Schabes1992] Schabes, Yves. 1992. Stochastic lexicalized tree-adjoining grammars. Proceedings of the 14th International Conference on Computational Linguistics, pages 425432. [Schutze1997] Schutze, Hinrich. 1997. Ambiguity Resolution in Language Learning: Computational and Cognitive Models. CSLI Stanford. [Schutze1998] Schutze, Hinrich. 1998. Automatic word sense discrimination. Compu- tational Linguistics, 24(1):97123. [Schutze and Singer1994] Schutze, Hinrich and Yoram Singer. 1994. Part-of-speech tagging using a variable memory Markov model. Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 181187. [Schwarz1978] Schwarz, G. 1978. Estimation of the dimension of a model. Annals of Statistics, 6:416446. [Sekine et al.1992] Sekine, Satoshi, Jeremy J. Carroll, Soﬁa Ananiadou, and Junichi Tsujii. 1992. Automatic learning for semantic collocation. Proceedings of the 3rd Conference on Applied Natural Language Processing, pages 104110. [Shirai et al.1998] Shirai, Kiyoaki, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998. A framework of intergrating syntactic and lexical statistics in sta- tistical parsing. Journal of Natural Language Processing, 5(3):85106. [Smadja1993] Smadja, Frank. 1993. Retrieving collocations from text: Xtract. Com- putational Linguistics, 19(1):143177. [Solomonoﬀ1964] Solomonoﬀ, R.J. 1964. A formal theory of inductive inference 1 and 2. Information and Control, 7:122,224254. [Stolcke and Omohundro1994] Stolcke, Andreas and Stephen Omohundro. 1994. In- ducing probabilistic grammars by Bayesian model merging. In Rafael C. Carrasco and Jose Oncina, editors, Grammatical Inference and Applications. Springer Verlag, pages 106118. References 123 [Stolcke and Segal1994] Stolcke, Andreas and Jonathan Segal. 1994. Precise n-gram probabilities from stochastic context-free grammars. Proceedings of the 32nd An- nual Meeting of the Association for Computational Linguistics, pages 7479. [Su and Chang1988] Su, Keh-Yih and Jing-Shin Chang. 1988. Semantic and syntactic aspects of score function. Proceedings of the 12th International Conference on Computational Linguistics, pages 642644. [Su et al.1989] Su, Keh-Yih, Jong-Nae Wang, Mei-Hui Su, and Jin-Shin Chang. 1989. A sequential truncation parsing algorithm based on the score function. Proceedings of the International Workshop on Parsing Technology 89, pages 95104. [Suzuki1993] Suzuki, Joe. 1993. A construction of Bayesian networks from databases based on an MDL principle. Proceedings of the 9th Conference on Uncertainty in Artiﬁcial Intelligence, pages 266273. [Tanaka1994] Tanaka, Hideki. 1994. Verbal case frame acquisition from a bilingual corpus: Gradual knowledge acquisition. Proceedings of the 15th International Con- ference on Computational Linguistics, pages 727731. [Tanaka1996] Tanaka, Hideki. 1996. Decision tree learning algorithm with structured attributes: Application to verbal case frame acquisition. Proceedings of the 16th International Conference on Computational Linguistics, pages 943948. [Tanner and Wong1987] Tanner, Martin A. and Wing Hung Wong. 1987. The calcu- lation of posterior distributions by data augmentation. Journal of the American Statistical Association, 82(398):528540. [Tokunaga, Iwayama, and Tanaka1995] Tokunaga, Takenobu, Makoto Iwayama, and Hozumi Tanaka. 1995. Automatic thesaurus construction based-on grammati- cal relations. Proceedings of the 14th International Joint Conference on Artiﬁcial Intelligency, pages 13081313. [Tsujii1987] Tsujii, Junichi. 1987. Knowledge Representation and Use  From AI View Point (in Japanese). Shoukoudou,Tokyo. [Ueda and Nakano1998] Ueda, Naonori and Ryohei Nakano. 1998. Deterministic an- nealing EM algorithm. Neural Networks, 11(2):271282. [Ushioda1996] Ushioda, Akira. 1996. Hierarchical clustering of words and application to NLP tasks. Proceedings of the 4th Workshop on Very Large Corpora, pages 2841. [Utsuro and Matsumoto1997] Utsuro, Takehito and Yuji Matsumoto. 1997. Learning probabilistic subcategorization preference by identifying case dependencies and op- timal noun class generalization level. Proceedings of the 5th Conference on Applied Natural Language Processing, pages 364371. 124 References [Utsuro, Matsumoto, and Nagao1992] Utsuro, Takehito, Yuji Matsumoto, and Makoto Nagao. 1992. Lexical knowledge acquisition from bilingual corpora. Proceedings of the 14th International Conference on Computational Linguistics, pages 581587. [Velardi, Pazienza, and Fasolo1991] Velardi, Paola, Maria Teresa Pazienza, and Michela Fasolo. 1991. How to encode semantic knowledge: A method for meaning represen- tation and computer-aided acquisition. Computational Linguistics, 17(2):153170. [Voorhees, Leacock, and Towell1995] Voorhees, Ellen M., Claudia Leacock, and Geof- frey Towell. 1995. Learning context to disambiguate word senses. In T. Petsche, S.J.Hanson, and J.W. Shavlik, editors, Computational Learning Theory and Natural Language Learning Systems 3: Selecting Good Models. MIT Press, pages 279305. [Wallace and Boulton1968] Wallace, C. and D. M. Boulton. 1968. An information measure for classiﬁcation. Computer Journal, 11:185195. [Webster and Marcus1989] Webster, Mort and Mitch Marcus. 1989. Automatic acqui- sition of the lexical semantics of verbs from sentence frames. Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 177184. [Wermter1989] Wermter, Stefan. 1989. Integration of semantic and syntactic con- straints for structural noun phrase disambiguation. Proceedings of the 11th In- ternational Joint Conference on Artiﬁcial Intelligency, pages 14861491. [Whittemore, Ferrara, and Brunner1990] Whittemore, Greg, Kathleen Ferrara, and Hans Brunner. 1990. Empirical study of predictive powers of simple attachment schemes for post-modiﬁer prepositional phrases. Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 2330. [Wilks1975] Wilks, Yorick. 1975. An intelligent analyzer and understander of English. Communications of the ACM, 18(5):264274. [Wright1990] Wright, J.H. 1990. Lr parsing of probabilistic grammars with input un- certainty for speech recognition. Computer Speech and Language, 4:297323. [Yamanishi1992a] Yamanishi, Kenji. 1992a. A learning criterion for stochastic rules. Machine Learning, 9:165203. [Yamanishi1992b] Yamanishi, Kenji. 1992b. A Statistical Approach to Computational Learning Theory. Ph.D. Thesis, Univ. of Tokyo. [Yamanishi1996] Yamanishi, Kenji. 1996. A randomized approximation of the MDL for stochastic models with hidden variables. Proceedings of the Ninth Annual Confer- ence on Computational Learning Theory, pages 99109. References 125 [Yarowsky1992] Yarowsky, David. 1992. Word-sense disambiguation using statistical models of Rogets categories trained on large corpora. Proceedings of the 14th International Conference on Computational Linguistics, pages 454460. [Yarowsky1993] Yarowsky, David. 1993. One sense per collocation. Proceedings of ARPA Workshop on Human Language Technology. [Yarowsky1994] Yarowsky, David. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 8895. [Yarowsky1995] Yarowsky, David. 1995. Unsupervised word sense disambiguation rival- ing supervised methods. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189196. 126 References Appendix A A.1 Derivation of Description Length: Two-stage Code We consider here minδ  log V δ1δk  log Pθ(xn)  . (A.1) We ﬁrst make Taylors expansion of  log Pθ(xn) around ˆθ:  log Pθ(xn)   log Pˆθ(xn)  ( log Pθ(xn)) θ ˆθ  δ  1 2  δT  2( log Pθ(xn)) θ2 ˆθ   δ  O(n  δ3), where δT denotes a transpose of δ. The second term equals 0 because ˆθ is the MLE estimate, and we ignore the fourth term. Furthermore, the third term can be written as 1 2  log e  n  δT  2( 1 n  ln Pθ(xn)) θ2 ˆθ   δ, where ln denotes the natural logarithm (recall that log denotes the logarithm to the base 2). Under certain suitable conditions, when n  ,  2( 1 n ln Pθ(xn)) θ2 ˆθ  can be approximated as a k-dimensional matrix of constants I(θ) known as the Fisher information matrix. Let us next consider min δ  log V δ1    δk  log Pˆθ(xn)  1 2  log e  n  δT  I(θ)  δ  . Diﬀerentiating this formula with each δi and having the results equal 0, we obtain the following equations: (n  I(θ)  δ)i  1 δi  0, (i  1,   , k). (A.2) 127 128 APPENDIX A. Suppose that the eigenvalues of I(θ) are λ1.   , λk, and the eigenvectors are (u1,    , uk). If we consider only the case in which the axes of a cell (k-dimensional rectangular solid) in the discretized vector space are in parallel with (u1,   , uk), then (A.2) becomes n      λ1 0 ... 0 λk         δ1 ... δk          1 δ1... 1 δk     . Hence, we have δi  1 n  λi and n  δT  I(θ)  δ  k. Moreover, since λ1    λk  I(θ) where A stands for the determinant of A, we have 1 δ1    δk  n k   I(θ). Finally, (A.1) becomes minδ  log V δ1δk  log Pθ(xn)   log(V  nk   I(θ))  log Pˆθ(xn)  1 2  log e  k  O( 1 n)   log Pˆθ(xn)  k 2  log n  k 2  log e  log V  1 2  log(I(θ))  O( 1 n)   log Pˆθ(xn)  k 2  log n  O(1). A.2 Learning a Soft Case Slot Model I describe here a method of learning the soft case slot model deﬁned in (3.2). We can ﬁrst adopt an existing soft clustering of words and estimate the word prob- ability distribution P(nC) within each class by employing a heuristic method (cf., (Li and Yamanishi, 1997)). We can next estimate the coeﬃcients (parameters) P(Cv, r) in the ﬁnite mixture model. There are two common methods for statistical estimation, Maximum Likelihood Estimation and Bayes Estimation. In their implementation for estimating the above coeﬃcients, however, both of them suﬀer from computational intractability. The EM algorithm (Dempster, Laird, and Rubin, 1977) can be used to approximate the max- imum likelihood estimates of the coeﬃcients. The Markov Chain Monte-Carlo tech- nique (Hastings, 1970; Geman and Geman, 1984; Tanner and Wong, 1987; Gelfand and Smith, 1990) can be used to approximate the Bayes estimates of the coeﬃcients. We consider here the use of an extended version of the EM algorithm (Helmbold et al., 1995). For the sake of notational simplicity, for some ﬁxed v and r, let us write A.3. NUMBER OF TREE CUTS 129 P(Cv, r), C  Γ as θi(i  1,   , m) and P(nC) as Pi(n). Then the soft case slot model in (3.2) may be written as P(nv, r)  m  i1 θi  Pi(n). Letting θ  (θ1,   , θm), for a given training sequence n1    nS, the maximum likeli- hood estimate of θ, denoted as ˆθ, is deﬁned as the value that maximizes the following log likelihood function L(θ)  1 S S  t1 log  m  i1 θi  Pi(nt)  . The EM algorithm ﬁrst arbitrarily sets the initial value of θ, which we denote as θ(0), and then successively calculates the values of θ on the basis of its most recent values. Let s be a predetermined number. At the lth iteration (l  1,   , s), we calculate θ(l)  (θ(l) 1 ,    , θ(l) m ) by θ(l) i  θ(l1) i  η(L(θ(l1))i  1)  1  , where η  0 (when η  1, Helmbold et als version simply becomes the standard EM algorithm), and L(θ) denotes L(θ)   L θ1    L θm  . After s numbers of calculations, the EM algorithm outputs θ(s)  (θ(s) 1 ,    , θ(s) m ) as an approximate of ˆθ. It is theoretically guaranteed that the EM algorithm converges to a local maximum of the likelihood function (Dempster, Laird, and Rubin, 1977). A.3 Number of Tree Cuts If we write F(i) for the number of tree cuts in a complete b-ary tree of depth i, we can show by mathematical induction that the number of tree cuts in a complete b-ary tree of depth d satisﬁes F(d)  Θ(2bd1), since clearly F(1)  1  1 and F(i)  (F(i  1))b  1 (i  2,   , d). Since the number of leaf nodes N in a complete b-ary tree equals bd, we conclude that the number of tree cuts in a complete b-ary tree is of order Θ(2 N b ). 130 APPENDIX A. Note that the number of tree cuts in a tree depends on the structure of that tree. If a tree is what I call a one-way branching b-ary tree, then it is easy to verify that the number of tree cuts in that tree is only of order Θ( N1 b1 ). Figure A.1 shows an example one-way branching b-ary tree. Note that a thesaurus tree is an unordered tree (for the deﬁnition of an unordered tree, see, for example, (Knuth, 1973)). Figure A.1: An example one-way branching binary tree. A.4 Proof of Proposition 1 For an arbitrary subtree T  of a thesaurus tree T and an arbitrary tree cut model M  (Γ, θ) in T, let MT   (ΓT , θT ) denote the submodel of M that is contained in T . Also, for any sample S, let ST  denote the subsample of S contained in T . (Note that MT  M, ST  S.) Then, in general for any submodel MT  and subsample ST , deﬁne L(ST ΓT , ˆθT ) to be the data description length of subsample ST  using submodel MT , deﬁne L(ˆθT ΓT ) to be the parameter description length for the submodel MT , and deﬁne L(MT , ST ) to be L(ST ΓT , ˆθT )  L(ˆθT ΓT ). First for any (sub)tree T, for any (sub)model MT  (ΓT, ˆθT) which is contained in T except the (sub)model consisting only of the root node of T, and for any (sub)sample ST contained in T, we have L(STΓT, ˆθT)  k  i1 L(STiΓTi, ˆθTi), (A.3) where Ti, (i  1,   , k) denote the child subtrees of T. For any (sub)tree T, for any (sub)model MT  (ΓT, ˆθT) which is contained in T A.5. EQUIVALENT DEPENDENCY TREE MODELS 131 except the (sub)model consisting only of the root node of T, we have L(ˆθT ΓT)  k  i1 L(ˆθTiΓTi), (A.4) where Ti, (i  1,   , k) denote the child subtrees of T. When T is the entire thesaurus tree, the parameter description length for a tree cut model in T should be L(ˆθT ΓT)  k  i1 L(ˆθTiΓTi)  log S 2 , (A.5) where S is the size of the entire sample. Since the second term log S 2 in (A.5) is common to each model in the entire thesaurus tree, it is irrelevant for the purpose of ﬁnding a model with the minimum description length. We will thus use identity (A.4) both when T is a proper subtree and when it is the entire tree. (This allows us to use the same recursive algorithm (Find-MDL) in all cases.) It follows from (A.3) and (A.4) that the minimization of description length can be done essentially independently for each subtree. Namely, if we let L min(MT, ST) denote the minimum description length (as deﬁned by (A.3) and (A.4)) achievable for (sub)model MT on (sub)sample ST contained in (sub)tree T, ˆP(η) the MLE estimate of the probability for node η, and root(T) the root node of T, then we have L min(MT, ST)  min{ k i1 L min(MTi, STi), L(([root(T)], [ ˆP(root(T))]), ST)}. (A.6) Here, Ti, (i  1,    , k) denote the child subtrees of T. The rest of the proof proceeds by induction. First, if T is a subtree having a single node, then there is only one submodel in T, and it is clearly the submodel with the minimum description length. Next, inductively assume that Find-MDL(T ) correctly outputs a submodel with the minimum description length for any subtree T  of size less than n. Then, given a (sub)tree T of size n whose root node has at least two child subtrees, say Ti : i  1,   , k, for each Ti, Find-MDL(Ti) returns a submodel with the minimum description length by inductive hypothesis. Then, since (A.6) holds, in whichever way the if-clause on lines 8, 9 of Find-MDL is evaluated, what is returned on line 11 or line 13 will still be a (sub)model with the minimum description length, completing the inductive step. It is easy to see that the time complexity of the algorithm is linear in the number of leaf nodes of the input thesaurus tree. A.5 Equivalent Dependency Tree Models We prove here that the dependency tree models based on a labeled free tree are equiv- alent to one another. Here, a labeled free tree means a tree in which each node is associated with one unique label and in which any node can be the root. 132 APPENDIX A. Suppose that the free tree we have is now rooted at X0 (Figure A.2). The depen- dency tree model based on this rooted tree will then be uniquely determined. Suppose that we randomly select one other node Xi from this tree. If we reverse the directions of the links from X0 to Xi, we will obtain another tree rooted at Xi. Another depen- dency tree model based on this tree will also be determined. It is not diﬃcult to see that these two distributions are equivalent to one another, since P(X0)  P(X1X0)   P(XiXi1)  P(X0X1)  P(X1)   P(XiXi1)      P(X0X1)   P(Xi1Xi)  P(Xi). Thus we complete the proof. X X 0 i X X 0 i Figure A.2: Equivalent dependency tree models. A.6 Proof of Proposition 2 We can represent any dependency forest model as P(X1,    , Xn)  P(X1Xq(1))   P(XiXq(i))    P(XnXq(n)) 0  q(i)  n, q(i)  i, (i  1,    , n) where Xq(i) denotes a random variable which Xi depends on. We let P(XiX0)  P(Xi). Note that there exists a j, (j  1,   , n) for which P(XjXq(j))  P(XjX0)  P(Xj). The sum of parameter description length and data description length for any de- A.6. PROOF OF PROPOSITION 2 133 pendency forest model equals n i1 ki1 2 kq(i)  log N   x1,,xn f(x1,   , xn)  log  ˆP(x1xq(1))    ˆP(xixq(i))    ˆP(xnxq(n))   n i1 ki1 2 kq(i)  log N  n i1  xi,xq(i) f(xi, xq(i))  log ˆP(xixq(i))  n i1  ki1 2 kq(i)  log N   xi,xq(i) f(xi, xq(i))  log ˆP(xixq(i))  , where N denotes data size, xi the possible values of Xi, and ki the number of possible values of Xi. We let k0  1 and f(xi, x0)  f(xi). Furthermore, the sum of parameter description length and data description length for the independent model (i.e., P(X1,    , Xn)  n i1 P(Xi)) equals n i1 ki1 2  log N   x1,,xn f(x1,   , xn)  log n i1 ˆP(xi)   n i1 ki1 2  log N  n i1  xi f(xi)  log ˆP(xi)  n i1  ki1 2  log N   xi f(xi)  log ˆP(xi)  . Thus, the diﬀerence between the description length of the independent model and the description length of any dependency forest model becomes n i1  xi,xq(i) f(xi, xq(i))  (log ˆP(xixq(i))  log ˆP(xi))  (ki1)(kq(i)1) 2  log N   n i1  N  ˆI(Xi, Xq(i))  (ki1)(kq(i)1) 2  log N   n i1 N  θ(Xi, Xq(i)). Any dependency forest model for which there exists an i satisfying θ(Xi, Xq(i))  0 is not favorable from the viewpoint of MDL because the model for which the corre- sponding i satisfying θ(Xi, Xq(i))  0 always exists and is clearly more favorable. We thus need only select the MDL model from those models for which for any i, θ(Xi, Xq(i))  0 is satisﬁed. Obviously, the model for which n i1 θ(Xi, Xq(i)) is maximized is the best model in terms of MDL. What Suzukis algorithm outputs is exactly this model, and this completes the proof. 134 APPENDIX A. Publication List Reviewed Journal Papers 1. Li, H.: A Probabilistic Disambiguation Method based on Psycholinguistic Prin- ciples, (in Japanese) Computer Software, Vol.13, No. 6, (1996) pp. 5365. 2. Li, H. and Abe, N.: Clustering Words with the MDL Principle, Journal of Natural Language Processing, Vol.4, No. 2, (1997), pp. 7188. 3. Li, H. and Abe, N.: Generalizing Case Frames Using a Thesaurus and the MDL Principle, Computational Linguistics, Vol.24, No.2 (1998), pp. 217-244. Reviewed Conference Papers 1. Li, H. and Abe, N.: Generalizing Case Frames Using a Thesaurus and the MDL Principle, Proceedings of Recent Advances in Natural Language Process- ing, (1995), pp. 239248. 2. Abe, N. and Li, H.: On-line Learning of Binary Lexical Relations Using Two- dimensional Weighted Majority Algorithms, Proceedings of the 12th International Conference on Machine Learning (ICML95), (1995), pp. 7188. 3. Li, H.: A Probabilistic Disambiguation Method based on Psycholinguistic Prin- ciples, Proceedings of the 4th Workshop on Very Large Corpora, (1996), pp. 141 154. 4. Li, H. and Abe, N.: Clustering Words with the MDL Principle, Proceedings of the 16th International Conference on Computational Linguistics (COLING96), (1996), pp. 49. 5. Li, H. and Abe, N.: Learning Dependencies between Case Frame Slots, Proceed- ings of the 16th International Conference on Computational Linguistics (COL- ING96), (1996), pp. 1015. 135 136 APPENDIX A. 6. Abe, N. and Li, H.:Learning Word Association Norms Using Tree Cut Pair Models , Proceedings of the 13th International Conference on Machine Learn- ing (ICML96), (1996), pp. 7188. 7. Li, H. and Yamanishi, K.: Document Classiﬁcation Using a Finite Mixture Model, Proceedings of the 35th Annual Meeting of Association for Computational Lin- guistics (ACLEACL97) , (1997). 8. Li, H. and Abe, N.: Word Clustering and Disambiguation Based on Co-occurrence Data, Proceedings of the 18th International Conference on Computational Lin- guistics and the 36th Annual Meeting of Association for Computational Linguis- tics (COLING-ACL98), (1998), to appear.",
  "21.pdf": "arXiv:cs9812005v1 [cs.CL] 4 Dec 1998 Optimal Multi-Paragraph Text Segmentation by Dynamic Programming Oskari Heinonen University of Helsinki, Department of Computer Science P.O. Box 26 (Teollisuuskatu 23), FIN00014 University of Helsinki, Finland Oskari.Heinonencs.Helsinki.FI Abstract There exist several methods of calculating a similar- ity curve, or a sequence of similarity values, repre- senting the lexical cohesion of successive text con- stituents, e.g., paragraphs. Methods for deciding the locations of fragment boundaries are, however, scarce. We propose a fragmentation method based on dynamic programming. The method is theoret- ically sound and guaranteed to provide an optimal splitting on the basis of a similarity curve, a pre- ferred fragment length, and a cost function deﬁned. The method is especially useful when control on fragment size is of importance. 1 Introduction Electronic full-text documents and digital libraries make the utilization of texts much more effective than before; yet, they pose new problems and re- quirements. For example, document retrieval based on string searches typically returns either the whole document or just the occurrences of the searched words. What the user often is after, however, is mi- crodocument: a part of the document that contains the occurrences and is reasonably self-contained. Microdocuments can be created by utilizing lex- ical cohesion (term repetition and semantic rela- tions) present in the text. There exist several meth- ods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohe- sion of successive constituents (such as paragraphs) of text (see, e.g., (Hearst, 1994; Hearst, 1997; Koz- ima, 1993; Morris and Hirst, 1991; Yaari, 1997; Youmans, 1991)). Methods for deciding the loca- tions of fragment boundaries are, however, not that common, and those that exist are often rather heuris- tic in nature. To evaluate our fragmentation method, to be ex- plained in Section 2, we calculate the paragraph similarities as follows. We employ stemming, re- move stopwords, and count the frequencies of the remaining words, i.e., terms. Then we take a pre- deﬁned number, e.g., 50, of the most frequent terms to represent the paragraph, and count the similar- ity using the cosine coefﬁcient (see, e.g., (Salton, 1989)). Furthermore, we have applied a sliding win- dow method: instead of just one paragraph, sev- eral paragraphs on both sides of each paragraph boundary are considered. The paragraph vectors are weighted based on their distance from the boundary in question with immediate paragraphs having the highest weight. The beneﬁt of using a larger win- dow is that we can smooth the effect of short para- graphs and such, perhaps example-type, paragraphs that interrupt a chain of coherent paragraphs. 2 Fragmentation by Dynamic Programming Fragmentation is a problem of choosing the para- graph boundaries that make the best fragment boundaries. The local minima of the similarity curve are the points of low lexical cohesion and thus the natural candidates. To get reasonably-sized mi- crodocuments, the similarity information alone is not enough; also the lengths of the created frag- ments have to be considered. In this section, we de- scribe an approach that performs the fragmentation by using both the similarities and the length infor- mation in a robust manner. The method is based on a programming paradigm called dynamic program- ming (see, e.g., (Cormen et al., 1990)). Dynamic programming as a method guarantees the optimal- ity of the result with respect to the input and the parameters. The idea of the fragmentation algorithm is as fol- lows (see also Fig. 1). We start from the ﬁrst bound- ary and calculate a cost for it as if the ﬁrst paragraph was a single fragment. Then we take the second boundary and attach to it the minimum of the two available possibilities: the cost of the ﬁrst two para- graphs as if they were a single fragment and the cost fragmentation(n, p, h, len[1..n], sim[1..n  1])  n no. of pars, p preferred frag length, h scaling   len[1..n] par lengths, sim[1..n  1] similarities  { sim[0] : 0.0; cost[0] : 0.0; B : ; for par : 1 to n { lensum : 0;  cumulative fragment length  cmin : MAXREAL; for i : par to 1 { lensum : lensum  len[i]; c : clen(lensum, p, h); if c  cmin {  optimization  exit the innermost for loop; } c : c  cost[i  1]  sim[i  1]; if c  cmin { cmin : c; loc cmin : i  1; } } cost[par] : cmin; link prev[par] : loc cmin; } j : n; while link prev[j]  0 { B : B  link prev[j]; j : link prev[j]; } return(B);  set of chosen fragment boundaries  } Figure 1: The dynamic programming algorithm for fragment boundary detection. of the second paragraph as a separate fragment. In the following steps, the evaluation moves on by one paragraph at each time, and all the possible loca- tions of the previous breakpoint are considered. We continue this procedure till the end of the text, and ﬁnally we can generate a list of breakpoints that in- dicate the fragmentation. The cost at each boundary is a combination of three components: the cost of fragment length clen, and the cost cost[] and similarity sim[] of some previous boundary. The cost function clen gives the lowest cost for the preferred fragment length given by the user, say, e.g., 500 words. A fragment which is either shorter or longer gets a higher cost, i.e., is punished for its length. We have experimented with two families of cost functions, a family of second degree functions (parabolas), clen(x, p, h)  h( 1 p2 x2  2 px  1), and V-shape linear functions, clen(x, p, h)  h(x p  1), 0 0.1 0.2 0.3 0.4 0.5 0.6 0 1000 2000 3000 4000 5000 6000 7000 similarity wordcount Mars. Chapter II. Section I. \"W6ClinH0.25L\" \"W6ClinH0.5L\" \"W6ClinH0.75L\" \"W6ClinH1.0L\" \"W6ClinH1.25L\" \"W6ClinH1.5L\" \"W6L\" (a) 0 0.1 0.2 0.3 0.4 0.5 0.6 0 1000 2000 3000 4000 5000 6000 7000 similarity wordcount Mars. Chapter II. Section I. \"W6CparH0.25L\" \"W6CparH0.5L\" \"W6CparH0.75L\" \"W6CparH1.0L\" \"W6CparH1.25L\" \"W6CparH1.5L\" \"W6L\" (b) Figure 2: Similarity curve and detected fragment boundaries with different cost functions. (a) Lin- ear. (b) Parabola. p is 600 words in both (a)  (b). H0.25, etc., indicates the value of h. Vertical bars indicate fragment boundaries while short bars below horizontal axis indicate paragraph boundaries. where x is the actual fragment length, p is the pre- ferred fragment length given by the user, and h is a scaling parameter that allows us to adjust the weight given to fragment length. The smaller the value of h, the less weight is given to the preferred fragment length in comparison with the similarity measure. 3 Experiments As test data we used Mars by Percival Lowell, 1895. As an illustrative example, we present the analysis of Section I. Evidence of it of Chapter II. Atmo- sphere. The length of the section is approximately 6600 words and it contains 55 paragraphs. The frag- ments found with different parameter settings can be seen in Figure 2. One of the most interesting is the one with parabola cost function and h  .5. In this case the fragment length adjusts nicely accord- ing to the similarity curve. Looking at the text, most fragments have an easily identiﬁable topic, like at- mospheric chemistry in fragment 7. Fragments 2 and 3 seem to have roughly the same topic: measur- ing the diameter of the planet Mars. The fact that they do not form a single fragment can be explained cost function h lavg lmin lmax davg linear .25 1096.1 501 3101 476.5 .50 706.4 501 1328 110.5 .75 635.7 515 835 60.1 1.00 635.7 515 835 59.5 1.25 635.7 515 835 59.5 1.50 635.7 515 835 57.6 parabola .25 908.2 501 1236 269.4 .50 691.0 319 1020 126.0 .75 676.3 371 922 105.8 1.00 662.2 371 866 94.2 1.25 648.7 466 835 82.4 1.50 635.7 473 835 69.9 Table 1: Variation of fragment length. Columns: lavg, lmin, lmax average, minimum, and maximum fragment length; and davg average deviation. by the preferred fragment length requirement. Table 1 summarizes the effect of the scaling fac- tor h in relation to the fragment length variation with the two cost functions over those 8 sections of Mars that have a length of at least 20 para- graphs. The average deviation davg with respect to the preferred fragment length p is deﬁned as davg  (m i1 p  li)m where li is the length of fragment i, and m is the number of fragments. The parametric cost function chosen affects the result a lot. As expected, the second degree cost function allows more variation than the linear one but roles change with a small h. Although the experiment is insufﬁcient, we can see that in this example a factor h  1.0 is unsuitable with the linear cost function (and h  1.5 with the parabola) since in these cases so much weight is given to the fragment length that fragment boundaries can appear very close to quite strong local maxima of the similarity curve. 4 Conclusions In this article, we presented a method for detect- ing fragment boundaries in text. The fragmentation method is based on dynamic programming and is guaranteed to give an optimal solution with respect to a similarity curve, a preferred fragment length, and a parametric fragment-length cost function de- ﬁned. The method is independent of the similarity calculation. This means that any method, not nec- essarily based on lexical cohesion, producing a suit- able sequence of similarities can be used prior to our fragmentation method. For example, the lexical cohesion proﬁle (Kozima, 1993) should be perfectly usable with our fragmentation method. The method is especially useful when control over fragment size is required. This is the case in passage retrieval since windows of 1000 bytes (Wilkinson and Zobel, 1995) or some hundred words (Callan, 1994) have been proposed as best passage sizes. Furthermore, we believe that frag- ments of reasonably similar size are beneﬁcial in our intended purpose of document assembly. Acknowledgements This work has been supported by the Finnish Technology Development Centre (TEKES) together with industrial partners, and by a grant from the 350th Anniversary Foundation of the University of Helsinki. The author thanks Helena Ahonen, Barbara Heikkinen, Mika Klemettinen, and Juha Karkkainen for their contributions to the work de- scribed. References J. P. Callan. 1994. Passage-level evidence in doc- ument retrieval. In Proc. SIGIR94, Dublin, Ire- land. T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990. Introduction to Algorithms. MIT Press, Cambridge, MA, USA. M. A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. ACL-94, Las Cruces, NM, USA. M. A. Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Compu- tational Linguistics, 23(1):3364, March. H. Kozima. 1993. Text segmentation based on sim- ilarity between words. In Proc. ACL-93, Colum- bus, OH, USA. J. Morris and G. Hirst. 1991. Lexical cohesion computed by thesaural relation as an indicator of the structure of text. Computational Linguistics, 17(1):2148. G. Salton. 1989. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Infor- mation by Computer. Addison-Wesley, Reading, MA, USA. R. Wilkinson and J. Zobel. 1995. Comparison of fragmentation schemes for document retrieval. In Overview of TREC-3, Gaithersburg, MD, USA. Y. Yaari. 1997. Segmentation of expository texts by hierarchical agglomerative clustering. In Proc. RANLP97, Tzigov Chark, Bulgaria. G. Youmans. 1991. A new tool for discourse anal- ysis. Language, 67(4):763789. Errata Table 1 is incorrect. Table 2 is correct. Figure 2 is tiny. Figure 4 has been enlarged. Figure 3 here is additional. cost function h lavg lmin lmax davg linear .25 1105.3 562 1754 518.0 .50 736.9 562 985 145.8 .75 663.2 603 724 63.2 1.00 663.2 603 724 63.2 1.25 663.2 603 724 63.2 1.50 663.2 603 724 63.2 parabola .25 829.0 562 1072 238.5 .50 829.0 562 1020 238.5 .75 736.9 562 832 151.3 1.00 663.2 466 817 113.8 1.25 663.2 466 804 113.8 1.50 663.2 603 724 63.2 Table 2: Variation of fragment length. Columns: lavg, lmin, lmax average, minimum, and maximum fragment length; and davg average deviation. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 similarity wordcount Mars. Chapter II. Section I. \"W6CparH0.25L\" \"W6L\" Figure 3: Similarity curve and detected fragment boundaries. Parabola cost function, p is 600 words, and h  .25. Vertical bars indicate fragment boundaries while short bars below horizontal axis indicate para- graph boundaries. 0 0.1 0.2 0.3 0.4 0.5 0.6 0 1000 2000 3000 4000 5000 6000 7000 similarity wordcount Mars. Chapter II. Section I. \"W6ClinH0.25L\" \"W6ClinH0.5L\" \"W6ClinH0.75L\" \"W6ClinH1.0L\" \"W6ClinH1.25L\" \"W6ClinH1.5L\" \"W6L\" (a) 0 0.1 0.2 0.3 0.4 0.5 0.6 0 1000 2000 3000 4000 5000 6000 7000 similarity wordcount Mars. Chapter II. Section I. \"W6CparH0.25L\" \"W6CparH0.5L\" \"W6CparH0.75L\" \"W6CparH1.0L\" \"W6CparH1.25L\" \"W6CparH1.5L\" \"W6L\" (b) Figure 4: Similarity curve and detected fragment boundaries with different cost functions. (a) Linear. (b) Parabola. p is 600 words in both (a)  (b). H0.25, etc., indicates the value of h. Vertical bars indicate fragment boundaries while short bars below horizontal axis indicate paragraph boundaries.",
  "22.pdf": "arXiv:cs9812018v1 [cs.CL] 16 Dec 1998 A FLEXIBLE SHALLOW APPROACH TO TEXT GENERATION Stephan Busemann and Helmut Horacek DFKI GmbH Stuhlsatzenhausweg 3, 66123 Saarbrucken, Germany {busemann, horacek}dfki.de1 Abstract In order to support the eﬃcient development of NL generation systems, two orthogonal methods are currently pursued with emphasis: (1) reusable, general, and linguistically motivated surface realization components, and (2) simple, task-oriented template-based techniques. In this paper we argue that, from an application-oriented perspective, the beneﬁts of both are still limited. In order to improve this situation, we suggest and evaluate shallow generation methods associated with increased ﬂexibility. We advise a close connection between domain-motivated and linguistic ontologies that supports the quick adaptation to new tasks and domains, rather than the reuse of general resources. Our method is especially designed for generating reports with limited linguistic variations. 1 Introduction In order to support the eﬃcient development of NL generation systems, two orthogonal methods are currently pursued with emphasis: (1) reusable, general, and linguistically motivated surface re- alization components, and (2) simple, task-oriented template-based techniques. Surface realization components impose a layer of intermediate representations that has become fairly standard, such as the Sentence Plan Language (SPL) [Kasper and Whitney, 1989]. This layer allows for the use of existing software with well-deﬁned interfaces, often reducing the development eﬀort for surface real- ization considerably. Template-based techniques recently had some sort of revival through several application-oriented projects such as idas [Reiter et al., 1995], that combine pre-deﬁned surface expressions with freely generated text in one or another way. However, the beneﬁts of both surface realization components and template-based techniques are still limited from an application-oriented perspective. Surface realization components are diﬃcult to use because of the diﬀerences between domain-oriented and linguistically motivated ontologies (as in SPL), and existing template-based techniques are too inﬂexible. In this paper we suggest and evaluate ﬂexible shallow methods for report generation applica- tions requiring limited linguistic resources that are adaptable with little eﬀort. We advise a close connection between domain-motivated and linguistic ontologies, and we suggest a layer of inter- mediate representation that is oriented towards the domain and the given task. This layer may contain representations of diﬀerent granularity, some highly implicit, others very elaborate. We show how this is used by the processing components in a beneﬁcial way. 1This work has been supported by a grant for the project TEMSIS from the European Union (Telematics Appli- cations Programme, Sector C9, contract no. 2945). The approach suggested does not only change the modularization generally assumed for NLG systems drastically, it also renders the system much more application-dependent. At ﬁrst glance, however, such an approach seems to abandon generality and reusability completely, but, as we will demonstrate, this is not necessarily the case. The rest of this paper is organized as follows: Section 2 identiﬁes deﬁcits with current approaches to surface realization that may occur for particular applications. In Section 3 we propose alternative methods implemented into our sample application, the generation of air-quality reports from current environmental data. In Section 4 we discuss the pros and cons of our approach, and we summarize the conditions for successful use. 2 In-Depth and Shallow Generation 2.1 Shallow generation Recently, the distinction between in-depth and shallow approaches to language processing has emerged from the need to build sensible applications. In language understanding deep analysis attempts to understand every part of the input, while shallow analysis tries to identify only parts of interest for a particular application. Shallow analysis is a key concept for information extraction from huge text bases and many other real-world application types. In language generation a corresponding distinction which we term in-depth vs. shallow genera- tion2 is becoming prominent. While in-depth generation is inherently knowledge-based and the- oretically motivated, shallow generation quite opportunistically models only the parts of interest for the application in hand. Often such models will turn out to be extremely shallow and simple, but in other cases much more detail is required. Thus, developing techniques for varying model- ing granularity according to the requirements posed by the application is a prerequisite for more custom-tailored systems. According to Reiter and Mellish, shallow techniques (which they call intermediate) are ap- propriate as long as corresponding in-depth approaches are poorly understood, less eﬃcient, or more costly to develop [Reiter and Mellish, 1993]. While our motivation for shallow techniques is in essence based on the cost factor, our assessment is even more pronounced than Reiters and Mellishs in that we claim that shallow approaches combining diﬀerent granularity in a ﬂexible way are better suited for small applications. We are convinced that shallow generation systems will have a similar impact on the development of feasible applications as shallow analyzers. 2.2 Potential shortcomings of approaches to surface realization Current approaches to surface realization are mostly in-depth, based on general, linguistically motivated, and widely reusable realization components, such as Penman [Penman, 1989], KPML [Bateman, 1997], and SURGE [Elhadad and Robin, 1996]. These components are domain-independent and based on sound linguistic principles. KPML and SURGE also exhibit a broad coverage of En- glish, while several other language models are also available or under development. Despite their being reusable in general, the fact that the modularization of grammatical knowledge follows lin- guistic criteria rather than the needs of diﬀerent types of applications may cause a number of problems for an eﬃcient development of concrete applications: 2We thus avoid confusion with the common distinction between deep and surface generation.  The substantial diﬀerences between domain- and linguistically motivated ontologies may ren- der the mapping between them diﬃcult; for instance, the use of case relations such as agent or objective requires compatible models of deep case semantics.  The need to encapsulate grammar knowledge within the surface realizer may require de- tails in the intermediate representation to be spelled out that are irrelevant to the intended application, even for rather small systems.  The ﬁxed granularity of grammatical modeling requires a realizer to cover many more lan- guages, language fragments, or stylistic variations than would be needed for one particular application, which can lead to a considerable ineﬃciency of the realizer. In addition, there may be linguistic constructs needed for some applications that are still outside the scope of the general tool. Their inclusion may require the intermediate representation layer to be modiﬁed. 2.3 Potential shortcomings of shallow generation methods A prominent example for an early shallow generation system is Ana [Kukich, 1983], which re- ports about stock market developments. While the kind of texts it produces can still be con- sidered valuable today, Ana is implemented as a widely unstructured rule-based system, which does not seem to be easily extendable and portable. Since then, various shallow methods in- cluding canned text parts and some template-based techniques have been utilized, e.g. in Cogen- tHelp [White and Caldwell, 1997], in the system described in [Cawsey et al., 1995], and in idas [Reiter et al., 1995]. They feature simplicity where the intended application does not require ﬁne- grained distinctions, such as the following techniques used in idas:  canned text with embedded KB references (Carefully slide [x] out along its guide),  case frames with textual slot ﬁllers, (gently in (manner: \"gently\")). Although these techniques seem to be able to provide the necessary distinctions for many practical applications in a much simpler way than in-depth surface realization components can do, a serious limitation lies in their inﬂexibility. The ﬁrst example above requires the realization of [x] to agree in number with the canned part; as this is not explicitly treated, the system seems to implicitly know that only singular descriptions will be inserted. Moreover, canned texts as case role ﬁllers may bear contextual inﬂuence, too, such as pronominals, or word order phenomena. Thus, the ﬂexibility of shallow generation techniques should be increased signiﬁcantly. 3 Shallow Generation in TEMSIS In order to tailor the design of a generation system towards an application, we must account for diﬀerent levels of granularity. We need a formalism capable of adapting to the expressivity of the domain-oriented information. Parts of the texts to be generated may be canned, some require templates, others require a more elaborate grammatical model. In this section we ﬁrst introduce an instance of the kind of applications we have in mind. We then proceed by discussing aspects of diﬀerent granularity from the point of view of the intermediate representation (IR) layer and the components it interfaces. These include text organization and text realization. The text organizer is also responsible for content selection. It retrieves the relevant data from the TEMSIS database. It combines ﬁxed text blocks with the results of the realizer in a language-neutral way. IR expressions are consumed by the text realizer, which is a version of the production system TG2 described in [Busemann, 1996]. 3.1 The TEMSIS application With TEMSIS a Transnational Environmental Management Support and Information System was created as part of a transnational cooperation between the communities in the French-German urban agglomeration, Moselle Est and Stadtverband Saarbrucken. Networked information kiosks are being installed in a number of communities to provide public and expert environmental infor- mation. The timely availability of relevant environmental information will improve the planning and reactive capabilities of the administration considerably. Current measurement data are made available on the TEMSIS web server. The data include the pollutant, the measurement values, the location and the time the measurements were taken, and a variety of thresholds. Besides such data, the server provides metadata that allow for descriptions of the measuring locations, of the pollutants measured and of regulations or laws according to which a comparison between measure- ments and thresholds can be performed. This information can be accessed via the internet through a hyperlink navigation interface (http:www-temsis.dfki.uni-sb.de). The verbalization of NL air quality information in German and French is an additional service reducing the need to look up multiple heterogeneous data. The generated texts can be comple- mented with diagrams of time series. The material can be edited and further processed by the administrations to ﬁt additional needs. In order to request a report, a user speciﬁes his demand by choosing from a hierarchy of options presented to him within the hyperlink navigation interface. He selects a report type by indicat- ing whether he is interested in average values, maximum values, or situations where thresholds are exceeded. Further choices include the language, the country the environmental legislation of which should apply, the measurement location, the pollutant, the period of time for which mea- surements should be retrieved, and in some cases comparison parameters. In addition, descriptions of pollutants and measurement stations can be requested. They are stored as canned texts in the TEMSIS database. Not all choices are needed in every case, and the TEMSIS navigator restricts the combination of choices to the meaningful ones. Let us assume that the user wants a French text comparing thresholds for sulfur dioxide with measurements taken in the winter period of 199697 at Volklingen City, and the applicable legis- lation should be from Germany. He also wants a conﬁrmation of some of his choices. The user receives the following text on his browser3 (translated into English for the readers convenience): You would like information about the concentration of sulfur dioxide in the air during the winter season 199697. At the measurement station of Volklingen City, the early warning threshold for sulfur dioxide at an exposition of three hours (600 µgm3 according to the German decree Smogverordnung) was not exceeded. In winter 199596, the early warning threshold was not exceeded either. 3A demo version of the system is available at http:www.dfki.deservicenlg-demo. Reports are organized into one or several paragraphs. Their length may range from a few lines to a page. [(COOP THRESHOLD-EXCEEDING) (LANGUAGE FRENCH) (TIME [(PRED SEASON) (NAME [(SEASON WINTER) (YEAR 1996)])]) (THRESHOLD-VALUE [(AMOUNT 600) (UNIT MKG-M3)]) (POLLUTANT SULFUR-DIOXIDE) (SITE \"Vo1lklingen-City\") (SOURCE [(LAW-NAME SMOGVERORDNUNG) (THRESHOLD-TYPE VORWARNSTUFE)]) (DURATION [(HOUR 3)]) (EXCEEDS [(STATUS NO) (TIMES 0)])] En hiver 199697 a la station de mesure de Volklingen-City, le seuil davertissement pour le dioxide de soufre pour une exposition de trois heures (600.0 µgm3 selon le decret allemand Smogverord- nung) na pas ete depassee. Figure 1: A sample intermediate representation for a report statement and its realization. 3.2 The intermediate representation The main purpose of the IR layer for the report generation system consists in ensuring that all facets of the domain with their diﬀerent degrees of speciﬁcity can be verbally expressed, and in keeping the realization task simple when no or little variety in language is needed. While SPL and similar languages interfacing to in-depth surface realization are either linguistic in nature or largely constrain the surface form of an utterance, the IR speciﬁes domain information to be conveyed to the user and logical predicates about it. Abstracting away from language-speciﬁc information in the IR like this has the additional advantage that multi-lingual aspects can be kept internal to the realizer. They depend on the LANGUAGE feature in an IR expression. The IR in Figure 1 roughly corresponds to the key statement of the sample report in the pre- vious section (the second sentence), which also appears at the end of each report as a summary. It constitutes a threshold comparison, as stated by the value of the COOP4 slot. There is only little indication as to how IR expressions should be expressed linguistically. Many semantic relations be- tween the elements of an IR expression are left implicit. For instance, the value of DURATION relates to the time of exposure according to the thresholds deﬁnition and not to the period of time the user is interested in (TIME). Another example is the relation between EXCEEDS and THRESHOLD-VALUE, which leads to the message that the early warning threshold was not exceeded at all. Wordings are not prescribed. For instance, our sample IR does not contain a basis for the generation of exposure or measurement station. IR expressions contain speciﬁcations at diﬀerent degrees of granularity. For coarse-grained speciﬁcations, it is up to the text realizer to make missing or underspeciﬁed parts explicit on the surface so that, in a sense, shallow text realization determines parts of the contents. For more ﬁne-grained speciﬁcations, such as time expressions, text realization behaves like a general surface 4The COOP value can correspond to the report type, as in the example, to conﬁrmations of user choices, or to meta comments such as an introductory statement to a diagram, generated by a dedicated component. generator with a fully-detailed interface. Ensuring an appropriate textual realization from IR expressions is left to the language template design within the realizer. The syntax of IR expressions is deﬁned by a standard Backus-Naur form. All syntactically correct expressions have a compositional semantic interpretation and can be realized as a surface text provided corresponding realization rules are deﬁned. Sharing the IR deﬁnitions between the text organization and the realization component thus avoids problems of realizability described in [Meteer, 1992]. 3.3 Text organization The goal of text organization in our context is to retrieve and express, in terms suitable for the deﬁnition of the IR, (1) report speciﬁcations provided by the user, (2) the relevant domain data accessed from the database according to these speciﬁcations, including e.g. explicit comparisons between measurements and threshold values, and (3) implicitly associated meta-information from the database, such as the duration of exposure, the decree and the value of the threshold. This task is accomplished by a staged process that is application-oriented rather than based on linguistically motivated principles. The process starts with building some sort of a representation sketch, by instantiating a report skeleton that consists of a sequence of assertion statement speciﬁcations. Assertion statements consist of a top level predicate that represents the assertions type (e.g. threshold-exceeding) and encapsulates the entire meaning of the associated assertion, except to attached speciﬁcations and domain data, to make local parameters and data dependencies explicit. In order to transform this initial representation to meet the application-oriented requirements of the IR, it is necessary to recast the information, which comprises augmenting, restructuring, and aggregating its components. Augmenting statement speciﬁcations means making information implicitly contained or available elsewhere explicitly at the place it is needed. This concerns reestablishing report-wide information, as well as making locally entailed information accessible. An example for the former is the number of diagrams copied into the introductory statement to these diagrams. This treatment is much simpler than using a reference generation algorithm, but it relies on knowing the number of diagrams in advance. An example for the latter is the unit in which the value of a measurement is expressed. Restructuring information imposes some manipulations on the speciﬁcations obtained so far to rearrange the pieces of information contained so that they meet the deﬁnition of the IR. The associated operations include reifying an attribute as a structured value and raising an embedded partial description. These operations are realized by mapping schemata similar to those elaborated for linguistically motivated lexicalization [Horacek, 1996]. However, some of our schemata are purely application-oriented and tailored to the domain, which manifests itself in the larger size of the structures covered. Aggregation, the last part of information recasting, comprises removing partial descriptions or adding simple structures. These operations are driven by a small set of declaratively represented rules that access a discourse memory. Most of the rules aim at avoiding repetitions of optional constituents (e.g., temporal and locative information) over adjacent statements. For example, the TIME speciﬁcation is elided in the second sentence of our sample text, since the time speciﬁcation in the ﬁrst sentence still applies. An example for adding a simple structure to an IR expression is the insertion of a marker indicating a strong correspondence between adjacent assertions, which (defproduction threshold-exceeding \"WU01\" (:PRECOND (:CAT DECL :TEST ((coop-eq threshold-exceeding) (threshold-value-p))) :ACTIONS (:TEMPLATE (:OPTRULE PPtime (get-param time)) (:OPTRULE SITEV (get-param site)) (:RULE THTYPE (self)) (:OPTRULE POLL (get-param pollutant)) (:OPTRULE DUR (get-param duration)) \"(\" (:RULE VAL (get-param threshold-value)) (:OPTRULE LAW (get-param law-name)) \") \" (:RULE EXCEEDS (get-param exceeds)) \".\" :CONSTRAINTS (:GENDER (THTYPE EXCEEDS) :EQ)))) Figure 2: A TGL rule deﬁning a sentence template for threshold exceeding statements. gives rise to inserting either in the sample text. Altogether, the underlying rules are formulated to meet application particularities, such as impacts of certain combinations of a value, a status, and a threshold comparison outcome, rather than to capture linguistic principles. 3.4 Text realization with TG2 TG2 is a ﬂexible and reusable application-oriented text realization system that can be smoothly combined with deep generation processes. It integrates canned text, templates, and context-free rules into a single production-rule formalism and is thus extremely well suited for coping with IR subexpressions of diﬀerent granularity. TG2 is based on production system techniques [Davis and King, 1977] that preserve the mod- ularity of processing and linguistic knowledge. Productions are applied through the familiar three- step processing cycle: (i) identify the applicable rules, (ii) select a rule on the basis of some conﬂict resolution mechanism, and (iii) apply that rule. Productions are used to encode grammar rules in the language TGL [Busemann, 1996]. A rule is applicable if its preconditions are met. The TGL rule in Figure 2 is applicable to input material as shown in Figure 1, because the COOP slot matches, and there is information about the THRESHOLD-VALUE available (otherwise a diﬀerent sentence pattern, and hence a diﬀerent rule, would be required). TGL rules contain categories as in a context-free grammar, which are used for rule selection (see below). The rules actions are carried out in a top-down, depth-ﬁrst and left-to-right manner. They include the activation of other rules (:RULE, :OPTRULE), the execution of a function, or the return of an ASCII string as a (partial) result. When selecting other rules by virtue of a category, the relevant portion of the input structure for which a candidate rule must pass its associated tests must be identiﬁed. The function get-param in Figure 2 yields the substructure of the current input depicted by the argument. The ﬁrst action selects all rules with category PPtime; the relevant substructure is the TIME slot of an IR. TGL rules are deﬁned according to the IR syntax deﬁnitions. This includes optional IR elements, many of which can simply be omitted without disturbing ﬂuency. In these cases, optional rules (OPTRULE) are deﬁned in TGL. Optional actions are ignored if the input structure does not contain relevant information. In certain cases, the omission of an IR element would suggest a diﬀerent sentence structure, which is accounted for by deﬁning alternative TGL rules with appropriate tests for the presence of some IR element. Agreement relations are encoded into TGL by virtue of a PATR style feature percolation mechanism [Shieber et al., 1983]. The rules can be annotated by equations that either assert equality of a features value at two or more constituents, or introduce a feature value at a constituent. The constraint in Figure 2 requires the categories THTYPE and EXCEEDS to agree in gender, thus implementing a subject-participle agreement relation in French. This general mechanism provides a considerable amount of ﬂexibility and goes beyond simple template ﬁlling techniques. A TGL rule is successfully applied if all actions are carried out. The rule returns the concate- nation of the substrings produced by the template actions. If an action fails, backtracking can be invoked ﬂexibly and eﬃciently using memoization techniques (see [Busemann, 1996]). 4 Costs and Beneﬁts As Reiter and Mellish note, the use of shallow techniques needs to be justiﬁed through a cost-beneﬁt analysis [Reiter and Mellish, 1993]. We specify the range of possible applications our approach is useful for, exempliﬁed by the report generator developed for the TEMSIS project. This application took an eﬀort of about eight person months, part of which were spent imple- menting interfaces to the TEMSIS server and to the database, and for making ourselves acquainted with details of the domain. The remaining time was spent on (1) the elicitation of user requirements and the deﬁnition of a small text corpus, (2) the design of IR according to the domain distinctions required for the corpus texts, and (3) text organization, adaptation of TG2 and grammar devel- opment. The grammars comprise 105 rules for the German and 122 for the French version. There are about twenty test predicates and IR access functions, most of which are needed for both languages. The French version was designed on the basis of the German one and took little more than a week to implement. The system covers a total of 384 diﬀerent report structures that diﬀer in at least one linguistic aspect. 4.1 Beneﬁts Altogether, the development eﬀort was very low. We believe that reusing an in-depth surface generator for this task would not have scored better. Our method has a number of advantages: (1) Partial reusability. Despite its domain-dependence, parts of the system are reusable. The TG2 interpreter has been adopted without modiﬁcations. Moreover, a sub-grammar for time expressions in the domain of appointment scheduling was reused with only minor extensions. (2) Modeling ﬂexibility. Realization techniques of diﬀerent granularity (canned text, templates, context-free grammars) allow the grammar writer to model general, linguistic knowledge as well as more speciﬁc task and domain-oriented wordings. (3) Processing speed. Shallow processing is fast. In our system, the average generation time of less than a second can almost be neglected (the overall run-time is longer due to database access). (4) Multi-lingual extensions. Additional languages can be included with little eﬀort because the IR is neutral towards particular languages. (5) Variations in wording. Alternative formulations are easily integrated by deﬁning conﬂicting rules in TGL. These are ordered according to a set of criteria that cause the system to prefer certain formulations to others (cf. [Busemann, 1996]). Grammar rules leading to preferred formulations are selected ﬁrst from a conﬂict set of concurring rules. The preference mechanisms will be used in a future version to tailor the texts for administrative and public uses. 4.2 Costs As argued above, the orientation towards the application task and domain yields some important beneﬁts. On the other hand, there are limitations in reusability and ﬂexibility: (1) IR cannot be reused for other applications. The consequences for the modules interfaced by IR, the text organizer and the text realizer, are a loss in generality. Since both modules keep a generic interpreter apart from partly domain-speciﬁc knowledge, the eﬀort of transporting the components to new applications is, however, restricted to modifying the knowledge sources. (2) By associating canned text with domain acts, TG2 behaves in a domain and task speciﬁc way. This keeps the ﬂexibility in the wording, which can only partly be inﬂuenced by the text organizer, inherently lower than with in-depth approaches. 4.3 When does it pay oﬀ? We take it for granted that the TEMSIS generation application stands for a class of comparable tasks that can be characterized as follows. The generated texts are information-conveying reports in a technical domain. The sublanguage allows for a rather straight-forward mapping onto IR expressions, and IR expressions can be realized in a context-independent way. For these kinds of applications, our methods provide suﬃcient ﬂexibility by omitting unnecessary or known informa- tion from both the schemes and its IR expressions, and by including particles to increase coherency. The reports could be generated in multiple languages. We recommend the opportunistic use of shal- low techniques for this type of application. Our approach is not suitable for tasks involving deliberate sentence planning, the careful choice of lexemes, or a sophisticated distribution of information onto linguistic units. Such tasks would not be compatible with the loose coupling of our components via IR. In addition, they would require complex tests to be formulated in TGL rules, rendering the grammar rather obscure. Finally, if the intended coverage of content is to be kept extensible or is not known precisely enough at an early phase of development, the eventual redesign of the intermediate structure and associated mapping rules for text organization may severely limit the usefulness of our approach. 5 Conclusion We have suggested shallow approaches to NL generation that are suited for small applications re- quiring limited linguistic resources. While these approaches ignore many theoretical insights gained through years of NLG research and instead revive old techniques once criticized for their lack of ﬂexibility, they nevertheless allow for the quick development of running systems. By integrating techniques of diﬀerent granularity into one formalism, we have shown that lack of ﬂexibility is not an inherent property of shallow approaches. Within the air quality report generation in TEMSIS, a non-trivial application was described. We also gave a qualitative evaluation of the domain char- acteristics to be met for our approach to work successfully. Further experience will show whether shallow techniques transpose to more complex tasks. We consider it a scientiﬁc challenge to combine shallow and in-depth approaches to analysis and generation in such a way that more theoretically motivated research ﬁnds its way into real applications. References [Bateman, 1997] John Bateman. KPML delvelopment environment: multilingual linguistic resource devel- opment and sentence generation. Report, German National Center for Information Technology (GMD), Institute for integrated publication and information systems (IPSI), Darmstadt, Germany, January 1997. Release 1.1. [Busemann, 1996] Stephan Busemann. Best-ﬁrst surface realization. In Donia Scott, editor, Eighth Inter- national Natural Language Generation Workshop. Proceedings, pages 101110, Herstmonceux, Univ. of Brighton, England, 1996. Also available at the Computation and Language Archive at cmp-lg9605010. [Cawsey et al., 1995] Alison Cawsey, Kim Binsted, and Ray Jones. Personalised explanations for patient education. In Fifth European Workshop on Natural Language Generation. Proceedings, pages 5974, Leiden, The Netherlands, 1995. [Davis and King, 1977] Randall Davis and Jonathan King. An overview of production systems. In E. W. Elcock and D. Michie, editors, Machine Intelligence 8, pages 300332. Ellis Horwood, Chichester, 1977. [Elhadad and Robin, 1996] Michael Elhadad and Jacques Robin. An overview of SURGE: a reusable com- prehensive syntactic realization component. In Donia Scott, editor, Eighth International Natural Language Generation Workshop. Demonstrations and Posters, pages 14, Herstmonceux, Univ. of Brighton, Eng- land, 1996. [Horacek, 1996] Helmut Horacek. Lexical choice in expressing metonymic relations in multiple languages. Machine Translation, 11:109158, 1996. [Kasper and Whitney, 1989] Robert Kasper and Richard Whitney. SPL: A sentence plan language for text generation. Technical report, USCInformation Sciences Institute, Marina del Rey, CA., 1989. [Kukich, 1983] Karen Kukich. Design and implementation of a knowledge-based report generator. In Pro- ceedings of the 21st Annual Meeting of the Association for Computational Linguistics, pages 145150, Cambridge, MA, 1983. [Meteer, 1992] M. Meteer. Expressibility and the Problem of Eﬃcient Text Planning. Frances Pinter, 1992. [Penman, 1989] Project Penman. penman documentation: the primer, the user guide, the reference manual, and the Nigel manual. Technical report, USCInformation Sciences Institute, Marina del Rey, CA, 1989. [Reiter and Mellish, 1993] Ehud Reiter and Chris Mellish. Optimizing the costs and beneﬁts of natural language generation. In Proc. 13th International Joint Conference on Artiﬁcial Intelligence, pages 1164 1169, Chambery, France, 1993. [Reiter et al., 1995] Ehud Reiter, Chris Mellish, and John Levine. Automatic generation of technical docu- mentation. Applied Artiﬁcial Intelligence, 9, 1995. [Shieber et al., 1983] Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson. The formalism and implementation of PATR-II. In Barbara J. Grosz and Mark E. Stickel, editors, Research on Interactive Acquisition and Use of Knowledge, pages 3979. AI Center, SRI International, Menlo Park, CA., 1983. [White and Caldwell, 1997] Michael White and David E. Caldwell. CogentHelp: NLG meets SE in a tool for authoring dynamically generated on-line help. In Proc. 5th Conference on Applied Natural Language Processing, pages 257264, Washington, DC., 1997.",
  "23.pdf": "arXiv:cs9901005v1 [cs.CL] 13 Jan 1999 Online Appendix to An Empirical Approach to Temporal Reference Resolution Janyce M. Wiebe, Thomas P. OHara, Thorsten Ohrstrom-Sandgren  Kenneth J. McKeever 1. Temporal Reference Resolution Rules 1.1 Introduction This document provides a detailed algorithm for temporal reference resolution as used in the Artwork system developed at New Mexico State Univsersity. The remainder of this section covers conventions used in the algorithm. The algorithm itself is in Section 1.2. 1.1.1 Abbreviations 1. SpanUtt  Spanish Utterance; a list of the Spanish words occurring in a sentence. For instance: cinco de marzo no would be represented as [cinco,de,marzo,no]. 2. TU  Temporal Unit. 3. ILT  A structured representation of the meaning of an SpanUtt, possibly containing a TU. The ILT representation is deﬁned by the Enthusiast project (Levin et al. 1995). 4. DE  Discourse Entity; Composed of a SpanUtt and an ILT. 5. FL  Focus List (ordered list of DEs). 6. RF  Reference Frame; a Temporal Unit used as a temporal frame of reference. 7. CF  Certainty Factor. 8. timeValue  One of {day, week, weekend, month, year}. 1.1.2 Functions 1. retrieveField(FieldName, Structure): returns the ﬁller for FieldName in Structure, if Structure contains a FieldName, otherwise NULL. 2. retrieveUtterance(DE): returns the Spanish utterance from DE, if one exists, oth- erwise NULL. 3. dateCopy(TU): Copies the start dates of a TU to their corresponding end dates. The function returns the modiﬁed TU and a CF, as a tuple. The CF is based on the combination of ﬁelds copied. 4. distanceFactor(DE): Returns a number reﬂecting how far back on the focus list DE is. 5. futureRange(N ILT): Returns a range in the future consistent with the ILT. For instance, second week in August, interpreted as the second Monday through the following Friday in August, closest in the future. If successful, a new TU is returned, otherwise, NULL. 1 6. last(timeV alue, RF): returns the last timeValue before reference frame RF. Ex: last(week, [Tues 20 Aug 1996] Mon-Fri, 12-16th, August, 1996 7. lastFLRange(timeV alue, N): N is either 1, or 2. If N  1 then lastFLRange returns the TU on the focus list that most closely matches timeValue from the focus list. If N  2, one matching TU on the list is skipped. 8. merge(TU1, TU2): if there exist conﬂicting ﬁelds in TU1 and TU2, return NULL. Otherwise return a TU which contains the union of all ﬁelds of TU1 and TU2. 9. mergeUpper(TU1, TU2): the same as merge, except only ﬁelds of the same or less speciﬁc levels than the most speciﬁc ﬁeld in TU1 are considered. 10. mostSpeciﬁc(X, TU): X is either start, end, or both to indicate the starting time, end time, or both the starting and end time, respectively. The function returns the speciﬁcity level of the most speciﬁc ﬁeld of TU, where month is level 1. Ex: mostSpeciﬁc(start, TU) returns the speciﬁcity level of the most speciﬁc ﬁeld in the start time of TU, and mostSpeciﬁc(both, TU) returns the speciﬁcity level of the most speciﬁc ﬁeld of the entire TU. 11. next(timeV alue, RF): returns the next timeValue that follows the reference frame RF. 12. nextInclToday(timeV alue, RF): Same as next, but this version considers today as a possibility. 13. resolveDeictic(TU, todaysDate): resolves the deictic term TU with respect to the dialog date. 14. applyRule(TU, RuleName [, subcase]): Invokes rule RuleName. Returns the return value of RuleName. 15. this(timeV alue): Returns the current timeV alue with respect to the conversation date. Ex: Dialog date is Thursday, 22th, August, 1996 This week has been long., Returns Mon-Fri, 19th-23rd, August, 1996 16. isDeictic(TU): returns TRUE if TU contains deictic information, otherwise NULL. 17. isRange(X, TU): returns TRUE, if TU is an X, where X  {week, day, time}, otherwise returns NULL. 18. moreEquSpeciﬁc(TU1, TU2): returns TRUE if TU1 is either more speciﬁc than TU2, or if the two have the same level of speciﬁcity, otherwise, NULL. 19. moreSpeciﬁcLow(TU1, TU2): same as moreEqu Speciﬁc, but only tests levels of less or equal speciﬁcity as time of day. 2 1.1.3 Typographic Conventions Variables are italicized (e.g., TU, startTimes). Variable values (e.g., month, date), key- words (e.g., if, then, else), and conceptual notations are in bold-face. Function names are in roman (e.g., merge(TU1, TU2)). XY refers to the ﬁeld(s) Y within X (e.g., TUstartTimes, TUendTimes, TUname)  Denotes comments in rules. forward- looking-adjective are adjectives that indicate a time in the future. For instance, next, following, or in Spanish proximo, siguiente, viene. 1.2 Rules Rules are invoked in two diﬀerent ways: one way for rules that do not access the focus list, and one for those that do. For rules that do not access the focus list the procedure is as follows. 1. Extract all of the TUs in the DE. 2. Apply the rule to each TU. 3. Put the resulting TUs back in the same order as they appeared in the original TUL. For rules the do access the focus list, a similar process is performed, but with one important diﬀerence: When accessing the speciﬁc rule for the ﬁrst Temporal Unit, which entity oﬀ the focus list that was used is recorded, and the remaining temporal units are forced to use the same one. 1.2.1 Rules for deictic relations Deictic Rule 1: (A type of relation D-simple) A deictic expression is resolved into a time interpreted with respect to the dialog date (e.g., Tomorrow, last week). if isDeictic(TU) then return 0.9, merge(TU, resolveDeictic(TU, todaysDate)) else fail  Ex: Lets meet tomorrow, with an interpretation of Mon 24 Sept Deictic Rule 2: (A type of relation D-frame-of-reference) A forward time is calculated with respect to today as a frame of reference. Subcase i: if not isDeictic(TU) then SpanUtt  retrieveUtterance(ILT) if forward-looking-adjective  SpanUtt  {} then if(mostSpeciﬁc(start, TU)  date) then new TU  next(mostSpeciﬁc(start,TU), todaysDate) return0.2, merge(TU, new TU) else fail 3 else fail else fail  Ex: How about if we meet next Monday?, interpreted as Monday, Sep 30 Subcase ii: if not isDeictic(TU) then if(mostSpeciﬁc(start, TU)  date) then new TU  nextInclToday(mostSpeciﬁc(start,TU), todaysDate) return0.3, merge(TU,new TU) else fail else fail  Ex: Hmmm, how about Monday? interpreted as the current Monday, that is Mon 23 Sept. This rule diﬀerers from the previous one only in that the current date is also taken into consideration. The following cover special cases not discussed in the body of the paper. Deictic Rule 3: (A type of relation D-simple) if not isDeictic(TU) then if(the rest of  TUSpec) then newTU  mergeAll(the rest of(Name)  Ex: I can meet Tuesday, resolved to Tue 25 Sept Good, I can meet for the rest of the week as well, resolved to Wed 26 Sept - Fri 28 Sept else if(the end of  Spec)  (this  Spec) then newTU  the end of(Name)  Ex: I can meet Monday, resolved to Mon 24 Sept No  better at the end of the week., resolved to Thu 27 Sept - Fri 28 Sept else if(the end of  Spec)  (next  Spec) then newTU  the end of next(Name)  Ex: I can meet at the end of next month, resolved to Mon 21 Oct - Thu 31 Oct (The last ten days of the coming month) else if(last  Spec) then newTU  last(Name, TU)  Ex: Last Thursdays meeting was very productive., resolved to Thu 17 Sept else if(this  Spec)  (coming  Spec) then newTU  next(Name, TU)  Ex: This coming Thursday is good., resolved to Thu 27 Sept else if(this  Spec) then newTU  this(Name, TU)  Ex: This has been a good month., resolved to Sat 1 Sept - Sun 30 Sept else newTU  futureRange(TU) 4 if(newTU)  null then fail  Ex: I can meet second week in October, under the interpretation Mon 8 Oct to Fri 12 Oct. return 0.5, merge(TU, newTU) Deictic Rule 4: (A special case for handling end times) The mentioned time is not an interval; resulting frames end dates correspond to the starting dates. if (simpleTime(TU)  {sday, sdate, smonth} is not null) then newTU, CF  dateCopy(TU) return CF, merge(TU, newTU) else fail  Ex: Lets meet Friday the 27 of September, under the interpretation that the start time and the end time both refer to Fri 27 Sept. Deictic Rule 5: (A type of relation D-simple) if at least one of {TUSMIN, TUSHOUR, TUSTIME DAY}  null  current utterance does not contain a date then  Make sure that no FL entry refers to time of day or lower if ( TU  FL : mostSpeciﬁc(both, TUfl)  time of day) then today TU  todays date return 0.5, merge(TU, today TU) else fail else fail  Ex: How about 3:00? resolved to 3:00 Mon 23 Sept Deictic Rule 6: (A type of relation D-simple) if(TUName  {weekend})  (TUSpec  {next, this, last}) then if(TUSpec  next then new TU  next(weekend) if(TUSpec  this then new TU  this(weekend) if(TUSpec  last then new TU  last(weekend) return 0.8, merge(TU, new TU) else fail else fail  Ex: Is next weekend OK? resolved to Sat 29-Sun 30 Sept 5 1.2.2 Rules for anaphoric relations Anaphoric Rule 1: (A type of relation A-co-reference) The times discussed are similar; the resulting frame is the union of both times. for each non-empty Temporal Unit TUfl from FL, starting with most recent if (moreEquSpeciﬁc(TU, TUfl))  (merge(TU, TUfl)  null) then return 0.8 - distanceFactor(TUfl), merge(TU, TUfl) else fail  Ex: Lets meet Tuesday. resolved to Tue 24 Sept How about 2? resolved to 2pm Tue 24 Sept Anaphoric Rule 2: (A type of relation A-less-speciﬁc) The current utterance evokes a time that includes the time evoked by a previous time, and the current time is less speciﬁc. for each non-empty Temporal Unit TUfl from FL, starting with most recent if not (moreEquSpeciﬁc(TU, TUfl))  (moreEquSpeciﬁc(TUfl, TU)) then if (merge(TU, TUfl)  null) then return 0.8 - distanceFactor(TU), merge(TU, TUfl) else fail else fail  Ex: Lets meet Tuesday. resolved to Tue 24 Sept How about 2? resolved to 2pm Tue 24 Sept On Tuesday? resolved to Tue 24 Sept This subcase diﬀers from the previous subcase in that in this case a relationship can be made to hold if the current utterance is less speciﬁc than the one on the focus list. Anaphoric Rule 3: (A type of relation A-frame-of-reference) A forward time is calculated with respect to a time on the Focus List as a frame of reference. Subcase i: if not isDeictic(TU) then if {forward-looking-adjective}  ILTSpanUtt  {} then if(leastSpeciﬁc(TUstartFields)  date) then for each non-empty Temporal Unit TUfl from FL, starting with most recent if (TUname  {week} then Base CF  0.4 else Base CF  0.3 if moreEquSpeciﬁc(TU, TUfl) then RF  retrieveStartDate(TUfl) new TU  next(mostSpeciﬁc(start,TU), RF) 6 return Base CF - distanceFactor(TU), merge(TU, new TU) else fail else fail else fail else fail  Ex: Lets meet next Monday. resolved to Mon 30 Sept Tuesday is better for me. resolved to Tue 1 Oct Subcase ii: if not isDeictic(TU) then if(leastSpeciﬁc(TUstartFields)  date) then for each non-empty Temporal Unit TUfl from FL, starting with most recent if moreEquSpeciﬁc(TU, TUfl) then RF  retrieveStartDate(TUfl) new TU  nextInclToday(mostSpeciﬁc(start,TU), RF) return 0.35 - distanceFactor(TU), merge(TU, new TU) else fail else fail else fail  This is nearly identical to the previous case, but were not looking for a forward-looking-adjective, and todays date is considered. Ex: Lets meet next Monday. resolved to Mon 30 Sept Monday is good. resolved to Mon 30 Sept Since the second sentence doesnt provide clues about the Monday (such as next, the following etc.), the current Monday on the focus list is used as opposed to the Monday following Monday the 30th. Anaphoric Rule 4: (A type of relation A-modify) The current time is a modiﬁcation of a previous time if not isDeictic(TU) then if(mostSpeciﬁc(both, TU)  time of day) for each non-empty Temporal Unit TUfl from FL, starting with most recent if (moreSpeciﬁcLow(TU, TUfl) then return 0.35 - distanceFactor(TU), mergeUpper(TUfl, TU) else fail else fail else fail  Ex: Lets meet Tuesday at 2:00. resolved to 2 pm, Tue 24 Sept 3:00 is better for me. resolved to 3 pm, Tue 24 Sept The following anaphoric rules do not appear in the paper. Anaphoric Rule 5: (A type of relation A-frame-of-reference) 7 if (TUstartFields  {})  (TUendFields  {}) then for each non-empty Temporal Unit TUfl from FL, starting with most recent if moreEquSpeciﬁc(TU, TUfl) then new TU  mergeUpper(TUfl, TU) if(leastSpeciﬁc(end, TU)  date) then RF  retrieveStartDate(TUfl) new TUendFields  next(mostSpeciﬁc(endFields, TU), RF) return 0.5 - distanceFactor(TU),new TU) else fail else fail else fail  Ex: How about Tuesday the 25th at 2? resolved to 2 pm, Tue 25 Sept I am busy until Friday, resolved to Tue 25-Friday 28 Sept Anaphoric Rule 6: (A type of relation A-frame-of-reference) Subcase i: if not(isDeictic(TU)) then if(TUSpec  {that, same, all range, less than, more than, long})  (TUName  {month, week, day, time}) then TUfl  lastFLRange(Name) if (mergeUpper(TUfl, TU))  null then return0.5,mergeUpper(TUfl, TU) else if (merge(TUfl, TU)) null then return0.5,merge(TUfl, TU) else fail else fail else fail  Ex: Lets meet Oct 8th to Oct 11th resolved to Tue 8 Oct - Fri 11 Oct that week sounds good resolved to Tue 8 Oct - Fri 11 Oct Subcase ii: if not(isDeictic(TU)) then Name  retrieveField(name, TU) Spec  retrieveField(speciﬁer, TU) if(the rest of  Spec)  (that  Spec) then startTUstartFields  applyRule(TU, ANA3, i) endTUendFields  next(Name, startFields) newTU  merge(startTU, endTU) else if(the end of  Spec)  (that  Spec) then RF  applyRule(TU, ANA3, i) newTU  the end of(RF) return 0.5,newTU else fail else fail 8  Ex: Lets meet Oct 8th to Oct 11th resolved to Tue 8 Oct - Fri 11 Oct The end of that week is better resolved to Thu 10 Oct - Fri 11 Oct Anaphoric Rule 7: (A type of relation A-co-reference) if(other  TUSpec)  (indeﬁnite not in TUSpec) then if(Name  {month, week, day, time}) then newTU  lastFLRange(TU,2) if(merge(newTU, TU)  null) then return 0.7 - distanceFactor(newTU),merge(newTU,TU) else fail else fail  Ex: Lets meet Tuesday. resolved to Tue 24 Sept How about Thursday? resolved to Thu 26 Sept No, the other day sounds better resolved to Tue 24 Sept distanceFactor(TUfl), new TU Anaphoric Rule 8: (A type of relation A-co-reference) Name :  retrieveField(name, TU) Spec  retrieveField(speciﬁer, TU) if(Name :  {week, day, time})  (Spec  {that, plural, both of}) then for each non-empty Temporal Unit TU1fl from FL, starting with most recent for each non-empty Temporal Unit TU2fl from FL, starting with TU1fl if(isRange(Name :, TU1fl))  (isRange(Name :, TU2fl)) then Speaker1  retrieveField(speaker, SpanUtt) Speaker2  retrieveField(speaker, SpanUtt) if(Speaker1  Speaker2) then if(Name  day)  (TU1fl  TU2fl are within the same week)  if(Name  week)  (TU1fl  TU2fl are within the same year)  if(Name  day)  (TU1fl  TU2fl are in the same day) then return 0.7, [merge(TU, TU1fl), merge(TU, TU2fl)] else fail else fail else fail else fail Ex: How about Tuesday and Wednesday? resolved to Tue 24 Sept, Wed 25 Sept Those days sound ﬁne. resolved to Tue 24 Sept, Wed 25 Sept  This rule heuristically uses the last two occurrences of Name :. 9 2. Normalized form of a Temporal Unit This appendix gives the speciﬁc structure, and possible information contained within a normalized Temporal Unit (TU), as used in the Artwork project, developed at New Mex- ico State University in conjunction with the Computing Research Laboratory. A list of Temporal Units LTU is a structure containing one or more TUs. 2.1 Notation Used The following BNF-style convention is used: 1. atom An atom is a single entry in a list. An atom cannot be expanded any further. 2. Non-terminal A Non-terminal indicates a structure that is constructed by either one or more atoms, or of other non-terminals. 3. [...] open-close angeled brackets denote the beginning and end of a list. For example, [this, is, a, list] is a list containing four atoms. 4.  An asterisk denotes a list that may be repeated zero or more times. For example: [Repeat, this, list]  [ ] (empty list) or [repeat, this, list],. . . , [repeat, this, list]. 5.  A plus sign denotes a list that occurs one or more times. time. 6.  The  sign indicates a choice. For example: Person  man  woman 2.2 High Level Format of a When Frame All structures in the Artwork project, adhere with the following format: [Structure Name, Structure Filler] For example, [Structure Name, Structure Filler] [sday week, SDAY WEEK] [sday week, thursday]. The general format of an LTU follows: LTU  [when-frame, TU] Thus, the name of the structure is when-frame and the ﬁller is either zero ([ ]), or more TU. 10 2.3 Detailed Format of a TU [when, [connective, CONNECTIVE], [gen spec, GEN SPEC], [duration, [length, DURATION], [dur speciﬁer, DUR SPECIFIER], [name, NAME], [interval, [speciﬁer, [SPECIFIER]], [start, [sday status, [DAY VALUE, DAY ORIGIN], [sday week, DAY WEEK], [sday, DAY], [stime day, TIME DAY], [sam pm, AM PM]], [smonth, [MONTH VALUE, MONTH ORIGIN]], [shour status, [HOUR VALUE, HOUR ORIGIN], [stime adv, TIME ADV], [shour, HOUR], [smin, MIN]]], [end, [eday status, [DAY VALUE, DAY ORIGIN], [eday week, DAY WEEK], [eday, DAY], [etime day, TIME DAY], [eam pm, AM PM]], [emonth, [MONTH VALUE, MONTH ORIGIN]], [ehour status, [HOUR VALUE, HOUR ORIGIN], [etime adv, TIME ADV], [ehour, HOUR], [emin, MIN]]]], [modiﬁers, MODIFIER]] Figure 1: Structural layout of a Temporal Unit. Figure 1 illustrates the structural layout of a single TU. The following tables and ﬁgures presents the possible values each temporal ﬁeld can take (Table 1, Figure 2 and Table 2). The NAME ﬁeld indicates the name referred to in the ﬁelds in the above structured TU. The VALUE column illustrates the set of values that each entry might take. A null value indicates that no information is available. The third column provides a description of the name ﬁeld. An in-depth description about each ﬁeld is given at the end of the table. The last ﬁeld in the table, R indicates whether the information can be repeated. A Y 11 indicates that it can be, and an N indicates that it cannot be. If a ﬁeld is repeated, it is preceded by the keyword multiple, otherwise it is not preceded by the keyword. For example, dur speciﬁer  {multiple, the end of, second} and dur speciﬁer  {the end of} Since dur speciﬁer can be repeated, it is indicated by the keyword multiple, as shown in the ﬁrst example. However, since there is only one ﬁller in the second example, the keyword multiple is omitted. Fields that can take on a large number of values or need special attention are underlined in the second column of the table, and expanded at the end of this section. A Temporal Unit comprises three parts: information about the time in general (e.g., duration, name, etc.), the start time, and the end time. Table 1 shows information on the general ﬁelds, and Figure 2 indicates the values these ﬁelds can take. Table 2 shows ﬁelds applicable to the starting time of a TU. The ending time ﬁelds are similar to the ﬁelds in Table 2, so no table is shown for the ending times. NAME POSSIBLE VALUES DESCRIPTION CONNECTIVE and  because  but  for example  if  or  so  then  therefore  unless  that is to say  null how multiple TUs are connected GEN SPEC generic  speciﬁc  null the genericity of a time DURATION 0..MAXINT (only whole num- bers)  epsilon  undetermined  null the diﬀerence (in hours) between the start and end time DUR SPECIFIER determined  part determined  not-complete the frame from which the dura- tion information was retrieved unless calculated or null; repeatable NAME Indexical  Special Name  Unit  null special information about the current TU not necessarily deter- minable by other ﬁelds. SPECIFIER Speciﬁer  0..MAXINT (only whole numbers)  null Info Table 1: General ﬁelds of a Temporal Unit 12 Speciﬁer :: all member  all range  also  another  any  anytime  approximate  at least  behind  both of  concrete  couple  deﬁnite  early  either of  even  exact  except  few  ﬁrst  following  fourth  in front of  indeﬁnite  last  late  less than  long  middle  more than  most member  most range  necessary  negative  next  only  other  particular  perhaps  plural  preceding  same  second  second last  short  some  sometime  that  the end of  the rest of  third  this  what  which Indexical :: now  today  tomorrow  date  time  then  that  there  here  later  when  what Special Name :: independence day  thanksgiving  thanksgiving day  labor day Unit :: minute  hour  day  week  month  year  decade  century  lunch time  working day  weekend  weekday  early  late  sometime  anytime Figure 2: Possible values for the general ﬁelds 13 NAME POSSIBLE VALUES DESCRIPTION R SDAY VALUE determined  part determined  unde- termined  same  null amount of information avail- able to determine the start date N SDAY ORIGIN when  clariﬁed  topic  null the frame from which the start day information was retrieved Y SDAY WEEK monday,. . . , sunday  null the start week-day N SDAY 1,. . . , 31  null the start date N STIME DAY afternoon  evening  mid afternoon  mid morning  midnight  morning  night  noon  null the general start time of a 24- hour period N SAM PM am  pm  null the start time (morning  afternoon) N SMONTH VALUE 1,. . . , 12  null the starting month in a nu- merical format. E.g.  Jan- uary  1,. . . , December  12 N SMONTH ORIGIN when  clariﬁed  topic  null the frame from which the start month was retrieved Y SHOUR VALUE determined  part determined  unde- termined  same  null amount of information avail- able to determine the start time N SHOUR ORIGIN when  clariﬁed  topic  null the frame from which the start hour information was retrieved Y STIME ADV after  at  before  from  til  to  until  null the adverb associated with the start time N SHOUR 1,. . . , 12  null This ﬁeld represents the start hour of a time. N SMIN 0,. . . , 59 the start minutes associated with the start hour N Table 2: Temporals ﬁelds of a Temporal Unit 2.4 Examples This section provides examples, and a brief discussion about selected ﬁeld representations given in Table 1 and Table reftemporal-ﬁelds. See Figure 3. 14 CONNECTIVE: Ex: I can meet Wednesday or Thursday This sentence will produce two TUs, both of which have CONNECTIVE set to or GEN SPEC: Ex: How about Wednesday? Since Wednesday is any wednesday, GEN SPEC is set to generic Ex: OK, that Wednesday sounds good In this example, the speaker is referring to a speciﬁc Wednesday, and, thus, GEN SPEC is set to speciﬁc DURATION: Ex: I can meet from 2:00 until 4:00 DURATION is set to 2 (hours) NAME: Ex: I can meet next Thursday NAME is set to next (It gives speciﬁc information about the time) SPECIFIER: Ex: The second week in August is good for me In this example, SPECIFIER is set to week (NAME  second) SDAY VALUE: Ex: I can meet Thursday SDAY VALUE is set to part-determined, since the exact Thursday cannot be determined. Ex: I can meet Thursday the 11th of August SDAY VALUE is set to determined, since we have enough information about the Thursday in question SHOUR VALUE: Ex: How about 4 or so? SHOUR VALUE is set to part-determined, since we dont know the exact time. (In this example, the minute information is missing) Ex: How about next Thursday SHOUR VALUE is set to undetermined, since no information about the time is available. Figure 3: Examples of ﬁeld values for temporal units 15 3. Coverage of Temporal Expressions The BNF grammar in Figure 4 describes the core set of the temporal expressions handled by our temporal reference resolution system. To simplify the grammar, some variations are not speciﬁed. Combinations of the expressions are also allowed, provided that this doesnt lead to contradictory temporal interpretations. This is based on English transcription of the Spanish phrases that are covered by the CMU semantic parser. Note that conjunction is allowed, although not speciﬁed here. Also, brackets are used for optional components, and parentheses are used for grouping items. TEMPORAL EXPRESSION :: [RELATIVE] DATE PHRASE  [RELATIVE] TIME PHRASE  [RELATIVE] DATE PHRASE [\"at\"] TIME PHRASE  [RELATIVE] TIME PHRASE [\"on\"] DATE PHRASE  \"from\" DATE PHRASE (\"to\"\"until\") DATE PHRASE  \"from\" TIME PHRASE (\"to\"\"until\") TIME PHRASE RELATIVE :: \"at\"  \"around\"  \"after\"  \"before\"  \"on\"  \"in\" DATE PHRASE :: DEICTIC  RELATIVE DATE  SPECIFIC DATE  ABSOLUTE DATE DEICTIC :: \"today\"  \"tomorrow\"  \"yesterday\"  \"now\" RELATIVE DATE :: SPECIFIER PERIOD SPECIFIC DATE :: DEMONSTRATIVE PERIOD SPECIFIER :: \"next\"  \"following\"  \"coming\"  \"the rest of\"  \"the end of\"  \"this\"  \"this coming\"  \"all\" DEMONSTRATIVE :: \"that\"  \"those\" PERIOD :: \"day\"  WEEKDAY  \"week\"  MONTH  \"month\"  \"morning\"  \"afternoon\"  \"lunchtime\"  \"evening\"  \"night\" ABSOLUTE DATE :: [WEEKDAY] [\"the\"] MONTH DATE [\"of\" MONTH]  [WEEKDAY] MONTH MONTH DATE  [\"in\"] MONTH  [\"on\"] WEEKDAY TIME PHRASE :: HOUR_MIN  TIME OF DAY HOUR_MIN :: HOUR MINUTE [TIME OF DAY] TIME OF DAY :: \"am\"  \"pm\"  \"in the morning\"  \"in the afternoon\"  \"lunch\"  \"night\" Figure 4: BNF speciﬁcation for system coverage of temporal expressions 16 arXiv:cs9901005v1 [cs.CL] 13 Jan 1999 Journal of Artiﬁcial Intelligence Research 9 (1998) 247-293 Submitted 598; published 1198 An Empirical Approach to Temporal Reference Resolution Janyce M. Wiebe wiebecs.nmsu.edu Thomas P. OHara tomoharacs.nmsu.edu Thorsten Ohrstrom-Sandgren sandgrenlucent.com Kenneth J. McKeever kmckeeveredwood.dn.hac.com Department of Computer Science and the Computing Research Laboratory New Mexico State University Las Cruces, NM 88003 Abstract Scheduling dialogs, during which people negotiate the times of appointments, are com- mon in everyday life. This paper reports the results of an in-depth empirical investigation of resolving explicit temporal references in scheduling dialogs. There are four phases of this work: data annotation and evaluation, model development, system implementation and evaluation, and model evaluation and analysis. The system and model were developed primarily on one set of data, and then applied later to a much more complex data set, to assess the generalizability of the model for the task being performed. Many diﬀerent types of empirical methods are applied to pinpoint the strengths and weaknesses of the approach. Detailed annotation instructions were developed and an intercoder reliability study was performed, showing that naive annotators can reliably perform the targeted an- notations. A fully automatic system has been developed and evaluated on unseen test data, with good results on both data sets. We adopt a pure realization of a recency-based focus model to identify precisely when it is and is not adequate for the task being addressed. In addition to system results, an in-depth evaluation of the model itself is presented, based on detailed manual annotations. The results are that few errors occur speciﬁcally due to the model of focus being used, and the set of anaphoric relations deﬁned in the model are low in ambiguity for both data sets. 1. Introduction Temporal information is often a signiﬁcant part of the meaning communicated in dialogs and texts, but is often left implicit, to be recovered by the listener or reader from the surrounding context. When scheduling a meeting, for example, a speaker may ask, How about 2? expecting the listener to determine which day is being speciﬁed. Recovering temporal information implicitly communicated in the discourse is important for many nat- ural language processing applications. For example, consider extracting information from memos and reports for entry into a database. It would be desirable to enter completely re- solved dates and times, rather than incomplete components such as the day or time alone. A speciﬁc application for which temporal reference resolution is important is appointment scheduling in natural language between human and machine agents (Busemann, Declerck, Diagne, Dini, Klein,  Schmeier, 1997). To fully participate, the machine agent must be able to understand the many references to times that occur in scheduling dialogs. Maintaining the temporal context can aid in other aspects of understanding. For exam- ple, Levin et al. (1995) and Rose et al. (1995) found that the temporal context, as part of c1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Wiebe, OHara, Ohrstrom-Sandgren,  McKeever the larger discourse context, can be exploited to improve various kinds of disambiguation, including speech act ambiguity, type of sentence ambiguity, and type of event ambiguity. This paper presents the results of an in-depth empirical investigation of temporal ref- erence resolution. Temporal reference resolution involves identifying temporal information that is missing due to anaphora, and resolving deictic expressions, which must be interpreted with respect to the current date. The genre addressed is scheduling dialogs, in which partic- ipants schedule meetings with one another. Such strongly task-oriented dialogs would arise in many useful applications, such as automated information providers and phone operators. A model of temporal reference resolution in scheduling dialogs was developed through an analysis of a corpus of scheduling dialogs. A critical component of any method for anaphora resolution is the focus model used. It appeared from our initial observations that a recency- based model might be adequate. To test this hypothesis, we made the strategic decision to limit ourselves to a local, recency-based model of focus, and to analyze the adequacy of such a model for temporal reference resolution in this genre. We also limit the complexity of our algorithm in other ways. For example, there are no facilities for centering within a discourse segment (Sidner, 1979; Grosz, Joshi,  Weinstein, 1995), and only very limited ones for performing tense and aspect interpretation. Even so, the methods investigated in this work go a long way toward solving the problem. From a practical point of view, the method is reproducible and relatively straightforward to implement. System results and the detailed algorithm are presented in this paper. The model and the implemented system were developed primarily on one data set, and then applied later to a much more complex data set to assess the generalizability of the model for the task being performed. Both data sets are challenging, in that they both include negotiation, contain many disﬂuencies, and show a great deal of variation in how dates and times are discussed. However, only in the more complex data set do the participants discuss their real life commitments or stray signiﬁcantly from the scheduling task. To support the computational work, the temporal references in the corpus were manually annotated. We developed explicit annotation instructions and performed an intercoder reliability study involving naive subjects, with excellent results. To support analysis of the problem and our approach, additional manual annotations were performed, including anaphoric chain annotations. The systems performance on unseen test data from both data sets is evaluated. On both, the system achieves a large improvement over the baseline accuracy. In addition, ablation (degradation) experiments were performed, to identify the most signiﬁcant aspects of the algorithm. The system is also evaluated on unambiguous input, to help isolate the contribution of the model itself to overall performance. The system is an important aspect of this work, but does not enable direct evaluation of the model, due to errors committed by the system in other areas of processing. Thus, we evaluate the model itself based on detailed manual annotations of the data. Important questions addressed are how many errors are attributable speciﬁcally to the model of focus and what kinds of errors they are, and how good is the coverage of the set of anaphoric relations deﬁned in the model and how much ambiguity do the relations introduce. The analysis shows that few errors occur speciﬁcally due to the model of focus, and the relations are low in ambiguity for the data sets. 248 An Empirical Approach to Temporal Reference Resolution The remainder of this paper is organized as follows. The data sets are described in Section 2. The problem is deﬁned and the results of an intercoder reliability study are presented in Section 3. An abstract model of temporal reference resolution is presented in Section 4 and the high-level algorithm is presented in Section 5. Detailed results of the implemented system are included in Section 6, and other approaches to temporal reference resolution are discussed in Section 7. In the ﬁnal part of the paper, we analyze the challenges presented by the dialogs to an algorithm that does not include a model of global focus (in Section 8.1), evaluate the coverage, ambiguity, and correctness of the set of anaphoric relations deﬁned in the model (in Section 8.2), and assess the importance of the architectural components of the algorithm (in Section 8.3). Section 9 is the conclusion. There are three online appendices. Online Appendix 1 contains a detailed speciﬁcation of the temporal reference resolution rules that form the basis of the algorithm. Online Appendix 2 gives a speciﬁcation of the input to the algorithm. Online Appendix 3 contains a BNF grammar describing the core set of the temporal expressions handled by the system. In addition, the annotation instructions, sample dialogs, and manual annotations of the dialogs are available on the project web site, http:www.cs.nmsu.eduwiebeprojects. 2. The Corpora The algorithm was primarily developed on a sample of a corpus of Spanish dialogs collected under the JANUS project at Carnegie Mellon University (Shum, Levin, Coccaro, Carbonell, Horiguchi, Isotani, Lavie, Mayﬁeld, Rose, Van Ess-Dykema  Waibel, 1994). These dialogs are referred to here as the CMU dialogs. The algorithm was later tested on a corpus of Spanish dialogs collected under the Artwork project at New Mexico State University by Daniel Villa and his students (Wiebe, Farwell, Villa, Chen, Sinclair, Sandgren, Stein, Zarazua,  OHara, 1996). These are referred to here as the NMSU dialogs. In both cases, subjects were asked to set up a meeting based on schedules given to them detailing their commitments. The NMSU dialogs are face-to-face, while the CMU dialogs are like telephone conversations. The participants in the CMU dialogs rarely discuss anything from their real lives, and almost exclusively stay on task. The participants in the NMSU dialogs embellish the schedule given to them with some of their real life commitments, and often stray from the task, discussing topics other than the meeting being planned. 3. The Temporal Annotations and Intercoder Reliability Study Consider the passage shown in Figure 1, which is from the CMU corpus (translated into English). An example of temporal reference resolution is that utterance (2) refers to 2-4pm, Thursday 30 September. Because the dialogs are centrally concerned with negotiating an interval of time in which to hold a meeting, our representations are geared toward such intervals. The basic representational unit is given in Figure 2. It is referred to throughout as a Temporal Unit (TU). 249 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever Temporal context: Tuesday 28 September s1 1 On Thursday I can only meet after two pm 2 From two to four 3 Or two thirty to four thirty 4 Or three to ﬁve s2 5 Then how does from two thirty to four thirty seem to you 6 On Thursday s1 7 Thursday the thirtieth of September Figure 1: Corpus Example ((start-month, start-date, start-day-of-week, start-hourminute, start-time-of-day) (end-month, end-date, end-day-of-week, end-hourminute, end-time-of-day)) Figure 2: The Temporal Unit Representation For example, the time speciﬁed1 in From 2 to 4, on Wednesday the 19th of August is represented as follows: ((August, 19, Wednesday, 2, pm) (August, 19, Wednesday, 4, pm)) Thus, the information from multiple noun phrases is often merged into a single representa- tion of the underlying interval speciﬁed by the utterance. Temporal references to times in utterances such as The meeting starts at 2 are also represented in terms of intervals. An issue this kind of utterance raises is whether or not a speculated end time of the interval should be ﬁlled in, using knowledge of how long meetings usually last. In the CMU data, the meetings all last two hours, by design. However, our annotation instructions are conservative with respect to ﬁlling in an end time given a starting time (or vice versa), specifying that it should be left open unless something in the dialog explicitly suggests otherwise. This policy makes the instructions applicable to a wider class of dialogs. Weeks, months, and years are represented as intervals starting with the ﬁrst day of the interval (for example, the ﬁrst day of the week), and ending with the last day of the interval (for example, the last day of the week). Some times are treated as points in time (for example, the time speciﬁed in It is now 3pm). These are represented as Temporal Units with the same starting and end times (as 1. Many terms have been used in the literature for the relation between anaphoric expressions and discourse entities. For example, Sidner (1983) and Webber (1983) argue that refer should be reserved for something people do with words, rather than something words do. Webber uses the term evoke for ﬁrst references to an entity and access for subsequent references. Sidner uses the term specify for the relation between a noun phrase and a discourse entity. We primarily use Sidners term, but use refer in a few contexts in which it seems more natural. 250 An Empirical Approach to Temporal Reference Resolution in Allen, 1984). If just the starting or end time is speciﬁed, all the ﬁelds of the other end of the interval are null. And, of course, all ﬁelds are null for utterances that do not contain any temporal information. In the case of an utterance that speciﬁes multiple, distinct intervals, the representation is a list of Temporal Units (for further details of the coding scheme, see OHara, Wiebe,  Payne, 1997). Temporal Units are also the representations used in the evaluation of the system. That is, the systems answers are mapped from its more complex internal representation (an ILT, see Section 5.2) into this simpler vector representation before evaluation is performed. The evaluation Temporal Units used to assess the systems performance were annotated by personnel working on the project. The training data were annotated by the second author of this paper, who also worked on developing the rules and other knowledge used in the system. However, the test data were annotated by another project member, Karen Payne, who contributed to the annotation instructions and to the integration of the system with the Enthusiast system (see below in Section 5.2), but did not contribute to developing the rules and other knowledge used in the system. As in much recent empirical work in discourse processing (see, for example, Arhenberg, Dahlback,  Jonsson, 1995; Isard  Carletta, 1995; Litman  Passonneau, 1995; Moser  Moore, 1995; Hirschberg  Nakatani, 1996), we performed an intercoder reliability study investigating agreement in annotating the times. The main goal in developing annotation instructions is to make them precise but intuitive so that they can be used reliably by non- experts after a reasonable amount of training (see Passonneau  Litman, 1993; Condon  Cech, 1995; Hirschberg  Nakatani, 1996). Reliability is measured in terms of the amount of agreement among annotators; high reliability indicates that the encoding scheme is re- producible given multiple annotators. In addition, the instructions also serve to document the annotations. The subjects were three people with no previous involvement in the project. They were given the original Spanish and the English translations. However, as they have limited knowledge of Spanish, in essence they annotated the English translations. The subjects annotated two training dialogs according to the instructions. After receiv- ing feedback, they annotated four unseen test dialogs. Intercoder reliability was assessed using Cohens Kappa statistic (κ) (Siegel  Castellan, 1988; Carletta, 1996). Agreement for each Temporal Unit ﬁeld (for example, start-month) was assessed independently. κ is calculated as follows: κ  Pa  Pe 1  Pe The numerator is the average percentage agreement among the annotators (Pa) less a term for expected chance agreement (Pe), and the denominator is 100 agreement less the same term for chance agreement (Pe). Pa and Pe are calculated as follows (Siegel  Castellan, 1988). Suppose that there are N objects, M classes, and K taggers. We have the following deﬁnitions.  nij is the number of assignments of object i to category j. Thus, for each i, M j1 nij  K.  Cj  N i1 nij, the total number of assignments of objects to category j. 251 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever  pj  Cj NK , the percentage of assignments to category j (note that N  K is the total number of assignments). We can now deﬁne Pe: Pe  M  j1 p2 j The extent of agreement among the taggers concerning the ith object is Si, deﬁned as follows. It is the total number of actual agreements for object i, over the maximum possible agreement for one object: Si  M j1  nij 2   K 2  Finally, Pa is the average agreement over objects: Pa  1 N N  i1 Si κ is 0.0 when the agreement is what one would expect under independence, and it is 1.0 when the agreement is exact (Hays, 1988). A κ value of 0.8 or greater indicates a high level of reliability among raters, with values between 0.67 and 0.8 indicating only moderate agreement (Hirschberg  Nakatani, 1996; Carletta, 1996). In addition to measuring intercoder reliability, we compared each coders annotations to the gold standard annotations used to assess the systems performance. Results for both types of agreement are shown in Table 1. The agreement among coders is shown in the column labeled κ, and the average pairwise κ values for the coders and the expert who performed the gold standard annotations are shown in the column labeled κavg. This was calculated by averaging the individual κ scores (which are not shown). There is a high level of agreement among annotators in all cases except the end time of day ﬁeld, a weakness we are investigating. There is also good agreement between the evaluation annotations and the naive coders evaluations: with the exception of the time of day ﬁelds, κavg indicates high average pairwise agreement between the expert and the naive subjects. Busemann et al. (1997) also annotate temporal information in a corpus of scheduling dialogs. However, their annotations are at the level of individual expressions rather than at the level of Temporal Units, and they do not present the results of an intercoder reliability study. 4. Model This section presents our model of temporal reference resolution in scheduling dialogs. Section 4.1 describes the cases of deictic reference covered and Section 4.2 presents the 252 An Empirical Approach to Temporal Reference Resolution Field Pa Pe κ κavg start Month .96 .51 .93 .94 Date .95 .50 .91 .93 DayofWeek .96 .52 .91 .92 HourMin .98 .82 .89 .92 TimeDay .97 .74 .87 .74 end Month .97 .51 .93 .94 Date .96 .50 .92 .94 DayofWeek .96 .52 .92 .92 HourMin .99 .89 .90 .88 TimeDay .95 .85 .65 .52 Table 1: Agreement Among Coders day of week ր ց month time of day  hourminute ց ր date Figure 3: Speciﬁcity Ordering anaphoric relations deﬁned. Section 4.3 gives some background information about focus models, and then describes the focus model used in this work. Anaphora is treated in this paper as a relationship between a Temporal Unit representing a time speciﬁed in the current utterance (TUcurrent) and one representing a time speciﬁed in a previous utterance (TUprevious). The resolution of the anaphor is a new Temporal Unit representing the interpretation, in context, of the contributing words in the current utterance. Fields of Temporal Units are partially ordered as in Figure 3, from least to most speciﬁc. The month has the lowest speciﬁcity value. In all cases of deictic reference listed in Section 4.1 and all cases of anaphoric reference listed in Section 4.2, after the resolvent has been formed, it is subjected to highly accurate, obvious inference to produce the ﬁnal interpretation. Examples are ﬁlling in the day of the week given the month and the date; ﬁlling in pm for modiﬁers such as afternoon; and ﬁlling in the duration of an interval from the starting and end points. In developing the rules, we found domain knowledge and task-speciﬁc linguistic conven- tions to be most useful. However, we observed some cases in the NMSU data for which syntactic information could be exploited (Grosz et al., 1995; Sidner, 1979). For example, until in the following suggests that the ﬁrst utterance speciﬁes an end time. 253 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever ... could it be until around twelve? 12:30 there A preference for parallel syntactic roles might be used to recognize that the second utterance speciﬁes an end time too. We intend to pursue such preferences in future work. 4.1 Deictic References The deictic expressions addressed in this work are those interpreted with respect to the dialog date (i.e., today in the context of the dialog). 4.1.1 Simple deictic relation A deictic expression such as tomorrow or last week is interpreted with respect to the dialog date. (See rule D-simple in Section 5.3.) 4.1.2 Frame of reference deictic relation A forward time reference is calculated using the dialog date as a frame of reference. Let F be the most speciﬁc ﬁeld in TUcurrent less speciﬁc than time of day (e.g., the date ﬁeld). The resolvent is the next F after the dialog date, augmented with the ﬁllers of the ﬁelds in TUcurrent that are at least as speciﬁc as time of day. (See rule D-frame-of-reference in Section 5.3.) Following is an example. Assume that the dialog date is Monday 19 August. Utterance Interpretation How about Wednesday at 2? 2 pm, Wednesday 21 August For both this and the frame of reference anaphoric relation, there are subcases for whether the starting andor end times are involved. 4.2 Anaphoric Relations Generally speaking, many diﬀerent kinds of relationships can be established between an anaphor and its antecedent. Examples are co-reference (John saw Mary. He. . .), part- whole (John bought a car. The engine. . .), and individual-class (John bought a truck. They are good for hauling. . .) (see, for example, Webber, 1983). The latter two involve bridging descriptions (see, for example, Clark, 1977; Heim, 1982; Poesio, Vieira,  Teufel, 1997): some reasoning is required to infer the correct interpretation. This section presents a set of anaphoric relations that have good coverage for temporal expressions in scheduling dialogs (see Section 8.2 for an evaluation). Many temporal references involve bridging inferences, in the sense that times are calculated by using the antecedent as a frame of reference or by modifying a previous temporal interpretation. 4.2.1 Co-reference anaphoric relation The same times are speciﬁed, or TUcurrent is more speciﬁc than TUprevious. The resolvent contains the union of the information in the two Temporal Units. (See rule A-co-reference in Section 5.3.) 254 An Empirical Approach to Temporal Reference Resolution For example (see also (1)-(2) of the corpus example in Figure 1): Utterance Interpretation How is Tuesday, January 30th? How about 2? 2pm, Tuesday 30 January 4.2.2 Less-specific anaphoric relation TUcurrent includes TUprevious, and TUcurrent is less speciﬁc than TUprevious. Let F be the most speciﬁc ﬁeld in TUcurrent. The resolvent contains all of the information in TUprevious of the same or lower speciﬁcity than F. (See rule A-less-speciﬁc in Section 5.3.) For example (see also (5)-(6) of the corpus example in Figure 1): Utterance Interpretation How about Monday at 2? Assume: 2pm, Monday 19 August Ok, well, Monday sounds good. Monday 19 August 4.2.3 Frame of reference anaphoric relation This is the same as the frame of reference deictic relation above, but the new time is calculated with respect to TUprevious instead of the dialog date. (See rule A-frame-of- reference in Section 5.3.) Following are two examples: Utterance Interpretation Would you like to meet Wednesday, August 2nd? No, how about Friday at 2. 2pm, Friday 4 August Utterance Interpretation How about the 3rd week of August? Lets see, Tuesday sounds good. Tuesday of the 3rd week in August In the ﬁrst example, the day speciﬁed in the ﬁrst utterance is used as the frame of reference. In the second example, the beginning day of the interval representing the 3rd week of August is used as the frame of reference. Note that tense can inﬂuence the choice of whether to calculate a forward or backward time from a frame of reference (Kamp  Reyle, 1993), but this is not accounted for because there is not much tense variation in the CMU corpus on which the algorithm was developed. However, errors can occur because backward calculations are not covered. For example, one might mention Friday and then Thursday, intending Thursday to be calculated as the day before that Friday, rather than the Thursday of the week following that Friday. We are investigating creating a new anaphoric relation to cover these cases. 4.2.4 Modify anaphoric relation TUcurrent is calculated by modifying the interpretation of the previous temporal reference. The times diﬀer in the ﬁller of a ﬁeld F, where F is at least as speciﬁc as time of day, but 255 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever are consistent in all ﬁelds less speciﬁc than F. The resolvent contains the information in TUprevious that is less speciﬁc than F together with the information in TUcurrent that is of the same or greater speciﬁcity as F. (See rule A-modify in Section 5.3.) For example (see also (3)-(5) of the corpus example in Figure 1): Utterance Interpretation Monday looks good. Assume: Monday 19 August How about 2? (co-reference relation) 2pm, Monday 19 August Hmm, how about 4? (modify relation) 4pm, Monday 19 August 4.3 Focus Models The focus model, or model of attentional state (Grosz  Sidner, 1986), is a model of which entities the dialog is most centrally about at each point in the dialog. It determines which previously mentioned entities are the candidate antecedents of anaphoric references. As such, it represents the role that the structure of the discourse plays in reference resolution. We consider three models of attentional state in this paper: (1) the linear-recency model (see, for example, the work by Hobbs (1978) and Walker2 (1996)), (2) Grosz and Sidners (1986) stack-based model, and (3) the graph structured stack model introduced by Rose, Di Eugenio, Levin, and Van Ess-Dykema (1995). Ordered from (1) to (3), the models are successively more complex, accounting for increasingly more complex structures in the discourse. In a linear-recency based model, entities mentioned in the discourse are stored on a focus list, ordered by recency. The corresponding structure in the dialog is shown in Figure 4a: a simple progression of references, uninterrupted by subdialogs. In Grosz and Sidners stack-based model, the entities in focus in a particular discourse segment are stored together in a focus space associated with that segment. To handle anaphoric references across discourse segments, focus spaces are pushed on and popped oﬀ the stack as appropriate to mirror the structure of the discourse. As each new segment is recognized, a focus space is created and pushed onto the stack. To interpret an anaphoric reference, the entities in the focus space on the top of the stack are considered ﬁrst. However, if the current utterance resumes a previous discourse segment, the intervening focus spaces are popped oﬀ. This allows anaphoric reference to an earlier entity, even if more recently mentioned entities are possible antecedents (for more details, see Grosz  Sidner, 1986). Figure 4b illustrates a discourse structure that the stack-based model is designed to handle. Suppose that both TU1 and TU2 are possible antecedents of TU3 (for example, suppose they are speciﬁed by pronouns that agree in number and gender), but TU2 is in a subsegment and is not a correct antecedent of TU3, even though it is mentioned more recently than TU1. In the stack-based model, the focus space containing TU2 is popped oﬀ the stack when the end of its segment is recognized, thus removing TU2 as a competitor for understanding TU3. Following is an example from the NMSU corpus (this is the dialog segment labeled 09-09, in row 7, in Figure 10 presented later). 2. Note that Walkers model is a cache-based model for which recency is a very important but not unique criterion for determining which entities are in the cache. 256 An Empirical Approach to Temporal Reference Resolution TU1 TU2 TU3 TU1 TU2 TU3 TU1 TU2 TU3 TU4 (a) (b) (c) Figure 4: Discourse Structures Targeted by Diﬀerent Focus Models Dialog Date: Monday 10 May 1 S1 Listen, daughter, I was thinking of inviting you to a demonstration on interior things, ornaments for decorating your house. 2 Uh, I would like to do it at two p.m. Wednesday, 3 But I dont know if you are free at that time or . . . TU1 4 S2 Uh, Wednesday, Mom, well Resolved to Wednesday, May 12 5 You know that, TU2,1 6 last week uh, I got a job and uh, a full-time job Unambiguous deictic; resolved to the week before the dialog date TU2,2 7 I go in from seven in the morning to ﬁve in the afternoon Habitual 8 S1 Oh, maybe it would be better TU3 9 S2 Well, I have lunch from twelve to one Utterance (4) is needed for the correct interpretation: 12-1, Wednesday 12 May In this passage, utterances (6)-(7) are in a subdialog about S2s job. To interpret twelve to one in utterance (9) correctly, one must go back to utterance (4). Incorrect interpretations involving the temporal references in (6) and (7) are possible (using the co-reference relation with (6) and the modify relation with (7)), so those utterances must be skipped. Rose et al.s graph structured stack is designed to handle the more complex structure depicted in Figure 4c. We will return to this structure later in Section 8.1, when the adequacy of our focus model is analyzed. Once the candidate antecedents are determined, various criteria can be used to choose among them. Syntactic and semantic constraints are common. 257 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever 4.3.1 Our Focus Model for Temporal Reference Resolution As mentioned earlier, our algorithm for temporal reference resolution is recency based. Speciﬁcally, the focus model is structured as a linear list of all times mentioned so far in the current dialog. The list is ordered by recency, and no entries are deleted from the list. The candidate antecedents are as follows. For each type of anaphoric relation, the most recent Temporal Unit on the focus list that satisﬁes that relation, if there is one, is a candidate antecedent. The antecedent is chosen from among the candidate antecedents based on a combined score reﬂecting a priori preferences for the type of anaphoric relation established, how recently the time was mentioned, and how plausible the resulting temporal interpretation would be (see Section 5). These numerical heuristics contribute to some extent to the success of the implementation, but are not critical components of the model, as shown in Section 8.3. 4.4 The Need for Explicit Identiﬁcation of Relations As mentioned in the introduction, one goal of this work is to assess the adequacy of a recency- based focus model for this task and genre. To be well founded, such evaluations must be made with respect to a particular set of relations. For example, the modify relation supports a recency-based approach. Consider the following example, reproduced from Section 4.2: Utterance Interpretation (1) Monday looks good. Assume: Monday 19 August (2) How about 2? (co-reference relation) 2pm, Monday 19 August (3) Hmm, how about 4? (modify relation) 4pm, Monday 19 August Because our model includes the modify anaphoric relation, the Temporal Unit in (2) is an appropriate antecedent for the one in (3). A model without this relation might require (3)s antecedent to be provided by (1). 5. Algorithm This section presents our high-level algorithm for temporal reference resolution. After an overview in Section 5.1, the rule application architecture is described in Section 5.2, and the main rules composing the algorithm are given in Section 5.3. The complete set of rules is given in detail in Online Appendix 1. 5.1 Overview An important feature of our approach is that the system is forced to choose among pos- sibilities only if the resulting interpretations would be inconsistent. If the results for two possibilities are consistent, the system merges the results together. At a high level, the algorithm operates as follows. There is a set of rules for each of the relations presented in Section 4.2. The rules include constraints involving the current utterance and another Temporal Unit. In the anaphoric cases, the other Temporal Unit is a potential antecedent from the focus list. In the deictic cases, it is the dialog date or a 258 An Empirical Approach to Temporal Reference Resolution . . . . . . . . . Utterancen ILT1,1,1 ILT1,1,p Transcription1,1 Transcription1,m Utterance1 . . . Augmented ILT1 Augmented ILTn CMU Speech Recognizer CMU Semantic Parser CMU  Artwork Discourse Processors Figure 5: The Enthusiast System later time. For the current temporal expression to be resolved, each rule is applied. For the anaphoric rules, the antecedent considered is the most recent one satisfying the constraints. All consistent maximal mergings of the results are formed, and the one with the highest score is the chosen interpretation. 5.2 Architecture Our system was developed to be integrated into the Enthusiast system developed at Carnegie Mellon University (see Qu, Eugenio, Lavie, Levin,  Rose, 1996; Levin et al., 1995; Rose et al., 1995; Lavie  Tomita, 1993). Enthusiast is a speech-to-speech machine translation system from Spanish into English. The aspects of the system needed for this paper are shown in Figure 5. The system processes all the utterances of a single speaker turn together (utterances 1 through n in the ﬁgure). Each spoken Spanish utterance is input to the speech recognizer, which produces one or more transcriptions of the utterance. The output of the speech recognition system is the input to a semantic parser (Lavie  Tomita, 1993; Levin et al., 1995), which produces a representation of the literal meaning of the sentence. This representation is called an Interlingual Text (ILT). The output of the semantic parser is ambiguous, consisting of multiple ILT representations of the input transcription. All of the ILT representations produced for an utterance are input to the discourse processor, which produces the ﬁnal, unambiguous representation of that utterance. This representation is called an augmented ILT. The discourse processor can be conﬁgured to be our system alone, a plan-based discourse processor developed at CMU (Rose et al., 1995), or the two working together in integrated mode. The main results, presented in Tables 2 and 3 in Section 6, are for our system working alone, taking as input the ambiguous output of the semantic parser. For the CMU 259 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever dialogs, the input to the semantic parser is the output of the speech recognition system. The NMSU dialogs were input to the semantic parser directly in the form of transcriptions.3 To produce one ILT, the semantic parser maps the main event and its participants into one of a small set of case frames (for example, a meet frame or an is busy frame). It also produces a surface representation of the temporal information in the utterance, which mirrors the form of the input utterance. Although the events and states discussed in the NMSU data are often outside the coverage of this parser, the temporal information generally is not. Thus, the parser provides a suﬃcient input representation for our purposes on both sets of data. As the Enthusiast system is conﬁgured, the input is presented to our discourse pro- cessor in the form of alternative sequences of ILTs. Each sequence contains one ILT for each utterance. For example, using the notation in Figure 5, a sequence might consist of ILT1,2,3, ILT2,1,1, . . ., ILTn,2,1. Our system resolves the ambiguity in batches. Speciﬁcally, it produces a sequence of Augmented ILTs for each input sequence, and then chooses the best sequence as its ﬁnal interpretation of the corresponding utterances. In this way, the input ambiguity is resolved as a function of ﬁnding the best temporal interpretations of the utterance sequences in context (as suggested by Qu et al., 1996). However, the num- ber of alternative sequences of ILTs for a set of utterances can be prohibitively large for our system. The total number of sequences considered by the system is limited to the top 125, where the sequences are ordered using statistical rankings provided by the Enthusiast system. Our method for performing semantic disambiguation is appropriate for this project, because the focus is on temporal reference resolution and not on semantic disambiguation. However, much semantic ambiguity cannot be resolved on the basis of the temporal discourse context alone, so this represents a potential area for improvement in the system performance results presented in Section 6. In fact, the Enthusiast researchers have already developed better techniques for resolving the semantic ambiguity in these dialogs (Shum et al., 1994). Because the ILT representation was designed to support various projects in discourse, semantic interpretation, and machine translation, the representation produced by the se- mantic parser is much richer than is required for our temporal reference resolution algorithm. We recommend that others who implement our algorithm for their application build an in- put parser to produce only the necessary temporal information. The speciﬁcation of our input is available in Online Appendix 2. As described in Section 4.3, a focus list records the Temporal Units that have been discussed so far in the dialog. After a ﬁnal Augmented ILT has been created for the current utterance, the Augmented ILT and the utterance are placed together on the focus list. In the case of utterances that specify more than one Temporal Unit, a separate entity is added for each to the focus list, in order of mention. Otherwise, the system architecture is similar to a standard production system, with one major exception: rather than choosing the results of just one of the rules that ﬁres, multiple results can be merged. This is a ﬂexible architecture that accommodates sets of rules targeting diﬀerent aspects of the interpretation. 3. The semantic parser but not the speech recognizer was available for us to process the NMSU data. Presumably, the speech recognizer would not perform as well on the NMSU dialogs as it does on the CMU dialogs, since it was trained on the latter. 260 An Empirical Approach to Temporal Reference Resolution Following are the basic steps in processing a single ILT. Step 1. The input ILT is normalized. In producing the ILTs that serve as input to our system, the semantic parser often represents pieces of information about the same time separately, mirroring the surface form of the utterance. This is done in order to capture relationships, such as topic-comment relationships, among clauses. Our system needs to know which pieces of information are about the same time, but does not need to know about the additional relationships. Thus, the system maps the input representation into a normalized form, to shield the reasoning component from the idiosyncracies of the input representation. A speciﬁcation of the normalized form is given in Online Appendix 2. The goal of the normalization process is to produce one Temporal Unit per distinct time speciﬁed in the utterance. The normalization program is quite detailed (since it must account for the various structures possible in the CMU input ILT), but the core strategy is straightforward: it merges information provided by separate noun phrases into one Temporal Unit, if it is consistent to do so. Thus, new Temporal Units are created only if necessary. Interestingly, few errors result from this process. Following are some examples. I can meet Wednesday or Thursday. Represented as two disjoint TUs. I can meet from 2:00 until 4:00 on the 14th. Represented as one TU. I can meet Thursday the 11th of August. Represented as one TU. After the normalization process, highly accurate, obvious inferences are made and added to the representation. Step 2. All of the rules are applied to the normalized input. The result of a rule application is a Partial Augmented ILTinformation this rule will contribute to the interpretation of the utterance, if it is chosen. This information includes a certainty factor representing an a priori preference for the type of anaphoric or deictic relation being established. In the case of anaphoric relations, this factor is adjusted by a term representing how far back on the focus list the antecedent is (in the anaphoric rules in Section 5.3, the adjustment is represented by distance factor in the calculation of the certainty factor CF). The result of this step is the set of Partial Augmented ILTs produced by the rules that ﬁred (i.e., those that succeeded). In the case of multiple Temporal Units in the input ILT, each rule is applied as follows. If the rule does not access the focus list, the rule is applied to each Temporal Unit. A list of Partial Augmented ILTs is produced, containing one entry for each successful match, retaining the order of the Temporal Units in the original input. If the rule does access the focus list, the process is the same, but with one important diﬀerence. The rule is applied to the ﬁrst Temporal Unit. If it is successful, then the same focus list entity used to apply the rule to this Temporal Unit is used to interpret the remaining Temporal Units in the list. Thus, all the anaphoric temporal references in a single utterance are understood with respect to the same focus list element. So, for example, the anaphoric interpretations of the temporal expressions in I can meet Monday or Tuesday both have to be understood with respect to the same entity in the focus list. When accessing entities on the focus list, an entry for an utterance that speciﬁes mul- tiple Temporal Units may be encountered. In this case, the Temporal Units are simply 261 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever accessed in order of mention (from most to least recent). Step 3. All maximal mergings of the Partial Augmented ILTs are created. Consider a graph in which the Partial Augmented ILTs are the vertices, and there is an edge between two Partial Augmented ILTs if they are compatible. Then, the maximal cliques of the graph (i.e., the maximal complete subgraphs) correspond to the maximal mergings. Each maximal merging is then merged with the normalized input ILT, resulting in a set of Augmented ILTs. Step 4. The Augmented ILT chosen is the one with the highest certainty factor. The certainty factor of an Augmented ILT is calculated as follows. First, the certainty factors of the constituent Partial Augmented ILTs are summed. Then, critics are applied to the resulting Augmented ILT, lowering the certainty factor if the information is judged to be incompatible with the dialog state. The merging process might have yielded additional opportunities for making obvious inferences, so this process is performed again, to produce the ﬁnal Augmented ILT. To process the alternative input sequences, a separate invocation to the core system is made for each sequence, with the sequence of ILTs and the current focus list as input. The result of each call is a sequence of Augmented ILTs, which are the systems best interpretations of the input ILTs, and a new focus list, representing the updated discourse context corresponding to that sequence of interpretations. The system assigns a certainty factor to each sequence of Augmented ILTs, speciﬁcally, the sum of the certainty factors of the constituents. It chooses the sequence with the highest certainty factor, and updates the focus list to the focus list calculated for that sequence. 5.3 Temporal Reference Resolution Rules Figure 6 presents the main temporal resolution rules, one for each of the cases described in Sections 4.1 and 4.2. In the complete set of rules, given in Online Appendix 1, many are broken down into subcases involving, for example, the end times or starting times. The rules apply to individual Temporal Units. They return a certainty factor, and either a more fully speciﬁed Temporal Unit or an empty structure indicating failure. Many of the rules calculate temporal information with respect to a frame of reference, using a separate calendar utility. Following are functions and conventions used in Figure 6. 1. next(TimeV alue, RF): returns the next timeV alue that follows reference frame RF. For example, next(Monday, [. . .Friday, 19th,. . .])  Monday, 22nd. 2. resolve deictic(DT, RF): resolves the deictic term DT with respect to the reference frame RF. 3. merge(TU1, TU2): if Temporal Units TU1 and TU2 contain no conﬂicting ﬁelds, returns a Temporal Unit containing all of the information in the two units; otherwise returns {}. 4. merge upper(TU1, TU2): similar to the previous function, except that the only ﬁelds from TU1 that are included are those that are of the same or less speciﬁcity as the most speciﬁc ﬁeld in TU2. 262 An Empirical Approach to Temporal Reference Resolution 5. speciﬁcity(TU): returns the speciﬁcity of the most speciﬁc ﬁeld in TU. 6. most speciﬁc(TU): returns the most speciﬁc ﬁeld in TU. 7. starting ﬁelds(TU): returns a list of starting ﬁeld names for those in TU having non-null values. 8. structurecomponent: returns the named component of the structure. 9. conventions: Values are in bold face and variables are in italics. TU is the current Temporal Unit being resolved. TodaysDate is a representation of the dialog date. FocusList is the list of discourse entities from all previous utterances. The algorithm does not cover some subcases of relations concerning the end times. For instance, rule D-frame-of-reference covers only the starting-time case of the frame of refer- ence deictic relation. An example of an end-time case that is not handled is the utterance Lets meet until Thursday, under the meaning that they should meet from today through Thursday. This is an area for future work. 6. Results As mentioned in Section 3, the main results are based on comparisons against human anno- tation of the held out test data. The results are based on straight ﬁeld-by-ﬁeld comparisons of the Temporal Unit representations introduced in Section 3. To be considered correct, information must not only be right, but it also has to be in the right place. Thus, for example, Monday correctly resolved to Monday 19 August, but incorrectly treated as a starting rather than an end time, contributes 3 errors of omission and 3 errors of commission (and receives no credit for the correct date). Detailed results for the test sets are presented in this section, starting with results for the CMU data (see Table 2). Accuracy measures the extent to which the system produces the correct answer, while precision measures the extent to which the systems answers are correct (see the formulas in Table 2). For each component of the extracted temporal structure, the systems correct and incorrect answers were counted. Since null values occur quite often, these counts exclude cases in which the systems answer, the correct answer, or both answers are null. Those cases were counted separately. Note that each test set contains three complete dialogs with an average of 72 utterances per dialog. These results show that the system achieves an overall accuracy of 81, which is signif- icantly better than the baseline accuracy (deﬁned below) of 43. In addition, the results show a high precision of 92. In some of the individual cases, however, the results could be higher due to several factors. For example, our system development was inevitably fo- cused more on some ﬁelds than others. An obvious area for improvement is the systems processing of the time of day ﬁelds. Also, note that the values in the Mis column are higher than those in the Ext column. This reﬂects the conservative coding convention, mentioned in Section 3, for ﬁlling in unspeciﬁed end points. The accuracy and precision ﬁgures for the hour  minute and time of day ﬁelds are very high because a large proportion of them are null. We include null correct answers in our 263 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever Rules for deictic relations Rule D-simple: All cases of the simple deictic relation. if there is a deictic term, DT, in TU then return 0.9, merge(TU, resolve deictic(DT, TodaysDate)) Rule D-frame-of-reference: The starting-time cases of the frame of reference deictic relation. if (most speciﬁc(starting ﬁelds(TU))  time of day) then Let f be the most speciﬁc ﬁeld in starting ﬁelds(TU) return 0.4, merge(TU, next(TUf, TodaysDate)) Rules for anaphoric relations Rule A-co-reference: All cases of the co-reference anaphoric relation. for each non-empty Temporal Unit TUfl from FocusList (starting with most recent) if speciﬁcity(TUfl)  speciﬁcity(TU) and not empty merge(TUfl, TU) then CF  0.8  distance factor(TUfl, FocusList) return CF, merge(TUfl, TU) Rule A-less-speciﬁc: All cases of the less-speciﬁc anaphoric relation. for each non-empty Temporal Unit TUfl from FocusList (starting with most recent) if speciﬁcity(TUfl)  speciﬁcity(TU) and not empty merge upper(TUfl, TU) then CF  0.5  distance factor(TUfl, FocusList) return CF, merge upper(TUfl, TU) Rule A-frame-of-reference: Starting-time case of the frame of reference anaphoric relation. if (most speciﬁc(starting ﬁelds(TU))  time of day) then for each non-empty Temporal Unit TUfl from FocusList (starting with most recent) if speciﬁcity(TU)  speciﬁcity(TUfl) then Let f be the most speciﬁc ﬁeld in starting ﬁelds(TU) CF  0.6  distance factor(TUfl, FocusList) return CF, merge(TU, next(TUf, TUflstart date)) Rule A-modify: All cases of the modify anaphoric relation. if (speciﬁcity(TU)  time of day) then for each non-empty Temporal Unit TUfl from FocusList (starting with most recent) if speciﬁcity(TU)  speciﬁcity(TUfl) and speciﬁcity(TUfl)  time of day then if not empty merge upper(TUfl, TU) then CF  0.5  distance factor(TUfl, FocusList) return CF, merge upper(TUfl, TU) Figure 6: Main Temporal Resolution Rules 264 An Empirical Approach to Temporal Reference Resolution Label Cor Inc Mis Ext Nul Poss Act BaseAcc Acc Prec start Month 49 3 7 3 0 59 55 0.338 0.831 0.891 Date 48 4 7 3 0 59 55 0.403 0.814 0.873 DayofWeek 46 6 7 3 0 59 55 0.242 0.780 0.836 HourMin 18 0 7 0 37 62 55 0.859 0.887 1.000 TimeDay 9 0 18 0 35 62 44 0.615 0.710 1.000 end Month 48 3 7 1 3 61 55 0.077 0.836 0.927 Date 47 5 6 3 1 59 56 0.048 0.814 0.857 DayofWeek 45 7 6 3 1 59 56 0.077 0.780 0.821 HourMin 9 0 9 0 44 62 53 0.862 0.855 1.000 TimeDay 4 0 13 1 44 61 49 0.738 0.787 0.980 Overall 323 28 87 17 165 534 604 0.428 0.809 0.916 Legend Cor(rect): System and key agree on non-null value Inc(orrect): System and key diﬀer on non-null value Mis(sing): System has null value for non-null key Ext(ra): System has non-null value for null key Nul(l): Both system and key give null answer Poss(ible): Correct  Incorrect  Missing  Null Act(ual): Correct  Incorrect  Extra  Null Base(line)Acc(uracy): Baseline accuracy (input used as is) Acc(uracy):  Key values matched correctly ((Correct  Null)Possible) Prec(ision):  System answers matching the key ((Correct  Null)Actual) Table 2: Evaluation of System on CMU Test Data 265 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever Label Cor Inc Mis Ext Nul Poss Act BaseAcc Acc Prec start TimeDay 9 0 18 0 35 62 44 0.615 0.710 1.000 Month 55 0 23 5 3 63 81 0.060 0.716 0.921 Date 49 6 23 5 3 63 81 0.060 0.642 0.825 DayofWeek 52 3 23 5 3 63 81 0.085 0.679 0.873 HourMin 34 3 7 6 36 79 80 0.852 0.875 0.886 TimeDay 18 8 31 2 27 55 84 0.354 0.536 0.818 end Month 55 0 23 5 3 63 81 0.060 0.716 0.921 Date 49 6 23 5 3 63 81 0.060 0.642 0.825 DayofWeek 52 3 23 5 3 63 81 0.060 0.679 0.873 HourMin 28 2 13 1 42 73 85 0.795 0.824 0.959 TimeDay 9 2 32 5 38 54 81 0.482 0.580 0.870 Overall 401 33 221 44 161 639 816 0.286 0.689 0.879 Table 3: Evaluation of System on NMSU Test Data ﬁgures because such answers often reﬂect valid decisions not to ﬁll in explicit values from previous Temporal Units. Table 3 contains the results for the system on the NMSU data. It shows that the system performs respectably, with 69 accuracy and 88 precision, on the more complex set of data. The precision is still comparable, but the accuracy is lower, since more of the entries are left unspeciﬁed (that is, the ﬁgures in the Mis column in Table 3 are higher than in Table 2). Furthermore, the baseline accuracy (29) is almost 15 lower than the one for the CMU data (43), supporting the claim that this data set is more challenging. The baseline accuracies for the test data sets are shown in Table 4. These values were derived by disabling all the rules and evaluating the input itself (after performing normalization, so that the evaluation software could be applied). Since null values are the most frequent for all ﬁelds, this is equivalent to using a naive algorithm that selects the most frequent value for each ﬁeld. Note that in Tables 2 and 3, the baseline accuracies for the end month, date, and day of week ﬁelds are quite low because the coding convention calls for ﬁlling in these ﬁelds, even though they are not usually explicitly speciﬁed. In this case, an alternative baseline would have been to use the corresponding starting ﬁeld. This has not been calculated, but the results can be approximated by using the baseline ﬁgures for the starting ﬁelds. The rightmost column of Table 4 shows that there is a small amount of error in the input representation. This ﬁgure is 1 minus the precision of the input representation (after normalization). Note, however, that this is a close but not exact measure of the error in the input, because there are a few cases of the normalization process committing errors and a few of it correcting errors. Recall that the input is ambiguous; the ﬁgures in Table 4 are based on the system selecting the ﬁrst ILT in each case. Since the parser orders the 266 An Empirical Approach to Temporal Reference Resolution Set Cor Inc Mis Ext Nul Act Poss Acc Input Error cmu 84 6 360 10 190 290 640 0.428 0.055 nmsu 65 3 587 4 171 243 826 0.286 0.029 Table 4: Baseline Figures for Both Test Sets seen cmu ambiguous, uncorrected Dialogs Utterances Acc Prec unseen nmsu unambiguous, partially corrected seen cmu ambiguous, uncorrected 12 659 0.883 0.918 seen cmu unambiguous, partially corrected 12 659 0.914 0.957 unseen cmu ambiguous, uncorrected 3 193 0.809 0.916 seen nmsu ambiguous, uncorrected 4 358 0.679 0.746 seen nmsu unambiguous, partially corrected 4 358 0.779 0.850 unseen nmsu ambiguous, uncorrected 3 236 0.689 0.879 Table 5: Overall Results ILTs based on a measure of acceptability, this choice is likely to have the relevant temporal information. The above results are for the system taking ambiguous semantic representations as input. To help isolate errors due to our model, the system was also evaluated on unambiguous, partially corrected input for all the seen data (the test sets were retained as unseen test data). The input is only partially corrected because some errors are not feasible to correct manually, given the complexity of the input representation. The overall results are shown in the Table 5. The table includes the results presented earlier in Tables 2 and 3, to facilitate comparison. In the CMU data set, there are twelve dialogs in the training data and three dialogs in a held out test set. The average length of each dialog is approximately 65 utterances. In the NMSU data set, there are four training dialogs and three test dialogs. In both data sets, there are noticeable gains in performance on the seen data going from ambiguous to unambiguous input, especially for the NMSU data. Therefore, the semantic ambiguity and input errors contribute signiﬁcantly to the systems errors. Some challenging characteristics of the seen NMSU data are vast semantic ambiguity, numbers mistaken by the input parser for dates (for example, phone numbers are treated as dates), and the occurrences of subdialogs. Most of the the systems errors on the unambiguous data are due to parser error, errors in applying the rules, errors in mistaking anaphoric references for deictic references (and vice versa), and errors in choosing the wrong anaphoric relation. As will be shown in Section 8.1, our approach handles focus eﬀectively, so few errors can be attributed to the wrong entities being in focus. 267 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever 7. Other Work on Temporal Reference Resolution To our knowledge, there are no other published results on unseen test data of systems performing similar temporal reference resolution tasks. Rose et al. (1995, Enthusiast), Alexandersson et al. (1997, Verbmobil), and Busemann et al. (1997, Cosma) describe other recent natural language processing systems that resolve temporal expressions in scheduling dialogs. Rose et al. also address focus issues; we compare our work to theirs in detail in Section 8.1. All of the systems share certain features, such as the use of a calendar utility to calculate dates, a speciﬁcity ordering of temporal components (such as in Figure 3), and a record of the temporal context. However, all of the other systems perform temporal reference resolution as part of their overall processing, in service of solving another problem such as speech act resolution. None of them lays out a detailed approach or model for temporal reference resolution, and none gives results of system performance on any temporal interpretation tasks. Kamp and Reyle (1993) address representational and processing issues in the interpre- tation of temporal expressions. However, they do not implement their ideas or present the results of a working system. They do not attempt coverage of a data set, or present a comprehensive set of relations, as we do, but consider only speciﬁc cases that are interest- ing for their Discourse Representation Theory. In addition, they do not address the issues of discourse structure and attentional state focused on here. For example, they recognize that references such as on Sunday may have to be understood with respect to a frame of reference. But they do not address how the frame of reference is chosen in context, so do not address the question of what type of focus model is required. Note that temporal reference resolution is a diﬀerent problem from tense and aspect in- terpretation in discourse (as addressed in, for example, Webber, 1988; Song  Cohen, 1991; Hwang  Schubert, 1992; Lascarides, Asher,  Oberlander, 1992; Kameyama, Passonneau,  Poesio, 1993). These tasks are brieﬂy reviewed here to clarify the diﬀerences. Temporal reference resolution is determining what time is being explicitly speciﬁed by noun phrases that are temporal referring expressions (e.g., Monday resolved to Monday 19 August). Tense and aspect interpretation involves determining implicit information about the states and events speciﬁed by verb phrases (e.g., that the kissing event speciﬁed in He had kissed her happened before some reference time in the past). While it could aid in performing temporal reference resolution, we are not addressing tense and aspect interpretation itself. Scheduling dialogs, or scheduling subdialogs of other kinds of dialogs, predominantly employ the present and future tenses, due to the nature of the task. As discussed further below in Section 8.1, a primary way that tracking the tense and aspect would aid in tem- poral reference resolution would be to recognize discourse segments that depart from the scheduling dialog or subdialog. In addition, Kamp and Reyle (1993) address some cases in which tense and aspect, temporal nouns, and temporal adverbs interact to aﬀect the temporal interpretation. We intend to pursue these ideas in future work. 8. Analysis The implementation is an important proof of concept. However, as discussed in Section 6, various kinds of errors are reﬂected in the results; many are not directly related to discourse 268 An Empirical Approach to Temporal Reference Resolution  TUs  TUs speciﬁed anaphorically CMU 196 167 NMSU 96 71 Total 292 238 Figure 7: Counts of Temporal Unit References in the Training Data processing or temporal reference resolution. Examples are (1) completely null inputs, when the semantic parser or speech recognizer fails, (2) numbers mistaken as dates, and (3) failures to recognize that a relation can be established, due to a lack of speciﬁc domain knowledge. To evaluate the algorithm itself, in this section we separately evaluate the components of our method for temporal reference resolution. Sections 8.1 and 8.2 assess the key con- tributions of this work: the focus model (in Section 8.1) and the deictic and anaphoric relations (in Section 8.2). These evaluations required us to perform extensive additional manual annotation of the data. In order to preserve the test dialogs as unseen test data, these annotations were performed on the training data only. In Section 8.3, we isolate the architectural components of our algorithm, such as the certainty factor calculation and the critics, to assess the eﬀects they have on performance. 8.1 Evaluation of the Focus Model The algorithm presented here does not include a mechanism for recognizing the global structure of the discourse, such as in the work of Grosz and Sidner (1986), Mann and Thompson (1988), Allen and Perrault (1980), and in descendent work. Recently in the literature, Walker (1996) argues for a more linear-recency based model of attentional state (though not that discourse structure need not be recognized), while Rose et al. (1995) argue for a more complex model of attentional state than is represented in most current computational theories of discourse. Many theories that address how attentional state should be modeled have the goal of performing intention recognition as well. We investigate performing temporal reference resolution directly, without also attempting to recognize discourse structure or intentions. We assess the challenges the data present to our model when only this task is attempted. The total number of Temporal Units and the number of them speciﬁed by anaphoric noun phrases in the two training data sets are given in Figure 7.4 There are diﬀerent units that could be counted, from the number of temporal noun phrases to the number of distinct times referred to in the dialog. Here, we count the entities that must be resolved by a temporal reference resolution algorithm, i.e., the number of distinct temporal units speciﬁed in each sentence, summed over all sentences. Operationally, this is a count of Temporal Units after the normalization phase, i.e., after Step 1 in Section 5.2. This is the unit considered in the remainder of this paper. 4. The anaphoric counts include the cases in which both deictic and anaphoric interpretations yield the correct result. 269 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever To support the evaluation presented in this section, antecedent information was man- ually annotated in the training data. For each Temporal Unit speciﬁed by an anaphoric noun phrase, all of the antecedents that yield the correct interpretation under one of the anaphoric relations were identiﬁed, except that, if both TUi and TUj are appropriate an- tecedents, and one is an antecedent of the other, only the more recent one is included. Thus, only the heads of the anaphoric chains existing at that point in the dialog are included. In addition, competitor discourse entities were also identiﬁed, i.e., previously mentioned Tem- poral Units for which some relation could be established, but the resulting interpretation would be incorrect. Again, only Temporal Units at the head of an anaphoric chain were considered. To illustrate these annotations, Figure 8 shows a graph depicting anaphoric chain annotations of an NMSU dialog (dialog 9). In the ﬁgure, solid lines link the correct antecedents, dotted lines show competitors, and edges to nowhere indicate deictics. 8.1.1 Cases in which the immediately preceding time is not an appropriate antecedent The main purpose of a focus model is to make an appropriate set of discourse entities available as candidate antecedents at each point in the discourse. As described above in Section 4.3, Grosz and Sidners model captures situations in which entities should not be available as candidate antecedents, and Rose et al. identify situations in which Grosz and Sidners model may incorrectly eliminate entities from consideration (i.e., dialogs with multiple threads). The potential challenge for a recency-based model like ours is that entities may be available as candidate antecedents that should not be. An entity E may occur to which an anaphoric relation could be established, but an entity mentioned before E is needed for the correct interpretation. (From another perspective, E yields the wrong interpretation but cannot be ruled out as a possible antecedent.) To assess the magnitude of this problem for our method, in this section we characterize the cases in which the most recent entity is not an appropriate antecedent. Before proceeding, we note that there is only one situation in which our model incorrectly makes a needed entity unavailable. Recall from Section 4.3 that, for a particular relation R, only the most recent Temporal Unit for which R can be established is a candidate (call it C). The problem arises when the correct interpretation requires that that same relation R be established with an entity mentioned earlier than C. This is a problem because the earlier time is not a candidate. If such cases were to occur in the training data, they would have been found by the analysis presented below. However, none were found. Based on the anaphoric chain annotations, we identiﬁed how far back on the focus list one must go to ﬁnd an antecedent that is appropriate according to the model. An antecedent is considered to be appropriate according to the model if there exists a relation deﬁned in the model such that, when established between the current utterance and the antecedent, it yields the correct interpretation. Note that we allow antecedents for which the anaphoric relation would be a trivial extension of one of the relations explicitly deﬁned in the model. For example, phrases such as after lunch should be treated as if they are simple times of day under the co-reference and modify anaphoric relations, but, as explicitly deﬁned, those relations do not cover such phrases. For example, given Wednesday 14 April, the reference after lunch should be interpreted as after lunch, Wednesday 14 April under the 270 An Empirical Approach to Temporal Reference Resolution 1 (s1): Listen, daughter, I was thinking of inviting you to a demonstration on interior things, ornaments for decorating your house 2 (s1): Uh, I would like to do it at two p.m. Wednesday, ((wed, may, 12, 2, afternoon), (wed, may, 12, null, null)) 3 (s1): But I dont know if you are free at that time or if we could change it to fit your schedule 4 (s2): Uh, Wednesday, Mom, well. ((wed, may, 12, null, null), (wed, may, 12, null, null)) less specific 5 (s2): You know that la..., la.... 7 (s2): I go in from seven in the morning to five in the afternoon. ((null, null, null, 7, morning), (null, null, null, 5, afternoon)) co-reference 9 (s2): Well, I have lunch from twelve to one. ((wed, may, 12, 12, afternoon), (wed, may, 12, 1, afternoon)) co-reference 6 (s2): last week uh, I got a job and uh, a full-time job. ((mon, may, 3, null, null), (fri, may, 7, null, null)) co-reference 8 (s1): Oh, maybe it would be better... modify 12 (s1): What would you think if we changed it to Saturday? ((sat, may, 15, null, null), (sat, may, 15, null, null)) co-reference 10 (s2): But I dont know if you could meet at that time since Dads lunch hour is at the same time. ((wed, may, 12, 12, afternoon), (wed, may, 12, 1, afternoon)) co-reference 11 (s2): So I think that... frame of reference Figure 8: Anaphoric Annotations of Part of NMSU Dialog 9. 271 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever TU1 TU2 TU3 Figure 9: Structure Challenging the Recency Model. co-reference relation. Similarly, given 10am, Wednesday, 14 April, After lunch in After lunch would be better should be interpreted as after lunch, Wednesday 14 April under the modify anaphoric relation. The results are striking. Between the two sets of training data, there are only nine anaphoric temporal references for which the immediately preceding Temporal Unit is not an appropriate antecedent, 3167  1.8 in the CMU data, and 671  8.4 in the NMSU data. Figure 9 depicts the structure involved in all nine cases. TU3 represents the anaphoric reference for which the immediately preceding Temporal Unit is not an appropriate an- tecedent. TU1 represents the most recent appropriate antecedent, and TU2 represents the intervening Temporal Unit or Units. The ellipses represent any intervening non-temporal utterances. Figure 10 characterizes the nine cases along a number of dimensions. To isolate the issues addressed, it was assumed in deriving these ﬁgures that the dialog is correctly interpreted up to and including TU1. In three of the cases (rows 2, 4, and 9, labeled 07-63, 08-57, 10-55, respectively), there is a correct deictic interpretation of TU3 under our model, in addition to the correct (with antecedent TU1) and incorrect (with antecedent TU2) anaphoric interpretations. Column 1 of Figure 10 shows that, in all three cases in the CMU data and in two cases in the NMSU data, the second most recently mentioned Temporal Unit is an appropri- ate antecedent. In the remaining four cases, the third most recently mentioned time is appropriate. In three of the cases, the references represented by TU2 in Figure 9 are in subdialogs oﬀ the main topic and scheduling task (indicated as Yes in column 2). All of these subdialogs are in the NMSU data. In four cases, the TU2 references are in subsegments that are directly in service of the main task (indicated as No in column 2), and in two cases, we judged them to be borderline. Column 3 characterizes the type of reference the TU2 references are. The two marked Anaphoric, main task are speciﬁc references to times that involve the main scheduling 272 An Empirical Approach to Temporal Reference Resolution 1 2 3 4 5 6 Distance to Subdialog? Type of T U2 T U2 T U2 a Potential most recent Correct? Competitor? Cumulative appropriate Errors antecedent 1 (07-37) 2 No Anaphoric, Yes Yes 21 CMU main task 2 (07-63) 2 No Habitual No Yes 0 CMU 3 (15-31) 2 No Anaphoric, Yes Yes 4 CMU main task 4 (08-57) 2 Yes Reference No Yes 2 minor NMSU outside dialog 5 (08-66) 3 Yes 1 deictic Yes Yes 10 NMSU 1 habitual No Yes (worst case) 6 (09-39) 2 No habitual No No 0 NMSU 7 (09-09) 3 Yes 1 deictic Yes Yes 4 NMSU 1 habitual No (worst case) 8 (09-45) 3 Borderline both habitual No Yes 6 NMSU 9 (10-55) 3 Borderline both habitual No Yes 3 NMSU Figure 10: Summary of Cases in Which Most Recent TU is not an Appropriate Antecedent 273 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever Dialog Date: Monday 10 May TU1: Its just that . . . this Thursday [Thursday May 13] is our second wedding anniversary and I dont know what to do.  31 non-temporal utterances about what to cook  Did you go with my mother? TU2: With my mother? Yes. I went at around six in the morning. Did you and Maura go for a walk? No, no we didnt. Hmmmmm. We got lazy. Ah Claudia. TU3 Well, yes. Listen Lily. What do you think if we see each other on, on Thursday at six and I, at six? Figure 11: Dialog Segment of the Case in Row 4 in Figure 10 task. The subdialog marked Reference outside dialog (row 4, label 8-57) is shown in Figure 11. The main topic of this dialog is a party for the anniversary mentioned in TU1. The TU2 reference, around six in the morning, involves the participants shared knowledge of an event that is not related to the scheduling task. The only interpretation possible in our model is six in the morning on the day speciﬁed in the TU1 reference, while in fact the participants are referring to six in the morning on the dialog date. (There is currently no coverage in our model for deictic references that mention only a time of day.) Thus, the interpretation of the TU2 reference is incorrect, as indicated in column 4. Many of the TU2 references are habitual (marked habitual in column 3 of Figure 10). For example, the participants discuss their usual work schedules, using utterances such as during the week I work from 3 to 6. Since there is no coverage of habituals in our model, the interpretations of all of the TU2 habitual references are incorrect, as indicated in column 4. We now turn to column 5, which asks a key question: is TU2 a competitor? TU2 is a competitor if there is some relation in the model that can be established between TU3 and TU2. In the cases in which TU2 represents multiple utterances (namely, the ﬁfth, seventh, eighth, and ninth rows of Figure 10), yes is indicated in column 5 if an interpretation of the segment involving both of the TU2 references is possible. Cumulative error (column 6) can be non-zero only if the entry in column 5 is Yes: if the TU2 references are not competitors, they cannot be antecedents under our model, so they cannot prevent TU3 from being recognized as a correct antecedent. It is important to note that the incorrect interpretation of TU3 and the cumulative errors indicated in column 6 are only potential errors. In all cases in Figure 10, the correct inter- pretation of TU3 involving TU1 is available as a possible interpretation. What is shown in column 6 is the number of cumulative errors that would result if an interpretation involving TU2 were chosen over a correct interpretation involving TU1. In many cases, the systems answer is correct because the (correct) TU3TU1 interpretation involves the co-reference 274 An Empirical Approach to Temporal Reference Resolution Correct Interpretation of the TU1 reference: Monday 22nd November TU2: of December? TU3: of November. Figure 12: Dialog Segment of the Case in Row 1 in Figure 10 TU1 TU2 TU3 Wed 14 April Fri 16 April Wed 14 Apr Later Month Figure 13: Structure of the Case in Row 3 of Figure 10 anaphoric relation, while the (incorrect) TU3TU2 interpretation involves the frame of ref- erence anaphoric relation; the certainty factor of the former is suﬃciently larger than that of the latter to overcome the distance-factor penalty. In addition, such interpretations often involve large jumps forward in time, which are penalized by the critics. The worst case of cumulative error, row 1, is an example. The segment is depicted in Figure 12. The incorrect interpretation involving TU2 is November of the following year, calculated under the frame of reference anaphoric relation. The participants do not discuss the year, so the system cannot recover. Thus, a large amount of cumulative error would result if that interpretation were chosen. The segment corresponding to row 3 is similar. Its structure is depicted in Figure 13. In this passage, two days are mentioned in sequence, Wednesday 14 April (the TU1 reference) and Friday 16 April (the TU2 reference). Then, the day mentioned ﬁrstWednesday 14 Aprilis referred to again as Wednesday the 14th (the TU3 reference). There is no relation in our model that enables the correct interpretation of TU3 to be obtained from TU2. If TU2 were taken to be the antecedent of TU3, the resulting incorrect interpretation would be the next possible Wednesday 14, in a later month (possibly in a later year), under the frame of reference anaphoric relation. What is required for the correct interpretation is the co-reference anaphoric relation to be established between TU1 and TU3. We saw exactly the same pattern above for the row 1 discourse segment, depicted in Figure 12, except that in that case a later month was calculated, rather than a later date. It should be noted that, if times rather than days or months were being discussed, the correct interpretation for TU3 could be obtained from TU2 under the modify anaphoric relation. A good example of this occurs in the corpus example in Figure 1, repeated here 275 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever Temporal context: Tuesday 28 September s1 1 On Thursday I can only meet after two pm 2 From two to four TU1 3 Or two thirty to four thirty TU2 4 Or three to ﬁve TU3 s2 5 Then how does from two thirty to four thirty seem to you 6 On Thursday s1 7 Thursday the thirtieth of September Figure 14: Corpus Example from Figure 1 as Figure 14. The modify anaphoric relation enables TU2 to be the antecedent of TU3. The same would be true in the simpler case of Two? Or Three? How about Two?. A promising future extension would be to develop a new modify anaphoric relation for these cases. Returning to column 6 of Figure 10, note that two of the cumulative error ﬁgures are listed as worst case. These are cases in which there are two TU2 references and there are many diﬀerent possible interpretations of the passage. Notice that the second and fourth rows correspond to cases in which TU2 is a competitor, yet no signiﬁcant potential cumulative error results (the minor errors listed for row 4 are due to the relation not ﬁtting exactly, rather than an error from choosing the wrong antecedent: six in the morning rather than in the morning is placed into the high speciﬁcity ﬁelds). In both of these cases, the error corrects itself: TU1 is incorrectly taken to be the antecedent of TU2, which is in turn incorrectly taken to be the antecedent of TU3. But TU2 in eﬀect copies over the information from TU1 that is needed to interpret TU3. As a result, the interpretation of TU3 is correct. In the cases for which there are only a few potential cumulative errors, either a new, unambiguous time is soon introduced, or a time being discussed before the oﬀending TU2 reference is soon reintroduced, getting things back on track. An important discourse feature of the dialogs is the degree of redundancy of the times mentioned (Walker, 1996). This limits the ambiguity of the times speciﬁed, and it also leads to a higher level of robustness, since additional Temporal Units with the same time are placed on the focus list and previously mentioned times are reintroduced. Table 6 presents measures of redundancy. The redundancy is broken down into the case where redundant plus additional information is provided (Redundant) versus the case where the temporal information is just repeated (Reiteration). This shows that roughly 27 of the CMU utterances with temporal information contain redundant temporal references, while 20 of the NMSU ones do. In considering how the model could be improved, in addition to adding a new modify anaphoric relation for cases such as those in Figures 12 and 13, habituals are clearly an area for investigation. Many of the oﬀending references are habitual, and all but one of the subdialogs and borderline subdialogs involve habituals. In a departure from the algorithm, 276 An Empirical Approach to Temporal Reference Resolution Dialog Set Temporal Utterances Redundant Reiteration  cmu 210 36 20 26.7 nmsu 122 11 13 19.7 Table 6: Redundancy in the Training Dialogs TU1 TU2 TU3 TU4 Figure 15: Temporal Multiple Thread Structure the system uses a simple heuristic for ignoring subdialogs: a time is ignored if the utterance evoking it is in the simple past or past perfect. This prevents some of the potential errors and suggests that changes in tense, aspect, and modality are promising clues to explore for recognizing subsegments in this kind of data (see, for example, Grosz  Sidner, 1986; Nakhimovsky, 1988). 8.1.2 Multiple threads Rose et al. (1995, p. 31) describe dialogs composed of multiple threads as negotiation dialogues in which multiple propositions are negotiated in parallel. According to Rose et al., dialogs with such multiple threads pose challenges to a stack-based discourse model on both the intentional and attentional levels. They posit a more complex representation of attentional state to meet these challenges, and improve their results on speech act resolution in a corpus of scheduling dialogs by using their model of attentional state.5 As discussed above, in this work we address only the attentional level. The relevant structure for temporal reference resolution, abstracting from the examples given by Rose et al., is shown in Figure 15. There are four Temporal Units mentioned in the order TU1, TU2, TU3, and TU4 (other times could be mentioned in between). The (attentional) multiple thread case is when TU1 is required to be an antecedent of TU3, but TU2 is also needed to 5. They do not report how many multiple thread instances appear in their data. 277 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever Assumed Dialog Date: Friday 11 April (1) S1: We need to set up a schedule for the meeting. (2) How does your schedule look for next week? (3) S2: Well, Monday and Tuesday both mornings are good. (4) Wednesday afternoon is good also. (5) S1: It looks like it will have to be Thursday then. (6) Or Friday would also possibly work. (7) Do you have time between twelve and two on Thursday? (8) Or do you think sometime Friday afternoon you could meet? (9) S2: No. (10) Thursday I have a class. (11) And Friday is really tight for me. (12) How is the next week? (13) If all else fails there is always video conferencing. (14) S1: Monday, Tuesday, and Wednesday I am out of town. (15) But Thursday and Friday are both good. (16) How about Thursday at twelve? (17) S2: Sounds good. (18) See you then. Figure 16: Example of Deliberating Over A Meeting Time (Rose et al., 1995, p. 32) interpret TU4. There are no realizations of this structure, in terms of our model, in either the NMSU or CMU training data set. The case represented by row three in Figure 10, whose structure is depicted above in 13, is the instance in our data that is most closely related to the situations addressed by Rose et al. This is a type of structure that Grosz and Sidners model addresses, but it is not a multiple thread case, since TU2 is not needed to interpret a Temporal Unit mentioned after TU3. Rose et al.s examples of dialogs containing multiple threads are shown in Figures 16 and 17, which are Rose et al.s Figures 1 and 2, respectively. Figure 16 is an extended example, and Figure 17 contains a simpliﬁed example which they analyze in greater detail. The passage in Figure 16 would be processed by our algorithm as follows. The dialog date is not given in (Rose et al., 1995). For concreteness, let us suppose that the dialog date is Friday 11 April. Then, next week is Monday 14 April through Friday 18 April (the dialog does not mention weekend days, so we exclude them for ease of discussion). Utterance 2 is deictic, introducing next week into the discourse. Utterances 3-6 all have both deictic and anaphoric readings, all of which yield the correct results. The deictic relation for all of them is the frame of reference deictic relation, under which the interpretations are forward references from the dialog date: 278 An Empirical Approach to Temporal Reference Resolution Utterance Deictic Interpretation 3 Monday 14 April  Tuesday 15 April 4 Wednesday 16 April 5 Thursday 17 April 6 Friday 18 April The correct interpretations of (3)-(6) are also established with the co-reference anaphoric relation, with antecedent next week in utterance 2: they each can be interpreted as specifying a more speciﬁc time than next week, that is, as a particular day of next week. Finally, the frame of reference anaphoric relation yields the correct result for Tuesday in (3)6 and for the times speciﬁed in utterances (4)-(6). The interpretation is the day calculated forward from the most recently mentioned Temporal Unit: Utterance Antecedent Interpretation 3 Monday 14 April, Utterance 3 Tuesday 15 April 4 Tuesday 15 April, Utterance 3 Wednesday 16 April 5 Wednesday 16 April, Utterance 4 Thursday 17 April 6 Thursday 17 April, Utterance 5 Friday 18 April Utterances (7) and (10) are potential challenges for our algorithm, representing instances of the situation depicted in Figure 13: Thursday 24 April is a possible incorrect interpretation of Thursday in these utterances, yielded by the frame of reference anaphoric relation. The correct interpretation is also a candidate, yielded by multiple relations: the frame of reference deictic relation and the co-reference anaphoric relation, with Thursday 17 April in utterance (5) as antecedent. The relative magnitude of the certainty factors of the co- reference and frame of reference anaphoric relations means that the correct interpretation is likely to be chosen in practice, as mentioned in Section 8.1.1. If the incorrect interpretation were chosen for utterances (7) and (10), then incorrect interpretations of Friday in each of (8) and (11) would be possible: the Friday after the incorrect date of Thursday 24 April, yielded by the frame of reference anaphoric relation. However, the correct interpretations would be possible too, yielded by the frame of reference deictic relation and the co-reference anaphoric relation. Utterances (12) through (16) have analogous interpretations, except that the deictic interpretations yield incorrect results (that is, due to utterance 12, How is the next week?, the days are actually of the week Monday 21 April through Friday 25 April; the deictic interpretations are of the week Monday 14 April through Friday 18 April). Thus, there are one correct and two incorrect interpretations for some of the utterances, making it less likely in practice that the correct interpretation would be chosen. Note that, generally speaking, which focus model is used does not directly address the deicticanaphoric ambiguity, so, for the purposes of this section, the two parts of the dialog pose the same challenge to the focus model. The dialog in Figure 17 is analogous. However, The other day in (5) brings up other issues. There is a special case of the co-reference anaphoric relation for such expressions 6. Recall that multiple Temporal Units speciﬁed in a single utterance are added to the focus list in order of mention and treated as separate discourse entities. 279 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever 1. When can you meet next week? 2. Tuesday afternoon looks good. 3. I could do it Wednesday morning too. 4. Tuesday I have a class from 12:00-1:30. 5. But the other day sounds good. A. Simple Stack Based Structure DS4 DS3 DS2 DS1 DS0 1. When can you meet next week? 2. Tuesday afternoon looks good. 3. I could do it Wednesday morning too. 4. Tuesday I have a class from 12:00-1:30. 5. But the other day sounds good. B. Graph-Structured Stack Structure DSE DSD DSC DSB DSA Figure 17: Sample Analysis (Rose et al., 1995, p. 33) 280 An Empirical Approach to Temporal Reference Resolution (i.e., the other daymonthyear; see Anaphoric Rule 7 in Online Appendix 1). In this case, the second most recent day, month, or year, as appropriate, is the candidate antecedent. Presumably, neither the most recently mentioned day nor a day mentioned before two or more others would be referred to as the other day; thus, we anticipate that this is a good heuristic. Nevertheless, if (5) explicitly mentioned Wednesday, our algorithm would have a correct and an incorrect interpretation to choose between. In summary, there were no instances of temporal multiple threads of the type addressed by Rose et al., either in the CMU training data upon which the algorithm was developed, or in the NMSU training data to which the algorithm was later applied. If segments such as those illustrated in Rose et al. were to appear, an incorrect interpretation by our algorithm would be possible, but, under our model, the correct antecedent would also be available. For the examples they present, the algorithm faces the same choice: establish a co-reference relation to a time before the last one (the correct interpretation), or establish a frame of reference relation with the immediately preceding time (an incorrect interpretation). If performing temporal reference resolution is the goal, and if one is faced with an application in which such temporal multiple threads do occur, our investigation of the problem suggests that this speciﬁc situation should be investigated before assuming that a more complex focus model is needed. Adding a new modify anaphoric relation could be investigated. Or, as implemented in our system, a speciﬁc preference could be deﬁned for the co-reference relation over the frame of reference relation when both are possible in a local context. Statistical techniques could be used to establish preferences appropriate for the particular application. The diﬀerent ﬁndings between Rose et al. and our work might be due to the fact that diﬀerent problems are being addressed. Having no intentional state, our model does not distinguish between times being negotiated and other times. It is possible that another structure is relevant for the intentional level. Rose et al. do not specify whether or not this is so. The diﬀerent ﬁndings may also be due to diﬀerences in the data: their protocol is like a radio conversation in which a button must be pressed in order to transmit a message, and the other participant cannot transmit a message until the speaker releases the button. This results in less dynamic interaction and longer turns (Villa, 1994). In the dialogs used here, the participants have free control over turn-taking. 8.2 Coverage and Ambiguity of the Relations Deﬁned in the Model A question naturally arises from the evaluation presented in the previous section: in using a less complex focus model, have we merely pushed aside the ambiguity into the set of deictic and anaphoric relations? In this section, we assess the ambiguity of the anaphoric relations for the NMSU and CMU training sets. This section also presents other evaluations of the relations, including an assessment of their coverage, redundancy, how often they are correct, and how often they are applicable. The evaluations presented in this section required detailed, time-consuming manual annotations. The systems annotations would not suﬃce, because the implementation does not perfectly recognize when a rule is applicable. A sample of four randomly selected dialogs in the CMU training set and the four dialogs in the NMSU training set were annotated. 281 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever The counts derived from the manual annotations for this section are deﬁned below. Because this section focuses on the relations, we consider them at the more speciﬁc level of the deictic and anaphoric rules presented in Online Appendix 1. In addition, we do not allow trivial extensions of the relations, as we did in the evaluation of the focus model (Section 8.1). The criterion for correctness in this section is the same as for the evaluation of the system: a ﬁeld-by-ﬁeld exact match with the manually annotated correct interpretations. There is one exception. The starting and end time of day ﬁelds are ignored, since these are known weaknesses of the rules, and they represent a relatively minor proportion of the overall temporal interpretation. The following were derived from manual annotations.  TimeRefs: the number of distinct times referred to in each sentence, summed over all sentences.  TimeRefsC: The number of TimeRefs for which a correct interpretation is available under our model (whether or not an incorrect interpretation is also possible).  Interp: The number of interpretations possible under the model. For the current Temporal Unit, there is one Interp for every rule that can be applied.  CorrI: The number of Interps that are correct, where correctness is deﬁned as an exact match with the manually annotated correct interpretation, except that the starting and end time of day ﬁelds are ignored.  IncI: The number of incorrect Interps (i.e., Interp  IncI  CorrI).  DiﬀI: The number of diﬀerent interpretations  DiﬀICorr: The number of diﬀerent interpretations, excluding interpretations of Tem- poral Units for which there is not a correct interpretation under our model. The values for each data set, together with coverage and ambiguity evaluations, are presented in Table 7. The ambiguity for both data sets is very low. The Ambiguity ﬁgure in Table 7 represents the average number of interpretations per temporal reference, considering only those for which the correct interpretation is possible (i.e., it is DiffICorr  TimeRefsC). The table also shows the ambiguity when all temporal references are included (i.e., DiffI  TimeRefs). As can be seen from the table, the average ambiguity in both data sets is much less than two interpretations per utterance. The coverage of the relations can be evaluated as (TimeRefsC  TimeRefs), the percentage of temporal references for which at least one rule yields the correct interpretation. While the coverage of the NMSU data set, 85, is not perfect, it is good, considering that the system was not developed on the NMSU data. The data also show that there is often more than one way to achieve the correct inter- pretation. This is another type of redundancy: redundancy of the data with respect to the model. It is calculated in Table 7 as (CorrI  TimeRefsC), that is, the number of correct interpretations over the number of temporal references that have a correct interpretation. 282 An Empirical Approach to Temporal Reference Resolution CMU Training Set 4 randomly selected dialogs TimeRefs TimeRefsC Interp CorrI IncI DiﬀI DiﬀICorr 78 74 165 142 23 91 85 Coverage (TimeRefsC  TimeRefs)  95 Ambiguity (DiﬀICorr  TimeRefsC)  1.15 Overall Ambiguity (DiﬀI  TimeRefs)  1.17 Rule Redundancy (CorrI  TimeRefsC)  14274  1.92  NMSU Training Set 4 dialogs TimeRefs TimeRefsC Interp CorrI IncI DiﬀI DiﬀICorr 98 83 210 154 56 129 106 Coverage (TimeRefsC  TimeRefs)  85 Ambiguity (DiﬀICorr  TimeRefsC)  1.28 Overall Ambiguity (DiﬀI  TimeRefs)  1.32 Rule Redundancy (CorrI  TimeRefsC)  154  83  1.86  Table 7: Coverage and Ambiguity 283 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever CMU Training Set 4 randomly selected dialogs Rule Correct Total Accuracy D1 4 4 1.00 D2i 0 0 0.00 D2ii 35 40 0.88 a frame-of-reference deictic relation D3 1 2 0.50 D4 0 0 0.00 D5 0 0 0.00 D6 2 2 1.00 A1 45 51 0.88 a co-reference anaphoric relation A2 0 0 0.00 A3i 1 1 1.00 A3ii 35 37 0.95 a frame-of-reference anaphoric rel. A4 14 18 0.78 a modify anaphoric relation A5 0 0 0.00 A6i 2 2 1.00 A6ii 1 1 1.00 A7 0 1 0.00 A8 0 0 0.00 NMSU Training Set 4 dialogs Rule Correct Total Accuracy D1 4 4 1.00 D2i 0 0 0.00 D2ii 24 36 0.67 a frame-of-reference deictic relation D3 6 9 0.67 D4 0 1 0.00 D5 0 0 0.00 D6 0 0 0.00 A1 57 68 0.84 a co-reference anaphoric relation A2 5 5 1.00 A3i 0 0 0.00 A3ii 21 32 0.66 a frame-of-reference anaphoric rel. A4 27 37 0.73 a modify anaphoric relation A5 0 1 0.00 A6i 7 9 0.78 A6ii 0 0 0.00 A7 0 0 0.00 A8 0 0 0.00 Table 8: Rule Applicability Based on Manual Annotations For both data sets, there are, on average, roughly two diﬀerent ways to achieve the correct interpretation. Table 8 shows the number of times each rule applies in total (column 3) and the number of times each rule is correct (column 2), according to our manual annotations. Column 4 shows the accuracies of the rules, i.e., (column 2  column 3). The rule labels are the ones used in Online Appendix 1 to identify the rules. The same four rules are responsible for the majority of applications in both data sets, the ones labeled D2ii, A1, A3ii, and A4. The ﬁrst is an instance of the frame of reference deictic relation, the second is an instance of the co-reference anaphoric relation, the third is an instance of the frame of reference anaphoric relation, and the fourth is an instance of the modify anaphoric relation. How often the system considers and actually uses each rule is shown in Table 9. Specif- ically, the column labeled Fires shows how often each rule applies, and the column labeled Used shows how often each rule is used to form the ﬁnal interpretation. To help isolate the accuracies of the rules, these experiments were performed on unambiguous data. Comparing this table with Table 8, we see that the same four rules shown to be the most important by 284 An Empirical Approach to Temporal Reference Resolution CMU data set Name Used Fires D1 16 16 D2i 1 3 D2ii 78 90 a frame-of-reference deictic relation D3 5 5 D4 9 9 D5 0 1 D6 2 2 A1 95 110 a co-reference anaphoric relation A2 2 24 A3i 1 1 A3ii 72 86 a frame-of-reference anaphoric rel. A4 45 80 a modify anaphoric relation A5 4 5 A6i 10 10 A6ii 0 0 A7 0 0 A8 1 1 NMSU data set Name Used Fires D1 4 4 D2i 2 2 D2ii 20 31 a frame-of-reference deictic relation D3 2 3 D4 0 0 D5 0 0 D6 0 0 A1 46 65 a co-reference anaphoric relation A2 6 12 A3i 0 2 A3ii 18 27 a frame-of-reference anaphoric rel. A4 24 42 a modify anaphoric relation A5 3 5 A6i 6 8 A6ii 0 0 A7 0 0 A8 0 0 Table 9: Rule Activation by the System on Unambiguous Data the manual annotations are also responsible for the majority of the systems interpretations. This holds for both the CMU and NMSU data sets. 8.3 Evaluation of the Architectural Components In this section, we evaluate the architectural components of our algorithm using degradation (ablation) studies. We perform experiments without each component in turn, and then with none of them, to observe the impact on the systems performance. Such studies have been useful in developing practical methods for other kinds of anaphora resolution as well (see, for example, Mitkov  Stys, 1997). Speciﬁcally, an experiment was performed testing each of the following variations. 1. The certainty factors of all of the rules are set to 1. Recall that all rules are applied to each utterance, and each rule that matches produces a Partial-Augmented-ILT (which is assigned the certainty factor of the rule). All maximal mergings of the Partial-Augmented-ILTs are then formed, to create a set of Augmented-ILTs. Then, the ﬁnal interpretation of the utterance is chosen from 285 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever among the set of Augmented-ILTs. The certainty factor of each Augmented-ILT is the sum of the certainty factors of the Partial-Augmented-ILTs composing it. Thus, setting the certainty factors to 1 implements the scheme in which the more partial results are merged into an interpretation, the higher the overall certainty factor of that interpretation. In other words, this scheme favors the Augmented-ILT resulting from the greatest number of rule applications. 2. The certainty factors of all of the rules are set to 0. This scheme is essentially random selection among the Augmented-ILTs that make sense according to the critics. If the critics did not exist, then setting the rule certainty factors to 0 would result in random selection. With the critics, any Augmented-ILTs to which the critics apply are excluded from consideration, because the critics will lower their certainty factors to negative numbers. 3. No merging of the rule results is performed. That is, the Partial-Augmented-ILTs are not merged prior to selection of the ﬁnal Augmented-ILT. The eﬀect of this is that the result of one single rule is chosen to be the ﬁnal interpretation. 4. The critics are not used. 5. The distance factors are not used. In this case, the certainty factors for rules that access the focus list are not adjusted based on how far back the chosen focus list item is. 6. All variations are applied, excluding case 2. Speciﬁcally, neither the critics nor the distance factors are used, no merging of partial results is performed, and the rules are all given the same certainty factor (namely, 1). Table 10 shows the results for each variation when run over the unambiguous but uncor- rected CMU training data. For comparison, the ﬁrst row shows the results for the system as normally conﬁgured. As with the previous evaluations, accuracy is the percentage of the correct answers the system produces, while precision is the percentage of the systems answers that are correct. Only two of the diﬀerences are statistically signiﬁcant (p  0.05), namely, the precision of the systems performance when the critics are not used, and the accuracy of the systems performance when all of the certainty factors are 0. The signiﬁcance analysis was performed using paired t-tests comparing the results for each variation with the results for the system as normally conﬁgured. The performance diﬀerence when the critics are not used is due to extraneous alternatives that the critics would have weeded out. The drop in accuracy when the certainty factors are all 0 shows that the certainty factors have some eﬀect. Experimenting with statistical methods to derive them would likely lead to further improvement. The remaining ﬁgures are all only slightly lower than those for the full system, and are all much higher than the baseline accuracies. 286 An Empirical Approach to Temporal Reference Resolution Variation Cor Inc Mis Ext Nul Act Poss Acc Prec system as is 1283 44 112 37 574 1938 2013 0.923 0.958 all CFs 1.0 1261 77 101 50 561 1949 2000 0.911 0.935 all CFs 0.0 1202 118 119 49 562 1931 2001 0.882 0.914 -critics 1228 104 107 354 667 2353 2106 0.900 0.805 -dist. factors 1265 52 122 50 591 1958 2030 0.914 0.948 -merge 1277 46 116 54 577 1954 2016 0.920 0.949 combo 1270 53 116 67 594 1984 2033 0.917 0.940 Legend Cor(rect): System and key agree on non-null value Inc(orrect): System and key diﬀer on non-null value Mis(sing): System has null value for non-null key Ext(ra): System has non-null value for null key Nul(l): Both system and key give null answer Poss(ible): Correct  Incorrect  Missing  Null Act(ual): Correct  Incorrect  Extra  Null Base(line)Acc(uracy): Baseline accuracy (input used as is) Acc(uracy):  Key values matched correctly ((Correct  Null)Possible) Prec(ision):  System answers matching the key ((Correct  Null)Actual) Table 10: Evaluation of the Variations on CMU UnambiguousUncorrected Data 287 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever It is interesting to note that the unimportance of the distance factors (variation 5) is consistent with the ﬁndings presented in Section 8.1 that the last mentioned time is an acceptable antecedent in the vast majority of cases. Otherwise, we might have expected to see an improvement in variation 5, since the distance factors penalize going further back on the focus list. 9. Conclusions Scheduling dialogs, during which people negotiate the times of appointments, are common in everyday life. This paper reports the results of an in-depth empirical investigation of resolving explicit temporal references in scheduling dialogs. There are four basic phases of this work: data annotation, model development, system implementation and evaluation, and model evaluation and analysis. The system and model were developed primarily on one set of data (the CMU dialogs), and then applied later to a much more complex set of data (the NMSU dialogs), to assess the generalizability of the model for the task being performed. Many diﬀerent types of empirical methods were applied to both data sets to pinpoint the strengths and weaknesses of the approach. In the data annotation phase, detailed coding instructions were developed and an inter- coder reliability study involving naive subjects was performed. The results of the study are very good, supporting the viability of the instructions and annotations. During the model development phase, we performed an iterative process of implementing a proposed set of anaphoric and deictic relations and then reﬁning them based on system performance (on the CMU training data), until we settled on the set presented here. We also developed our focus model during this phase. The question of what type of focus model is required for various tasks is a question of ongoing importance in the literature. It appeared from our initial observations of the data that, contrary to what we expected, a recency-based focus model might be adequate. To test this hypothesis, we made the strategic decision to limit ourselves to a recency-based model, rather than build some kind of hybrid model whose success or failure would not have told us as much. During system implementation and evaluation, a system implementing the model was implemented and evaluated on unseen test data, using a challenging ﬁeld-by-ﬁeld compari- son of system and human answers. To be considered the right answer, the information must not only be correct, but must also be included in the correct ﬁeld of the output representa- tion. Taking as input the ambiguous output of a semantic grammar, the system achieves an overall accuracy of 81 on unseen CMU test data, a large improvement over the baseline accuracy of 43. On an unseen test set from the more complex NMSU data, the results are very respectable: an overall accuracy of 69, with a much lower baseline accuracy of 29. This also shows the robustness of the CMU semantic parser (Lavie  Tomita, 1993; Levin et al., 1995), which was given the NMSU dialogs as input without being modiﬁed in any way to handle them. The implementation is an important proof of concept. However, it is not a direct evaluation of the model, because there are errors due to factors we do not focus on in this work. Some of the error is simply due to utterance components being outside the coverage of the CMU parser, or having high semantic ambiguity. The only information we use to perform semantic disambiguation is the temporal context. The Enthusiast researchers have 288 An Empirical Approach to Temporal Reference Resolution already developed better techniques for resolving the semantic ambiguity in these dialogs (Shum et al., 1994), which could be used to improve performance. Thus, in the model evaluation and analysis phase, we performed extensive additional evaluation of the algorithm itself. We focus on the relations and the focus model, because they are the main contributions of this work. Our degradation studies support this, as they show that the other aspects of the algorithm, such as the distance factors and merging process, are responsible for little of the systems success (see Section 8.3). Our evaluations show the strength of the focus model for the task, not only for the CMU data on which it was developed, but also for the more complex NMSU data. While the NMSU data is more complex, there are few cases in which the last mentioned time is not an appropriate antecedent, highlighting the importance of recency (Walker, 1996); see Section 8.1. We characterized those cases along a number of dimensions, to identify the particular types of challenges they pose (see Figure 10). In order to compare our work to that of others, we formally deﬁned subdialogs and the multiple thread structures addressed by Rose et al. (1995) with respect to our model and the speciﬁc problem of temporal reference resolution. An interesting ﬁnding is that, while subdialogs of the types addressed by Grosz and Sidner (1986) were found in the data, no cases of multiple threads were found. That is, some subdialogs, all in the NMSU data, mention times that potentially interfere with the correct antecedent. But in none of these cases would subsequent errors result if, upon exiting the subdialog, the oﬀending information were popped oﬀ a discourse stack or otherwise made inaccessible. Changes in tense, aspect, and modality are promising clues for recognizing subdialogs in this data, which we plan to explore in future work. To assess whether or not using a simpler focus model requires one to use a highly ambiguous set of relations, we performed a separate evaluation of the relations, based on detailed, manual annotations of a set of dialogs. The ambiguity of the relations for both data sets is very low, and the coverage is good (see Table 7). In a comparison of system and human annotations, the same four rules identiﬁed to be most important in the manual annotations are responsible for the majority of the systems interpretations for both data sets (see Tables 8 and 9), suggesting that the system is a good implementation of the model. Recently, many in computational discourse processing have turned to empirical studies of discourse, with a goal to develop general theories by analyzing speciﬁc discourse phenom- ena and systems that process them (Walker  Moore, 1997). We contribute to this general enterprise. We performed many diﬀerent evaluations, on the CMU data upon which the model was developed, and on the more complex NMSU data. The task and model compo- nents were explicitly speciﬁed to facilitate evaluation and comparison. Each evaluation is directed toward answering a particular question; together, the evaluations paint an overall picture of the diﬃculty of the task and of the success of the proposed model. As a contribution of this work, we have made available on the project web page the coding instructions, the NMSU dialogs, and the various kinds of manual annotations we performed. 289 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever 10. Acknowledgements This research was supported in part by the Department of Defense under grant number 0-94-10. A number of people contributed to this work. We want to especially thank David Farwell, Daniel Villa, Carol Van Ess-Dykema, Karen Payne, Robert Sinclair, Rocio Guillen, David Zarazua, Rebecca Bruce, Gezina Stein, Tom Herndon, and the project members of Enthusiast at CMU, whose cooperation greatly aided our project. We wholeheartedly thank the anonymous reviewers, whose comments and criticisms were very helpful. We also thank Esther Steiner, Philip Bernick, and Julie England for participating in the intercoder reliability study, and Linda Fresques for proofreading the paper. References Alexandersson, J., Reithinger, N.,  Elisabeth, M. (1997). Insights into the dialogue pro- cessing of Verbmobil. In Proc. 5th Conference on Applied Natural Language Process- ing, pp. 3340. Association for Computational Linguistics. Allen, J. (1984). Toward a general theory of action and time. Artiﬁcial Intelligence, 23, 123154. Allen, J.,  Perrault, C. (1980). Analyzing intention in utterances. Artiﬁcial Intelligence, 15, 143178. Arhenberg, L., Dahlback, N.,  Jonsson, A. (1995). Coding schemes for natural language dialogues. In Working Notes of AAAI Spring Symposium: Empirical Methods in Discourse Interpretation and Generation, pp. 813. Busemann, S., Declerck, T., Diagne, A. K., Dini, L., Klein, J.,  Schmeier, S. (1997). Natural language dialogue service for appointment scheduling agents. In Proc. 5th Conference on Applied Natural Language Processing, pp. 2532. Association for Com- putational Linguistics. Carletta, J. (1996). Assessing agreement on classiﬁcation tasks: the kappa statistic. Com- putational Linguistics, 22(2), 249254. Clark, H. H. (1977). Bridging. In Johnson-Laird, P. N.,  Wason, P. C. (Eds.), Thinking: Readings in Cognitive Science. Cambridge University Press. Condon, S.,  Cech, C. (1995). Problems for reliable discourse coding schemes. In Proc. AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Gen- eration, pp. 2733. Grosz, B., Joshi, A.,  Weinstein, S. (1995). Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2), 203225. Grosz, B.,  Sidner, C. (1986). Attention, intention, and the structure of discourse. Com- putational Linguistics, 12(3), 175204. Hays, W. L. (1988). Statistics (Fourth edition). Holt, Rinehart, and Winston. 290 An Empirical Approach to Temporal Reference Resolution Heim, I. (1982). The Semantics of Deﬁnite and Indeﬁnite Noun Phrases. Ph.D. thesis, University of Massachusetts at Amherst. Hirschberg, J.,  Nakatani, C. (1996). A prosodic analysis of discourse segments in direction- giving monologues. In Proc. 34th Annual Meeting of the Association for Computa- tional Linguistics, pp. 286293. Hobbs, J. (1978). Resolving pronoun references. Lingua, 44, 311338. Hwang, C.,  Schubert, L. (1992). Tense trees as the ﬁne structure of discourse. In Proc. 30th Annual Meeting of the Association for Computational Linguistics, pp. 232240. Isard, A.,  Carletta, J. (1995). Replicability of transaction and action coding in the map task corpus. In Working Notes of AAAI Spring Symposium: Empirical Methods in Discourse Interpretation and Generation, pp. 6066. Kameyama, M., Passonneau, R.,  Poesio, M. (1993). Temporal centering. In Proc. of the 31st Annual Meeting of the Association for Computational Linguistics, pp. 7077. Kamp, H.,  Reyle, U. (1993). From Discourse to Logic. Kluwer Academic Publisher, Dordrecht, The Netherlands. Lascarides, A., Asher, N.,  Oberlander, J. (1992). Inferring discourse relations in context. In Proc. 30th Annual Meeting of the Association for Computational Linguistics, pp. 18. Lavie, A.,  Tomita, M. (1993). GLR - an eﬃcient noise skipping parsing algorithm for context free grammars. In Proc. 3rd International Workshop on Parsing Technologies. Levin, L., Glickman, O., Qu, Y., Gates, D., Lavie, A., Rose, C., Van Ess-Dykema, C.,  Waibel, A. (1995). Using context in the machine translation of spoken language. In Proc. Theoretical and Methodological Issues in Machine Translation (TMI-95). Litman, D.,  Passonneau, R. (1995). Combining multiple knowledge sources for discourse segmentation. In Proc. 33rd Annual Meeting of the Association for Computational Linguistics, pp. 130143. Mann, W.,  Thompson, S. (1988). Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8(3), 243281. Mitkov, R.,  Stys, M. (1997). Robust reference resolution with limited knowledge: high precision genre-speciﬁc approach for English and Polish. In Recent Advances in Nat- ural Language Processing (RANLP-97), pp. 7481. European Commission, DG XIII. Moser, M.,  Moore, J. (1995). Investigating cue selection and placement in tutorial dis- courses. In Proc. 33rd Annual Meeting of the Association for Computational Linguis- tics, pp. 130143. Nakhimovsky, A. (1988). Aspect, aspectual class, and the temporal structure of narrative. Computational Linguistics, 14(2), 2943. 291 Wiebe, OHara, Ohrstrom-Sandgren,  McKeever OHara, T., Wiebe, J.,  Payne, K. (1997). Instructions for annotating temporal informa- tion in scheduling dialogs. Tech. rep. MCCS-97-308, Computing Research Laboratory, New Mexico State University. Passonneau, R.,  Litman, D. (1993). Intention-based segmentation: human reliability and correlation with linguistic cues. In Proc. of the 31st Annual Meeting of the Association for Computational Linguistics, pp. 148155. Poesio, M., Vieira, R.,  Teufel, S. (1997). Resolving bridging references in unrestricted text. In Proc. Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts. Association for Computational Linguistics. Qu, Y., Eugenio, B. D., Lavie, A., Levin, L.,  Rose, C. (1996). Minimizing cumulative error in discourse context. In ECAI Workshop Proceedings on Dialogue Processing in Spoken Language Systems. Rose, C., Eugenio, B. D., Levin, L.,  Van Ess-Dykema, C. (1995). Discourse processing of dialogues with multiple threads. In Proc. 33rd Annual Meeting of the Association for Computational Linguistics, pp. 3138. Shum, B., Levin, L., Coccaro, N., Carbonell, J., Horiguchi, K., Isotani, H., Lavie, A., Mayﬁeld, L., Rose, C., Van Ess-Dykema, C.,  Waibel, A. (1994). Speech-language integration in a multi-lingual speech translation system. In Proceedings of the AAAI Workshop on Integration of Natural Language and Speech Processing. Sidner, C. (1979). Towards a Computational Theory of Deﬁnite Anaphora Comprehension in English Discourse. Ph.D. thesis, MIT. Sidner, C. (1983). Focusing in the comprehension of deﬁnite anaphora. In Brady, M.,  Berwick, R. C. (Eds.), Computational Models of Discourse, pp. 267330. MIT Press, Cambridge, MA. Siegel, S.,  Castellan, Jr., N. J. (1988). Nonparametric Statistics for the Behavioral Sci- ences (Second edition). McGraw-Hill, New York. Song, F.,  Cohen, R. (1991). Tense interpretation in the context of narrative. In Proc. 9th National Conference on Artiﬁcial Intelligence (AAAI-91), pp. 131136. Villa, D. (1994). Eﬀects of protocol on discourse internal and external illocutionary markers in Spanish dialogs. In Linguistic Association of the Southwest Conference XXIII. Walker, L.,  Moore, J. (1997). Empirical studies in discourse. Computational Linguistics, 23(1), 112. Walker, M. (1996). Limited attention and discourse structure. Computational Linguistics, 22(2), 255264. Webber, B. L. (1983). So what can we talk about now?. In Brady, M.,  Berwick, R. (Eds.), Computational Models of Discourse. MIT Press, Cambridge. 292 An Empirical Approach to Temporal Reference Resolution Webber, B. (1988). Tense as discourse anaphor. Computational Linguistics, 14(2), 6173. Wiebe, J., Farwell, D., Villa, D., Chen, J.-L., Sinclair, R., Sandgren, T., Stein, G., Zarazua, D.,  OHara, T. (1996). Artwork: Discourse processing in machine translation of dialog. Tech. rep. MCCS-96-294, Computing Research Laboratory, New Mexico State University. 293",
  "24.pdf": "arXiv:cs9902001v1 [cs.CL] 31 Jan 1999 Compacting the Penn Treebank Grammar Alexander Krotov and Mark Hepple and Robert Gaizauskas and Yorick Wilks Department of Computer Science, Sheﬃeld University 211 Portobello Street, Sheﬃeld S1 4DP, UK {alexk, hepple, robertg, yorick}dcs.shef.ac.uk Abstract Treebanks, such as the Penn Treebank (PTB), oﬀer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar oﬀ the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more tree- banked text would be required to obtain a com- plete grammar, if one exists at some limit. However, we oﬀer an alternative explanation in terms of the underspeciﬁcation of structures within the treebank. This hypothesis is ex- plored by applying an algorithm to compact the derived grammar by eliminating redundant rules  rules whose right hand sides can be parsed by other rules. The size of the result- ing compacted grammar, which is signiﬁcantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance ﬁgures. A version of the com- paction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58 reduction in grammar size without signif- icant change in parsing performance, and can produce a 69 reduction with some gain in re- call, but a loss in precision. 1 Introduction The Penn Treebank (PTB) (Marcus et al., 1994) has been used for a rather simple approach to deriving large grammars automatically: one where the grammar rules are simply read oﬀ the parse trees in the corpus, with each local subtree providing the left and right hand sides of a rule. Charniak (Charniak, 1996) reports precision and recall ﬁgures of around 80 for a parser employing such a grammar. In this paper we show that the huge size of such a tree- bank grammar (see below) can be reduced in size without appreciable loss in performance, and, in fact, an improvement in recall can be achieved. Our approach can be generalised in terms of Data-Oriented Parsing (DOP) methods (see (Bonnema et al., 1997)) with the tree depth of 1. However, the number of trees produced with a general DOP method is so large that Bonnema (Bonnema et al., 1997) has to resort to restrict- ing the tree depth, using a very domain-speciﬁc corpus such as ATIS or OVIS, and parsing very short sentences of average length 4.74 words. Our compaction algorithm can be extended for the use within the DOP framework but, because of the huge size of the derived grammar (see be- low), we chose to use the simplest PCFG frame- work for our experiments. We are concerned with the nature of the rule set extracted, and how it can be improved, with regard both to linguistic criteria and processing eﬃciency. In what follows, we report the worry- ing observation that the growth of the rule set continues at a square root rate throughout pro- cessing of the entire treebank (suggesting, per- haps that the rule set is far from complete). Our results are similar to those reported in (Krotov et al., 1994). 1 We discuss an alternative pos- sible source of this rule growth phenomenon, partial bracketting, and suggest that it can be alleviated by compaction, where rules that are redundant (in a sense to be deﬁned) are elimi- nated from the grammar. Our experiments on compacting a PTB tree- 1For the complete investigation of the grammar ex- tracted from the Penn Treebank II see (Gaizauskas, 1995) 0 5000 10000 15000 20000 0 20 40 60 80 100 Number of rules Percentage of the corpus Figure 1: Rule Set Growth for Penn Treebank II bank grammar resulted in two major ﬁndings: one, that the grammar can at the limit be com- pacted to about 7 of its original size, and the rule number growth of the compacted grammar stops at some point. The other is that a 58 re- duction can be achieved with no loss in parsing performance, whereas a 69 reduction yields a gain in recall, but a small loss in precision. This, we believe, gives further support to the utility of treebank grammars and to the compaction method. For example, compaction methods can be applied within the DOP frame- work to reduce the number of trees. Also, by partially lexicalising the rule extraction process (i.e., by using some more frequent words as well as the part-of-speech tags), we may be able to achieve parsing performance similar to the best results in the ﬁeld obtained in (Collins, 1996). 2 Growth of the Rule Set One could investigate whether there is a ﬁ- nite grammar that should account for any text within a class of related texts (i.e. a domain oriented sub-grammar of English). If there is, the number of extracted rules will approach a limit as more sentences are processed, i.e. as the rule number approaches the size of such an underlying and ﬁnite grammar. We had hoped that some approach to a limit would be seen using PTB II (Marcus et al., 1994), which larger and more consistent for an- notation than PTB I. As shown in Figure 1, however, the rule number growth continues un- abated even after more than 1 million part-of- speech tokens have been processed. 3 Rule Growth and Partial Bracketting Why should the set of rules continue to grow in this way? Putting aside the possibility that natural languages do not have ﬁnite rule sets, we can think of two possible answers. First, it may be that the full underlying grammar is much larger than the rule set that has so far been produced, requiring a much larger tree- banked corpus than is now available for its extraction. If this were true, then the out- look would be bleak for achieving near-complete grammars from treebanks, given the resource demands of producing such resources. However, the radical incompleteness of grammar that this alternative implies seems incompatible with the promising parsing results that Charniak reports (Charniak, 1996). A second answer is suggested by the presence in the extracted grammar of rules such as (1).2 This rule is suspicious from a linguistic point of view, and we would expect that the text from which it has been extracted should more prop- erly have been analysed using rules (2,3), i.e. as a coordination of two simpler NPs. NP  DT NN CC DT NN (1) NP  NP CC NP (2) NP  DT NN (3) Our suspicion is that this example reﬂects a widespread phenomenon of partial bracketting within the PTB. Such partial bracketting will arise during the hand-parsing of texts, with (hu- man) parsers adding brackets where they are conﬁdent that some string forms a given con- stituent, but leaving out many brackets where they are less conﬁdent of the constituent struc- ture of the text. This will mean that many rules extracted from the corpus will be ﬂat- ter than they should be, corresponding prop- erly to what should be the result of using sev- eral grammar rules, showing only the top node and leaf nodes of some unspeciﬁed tree structure (where the leaf nodes here are category sym- bols, which may be nonterminal). For the exam- ple above, a tree structure that should properly have been given as (4), has instead received only 2PTB POS tags are used here, i.e. DT for determiner, CC for coordinating conjunction (e.g and), NN for noun the partial analysis (5), from the ﬂatter partial- structure rule (1). i. NP    NP    DT NN CC NP    DT NN (4) ii. NP       DT NN CC DT NN (5) 4 Grammar Compaction The idea of partiality of structure in treebanks and their grammars suggests a route by which treebank grammars may be reduced in size, or compacted as we shall call it, by the elimination of partial-structure rules. A rule that may be eliminable as a partial-structure rule is one that can be parsed (in the familiar sense of context- free parsing) using other rules of the grammar. For example, the rule (1) can be parsed us- ing the rules (2,3), as the structure (4) demon- strates. Note that, although a partial-structure rule should be parsable using other rules, it does not follow that every rule which is so parsable is a partial-structure rule that should be elimi- nated. There may be linguistically correct rules which can be parsed. This is a topic to which we will return at the end of the paper (Sec. 6). For most of what follows, however, we take the simpler path of assuming that the parsability of a rule is not only necessary, but also suﬃcient, for its elimination. Rules which can be parsed using other rules in the grammar are redundant in the sense that eliminating such a rule will never have the ef- fect of making a sentence unparsable that could previously be parsed.3 The algorithm we use for compacting a gram- mar is straightforward. A loop is followed whereby each rule R in the grammar is ad- dressed in turn. If R can be parsed using other rules (which have not already been eliminated) then R is deleted (and the grammar without R is used for parsing further rules). Otherwise R 3Thus, wherever a sentence has a parse P that em- ploys the parsable rule R, it also has a further parse that is just like P except that any use of R is replaced by a more complex substructure, i.e. a parse of R. is kept in the grammar. The rules that remain when all rules have been checked constitute the compacted grammar. An interesting question is whether the result of compaction is independent of the order in which the rules are addressed. In general, this is not the case, as is shown by the following rules, of which (8) and (9) can each be used to parse the other, so that whichever is addressed ﬁrst will be eliminated, whilst the other will remain. B  C (6) C  B (7) A  B B (8) A  C C (9) Order-independence can be shown to hold for grammars that contain no unary or epsilon (empty) rules, i.e. rules whose righthand sides have one or zero elements. The grammar that we have extracted from PTB II, and which is used in the compaction experiments reported in the next section, is one that excludes such rules. Unary and sister rules were collapsed with the sister nodes, e.g. the structure (S (NP -NULL-) (VP VB (NP (QP ...))) .) will produce the following rules: S - VP ., VP - VB QP and QP - ....4 For further discussion, and for the proof of the order independence see (Krotov, 1998). 5 Experiments We conducted a number of compaction exper- iments: 5 ﬁrst, the complete grammar was parsed as described in Section 4. Results ex- ceeded our expectations: the set of 17,529 rules reduced to only 1,667 rules, a better than 90 reduction. To investigate in more detail how the com- pacted grammar grows, we conducted a third experiment involving a staged compaction of the grammar. Firstly, the corpus was split into 10 chunks (by number of ﬁles) and the rule sets extracted from each. The staged compaction proceeded as follows: the rule set of the ﬁrst 10 was compacted, and then the rules for the 4See (Gaizauskas, 1995) for discussion. 5For these experiments, we used two parsers: Stol- ckes BOOGIE (Stolcke, 1995) and Sekines Apple Pie Parser (Sekine and Grishman, 1995). 0 500 1000 1500 2000 0 20 40 60 80 100 Number of rules Percentage of the corpus Figure 2: Compacted Grammar Size next 10 added and the resulting set again com- pacted, and then the rules for the next 10 added, and so on. Results of this experiment are shown in Figure 2. At 50 of the corpus processed the com- pacted grammar size actually exceeds the level it reaches at 100, and then the overall gram- mar size starts to go down as well as up. This reﬂects the fact that new rules are either redun- dant, or make old rules redundant, so that the compacted grammar size seems to approach a limit. 6 Retaining Linguistically Valid Rules Even though parsable rules are redundant in the sense that has been deﬁned above, it does not follow that they should always be removed. In particular, there are times where the ﬂatter structure allowed by some rule may be more lin- guistically correct, rather than simply a case of partial bracketting. Consider, for example, the (linguistically plausible) rules (10,11,12). Rules (11) and (12) can be used to parse (10), but the latter should not be eliminated, as there are cases where the ﬂatter structure it allows is more linguistically correct. VP  VB NP PP (10) VP  VB NP (11) NP  NP PP (12) i. VP     VB NP    NP PP ii. VP    VB NP PP (13) We believe that a solution to this problem can be found by exploiting the data provided by the corpus. Frequency of occurrence data for rules which have been collected from the cor- pus and used to assign probabilities to rules, and hence to the structures they allow, so as to produce a probabilistic context-free grammar for the rules. Where a parsable rule is correct rather than merely partially bracketted, we then expect this fact to be reﬂected in rule and parse probabilities (reﬂecting the occurrence data of the corpus), which can be used to decide when a rule that may be eliminated should be elimi- nated. In particular, a rule should be eliminated only when the more complex structure allowed by other rules is more probable than the simpler structure that the rule itself allows. We developed a linguistic compaction algo- rithm employing the ideas just described. How- ever, we cannot present it here due to the space limitations. The preliminary results of our ex- periments are presented in Table 1. Simple thresholding (removing rules that only occur once) was also to achieve the maximum com- paction ratio. For labelled as well as unlabelled evaluation of the resulting parse trees we used the evalb software by Satoshi Sekine. See (Kro- tov, 1998) for the complete presentation of our methodology and results. As one can see, the fully compacted grammar yields poor recall and precision ﬁgures. This can be because collapsing of the rules often produces too much substructure (hence lower precision ﬁgures) and also because many longer rules in fact encode valid linguistic information. How- ever, linguistic compaction combined with sim- ple thresholding achieves a 58 reduction with- out any loss in performance, and 69 reduction even yields higher recall. 7 Conclusions We see the principal results of our work to be the following:  the result showing continued square-root growth in the rule set extracted from the PTB II;  the analysis of the source of this contin- ued growth in terms of partial bracketting and the justiﬁcation this provides for com- paction via rule-parsing;  the result that the compacted rule set does approach a limit at some point during Full Simply thresholded Fully compacted Linguistically compacted Grammar 1 Grammar 2 Labelled evaluation Recall 70.55 70.78 30.93 71.55 70.76 Precision 77.89 77.66 19.18 72.19 77.21 Unlabelled evaluation Recall 73.49 73.71 43.61 74.72 73.67 Precision 81.44 80.87 27.04 75.39 80.39 Grammar size 15,421 7,278 1,122 4,820 6,417 reduction (as  of full) 0 53 93 69 58 Table 1: Preliminary results of evaluating the grammar compaction method staged rule extraction and compaction, af- ter a suﬃcient amount of input has been processed;  that, though the fully compacted grammar produces lower parsing performance than the extracted grammar, a 58 reduction (without loss) can still be achieved by us- ing linguistic compaction, and 69 reduc- tion yields a gain in recall, but a loss in precision. The latter result in particular provides further support for the possible future utility of the compaction algorithm. Our method is similar to that used by Shirai (Shirai et al., 1995), but the principal diﬀerences are as follows. First, their algorithm does not employ full context- free parsing in determining the redundancy of rules, considering instead only direct composi- tion of the rules (so that only parses of depth 2 are addressed). We proved that the result of compaction is independent of the order in which the rules in the grammar are parsed in those cases involving mutual parsability (dis- cussed in Section 4), but Shirais algorithm will eliminate both rules so that coverage is lost. Secondly, it is not clear that compaction will work in the same way for English as it did for Japanese. References Remko Bonnema, Rens Bod, and Remko Scha. 1997. A DOP model for semantic interpretation. In Proceedings of European Chapter of the ACL, pages 159167. Eugene Charniak. 1996. Tree-bank grammars. In Proceedings of the Thirteenth National Confer- ence on Artiﬁcial Intelligence (AAAI-96), pages 10311036. MIT Press, August. Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceed- ings of the 34th Annual Meeting of the ACL. Robert Gaizauskas. 1995. Investigations into the grammar underlying the Penn Treebank II. Re- search Memorandum CS-95-25, University of Sheﬃeld. Alexander Krotov, Robert Gaizauskas, and Yorick Wilks. 1994. Acquiring a stochastic context-free grammar from the Penn Treebank. In Proceed- ings of Third Conference on the Cognitive Sci- ence of Natural Language Processing, pages 79 86, Dublin. Alexander Krotov. 1998. Notes on compacting the Penn Treebank grammar. Technical Memo, Department of Computer Science, University of Sheﬃeld. M. Marcus, G. Kim, M.A. Marcinkiewicz, R. MacIn- tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas- berger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of ARPA Speech and Natural language workshop. Satoshi Sekine and Ralph Grishman. 1995. A corpus-based probabilistic grammar with only two non-terminals. In Proceedings of Fourth Interna- tional Workshop on Parsing Technologies. Kiyoaki Shirai, Takenobu Tokunaga, and Hozumi Tanaka. 1995. Automatic extraction of Japanese grammar from a bracketed corpus. In Proceedings of Natural Language Processing Paciﬁc Rim Sym- posium, Korea, December. Andreas Stolcke. 1995. An eﬃcient probabilis- tic context-free parsing algorithm that computes preﬁx probabilities. Computational Linguistics, 21(2):165201.",
  "25.pdf": "arXiv:cs9902029v1 [cs.CL] 25 Feb 1999 The Fodor-FODOR fallacy bites back Yorick Wilks Abstract The paper argues that Fodor and Lepore are misguided in their attack on Pustejovskys Generative Lexicon, largely because their argument rests on a traditional, but implausible and discredited, view of the lexicon on which it is eﬀectively empty of content, a view that stands in the long line of explaining word meaning (a) by ostension and then (b) explaining it by means of a vacuous symbol in a lexicon, often the word itself after typographic transmogriﬁcation. (a) and (b) both share the wrong be- lief that to a word must correspond a simple entity that is its meaning. I then turn to the semantic rules that Pustejovsky uses and argue ﬁrst that, although they have novel features, they are in a well-established Artiﬁ- cial Intelligence tradition of explaining meaning by reference to structures that mention other structures assigned to words that may occur in close proximity to the ﬁrst. It is argued that Fodor and Lepores view that there cannot be such rules is without foundation, and indeed systems us- ing such rules have proved their practical worth in computational systems. Their justiﬁcation descends from line of argument, whose high points were probably Wittgenstein and Quine that meaning is not to be understood by simple links to the world, ostensive or otherwise, but by the relation- ship of whole cultural representational structures to each other and to the world as a whole1. 1 Introduction Fodor and Lepore (FL from here on) have saddled up recently and ridden again at the Windmills of Artiﬁcial Intelligence (AI): this time against Pustejovskys Generative Lexicon (Pustejovsky, 1995: FL call the work TGL), so as to make an example for the rest of us. I want to join in because FL claim he is part of a wider movement they call Informational Role Semantics (which I will call IRS as they do), and I count myself a long term member of that movement. But their weapons are rusty: they wave about as their sword of truth an old and much satirised fallacy, which Ryle (1957) called the Fido-Fido fallacy: that 1One might say: the ostensive deﬁnition explains the use the meaning of a word when the overall role of the word in language is clear. Thus if I know that someone means to explain a colour-word to me the ostensive deﬁnition That is called sepia will help me to understand the word (Wittgensein, 1953, p. 30). 1 to every word corresponds a meaning, be it abstract denotation (as for FL), a concept, or a real world object. The special quality of the fallacy is the simple one-to-one mapping, and not the nature of what corresponds to a word. In the ﬁrst part of this paper I want to show that the fallacy cannot be pressed back into service: it is old and overexposed. It is important to do this (again) even though, as the paper progresses, FL relent a little about the need for the fallacy, and even seem to accept a part of the IRS position. But they do this as men convinced that, really and truly and after all concessions are made, the fallacy is still true. It is not, and this needs saying yet again. In the second part of the paper, I will brieﬂy touch on issues speciﬁc to Pustejovskys (JP) claims; only brieﬂy because he is quite capable of defending his own views. In the third and ﬁnal part I will make some points to do with the general nature of the IRS position, within AI and computational natural language processing, and argue that the concession FL oﬀer is unneeded: IRS is a perfectly reasonable doctrine in its own right and needs no defence from those who really believe in the original fallacy. 2 Fido and FIDO Fodor and Le Pores dissection of JPs book is, and is intended to be, an at- tack on a whole AI approach to natural language processing based on symbolic representations, so it is open to any other member of that school to join in the defence. IRS has its faults but also some technological successes to show in the areas of machine translation and information extraction (e.g. Wilks et al., 1993), but is it well-founded and philosophically defensible? Many within IRS would say that does not matter, in that the only defence lexical or other machine codings need in any information processing system is that the system containing them works to an acceptable degree; but I would agree with those who say it is defensible, or is at least as well founded as the philosophical foundation on which FL stand. That is, I believe, one of the shakiest and most satirised of this century, and loosely related to what what Ryle (1957) called the Fido-Fido fallacy: the claim that to every word corresponds a concept andor a denotation, a view that has crept into everyday philosophical chat as the joke that the meaning of life is life (life prime, the object denoted by life2. It is a foundation of the utmost triviality, that comes from FL (op.cit., p.1) in the form: (1) The meaning of dog is DOG. 2Fido or Fido-prime are common notations for denotations corresponding to words. FL seem to prefer small caps FIDO, and I will use that form from their paper in the argument that follows. 2 They seem to embrace it wholeheartedly, and prefer it to any theory, like TGL, oﬀering complex structured dictionary entries, or even any paper dictionary oﬀ a shelf, like Websters, that oﬀers even more complex structures than TGL in a form of English. FL embrace an empty lexicon, willingly and with open eyes: one that lists just DOG as the entry for dog. The questions we must ask, though the answer is obviously no in each case, are:  is (1) even a correct form of what FL wants to say?  could such a dictionary consisting of statements like (1) serve any purpose whatever, for humans or for machines?  would one even need to write such a dictionary, supposing one believed in a role for such a compilation, as opposed to say, saving space by storing one as a simple rule for capitalizing any word whose meaning was wanted? The ﬁrst of these points brings back an age of linguistic analysis contemporary with Ryles, in particular the work of writers like Lewy (1964); put brieﬂy, the issue is whether or not (1) expresses anything determinate (and remember it is the mantra of the whole FL paper), or preferable to alternatives such as: (2) The meaning of dog is a domestic canine animal. or (3) The meaning of dog is a dog. or even (4) The meaning of dog is domestic canine animal. not to mention (5) The meaning of dog is dog. The two sentences (2) and (3) are perfectly sensible, depending on the circum- stances: (2) is roughly what real, non-Fodorian, dictionaries tell you, which seems unnecessary for dogs, but would be more plausible if the sentence was about marmosets or wombats. (3) is unhelpful, as it stands, but perhaps that is accidental, for if we translate it into German we get something like : (3a) Die Bedeutung von dog ist ein Hund. which could be very helpful to a German with little or no knowledge of English, as would be 3 (2a) Die Bedeutung von dog ist ein huendliche Haustier. To continue with this line of argument one needs all parties to accept the reality of translation and its role in argument: that there are translations, at least between close languages for simple sentences, and no amount of philosophical argument can shift that fact. For anyone who cannot accept this, there is probably no point in arguing about the role of lexicons at all. Both (2) and (3), then, are sensible and, in the right circumstances, infor- mative: they can be synonymous in some functional sense since both, when translated, could be equally informative to a normal ﬂuent speaker of another language. But (4) and (5) are a little harder: their translations would be un- informative to a German when translated, since translation does not translate quotations and so we get forms like: (5a) Die Bedeutung von dog ist dog. and similarly for a German (4a) version of the English (4). These sentences therefore cannot be synonymous with (3) and (2) respectively. But (4) might be thought no more than an odd form of a lexical entry sentence like (3), spoken by an English speaker. But what of (1); who could that inform about anything? Suppose we sharpen the issue by again asking who its translation could inform and about what: (1a) Die Bedeutung von dog ist DOG. (1a) tells the German speaker nothing, at which point we may be told that DOG stands for a denotation and its name is arbitrary. But that is just old nonsense on horseback: it implies that the English speaker cannot understand (1) either, since DOG might properly be replaced by G00971 if the ﬁnal symbol in (1) is truly arbitrary. It is surely (3), not (1), that tells us what the denotation of dog is, in the way language is normally used to do such things. DOG in (1) is simply a conﬁdence trick: it is put to us as having the role of the last term in (3). When and only when it is in the same language as the ﬁnal symbol of (3) (a fact we are conﬁdently assured is arbitrary) it does appear to point to dogs. However, taken as the last term in the synonymous (1a) it cannot possibly be doing that for it is incomprehensible, and functioning as an (untranslated) English word, exactly as in the last term of (5). But, as we saw, (5) and (3) cannot be synonymous, and so DOG in (1) has two incompatible roles at once, which is the trick that gives (1) interpretations that ﬂip uncontrollably between the (non-synonymous) (3) and (5). It is an optical illusion of a sentence. In conclusion, then, (1) is a dangerous sentence, one whose upper case- inﬂation suggests it has a function but which, on careful examination, proves not to be there: it is either (case-deﬂated) a form of the commonsense (3), in 4 which case it loses its capitals and all the role FL assign to it, since it is vacuous in English, or just a simple bilingual dictionary entry in German or some other language. Or it is a form or (5), uninformative in any language or lexicon but plainly a triviality, shorn of any philosophical import. Those who still have worries about this issue, and wonder if capitalizing may not still have some merit, should ask themselves the following question: which dog is the real DOG? The word dog has 24 entries even in a basic English dictionary like Collins, so how do FL know which one is intended by (1)? If one is tempted to reply, well DOG will have to be subscripted then, as in DOG1, DOG2 etc, then I shall reply that we will then be into another theory of meaning, and not one of simple denotations. My own suspicion is that all this can only be understood in terms of Fodors Language of Thought (1975) and that DOG for FL is a simple primitive in that language, rather than a denotation in the world or logical space. However, we have no access whatever to such a language, though Kay among others has given arguments that, if anything like an LOT exists, it will have to be subscripted (Kay, 1989), in which case the role of (1) will have to be rethought from scratch. All the discussion above will still remain relevant to such a development, and the issue of translation into LOT will then be the key one. However, until we can do that, and in the presence of a LOT native speaker, we may leave that situation aside and await developments. The moral for the rest of the discussion, and the role of IRS and TGL, is sim- ple: some of the sentences numbered above are like real, useful, lexical entries: (3) is a paradigm of an entry in a bilingual lexicon, where explanations are not crucial, while (2) is very like a monolingual lexical entry, where explanations are the stuﬀ of giving meaning. 3 Issues concerning TGL The standard of the examples used by FL to a attack TGL is not at all chal- lenging; they claim that JPs: (6) He baked a cake. is in fact ambiguous between JPs create and warm up aspects of bake, where baking a cake yields the ﬁrst, but baking a potato the second. JP does not want to claim this is a sense ambiguity, but a systematic diﬀerence in interpretation given by inferences cued by features of the two objects, which could be labels such as ARTIFACT in the case of the cake but not the potato. But in fact, bake a cake is ambiguous. To be sure, you can make a cake by baking it; but also you can do to a (pre-existent) cake just what you can do to a (pre-existent) potato: viz. put it in the oven and (non creatively) bake it. (op.cit. p.7) From this, FL conclude, bake must be ambiguous, since cake is not. But 5 all this is absurd and untrue to the simplest facts. Of course, warming up a (pre-existent) cake is not baking it; whoever could think it was? That activity would be referred to as warming a cake up, or through, never as baking it. You can no more bake a cake again, with the other interpretation, than you can bake a potato again and turn it into an artifact. FL like syntactically correlated evidence in semantics, and they should have noticed that a baked potato is ﬁne but a baked cake is not, which correlates with just the diﬀerence JP requires (cf. baked ﬁshmeat). It gets worse: FL go on to argue that if ARTIFACTs are the distinguishing feature for JP then bake a trolley car should take the creative reading since it is an artifact, completely ignoring the fact that the whole JP analysis is based on the (natural) assumption that potatoes and cakes both share some attribute like FOOD (as trolley cars do not) which is the only way the discussion can get under way: being a FOOD is a necessary condition for this analysis of bake to get under way. FLs key argument against TGL is that it is not possible to have a rule, of the sort JP advocates, that expands the content or meaning of a word in virtue of (the meaning content of) a neighbouring word in a context, namely, a word in some functional relation to the ﬁrst. So, JP, like many in the IRS tradition, argues that in: (7) John wants a beer. the meaning of wants in that context, which need not be taken to be any new or special or even existing sense of the word, is to be glossed as wants to drink a beer. This is done by a process that varies in detail from IRS researcher to researcher, but comes down to some form of: (8) X wants Y  X wants to do with Y whatever is normally done with Y. An issue over which AI researchers have diﬀered is whether this knowledge of normal or default use is stored in a lexical entry or in some other computational knowledge form such as one that was sometimes called a script (Schank and Abelson, 1997) and thought of as indexed by words but was much more than a lexical entry. It is not clear that one needs to discriminate between structures, however complex, if they are indexed by a word or words. JP stores the inference captured in (8) within the lexical entry under a label TELIC that shows purpose. In earlier AI systems, such information about function might be stored as part of a lexical semantic formulas attached to a primitive GOAL (Charniak and Wilks, 6 19763), or (depending on its salience) within an associated knowledge structure4. Some made the explicit assumption that a system should be suﬃciently robust that it would not matter if such functional information was stored in more than one place in a system, perhaps even in diﬀerent formats. For FL all this is unthinkable, and they produce a tortuous argument roughly as follows:  Fido-FIDO may not be the only form for a lexicon, but an extension could only be one where an expansion of meaning for a term was indepen- dent of the control of all other terms, as it is plainly not in the case of JPs (8)).  Any such extension would be to an underlying logical form, one that should also be syntactically motivated. FL then produce a complex algorithm (op.cit. p.10) that expands want con- sistently with these assumptions, one which is hard to follow and comes down to no more than the universal applicability (i.e. if accepted it must be applied to all occurrences of want regardless of lexical context) of the rule: (9) X wants Y  X wants to have Y. This, of course, avoids, as it is intended to, any poking about in the lexical content of Y. But it is also an absurd rule, no matter what dubious chat is appended to it about the nature of logical form. Consider: (10a) I want an orange. (10b) I want a beer. (10c) I want a rest. (10d) I want love. 3 In preference semantics (Wilks) door was coded as a formula (that could be displayed as a binary tree) such as: ((THIS((PLANT STUFF)SOUR)) ((((((THRU PART)OBJE) (NOTUSE ANI))GOAL) ((MAN USE) (OBJE THING)))) where the subformula: ((((THRU PART)OBJE) (NOTUSE ANI))GOAL) was intended to convey that the function of a door is to prevent passage through an aperture by a human. 4Such larger knowledge structures were called pseudo-texts (Wilks) in preference semantics (to emphasize the continuity of language and world knowledge): one for car (written [car]) would contain a clause like [car consume gasoline] where each lexical item in the pseudo-text was an index to a semantic formula (in the sense of note 3) that explicated it. 7 (10a) and (10b) seem intuitively to ﬁt the IRS rule (8) and the FL rule (9). (10c) might conform to some appropriate IRS coding to produce (from (8)): X wants to experience a rest, but the apparently felicitous application of FLs (9), yielding X wants to have a rest, is purely fortuitous, since have a rest is a lexicalised form having nothing at all to do with possession, which is the only natural interpretation of (9). This just serves to show the absurdity of FLs content-free rule (9) since its application to (10c) cannot possibly be interpreted in the same way as it was when producing X wants to have a beer. Only the IRS rule (8) distinguishes appropriate from inappropriate applica- tions of rules to (10c). One could make the same case with (10d), where the FL rule (9) produces only ambiguous absurdity, and the applicability of the IRS rule (8) depends entirely on how the function of love has been coded, if at all. However, the purpose of this section has been not to defend an IRS view or rule (8) in particular, but to argue that there is no future at all in FLs grudging, context free, rule (9), in part because it is context free. JPs speciﬁc claim is not that the use of rule (8) produces a new sense, or we would have a new sense corresponding to many or most of the possible objects of wanting, a more promiscuous expansion of the lexicon (were it augmented by every rule application) than would be the case for bake a potatocake where JP resisted augmentation of the lexicon, though other researchers would probably accept it. Nor is this like the application of normal function to the transfor- mation of (11) My car drinks gasoline. in (Wilks, 1980) where it was suggested that drink should be replaced by the structure for consume (as in note 4 below) in a context containing bro- ken preferences (unlike (7)) and where augmentation of the lexicon would be appropriate (like (11)) and if the relaxation, as some would call it, became statistically signiﬁcant, as it has in the case of (11). It is not easy to pin down quite why FL ﬁnd the rule (8) so objectionable, since their rule (9), like (8), is not, as they seem to believe, distinguished by logical form considerations. The traditional (Quine, 1953) logical opacity of want is such that inferences like (8) and (9) can never be supported by strong claims about logical form whose transformations must be deductive, and one can always want X without wanting Y, no matter what the logical relations of X and Y. Hence, neither (8) nor (9) are deductive rules, and FL have no ground in context-dependence to swallow the one but not the other. Contrary to what FL seem to assume, an NLP algorithm to incorporate or graft part of the lexical entry for one word (e.g. beer) into another (e.g. want) is not practically diﬃcult. The only issue for NLP research is whether and when such inferences should be drawn: at ﬁrst encounter with such a col- location, or when needed in later inference, a distinction corresponding roughly to what is distinguished by the oppositions forward and backward inference, or 8 data-driven and demand-driven inference. This issue is connected with whether a lexical entry should be adapted rather than a database of world knowledge and, again contrary to FLs assumptions, no NLP researcher can accept a ﬁrm dis- tinction between these, nor is there one, any more than a ﬁrm analytic-synthetic distinction has survived decades of scepticism. One can always force such a distinction into ones system at trivial cost, as Carnap (1947) did with his formal and material modes of sentences containing the same information: (12f) Edelweiss has the property ﬂower. (12m) An Edelweiss is a ﬂower. but the distinction is wholly arbitrary5. JPs treatment of more structural intensional verbs like believe is far more ingenious than FL would have us believe, and an interesting advance on previ- ous work: it is based on a richer notion of default than earlier IRS treatments. JPs position, as I understand it, is that the default, or underlying, structure associated with believe is: X believes p where p is expanded by default by the rule: (13) X believes p  X believes (Y communicates p). FL of course object again to another expansion beyond their self-imposed limit of context-freeness for which, as we saw, there is no principled defence, while failing to notice that (13) is in fact context-free in their sense. For me, the originality of (13) is not only that it can expand forms like: (14) John believes Mary. but can also be a general (context-free) default, overriding forms like: (15) John believes pigs eat carrots. 5Provided one remembers always that forms like: Edelweiss has nine letters is in material mode even though it could look like the formal mode. The formal mode of what it expresses is: Edelweiss has the property nine letters. 9 in favour of the more complex: (16) John believes (Y communicates (pigs eat carrots)). which is an original expansion according to which all beliefs can be seen as the result of some communication, often from oneself (when Y  John in (16)). There certainly were default expansions of believe in IRS before JP but not of this boldness6. 4 Some general IRS principles Once (Wilks, 1971, 1972) I tried to lay out principles for something very like IRS, and which still seem to underlie the position arrived at in this discussion; it would be helpful for FL to see IRS not simply as some form of undisciplined, opportunistic, discipline neighbouring their own professional interests. Let me restate two of these principles that bear on this discussion:  Meaning, in the end, is best thought of as other words, and that is the only position that can underpin a lexicon- and procedure-based approach to meaning, and one should accept this, whatever its drawbacks, because the alternative is untenable and not decisively supported by claims about ostension. Quine (1953) has argued a much more sophisticated version of this for many years, one in which representational structures are only compared against the world as wholes, and local comparisons are wholly symbolic. Meanings depend crucially upon explanations and these, for- mally or discursively, are what dictionaries oﬀer. This solution to the problem is indeed circular, but not viciously so, since dictionaries rarely oﬀer small dictionary circles (Sparck Jones, 1966) like the classic, and un- satisfying, case where furze is deﬁned as gorse and vice versa. Mean- ings, in terms of other words, is thus a function of circle size: furze gorse is pathological, in the sense of unhelpful, yet, since a dictionary deﬁnition set is closed, and must be circular, not all such circles can be unhelpful or dictionaries are all and always vacuous. On the other hand, FLs original position of the section 2 above, is not really renounced by the end of their paper, and is utterly untenable, not only for the analytic reasons we have given, but because it could not form the basis of any possible dictionary, for humans (seeking meaning explanations) or for NLP. 6In preference semantics (Wilks, 1972) believe had a lexical entry showing a preference for a propositional object, so that John believes Mary was accepted as a preference-breaking form but with a stored expansion of the object in the lexical entry for believe of a simple propositional form (Mary DO Y) with what is really an empty verb variable DO, and not a communication speciﬁcally act like TGL. 10 Indeed, as we pointed out earlier, no lexicon is needed for the dog-DOG theory, since a simple macro to produce upper-case forms will do, give or take a little morphological tweaking for the boil-BOILING cases.  Semantic well-formedness is not a property that can decidably be assigned to utterances, in the way that truth can to formulas in parts of the predi- cate calculus, and as it was hoped for many years that syntactically-well formed would be assignable to sentences. This point was made in some detail in (Wilks, 1971) on the basis that no underlying intuition is available to support semantic well-formedness7, since our intuitions are dependent on the state of our (or our machines) lexicons when considering an utterances status, and that we are capa- ble of expanding our lexicons (in something like the ways discussed in this paper) so as to bring utterances iteratively within the boundary of semantic well-formedness, and in a way that has no analogy in truth or syntax. Thus, no boundary drawing, of the sort required for a decidable property, can be done for the predicate Semantically-well-formed. Belief in the opposite seems one of the very few places where JP and FL agree, so further discussion may prove necessary. References [1] Carnap, R. (1947). Meaning and Necessity, Chicago:University of Chicago Press. [2] Charniak, E. and Wilks, Y. (eds.) (1976). Computational Semantics, North Holland: Amsterdam. [3] Fodor, J. and Lepore E. (1996). The emptiness of the Lexicon: critical re- ﬂections on J. Pustejovskys The Generative Lexicon. Unpublished MS. (this volume). [4] Fodor, J. (1975). The Language of Thought, New York: Crowell. [5] Kay, M. (1989). The concrete lexicon and the abstract dictionary. Proc. Fifth Annual Conference of the University of Waterloo Centre for the New Oxford English Dictionary, Oxford. [6] Lewy, C. (1964). Entailment and Propositional Identity. Proc. Arist. Soc., Vol. LXIV. [7] Pustejovsky, J. (1995). The Generative Lexicon, Cambridge, MA: MIT Press. 7This property must intuitively underlie all decidability claims and procedures: Goedels proof that there are true but undecidable sentences in a class of calculi only makes sense on the assumption that we have some (intuitive) way of seeing those sentences are true (outside of proof) 11 [8] Quine, W.V.O. (1953). Meaning and Translation, In From a Logical Point of View, Cambridge, MA: MIT Press. [9] Ryle, G. (1957). The Theory of Meaning. In C.A. Mace (ed.), British Phi- losophy in Mid-Century, London: Allen and Unwin. [10] Schank, R. and Abelson R.P. (1977). Scripts, Plans, Goals and Understand- ing, Hillsdale NJ: Lawrence Erlbaum. [11] Sparck Jones, K. (1966). Dictionary Circles, Technical Memorandum SP- 3304, System Development Corporation, Santa Monica, California. [12] Wittgenstein, L. (1953). Philosophical Investigations, Oxford: Blackwell. [13] Wilks, Y. (1980) Frames, Semantics and Novelty. In D. Metzing (ed.), Frame Conceptions and Text Understanding, Berlin: de Gruyter. [14] Wilks, Y. (1971). Decidability and Meaning. Mind, vol. LXXX. [15] Wilks, Y. (1972). Grammar Meaning and the Machine Analysis of Lan- guage, London and Boston: Routledge. [16] Wilks, Y., Pustejovsky, J., and Cowie, J. (1993). DIDEROT: automatic data extraction from text using semantic analysis. Proc. ARPA Conference of Human Language Technology, Princeton, NJ. and Menlo Park, CA: Morgan Kaufmann. 12",
  "26.pdf": "arXiv:cs9902030v1 [cs.CL] 25 Feb 1999 Is Word Sense Disambiguation just one more NLP task? Yorick Wilks Abstract The paper compares the tasks of part-of-speech (POS) tagging and word-sense-tagging or disambiguation (WSD), and argues that the tasks are not related by ﬁneness of grain or anything like that, but are quite diﬀerent kinds of task, particularly because there is nothing in POS cor- responding to sense novelty. The paper also argues for the reintegration of sub-tasks that are being separated for evaluation. 1 Introduction I want to make clear right away that I am not writing as a sceptic about word- sense disambiguation (WSD) let alone as a recent convert: on the contrary, since my PhD thesis was on the topic thirty years ago. That (Wilks, 1968) was what we would now call a classic AI toy system approach, one that used techniques later called Preference Semantics, but applied to real newspaper texts, as controls on the philosophical texts that were my real interest at the time. But it did attach single sense representations to words drawn from a polysemous lexicon of 800 or so. If Boguraev was right, in his informal survey twelve years ago, that the average NLP lexicon was under ﬁfty words, then that work was ahead of its time and I do therefore have a longer commitment to, and perspective on, the topic than most, for whatever that may be worth!. I want to raise some general questions about WSD as a task, aside from all the busy work in SENSEVAL: questions that should make us worried and wary about what we are doing here, but deﬁnitely NOT stop doing it. I can start by reminding us all of the obvious ways in which WSD is not like part- of-speech (POS) tagging, even though the two tasks are plainly connected in information terms, as Stevenson and I pointed out in (Wilks and Stevenson, 1998a), and were widely misunderstood for doing so. From these diﬀerences, of POS and WSD, I will conclude that WSD is not just one more partial task to be hacked oﬀ the body of NLP and solved. What follows acknowledges that Resnik and Yarowsky made a similar comparison in 1997 (Resnik and Yarowsky, 1997) though this list is a little diﬀerent from theirs: 1 1. There is broad agreement about POS tags in that, even for those commit- ted to diﬀering sets, there is little or no dispute that they can be put into one-many correspondence. That is not generally accepted for the sets of senses for the same words from diﬀerent lexicons. 2. There is little dispute that humans can POS tag to a high degree of consis- tency, but again this is not universally agreed for WS tagging, as various email discussions leading up to this workshop have shown. Ill come back to this issue below, but its importance cannot be exaggerated  if humans cannot do it then we are wasting our time trying to automate it. I assume that fact is clear to everyone: whatever maybe the case in robotics or fast arithmetic, in the NL parts of AI there is no point modelling or training for skills that humans do not have! 3. I do not know the genesis of the phrase lexical tuning, but the phe- nomenon has been remarked, and worked on, for thirty years and everyone seems agreed that it happens, in the sense that human generators create, and human analysers understand, words in quite new senses, ungenerated before or, at least, not contained in the point-of-reference lexicon, whether that be thought of as in the head or in the computer. Only this view is consistent with the evident expansion of sense lists in dictionaries with time; these new additions cannot plausibly be taken as established usages not noticed before. If this is the case, it seems to mark an absolute diﬀerence from POS tagging (where novelty does not occur in the same way), and that should radically alter our view of what we are doing here, because we cannot apply the standard empirical modelling method to that kind of novelty. The now standard empirical paradigm of [mark-up, modeltrain, and test] assumes prior markup, in the sense of a positive answer to the question (2) above. But we cannot, by deﬁnition, mark up for new senses, those not in the list we were initially given, because the text analysed creates them, or they were left out of the source from which the mark up list came. If this phenomenon is real, and I assume it is, it sets a limit on phenomenon (2), the human ability to pre-tag with senses, and therefore sets an upper bound on the percentage results we can expect from WSD, a fact that marks WSD out quite clearly from POS tagging. The contrast here is in fact quite subtle as can be seen from the interesting intermediate case of semantic tagging: which is the task of attaching semantic, rather than POS, tags to words automatically, a task which can then be used to do more of the WSD task (as in Dini et al., 1998) than POS tagging can, since the ANIMAL or BIRD versus MACHINE tags can then separate the main senses of crane. In this case, as with POS, one need not assume novelty in the tag set, but must allow for novel assignments from it to corpus words e.g. when a word like dog or pig was ﬁrst used in a human sense. It is just this 2 sense of novelty that POS tagging does also have, of course, since a POS tag like VERB can be applied to what was once only a noun, as with ticket. This kind of novelty, in POS and semantic tagging, can be pre-marked up with a ﬁxed tag inventory, hence both these techniques diﬀer from genuine sense novelty which cannot be premarked. As I said earlier, the thrust of these remarks is not intended sceptically, either about WSD in particular, or about the empirical linguistic agenda of the last ten years more generally. I assume the latter has done a great deal of good to NLPCL: it has freed us from toy systems and fatuous example mongering, and shown that more could be done with superﬁcial knowledge-free methods than the whole AI knowledge-based-NLP tradition ever conceded: the tradition in which every example, every sentence, had in principle to be subjected to the deepest methods. Minsky and McCarthy always argued for that, but it seemed to some even then an implausible route for any least-eﬀort-driven theory of evolution to have taken. The caveman would have stood paralysed in the path of the dinosaur as he downloaded deeper analysis modules, trying to disprove he was only having a nightmare. However, with that said, it may be time for some corrective: time to ask not only how we can continue to slice oﬀ more fragments of partial NLP as tasks to model and evaluate, but also how to reintegrate them for real tasks that humans undoubtedly can evaluate reliably, like MT and IE, and which are therefore unlike some of the partial tasks we have grown used to (like syntactic parsing) but on which normal language users have no views at all, for they are expert-created tasks, of dubious signiﬁcance outside a wider framework. It is easy to forget this because it is easier to keep busy, always moving on. But there are few places left to go after WSD:empirical pragmatics has surely started but may turn out to be the ﬁnal leg of the journey. Given the successes of empirical NLP at such a wide range of tasks, it is not to soon to ask what it is all for, and to remember that, just because machine translation (MT) researchers complained long ago that WSD was one of their main problems, it does not follow that high level percentage success at WSD will advance MT. It may do so, and it is worth a try, but we should remem- ber that Martin Kay warned years ago that no set of individual solutions to computational semantics, syntax, morphology etc. would necessarily advance MT. However, unless we put more thought into reintegrating the new techniques developed in the last decade we shall never ﬁnd out. 2 Can humans sense tag? I wish now to return to two of the topics raised above: ﬁrst, the human task: itself. It seems obvious to me that, aside from the problems of tuning and other phenomena that go under names like vagueness, humans, after training, can 3 sense-tag texts at reasonably high levels and reasonable inter-annotator consis- tency. They can do this with alternative sets of senses for words for the same text, although it may be a task where some degree of training and prior literacy are essential, since some senses in such a list are usually not widely known to the public. This should not be shocking: teams of lexicographers in major pub- lishing houses constitute literate, trained teams and they can normally achieve agreement suﬃcient for a large printed dictionary for publication (about sense sets, that is, a closely related skill to sense-tagging). Those averse to claims about training and expertise here should remember that most native speak- ers cannot POS tag either, though there seems substantial and uncontentious consistency among the trained. There is strong evidence for this position on tagging ability, which includes (Green, 1989 see also Jorgensen, 1990) and indeed the high ﬁgures obtained for small word sets by the techniques pioneered by Yarowsky (Yarowsky, 1995). Many of those ﬁgures rest on forms of annotation (e.g. assignment of words to thesaurus head sets in Roget), and the general plausibility of the methodology serves to conﬁrm the reality of human annotation (as a consistent task) as a side eﬀect. The counterarguments to this have come explicitly from the writings of Kil- garriﬀ (1993), and sometimes implicitly from the work of those who argue from the primacy of lexical rules or of notions like vagueness in regard to WSD. In Kilgarriﬀs case I have argued elsewhere (Wilks, 1997) that the ﬁgures he produced on human annotation are actually consistent with very high levels of human ability to sense-tag and are not counter-arguments at all, even though he seems to remain sceptical about the task in his papers. He showed only that for most words there are some contexts for which humans cannot assign a sense, which is of course not an argument against the human skill being generally successful. On a personal note, I would hope very much to be clearer when I see his published reaction to the SENSEVAL workshop what his attitude to WSD really is. In writing he is a widely published sceptic, in the ﬂesh he is the prime organiser of this excellent event (SENSEVAL Workshop) to test a skill he may, or may not, believe in. There need be no contradiction there, but a fascinating question about motive lingers in the air. Has he set all this up so that WSD can destroy itself when rigourously tested? One does not have to be a student of double-blind tests, and the role of intention in experimental design, to take these questions seriously, particularly as he has designed the SENSEVAL methodology and the use of the data himself. The motive question here is not mere ad hominem argument but a serious question needing an answer. These are not idle questions, in my view, but go to the heart of what the SENSEVAL workshop is for: is it to show how to do better at WSD, or is to say something about wordsense itself (which might involve saying that you cannot do WSD by computer at all, or cannot do it well enough to be of interest?). In all this discussion we should remember that, if we take the improvement 4 of (assessable) real tasks as paramount, those like MT, Information Retrieval and Information Extraction (IE), then it may not in the end matter whether humans are ever shown psycholinguistically to need POS tagging or WSD for their own language performance;there is much evidence they do not. But that issue is wholly separate from what concerns us here; it may still be useful to advance MTIE via partial tasks like WSD, if they can be shown performable, assessable, and modelable by computers, no matter how humans turn out to work. The implicit critique of the broadly positive position above (i.e. that WSD can be done by people and machines and we should keep at it) sometimes seems to come as well from those who argue (a) for the inadequacy of lexical sense sets over productive lexical rules and (b) for the inherently vague quality of the diﬀerence between senses of a given word. I believe both these approaches are muddled if their proponents conclude that WSD is therefore fatally ﬂawed as a task;- and clearly not all do since some of them are represented here as participants. 3 Lexical Rules Lexical rules go back at least to Givons (1967) thirty-year old sense-extension rules and they are in no way incompatible with a sense-set approach, like that found in a classic dictionary. Such sense sets are normally structured (often by part of speech and by general and speciﬁc senses) and the rules are, in some sense, no more than a compression device for predicting that structuring. But the set produced by any set of lexical rules is still a set, just as a dictionary list of senses is a set, albeit structured. It is mere confusion to think one is a set and one not: Nirenburg and Raskin (1997) have pointed out that those who argue against lists of senses (in favour of rules, e.g. Pustejovsky 1995) still produce and use such lists. What else could they do? I myself cannot get suﬃcient clarity at all on what the lexical rule approach, whatever its faults or virtues, has to do with WSD? The email discussion preced- ing this workshop showed there were people who think the issues are connected, but I cannot see it, but would like to be better informed before I go home from here. If their case is that rules can predict or generate new senses then their po- sition is no diﬀerent (with regard to WSD) from that of anyone else who thinks new senses important, however modelled or described. The rulecompression issue itself has nothing essential to do with WSD: it is simply one variant of the noveltytuningnew-sensemetonymy problem, however that is described. The vagueness issue is again an old observation, one that, if taken seriously, must surely result in a statistical or fuzzy-logic approach to sense discrimina- tion, since only probabilistic (or at least quantitative) methods can capture real vagueness. That, surely, is the point of the Sorites paradox: there can be no plausible or rational qualitatively-based criterion (which would include any 5 quantitative system with clear limits: e.g. tall  over 6 feet) for demarcating tall, green or any inherently vague concept. If, however, sense setslistsinventories are to continue to play a role, vague- ness can mean no more than highlighting what all systems of WSD must have, namely some parameter or threshold for the assignment to one of a list of senses versus another, or setting up a new sense in the list. Talk of vagueness adds nothing speciﬁc to help that process for those who want to assign on some quan- titative basis to one sense rather than another; algorithms will capture the usual issue of tuning to see what works and ﬁts our intuitions. Vagueness would be a serious concept only if the whole sense list for a word (in rule form or not) was abandoned in favour of statistically-based unsuper- vised clusters of usages or contexts. There have been just such approaches to WSD in recent years (e.g. Bruce and Wiebe, 1994, Pedersen and Bruce, 1997, Schuetze  Pederson, 1995) and the essence of the idea goes back to Sparck Jones 19641986) but such an approach would ﬁnd it impossible to take part in any competition like SENSEVAL because it would inevitably deal in nameless entities which cannot be marked up for. Vague and Lexical Rule based approaches also have the consequence that all lexicographic practice is, in some sense, misguided: dictionaries according to such theories are fraudulent documents that could not help users, whom they systematically mislead by listing senses. Fortunately, the market decides this issue, and it is a false claim. Vagueness in WSD is either false (the last position) or trivial, and known and utilised within all methodologies. This issue owes something to the systematic ignorance of its own history so often noted in AI. A discussion email preceding this workshop referred to the purported beneﬁts of underspeciﬁcation in lexical entries, and how recent formalisms had made that possible. How could anyone write such a thing in ignorance of the 1970s and 80s work on incremental semantic interpretation of Hirst, Mellish and Small (Hirst, 1987; Mellish, 1983; Small et al., 1988) among others? None of this is a surprise to those with AI memories more than a few weeks long: in our ﬁeld people read little outside their own notational clique, and constantly rediscover old work with a new notation. This leads me to my ﬁnal point which has to do, as I noted above, with the need for a fresh look at technique integration for real tasks. We all pay lip service to this while we spend years on fragmentary activity, arguing that that is the method of science. Well, yes and no, and anyway WSD is not science: what we are doing is engineering and the scientiﬁc method does not generally work there, since engineering is essentially integrative, not analytical. We often write or read of hybrid systems in NLP, which is certainly an integrative notion, but we have little clear idea of what it means. If statistical or knowledge-free methods are to solve some or most cases of any linguistic phenomenon, like WSD, how do we then locate that subclass of the phenomena that other, deeper, techniques like AI and knowledge-based reasoning are then to deal with? Conversely, how 6 can we know which cases the deeper techniques cannot or need not deal with? If there is an upper bound to empirical methods, and I have argued that that will be lower for WSD than for some other NLP tasks for the reasons set out above, then how can we pull in other techniques smoothly and seamlessly for the hard examples? The experience of POS tagging, to return to where we started, suggests that rule-driven taggers can do as well as purely machine learning-based taggers, which, if true, suggests that symbolic methods, in a broad sense, might still be the right approach for the whole task. Are we yet sure this is not the case for WSD? I simply raise the question. Ten years ago, it was taken for granted in most of the AINLP community that knowledge-based methods were essential for serious NLP. Some of the successes of the empirical program (and especially the TIPSTER program) have caused many to reevaluate that assumption. But where are we now, if a real ceiling to such methods is already in sight? Informa- tion Retrieval languished for years, and maybe still does, as a technique with a practical use but an obvious ceiling, and no way of breaking through it; there was really nowhere for its researchers to go. But that is not quite true for us, because the claims of AINLP to oﬀer high quality at NLP tasks have never been really tested. They have certainly not failed, just got left behind in the rush towards what could be easily tested! 4 Large or Small-scale WSD? Which brings me to my ﬁnal point: general versus small-scale WSD. Our group is one of the few that has insisted on continuing with general WSD: the tagging and test of all content words in a text, a group that includes CUP, XERC- Grenoble and CRL-NMSU. We currently claim about 90 correct sense assign- ment (Wilks and Stevenson, 1998b) and do not expect to be able to improve much on that for the reasons set out above; we believe the rest is AI or lexical tuning! The general argument for continuing with the all-word paradigm, rather than the highly successful paradigm of Yarowsky et al., is that that is the real task, and there is no ﬁrm evidence that the small scale will scale up to the large because much of sense-disambiguation is mutual between the words of the text, which cannot be used by the small set approach. I am not sure this argument is watertight but it seems plausible to me. Logically, if you claim to do all the content words you ought, in principle, to be able to enter a contest like SENSEVAL that does only some of the words with an unmodiﬁed system. This is true, but you will also expect to do worse, as you have not have had as much training data for the chosen word set. Moreover you will have to do far more preparation to enter if you insist, as we would, on bringing the engines and data into play for all the training and test set words; the eﬀort is that much greater and it makes such an entry self-penalising in terms of both eﬀort and likely outcome, which is why we decided not to enter in the ﬁrst 7 round, regretfully, but just to mope and wail at the sidelines. The methodology chosen for SENSEVAL was a natural reaction to the lack of training and test data for the WSD task, as we all know, and that is where I would personally like to see eﬀort put in the future, so that everyone can enter all the words; I assume that would be universally agreed to if the data were there. It is a pity, surely, to base the whole structure of a competition on the paucity of the data. 5 Conclusion What we would like to suggest positively is that we cooperate to produce more data, and use existing all-word systems, like Grenoble, CUP, our own and others willing to join, possibly in combination, so as to create large-scale tagged data quasi-automatically, rather in the way that the Penn tree bank was produced with the aid of parsers, not just people. We have some concrete suggestions as to how this can be done, and done consistently, using not only multiple WSD systems but also by cross comparing the lexical resources available, e.g. WordNet (or EuroWordNet) and a major monolingual dictionary. We developed our own reasonably large testtraining set with the WordNet-LDOCE sense translation table (SENSUS, Knight and Luk, 1994) from ISI. Some sort of organised eﬀort along those lines, before the next SENSEVAL, would enable us all to play on a ﬁeld not only level, but much larger. References [1] Bruce, R. and Wiebe, J. (1994) Word-sense disambiguation using decom- posable models, Proc. ACL-94. [2] Dini, L., di Tommaso, V. and Segond, F. (1998) Error-driven word sense disambiguation. In Proc. COLING-ACL98, Montreal. [3] Givon, T. (1967) Transformations of Ellipsis, Sense Development and Rules of Lexical Derivation. SP-2896, Systems Development Corp., Sta. Monica, CA. [4] Green, G. (1989) Pragmatics and Natural Language Understanding. Erl- baum: Hillsdale, NJ. [5] Hirst, G. (1987) Semantic Interpretation and the Resolution of Ambiguity, CUP: Cambridge, England. [6] Jorgensen, J. (1990) The psychological reality of word senses, Journal of Psycholinguistic Research, vol 19. 8 [7] Kilgarriﬀ, A. (1993) Dictionary word-sense distinctions: an enquiry into their nature, Computers and the Humanities, vol 26. [8] Knight, K. and Luk, S. (1994) Building a Large Knowledge Base for Ma- chine Translation, Proceedings of the American Association for Artiﬁcial In- telligence Conference AAAI-94, pp. 185-109, Seattle, WA. [9] Mellish, C. (1983) Incremental semantic interpretation in a modular parsing system. In K. Sparck-Jones and Y. Wilks (eds.) Automatic Natural Language Parsing, Ellis HorwoodWiley: ChichesterNYC. [10] Nirenburg, S. and Raskin., V. (1997) Ten choices for lexical semantics. Research Memorandum, Computing Research Laboratory, Las Cruces, NM. [11] Pedersen, T. and Bruce, R. (1997) Distinguishing Word Senses in Untagged Text, Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pp. 197-207, Providence, RI. [12] Pustejovsky, J. (1995) The Generative Lexicon, MIT Press: Cambridge, MA. [13] Resnik, P. and Yarowsky, D. (1997) A Perspective on Word Sense Dis- ambiguation Techniques and their Evaluation, Proceedings of the SIGLEX Workshop Tagging Text with Lexical Semantics: What, why and how?, pp. 79-86, Washington, D.C. [14] Schutze, H. (1992) Dimensions of Meaning, Proceedings of Supercomputing 92, pp. 787-796, Minneapolis, MN. [15] Schutze, H. and Pederson, J. (1995) Information Retrieval based on Word Sense, Proc. Fourth Annual Symposium on Document Analysis and Informa- tion Retrieval. Las Vegas, NV. [16] Small, S., Cottrell, G., and Tanenhaus, M. (Eds.) (1988) Lexical Ambiguity Resolution, Morgan Kaufmann: San Mateo, CA. [17] Sparck Jones, K. (19641986) Synonymy and Semantic Classiﬁcation. Ed- inburgh UP: Edinburgh. [18] Wilks, Y. (1968) Argument and Proof. Cambridge University PhD thesis. [19] Wilks, Y. (1997) Senses and Texts. Computers and the Humanities. [20] Wilks, Y. and Stevenson, M. (1998a) The Grammar of Sense: Using part- of-speech tags as a ﬁrst step in semantic disambiguation, Journal of Natural Language Engineering, 4(1), pp. 1-9. 9 [21] Wilks, Y. and Stevenson, M. (1998b) Optimising Combinations of Knowl- edge Sources for Word Sense Disambiguation, Proceedings of the 36th Meet- ing of the Association for Computational Linguistics (COLING-ACL-98), Montreal, Canada. [22] Yarowsky, D. (1995) Unsupervised Word-Sense Disambiguation Rivaling Supervised Methods, Proceedings of the 33rd Annual Meeting of the Associ- ation for Computational Linguistics (ACL-95), pp. 189-196, Cambridge, MA. 10",
  "27.pdf": "arXiv:cs9903003v1 [cs.CL] 2 Mar 1999 A Formal Framework for Linguistic Annotation Steven Bird and Mark Liberman Linguistic Data Consortium, University of Pennsylvania 3615 Market St, Philadelphia, PA 19104-2608, USA Email: {sb,myl}ldc.upenn.edu Technical Report MS-CIS-99-01 Department of Computer and Information Science March 1999 Abstract Linguistic annotation covers any descriptive or analytic notations applied to raw lan- guage data. The basic data may be in the form of time functions  audio, video andor physi- ological recordings  or it may be textual. The added notations may include transcriptions of all sorts (from phonetic features to discourse structures), part-of-speech and sense tagging, syntactic analysis, named entity identiﬁcation, co-reference annotation, and so on. While there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases, the lack of widely accepted standards is becoming a critical problem. Proposed standards, to the extent they exist, have focussed on ﬁle formats. This paper focuses instead on the logical structure of linguistic annotations. We survey a wide variety of existing annotation formats and demonstrate a common conceptual core, the annotation graph. This provides a formal framework for constructing, maintaining and searching linguistic annotations, while remaining consistent with many alternative data structures and ﬁle formats. Copyright c 1999 Steven Bird  Mark Liberman MS-CIS-99-01: Bird  Liberman 1 1 Introduction In the simplest and commonest case, linguistic annotation is an orthographic transcription of speech, time-aligned to an audio or video recording. Other central examples include morphological analysis, part-of-speech tagging and syntactic bracketing; phonetic segmentation and labeling; annotation of disﬂuencies, prosodic phrasing, intonation, gesture, and discourse structure; marking of co-reference, named entity tagging, and sense tagging; and phrase-level or word-level translations. Linguistic annotations may describe texts or recorded signals. Our focus will be on the latter, broadly construed to include any kind of audio, video or physiological recording, or any combination of these, for which we will use the cover term linguistic signals. However, our ideas also apply to the annotation of texts. Linguistic annotations have seen increasingly broad use in the scientiﬁc study of language, in research and development of language-related technologies, and in language-related applications more broadly, for instance in the entertainment industry. Particular cases range from speech databases used in speech recognition or speech synthesis development, to annotated ethnographic materials, to cartoon sound tracks. There have been many independent efforts to provide tools for creating linguistic annotations, to provide general formats for expressing them, and to provide tools for creating, browsing and searching databases containing them  see [www.ldc.upenn.eduannotation]. Within the area of speech and language technology development alone, hundreds of annotated linguistic databases have been published in the past ﬁfteen years. While the utility of existing tools, formats and databases is unquestionable, their sheer variety  and the lack of standards able to mediate among them  is becoming a critical problem. Particular bodies of data are created with particular needs in mind, using formats and tools tailored to those needs, based on the resources and practices of the community involved. Once created, a linguistic database may subsequently be used for a variety of unforeseen purposes, both inside and outside the community that created it. Adapting existing software for creation, update, indexing, search and display of foreign databases typ- ically requires extensive re-engineering. Working across a set of databases requires repeated adaptations of this kind. Previous attempts to standardize practice in this area have primarily focussed on ﬁle formats and on the tags, attributes and values for describing content (e.g. [24], [28]; but see also [31]). We contend that ﬁle formats and content speciﬁcations, though important, are secondary. Instead, we focus on the logical structure of linguistic annotations. We demonstrate that, while different existing annotations vary greatly in their form, their logical structure is remarkably consistent. In order to help us think about the form and meaning of annotations, we describe a simple mathematical framework endowed with a practically useful formal structure. This opens up an interesting range of new possibilities for creation, maintenance and search. We claim that essentially all existing annotations can be expressed in this framework. Thus, the framework should provide a useful interlingua for translation among the multiplicity of current annotation formats, and also should permit the development of new tools with broad applicability. Before we embark on our survey, a terminological aside is necessary. As far as we are aware, there is no existing cover term for the kinds of transcription, description and analysis that we address here. Transcription may refer to the use of ordinary orthography, or a phonetic orthography; it can plausibly be extended to certain aspects of prosody (intonational transcription), but not to other kinds of analysis (morphological, syntactic, rhetorical or discourse structural, semantic, etc). One does not talk about a syntactic transcription, although this is at least as determinate a representation of the speech stream as is a phonetic transcription. Coding has been used by social scientists to mean something like the 2 A Formal Framework for Linguistic Annotation assignment of events to stipulated symbolic categories, as a generalization of the ordinary language meaning associated with translating words and phrases into references to a shared, secret code book. It would be idiosyncratic and confusing (though conceptually plausible) to refer to ordinary orthographic transcription in this way. The term markup has come to have a speciﬁc technical meaning, involving the addition of typographical or structural information to a document. In ordinary language, annotation means a sort of commentary or explanation (typically indexed to particular portions of a text), or the act of producing such a commentary. Like markup, this terms ordinary meaning plausibly covers the non-transcriptional kinds of linguistic analysis, such as the annotation of syntactic structure or of co-reference. Some speech and language engineers have begun to use annotation in this way, but there is not yet a speciﬁc, widely-accepted technical meaning. We feel that it is reasonable to generalize this term to cover the case of transcribing speech, by thinking of annotation as the provision of any symbolic description of particular portions of a pre-existing linguistic object. If the object is a speech recording, then an ordinary orthographic transcription is certainly a kind of annotation in this sense  though it is one in which the amount of critical judgment is small. In sum, annotation is a reasonable candidate for adoption as the needed cover term. The alternative would be to create a neologism (scription?). Extension of the existing term annotation seems prefer- able to us. 2 Existing Annotation Systems In order to justify our claim that essentially all existing linguistic annotations can be expressed in the framework that we propose, we need to discuss a representative set of such annotations. In addition, it will be easiest to understand our proposal if we motivate it, piece by piece, in terms of the logical structures underlying existing annotation practice. This section reviews nine bodies of annotation practice, with a concrete example of each. For each example, we show how to express its various structuring conventions in terms of our annotation graphs, which are networks consisting of nodes and arcs, decorated with time marks and labels. Following the review, we shall discuss some general architectural issues (3), give a formal presentation of the annotation graph concept (4), and describe some indexing methods (5). The paper concludes in 6 with an evaluation of the proposed formalism and a discussion of future work. The nine annotation models to be discussed in detail are TIMIT [15], Partitur [31], CHILDES [24], the LACITO Archiving Project [26], LDC Broadcast News, LDC Telephone Speech, NIST UTF [28], Emu [11] and Festival [34]. These models are widely divergent in type and purpose. Some, like TIMIT, are associated with a speciﬁc database, others, like UTF, are associated with a speciﬁc linguistic domain (here conversation), while still others, like Festival, are associated with a speciﬁc application domain (here, speech synthesis). Several other systems and formats have been considered in developing our ideas, but will not be dis- cussed in detail. These include Switchboard [17], HCRC MapTask [2], TEI [36], and MATE [13]. The Switchboard and MapTask formats are conversational transcription systems that encode a subset of the information in the LDC and NIST formats cited above. The TEI guidelines for Transcriptions of Speech [36, p11] are also similar in content, though they offer access to a very broad range of representational techniques drawn from other aspects of the TEI speciﬁcation. The TEI report sketches or alludes to a correspondingly wide range of possible issues in speech annotation. All of these seem MS-CIS-99-01: Bird  Liberman 3 to be encompassed within our proposed framework, but it does not seem appropriate to speculate at much greater length about this, given that this portion of the TEI guidelines does not seem to have been used in any published transcriptions to date. As for MATE, it is a new SGML- and TEI-based standard for dialogue annotation, in the process of being developed. It also appears to fall within the class of annotation systems that our framework covers, but it would be premature to discuss the correspondences in detail. Still other models that we are aware of include [1, 21, 30]. Note that there are many kinds of linguistic database that are not linguistic annotations in our sense, although they may be connected with linguistic annotations in various ways. One example is a lexical database with pointers to speech recordings along with transcriptions of those recordings (e.g. HyperLex [5]). Another example would be collections of information that are not speciﬁc to any particular stretch of speech, such as demographic information about speakers. We return to such cases in 6.2. 2.1 TIMIT The TIMIT corpus of read speech was designed to provide data for the acquisition of acoustic-phonetic knowledge and to support the development and evaluation of automatic speech recognition systems. TIMIT was the ﬁrst annotated speech database to be published, and it has been widely used and also republished in several different forms. It is also especially simple and clear in structure. Here, we just give one example taken from the TIMIT database [15]. The ﬁle traindr1fjsp0sa1.wrd contains: 2360 5200 she 5200 9680 had 9680 11077 your 11077 16626 dark 16626 22179 suit 22179 24400 in 24400 30161 greasy 30161 36150 wash 36720 41839 water 41839 44680 all 44680 49066 year This ﬁle combines an ordinary string of orthographic words with information about the starting and ending time of each word, measured in audio samples at a sampling rate of 16 kHz. The path name traindr1fjsp0sa1.wrd tells us that this is training data, from dialect region 1, from female speaker jsp0, containing words and audio sample numbers. The ﬁle traindr1fjsp0sa1.phn contains a corresponding broad phonetic transcription, which begins as follows: 0 2360 h 2360 3720 sh 3720 5200 iy 5200 6160 hv 6160 8720 ae 8720 9680 dcl 9680 10173 y 10173 11077 axr 11077 12019 dcl 12019 12257 d We can interpret each line: time1 time2 label as an edge in a directed acyclic graph, where the two times are attributes of nodes and the label is a property of an edge connecting those nodes. 4 A Formal Framework for Linguistic Annotation 0 0 1 2360 Ph 2 3270 Psh 3 5200 Wshe Piy 4 6160 Phv 6 9680 Whad 5 8720 Pae Pdcl 7 10173 Py 8 11077 Wyour Paxr Figure 1: Graph Structure for TIMIT Example 0 4160 1 5280 Mj 2 7520 Oja 10 17120 D(THANK_INIT BA) Ma: 3 9920 MS 6 12480 Osch\"onen 4 11520 M2: 5 12000 Mn Mn 7 12960 Mnib 8 13440 Md ODank 9 15840 Ma MN Figure 2: Graph Structure for Partitur Example MS-CIS-99-01: Bird  Liberman 5 The resulting annotation graph for the above fragment is shown in Figure 1. Observe that edge labels have the form typecontent where the type here tells us what kind of label it is. We have used P for the (phonetic transcription) contents of the .phn ﬁle, and W for the (orthographic word) contents of the .wrd ﬁle. The top number for each node is an arbitrary node identiﬁer, while the bottom number is the time reference. We distinguish node identiﬁers from time references since nodes may lack time references, as we shall see later. 2.2 Partitur The Partitur format of the Bavarian Archive for Speech Signals [31] is founded on the collective experience of a broad range of German speech database efforts. The aim has been to create an open (that is extensible), robust format to represent results from many different research labs in a common source. Partitur is valuable because it represents a careful attempt to present a common low-level core for all of those independent efforts, similar in spirit to our effort here. In essence, Partitur extends and reconceptualizes the TIMIT format to encompass a wide range of annotation types. The Partitur format permits time-aligned, multi-tier description of speech signals, along with links between units on different tiers which are independent of the temporal structure. For ease of presentation, the example Partitur ﬁle will be broken into a number of chunks, and certain details (such as the header) will be ignored. The fragment under discussion is from one of the Verbmobil corpora at the Bavarian Archive of Speech Signals. The KAN tier provides the canonical transcription, and introduces a numerical identiﬁer for each word to serve as an anchor for all other material. KAN: 0 ja: KAN: 1 S2:nn KAN: 2 daNk KAN: 3 das KAN: 4 vE:r KAN: 5 ze:6 KAN: 6 nEt Tiers for orthographic and transliteration information then reference these anchors as shown below, with orthographic information (ORT) on the left and transliteration information (TRL) on the right. ORT: 0 ja TRL: 0 A ORT: 1 sch\"onen TRL: 0 ja , ORT: 2 Dank TRL: 1 sch\"onen ORT: 3 das TRL: 1 :Klopfen ORT: 4 w\"are TRL: 2 Dank: , ORT: 5 sehr TRL: 3 das ORT: 6 nett TRL: 4 w\"ar TRL: 5 sehr TRL: 6 nett . Higher level structure representing dialogue acts refers to extended intervals using contiguous sequences of anchors, as shown below: DAS: 0,1,2 (THANK_INIT BA) DAS: 3,4,5,6 (FEEDBACK_ACKNOWLEDGEMENT BA) Speech data can be referenced using annotation lines containing offset and duration information. As before, links to the KAN anchors are also speciﬁed (as the second-last ﬁeld). 6 A Formal Framework for Linguistic Annotation MAU: 4160 1119 0 j MAU: 17760 1119 3 a MAU: 5280 2239 0 a: MAU: 18880 1279 3 s MAU: 7520 2399 1 S MAU: 20160 959 4 v MAU: 9920 1599 1 2: MAU: 21120 639 4 E: MAU: 11520 479 1 n MAU: 21760 1119 4 6 MAU: 12000 479 1 n MAU: 22880 1119 5 z MAU: 12480 479 -1 nib MAU: 24000 799 5 e: MAU: 12960 479 2 d MAU: 24800 1119 5 6 MAU: 13440 2399 2 a MAU: 25920 1279 6 n MAU: 15840 1279 2 N MAU: 27200 1919 6 E MAU: 17120 639 3 d MAU: 29120 2879 6 t MAU: 32000 2559 -1 p: The content of the ﬁrst few words of the ORT (orthography), DAS (dialog act) and MAU (phonetic segment) tiers can apparently be expressed as in Figure 2. Note that we abbreviate the types, using O for ORT, D for DAS, and M for MAU. 2.3 CHILDES With its extensive user base, tools and documentation, and its coverage of some two dozen languages, the Child Language Data Exchange System, or CHILDES, represents the largest scientiﬁc  as opposed to engineering  enterprise involved in our survey. The CHILDES database includes a vast amount of transcript data collected from children and adults who are learning languages [24]. All of the data are transcribed in the so-called CHAT format; a typical instance is provided by this opening fragment of a CHAT transcription: Begin Filename: boys73.cha Participants: ROS Ross Child, MAR Mark Child, FAT Brian Father, MOT Mary Mother Date: 4-APR-1984 Age of ROS: 6;3.11 Sex of ROS: Male Birth of ROS: 25-DEC-1977 Age of MAR: 4;4.15 Birth of MAR: 19-NOV-1979 Sex of MAR: male Situation: Room cleaning ROS: yahoo. snd: \"boys73a.aiff\" 7349 8338 FAT: you got a lot more to do  dont you? snd: \"boys73a.aiff\" 8607 9999 MAR: yeah. snd: \"boys73a.aiff\" 10482 10839 MAR: because Im not ready to go to the bathroom [] . snd: \"boys73a.aiff\" 11621 13784 The snd lines, by the conventions of this notation, provide times for the previous transcription lines, in milliseconds relative to the beginning of the referenced ﬁle. The ﬁrst two lines of this transcript might then be represented graphically as in Figure 3. Observe that the gap between the conversational turns results in a disconnected graph. Note also that the snd annotations in the original chat ﬁle included a ﬁle name; see 3.6 for a discussion of associations between annotations and ﬁles. The representation in Figure 3 is inadequate, for it treats entire phrases as atomic arc labels, complicating indexing and search. We favor the representation in Figure 4, where labels have uniform ontological MS-CIS-99-01: Bird  Liberman 7 0 7349 1 8338 Wyahoo. SRoss 2 8607 3 9999 Wyou got a lot more to do  dont you? SFather Figure 3: Graph Structure for CHILDES Example (Version 1) 0 7349 1 W yahoo 2 8338 SRoss W. 3 8607 4 W you 14 9999 SFather 5 W got 6 W a 7 W lot 8 W more 9 W to 10 W do 11 W  12 W dont 13 W you W ? Figure 4: Graph Structure for CHILDES Example (Version 2) 8 A Formal Framework for Linguistic Annotation status regardless of the presence vs. absence of time references. Observe that most of the nodes in Figure 4 could have been given time references in the CHAT format but were not. Our approach maintains the same topology regardless of the sparseness of temporal information. Notice that some of the tokens of the transcript, i.e. the punctuation marks, are conceptually not refer- ences to discrete stretches of time in the same way that orthographic words are. (The distinction could be reﬂected by choosing a different type for punctuation labels.) Evidently it is not always meaningful to assign time references to the nodes of an annotation. We shall see a more pervasive example of this atemporality in the next section. 2.4 LACITO Linguistic Data Archiving Project LACITO  Langues et Civilisations a Tradition Orale  is a CNRS organization concerned with research on unwritten languages. The LACITO Linguistic Data Archiving Project was founded to conserve and distribute the large quantity of recorded, transcribed speech data collected by LACITO members over the last three decades [26]. In this section we discuss a transcription for an utterance in Hayu, a Tibeto- Burman language of Nepal. The gloss and free translation are in French. ?XML version\"1.0\" encoding\"ISO-8859-1\" ? !DOCTYPE ARCHIVE SYSTEM \"Archive.dtd\" ARCHIVE HEADER TITLEDeux soeursTITLE SOUNDFILE href\"hayu.wav\" HEADER TEXT lang\"hayu\" S id\"s1\" TRANSCR WnakpuW WnonotsoW Wsix014b;W WpaW Wlax0294;natshemW Ware.W TRANSCR AUDIO type\"wav\" start\"0.0000\" end\"5.5467\" TRADUCOn raconte que deux soeurs allegrave;rent un jour chercher du bois.TRADUC MOTAMOT WdeuxW WsoeursW WboisW WfaireW Wallegrave;rent(D)W Wdit.on.W MOTAMOT S TEXT ARCHIVE A possible graphical representation of the annotation of the sentence, expressed as a labeled directed acyclic graph of the type under discussion, is shown in Figure 5. Here we have three types of edge labels: W for the words of the Hayu story; M for a word-by-word interlinear translation into French; and T for a phrasal translation into French. MS-CIS-99-01: Bird  Liberman 9 0 0.0 1 W nakpu M deux 9 T on 2 W nonotso M soeurs 3 W siG M bois 4 W pa M faire 5 W la7natshem 7 M allŁrent 6 5.5467 W are 8 M dit M (D) M on 10 T raconte 11 T qu 12 T deux 13 T soeurs 14 T allŁrent 15 T un 16 T jour 17 T chercher 18 T du T bois Figure 5: Graph Structure for the LACITO Archiving Project Example 0 0.000 1 4.233 Mhigh 2 Wit 10 Mlow 3 Wwill 4 5 Whave 6 8.015 Wbeen 7 Wso 8 Wthere 9 13.981 Figure 6: Graph Structure for LDC Broadcast Transcript Example 10 A Formal Framework for Linguistic Annotation (We have taken a small liberty with the word-by-word annotation in the original ﬁle, which is arranged so that the W (for word) tokens in the Hayu are in one-to-one correspondence with the W tokens in the French MOTAMOT interlinear version. In such cases, it is normal for individual morphemes in the source language to correspond to several morphemes in the target language. This happens twice in the sentence in question, and we have split the interlinear translations to reﬂect the natural tokenization of the target language.) In this example, the time references (which are in seconds) are again given only at the beginning and end of the phrase, as required by the LACITO Archiving Project format. Nevertheless, the individual Hayu words have temporal extent and one might want to indicate that in the annotation. Observe that there is no meaningful way of assigning time references to word boundaries in the phrasal translation. Whether the time references happen to be unknown, as in the upper half of Figure 5, or are intrinsically un-knowable, as in the lower half of Figure 5, we can treat the W, M and T annotations in identical fashion. 2.5 LDC Broadcast News Transcripts The Linguistic Data Consortium (LDC) is an open consortium of universities, companies and govern- ment research laboratories, hosted by the University of Pennsylvania, that creates, collects and publishes speech and text databases, lexicons, and similar resources. Since its foundation in 1992, it has published some 150 digital databases, most of which contain material that falls under our deﬁnition of linguistic annotation. The Hub-4 English broadcast news corpora from the LDC contain some 200 hours of speech data with SGML annotation [www.ldc.upenn.eduCatalogLDC{97T22,98T28}.html]. About 60 hours of similar material has been published in Mandarin and Spanish, and an additional corpus of some 700 hours of English broadcast material will be published this year. What follows is the beginning of a radio program transcription from the Hub-4 corpus. Background TypeMusic Time0.000 LevelHigh Background TypeMusic Time4.233 LevelLow Section S_time4.233 E_time59.989 TypeFiller Segment S_time4.233 E_time13.981 Speaker\"Tad_Bile\" FidelityLow ModeSpontaneous it will certainly make some of these districts more competitive than they have been Sync Time8.015 so there will be some districts which are republican Sync Time11.040 but all of a sudden they may be up for grabs Segment Segment S_time13.981 E_time40.840 Speaker\"Noah_Adams\" FidelityHigh ModePlanned politicians get the maps out again Sync Time15.882 for friday june fourteenth this is n. p. r.s all things considered Sync Time18.960 Background TypeMusic Time23.613 LevelLow Sync Time23.613 in north carolina and other states officials are trying to figure out the effects of the supreme court ruling against minority voting districts breath Sync Time29.454 a business week magazine report of a federal criminal investigation breath Sync Time33.067 into the cause and the aftermath of the ValuJet crash in florida breath Sync Time36.825 efforts in education reform breath and the question will the public pay Segment MS-CIS-99-01: Bird  Liberman 11 Transcriptions are divided into sections (see the Section tag), where each section consists of a number of Segment blocks. At various times during a segment a Sync Time element is inserted to align a word boundary with an offset into a speech ﬁle. Elements specifying changes in background noise and signal quality function independently of the hierarchy. For example, a period of background music might bridge two segments, beginning in one segment and ending in the next. Figure 6 represents the structure of this annotation. Dotted arcs represent elided material, W is for words and M is for background music level. 2.6 LDC Telephone Speech Transcripts The LDC-published CALLHOME corpora include digital audio, transcripts and lexicons for telephone conversations in several languages. The corpora are designed to support research on speech recogni- tion algorithms [www.ldc.upenn.eduCatalogLDC96S46.html]. The transcripts exhibit abun- dant overlap between speaker turns in two-way telephone conversations. What follows is a typical fragment of an annotation. Each stretch of speech consists of a begin time, an end time, a speaker designation (A or B in the example below), and the transcription for the cited stretch of time. We have augmented the annotation with  and  to indicate partial and total overlap (respectively) with the previous speaker turn. 962.68 970.21 A: He was changing projects every couple of weeks and he said he couldnt keep on top of it. He couldnt learn the whole new area  968.71 969.00 B: mm. 970.35 971.94 A: that fast each time.  971.23 971.42 B: mm. 972.46 979.47 A: um, and he says he went in and had some tests, and he was diagnosed as having attention deficit disorder. Which 980.18 989.56 A: you know, given how hes how far hes gotten, you know, he got his degree at Tufts and all, I found that surprising that for the first time as an adult theyre diagnosing this. um  989.42 991.86 B: mm. I wonder about it. But anyway.  991.75 994.65 A: yeah, but thats what he said. And um  994.19 994.46 B: yeah. 995.21 996.59 A: He um  996.51 997.61 B: Whatevers helpful.  997.40 1002.55 A: Right. So he found this new job as a financial consultant and seems to be happy with that. 1003.14 1003.45 B: Good.  1003.06 1006.27 A: And then we saw Leo and Julie at Christmas time.  1005.45 1006.00 B: uh-huh. 1006.70 1009.85 A: And theyre doing great. um, they had just moved to  1009.25 1010.58 B: Hes in New York now, right?  1010.19 1013.55 A: a really nice house in Westchester. yeah, an o-  1013.38 1013.61 B: Good.  1013.52 1018.57 A: an older home that you know Julie is of course carving up and making beautiful. um  1018.15 1018.40 B: uh-huh. 1018.68 1029.75 A: Now she had a job with an architectural group when she first got out to New York, and that didnt work out. She said they had her doing things that she really wasnt qualified to do Long turns (e.g. the period from 972.46 to 989.56 seconds) were broken up into shorter stretches for the convenience of the annotators. Thus this format is ambiguous as to whether adjacent stretches by the same speaker should be considered parts of the same unit, or parts of different units  in translating to an annotation graph representation, either choice could be made. However, the intent is clearly just 12 A Formal Framework for Linguistic Annotation to provide additional time references within long turns, so the most appropriate choice seems to be to merge abutting same-speaker structures while retaining the additional time-marks. A section of this annotation including an example of total overlap is represented in annotation graph form in Figure 7. The turns are attributed to speakers using the speaker type. All of the words, punctuation and disﬂuencies are given the W type, though we could easily opt for a more reﬁned version in which these are assigned different types. Observe that the annotation graph representation preserves the non-explicitness of the original ﬁle format concerning which of speaker As words overlap which of speaker Bs words. Of course, additional time references could specify the overlap down to any desired level of detail (including to the level of phonetic segments or acoustic events if desired). 2.7 NIST Universal Transcription Format The US National Institute of Standards and Technology (NIST) has recently developed a set of annotation conventions intended to provide an extensible universal format for transcription and annotation across many spoken language technology evaluation domains [28]. This Universal Transcription Format (UTF) was based on the LDC Broadcast News format, previously discussed. A key design goal for UTF was to provide an SGML-based format that would cover both the LDC broadcast transcriptions and also various LDC-published conversational transcriptions, while also providing for plausible extensions to other sorts of material. A notable aspect of UTF is its treatment of overlapping speaker turns. In the following fragment (from the Hub-4 1997 evaluation set), overlapping stretches of speech are marked with the b overlap (begin overlap) and e overlap (end overlap) tags. turn speaker\"Roger_Hedgecock\" spkrtype\"male\" dialect\"native\" startTime\"2348.811875\" endTime\"2391.606000\" mode\"spontaneous\" fidelity\"high\" ... time sec\"2378.629937\" now all of those things are in doubt after forty years of democratic rule in b_enamex type\"ORGANIZATION\"congresse_enamex time sec\"2382.539437\" {breath because contraction e_form\"[youyou][vehave]\"youve got quotas {breath and sethyphenasides and rigidities in this system that keep you time sec\"2387.353875\" on welfare and away from real ownership {breath and contraction e_form\"[thatthat][sis]\"thats a real problem in this b_overlap startTime\"2391.115375\" endTime\"2391.606000\" country e_overlap turn turn speaker\"Gloria_Allred\" spkrtype\"female\" dialect\"native\" startTime\"2391.299625\" endTime\"2439.820312\" mode\"spontaneous\" fidelity\"high\" b_overlap startTime\"2391.299625\" endTime\"2391.606000\" well i e_overlap think the real problem is that uh these kinds of republican attacks time sec\"2395.462500\" i see as code words for discrimination ... turn Observe that there are two speaker turns, where the ﬁrst speakers utterance of country overlaps the second speakers utterance of well I. Note that the time attributes for overlap are not required to MS-CIS-99-01: Bird  Liberman 13 15 16 Wand 31 994.19 32 994.46 speakerB Wyeah 17 994.65 Wum 33 996.51 19 20 996.59 Wum 35 997.61 speakerB 34 Wwhatevers 22 23 W. 11 991.75 12 speakerA 13 Whe 14 Wsaid W, 18 995.21 Whe speakerA 21 997.40 Wright 25 1002.55 speakerA 24 Wso Whelpful Figure 7: Graph Structure for LDC Telephone Speech Example 19 2391.11 20 2391.60 Wcountry 21 2391.29 22 Wwell 25 2439.82 speakerGloria-Allred spkrtypefemale 23 2391.60 24 Wthink 11 2348.81 speakerRoger-Hedgecock spkrtypemale 12 13 Lthat 14 Wthats Lis 15 Wa 16 Wreal 17 Wproblem 18 Win Wthis Wi Figure 8: Graph Structure for NIST UTF Example 0 0.300 1 0.354 SD PD 2 0.404 SylW Wthe 13 1.116 IntermediateL- S P 3 0.465 Sp 4 0.520 Pp 8 0.796 SylS AccentH Wprice SH 5 0.540 SOr 6 0.568 Pr Sr 7 0.709 Pai Sai Ps Ss 9 0.819 SOr 10 0.871 Pr SylS Accent!H Wrange Sr 11 0.998 Sei Pei 12 1.069 Sn Pn SZ PZ Figure 9: Graph Structure for Emu Annotation Example 14 A Formal Framework for Linguistic Annotation coincide, since they are aligned to the most inclusive word boundaries for each speaker turn involved in the overlap. The coincidence of end times in this case is almost surely an artifact of the user interface of the system used to create the annotations, which required overlaps to be speciﬁed relative to word boundaries. The structure of overlapping turns can be represented using annotation graphs as shown in Figure 8. Each speaker turn is a separate connected subgraph, disconnected from other speaker turns. This situation neatly reﬂects the fact that the time courses of utterances by various speakers in conversation are logically asynchronous. Observe that the information about overlap is implicit in the time references and that partial word overlap can be represented. This seems like the best choice in general, since there is no necessary logical structure to conversational overlaps  at base, they are just two different actions unfolding over the same time period. The cited annotation graph structure is thus less explicit about word overlaps than the UTF ﬁle. However, if a more explicit symbolic representation of overlaps is desired, specifying that such-and-such a stretch of one speaker turn is associated with such-and-such a stretch of another speaker turn, this can be represented in our framework using the inter-arc linkage method described in 3.5, or using the extension described in 6.2. Of course, the same word-boundary-based representation of overlapping turns could also be expressed in annotation graph form, by allowing different speakers transcripts to share certain nodes (representing the word boundaries at which overlaps start or end). We do not suggest this, since it seems to us to be based on an inappropriate model of overlapping, which will surely cause trouble in the end. Note the use of the L lexical type to include the full form of a contraction. The UTF format employed special syntax for expanding contractions. No additional ontology was needed in order to do this in the annotation graph. (A query to ﬁnd instances of Wthat or Lthat would simply disjoin over the types.) Note also that it would have been possible to replicate the type system, replacing W with W1 for speaker 1 and W2 for speaker 2. However, we have chosen instead to attribute material to speakers using the speaker type on an arc spanning an entire turn. The disconnectedness of the graph structure means there can be no ambiguity about the attribution of each component arc to a speaker. As we have argued, annotation graphs of the kind shown in Figure 8 are actually more general and ﬂexible than the UTF ﬁles they model. The UTF format imposes a linear structure on the speaker turns and assumes that overlap only occurs at the periphery of a turn. In contrast, the annotation graph structure is well-behaved for partial word overlap, and it scales up naturally and gracefully to the situation where multiple speakers are talking simultaneously (e.g. for transcribing a radio talk-back show with a compere, a telephone interlocutor and a panel of discussants). It also works for arbitrary kinds of overlap (e.g. where one speaker turn is fully contained inside another), as discussed in the previous section. 2.8 Emu The Emu speech database system [11] grew out of the earlier Mu (Macquarie University) system [20], which was designed to support speech scientists who work with large collections of speech data, such as the Australian National Database of Spoken Language [andosl.anu.edu.auandosl]. Emu permits hierarchical annotations arrayed over any number of levels, where each level is a linear ordering. An annotation resides in a single ﬁle linked to an xwaves label ﬁle. The ﬁle begins with a declaration of the levels of the hierarchy and the immediate dominance relations. MS-CIS-99-01: Bird  Liberman 15 level Utterance level Intonational Utterance level Intermediate Intonational level Word Intermediate level Syllable Word level Phoneme Syllable level Phonetic Phoneme many-to-many The ﬁnal line licenses a many-to-many relationship between phonetic segments and phonemes, rather than the usual many-to-one relationship. According to the users manual, this is only advisable at the bottom of the hierarchy, otherwise temporal ambiguities may arise. At any given level of the hierarchy, the elements may have more than one attribute. For example, in the following declarations we see that elements at the Word level may be decorated with Accent and Text information, while syllables may carry a pitch accent. label Word Accent label Word Text label Syllable Pitch_Accent The next line sets up a dependency between the Phonetic level and an xwaves label ﬁle linked to ESPS-formatted audio data. labfile Phonetic :format ESPS :type SEGMENT :mark END :extension lab :time-factor 1000 The type declaration distinguishes segments with duration from events which are instantaneous. Here, the time associated with a segment will mark its endpoint rather than its starting point, as indicated by the mark END declaration. The timing information from the label ﬁle is adopted into the hierarchy (scaled from µs to ms), and can propagate upwards. In this way, the end of a phonetic segment may also become the end of a syllable, for example. The sequence of labels from the xwaves label ﬁle is reproduced in the Emu annotation, while the timing information remains in the xwaves label ﬁle. Therefore the latter ﬁle is an essential part of an Emu annotation and must be explicitly referenced. The labels are assigned unique numerical identiﬁers, as shown below for the sentence the price range is smaller than any of us expected. (For compactness, multiple lines have been collapsed to a single line.) Phonetic Phonetic 0 D 9  11 p 16 H 17 Or 19 r 20 ai 22 s 24 Or 30 r 31 ei 33 n 35 Z 37 I 44 zs 50 Om 52 m 53 o: 55 l 58  60 D 65  67 n 69 EC 76 E 77 n 80 i: 82  88 v 90  95 s 97 I 102 k 104 H 105 s 109 p 111 H 112 E 114 k 116 H 117 t 120 H 121  123 d 125 H The labels on the more abstract, phonemic level are assigned a different set of numerical identiﬁers. Phoneme Phoneme 1 D 10  12 p 18 r 21 ai 23 s 25 r 32 ei 34 n 36 Z 38 I 45 z 46 s 51 m 54 o: 56 l 59  61 D 66  68 n 70 E 78 n 81 i: 83  89 v 91  96 s 98 I 103 k 106 s 110 p 113 E 115 k 118 t 122  124 d Here is the remainder of the hierarchy. 16 A Formal Framework for Linguistic Annotation Utterance Utterance 8 Intonational Intonational 7 L Intermediate Intermediate 5 L- 42 L- 74 L- Word Word Accent Text 2 F W the 13 C S price 26 C S range 39 F W is 47 C S smaller 62 F W than 71 F S any 84 F W of 92 F W us 99 C S expected Syllable Syllable Pitch_Accent 4 W 15 S H 28 S !H 41 W 49 S H 57 W 64 W 73 S 79 W H 86 W 94 W 101 W 108 S H 119 W A separate section of an Emu annotation ﬁle lists each identiﬁer, followed by all those identiﬁers which it dominates. For example, the line 4 0 1 9 10 states that the ﬁrst W syllable (id4) directly or indirectly dominates phonetic segments D (id0) and  (id9) and phonemes D (id1) and  (id10). The ﬁrst intermediate phrase label L- (id5) dominates this material and much other material besides: 5 0 1 2 4 9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26 28 30 31 32 33 34 35 36 This exhaustive approach greatly facilitates the display of parts of the annotation hierarchy. If the syllable level is switched off, it is a trivial matter to draw lines directly from words to phonemes. The ﬁrst three words of this annotation are displayed as an annotation graph in Figure 9. Here S is used for phonetic segments, P for phonemes and Syl for strong (S) and weak (W) syllables. 2.9 Festival The Festival speech synthesis system [34, 35] is driven by richly-structured linguistic input. The Festival data structure, called a heterogeneous relation graph (HRG) is a collection of binary relations over attribute-value matrices (AVMs). Each matrix describes the local properties of some linguistic unit, such as a segment, a syllable, or a syntactic phrase. The value of an attribute could be atomic (such as a binary feature or a real number), or another (nested) AVM, or a function. Functions have the ability to traverse one or more binary relations and incorporate values from other AVMs. For example, if duration was an attribute of a syllable, its value would be a function subtracting the start time of the ﬁrst dominated segment from the end time of the last dominated segment. Typically, each level of structure includes these function-valued attributes so that temporal information is correctly propagated and does not need to be stored more than once. An example HRG is shown in Figure 10. Each box contains an abbreviated form of an AVM. The lines represent the binary relations. Observe, for example, that the phonemes and the surface segments are organized into two sequences, the two parallel lines spanning the bottom of the ﬁgure. Each sequence is a distinct binary relation. The hierarchical structures of the metrical and the syllable trees are two more binary relations. And the linear ordering of words is still another binary relation. Figure 11 gives the annotation graph representing the second half of the HRG structure. Given the abundance of arcs and levels, we have expanded the vertical dimension of the nodes, but this is not MS-CIS-99-01: Bird  Liberman 17 Figure 10: Annotation Structure from Festival Sax Pey Syln Sylr Metw Wa Metw Mets Ss Ps Sylo Mets Mets Wsocial Pow Sow Syln Sylr Psh Sylo Ssh Metw Pax Syln Sylr Sel Pl Sylc Paw Syln Saw Sylr Metw Mets Woutcast Pt Sylc St Pk Sylo Sk Mets Pae Syln Sae Sylr Ps Ss Sylc St Pt Figure 11: Graph Structure for Festival Example 18 A Formal Framework for Linguistic Annotation signiﬁcant. Node identiﬁers and time references have been omitted. Like the HRG, the annotation graph represents temporal information only once. Yet unlike the HRG, there is no need to deﬁne explicit propagation functions. 3 Architectural Considerations A diverse range of annotation models have now been considered. Our provision of annotation graphs for each one already gives a foretaste of the formalism we present in 4. However, before launching into the formalism, we want to stand back from the details of the various models, and try to take in the big picture. In this section we describe a wide variety of architectural issues which we believe should be addressed by any general purpose model for annotating linguistic signals. 3.1 Representation of Partial Information In the discussion of CHILDES and the LACITO Archiving Project above, there were cases where our graph representation had nodes which bore no time reference. Perhaps times were not measured, as in typical annotations of extended recordings where time references might only be given at major phrase boundaries (c.f. CHILDES). Or perhaps time measurements were not applicable in principle, as for phrasal translations (c.f. the LACITO Archiving Project). Various other possibilities suggest themselves. We might create a segment-level annotation automatically from a word-level annotation by looking up each word in a pronouncing dictionary and adding an arc for each segment, prior to hand-checking the segment annotations and adding time references to the newly created nodes. The annotation should remain well-formed (and therefore usable) at each step in this enrichment process. Just as the temporal information may be partial, so might the label information. For example, we might label indistinct speech with whatever information is available  so-and-so said something here that seems to be two syllables long and begins with a t. Beyond these two kinds of partiality, there is an even more obvious kind of partiality we should recog- nize. An annotated corpus might be annotated in a fragmentary manner. It might be that only 1 of a certain recording has any bearing on the research question that motivated the collection and annotation work. Therefore, it should be possible to have a well-formed annotation structure with arbitrary amounts of annotation detail at certain interesting loci, and limited or no detail elsewhere. This is a typical situation in phonetic or sociolinguistic research, where a large body of recordings may be annotated in detail with respect to a single, relatively infrequent phenomenon of interest. Naturally, one could always extract a sub-corpus and annotate that material completely, thereby removing the need for partiality, but this may have undesirable consequences for managing a corpus: (i) special intervention is required each time one wants to expand the sub-corpus as the research progresses; (ii) it is difﬁcult to make annotations of a sub-corpus available to someone working on a related research question with an overlapping sub-corpus, and updates cannot be propagated easily; (iii) provenance issues arise, e.g. it may be difﬁcult to identify the origin of any given fragment, in case access to broader context is necessary to retrieve the value of some other independent variable one might need to know; and (iv) it is difﬁcult to combine the various contributions into the larger task of annotating a standard corpus for use in perpetuity. MS-CIS-99-01: Bird  Liberman 19 By pointing out these problems we do not mean to suggest that all annotations of a corpus should be physically or logically combined. On the contrary, even with one physical copy of a corpus, we would want to allow several independent (partial) annotations to coexist, where these may be owned by different people and stored remotely from each other. Nor do we wish to suggest that the creation of sub-corpora is never warranted. The point is simply that an annotation formalism should not force users to create a derived corpus just so that a partial annotation is well-formed. 3.2 Encoding Hierarchical Information Existing annotated speech corpora always involve a hierarchy of several levels of annotation, even if they do not focus on very elaborate types of linguistic structure. TIMIT has sentences, words and phonetic segments; a broadcast news corpus may have designated levels for shows, stories, speaker turns, sentences and words. Some annotations may express much more elaborate hierarchies, with multiple hierarchies sometimes created for a single underlying body of speech data. For example, the Switchboard corpus of conver- sational speech [17] began with the three basic levels: conversation, speaker turn, and word. Various parts of it have since been annotated for syntactic structure [25], for breath groups and disﬂuencies [33], for speech act type [22, 23], and for phonetic segments [18]. These various annotations have been done as separate efforts, and presented in formats that are fairly easy to process one-by-one, but difﬁcult to compare or combine. Considering the variety of approaches that have been adopted, it is possible to identify at least three general methods for encoding hierarchical information. Token-based hierarchy Here, hierarchical relations among annotations are explicitly marked with respect to particular tokens: this particular segment is a daughter of this particular syllable. Systems that have adopted this approach include Partitur, Emu and Festival. Type-based hierarchy Here, hierarchical information is given with respect to types  whether once and for all in the database, or ad hoc by a user, or both. In effect, this means that a grammar of some sort is speciﬁed, which induces (additional) structure in the annotation. This allows (for instance) the subordination of syllables to words to be indicated, but only as a general fact about all syllables and words, not as a speciﬁc fact about particular syllables and words. An SGML DTD is an example of this: it speciﬁes a context-free grammar for any textual markup that uses it. In some cases, the hierarchical structure of a particular stretch of SGML markup cannot be determined without reference to the applicable DTD. Graph-based hierarchy Here, annotations are akin to the arcs in so-called parse charts [16, 179ff]. A parse chart is a particular kind of acyclic digraph, which starts with a string of words and then adds a set of arcs representing hypotheses about constituents dominating various substrings. In such a graph, if the substring spanned by arc ai properly contains the substring spanned by arc aj, then the constituent corresponding to ai must dominate the constituent corresponding to aj (though of course other structures may intervene). Hierarchical relationships are encoded in a parse chart only to the extent that they are implied by this graph-wise inclusion  thus two arcs spanning the same substring are unspeciﬁed as to their hierarchical relationship, and arcs ordered by temporal inclusion acquire a hierarchical relationship even when this is not appropriate given the types of 20 A Formal Framework for Linguistic Annotation those arcs (though a grammar, external to the parse chart for a particular sentence, may settle the matter; see also 5.3). As we have seen, many sorts of linguistic annotations are naturally encoded as graph structures with labeled arcs and time-marked nodes. Such a representation arises naturally from the fact that elementary annotations are predicates about stretches of signal. Thus in our TIMIT example, we can construe the underlying sequence of audio samples as a sort of terminal string, with annotations representing hypotheses about constituents of various types that dominate designated subsequences. In the example cited, the word she spans the sequence from sample 2360 to sample 5200; the phoneme sh spans the sequence from 2360 to 3720; and the phoneme iy spans the sequence from 3720 to 5200. This graph structure itself implies a sort of hierarchical structure based on temporal inclusion. If we interpret it as a parse chart, it tells us that the word she dominates the phoneme sequence sh iy. Examples of annotation systems that encode hierarchy using this approach are TIMIT, CHILDES and Delta [21]. (Note that, once equipped with the full annotation graph formalism, we will be able to distinguish graph-based and time-based inclusion, conﬂated here.) A particular system may present some mixture of the above techniques. Thus an SGML labeled bracketing may specify an unambiguous token-based hierarchy, with the applicable DTD grammar being just a redundant type-based check; but in some cases, the DTD may be necessary to determine the structure of a particular stretch of markup. Similarly, the graph structures implicit in TIMITs annotation ﬁles do not tell us, for the word spelled I and pronounced ay, whether the word dominates the phoneme or vice versa; but the structural relationship is implicit in the general relationship between the two types of annotations. An annotation framework (or its implementation) may also choose to incorporate arbitrary amounts of redundant encoding of structural information. It is often convenient to add redundant links explicitly  from children to parents, from parents to children, from one child to the next in order, and so on  so that a program can navigate the structure in a way that is clearer or more efﬁcient. Although such redundant links can be speciﬁed in the basic annotation itself  as in Festival  they might equally well be added automatically, as part of a compilation or indexing process. In our view, the addition of this often-useful but predictable structure should not be an intrinsic part of the deﬁnition of general-purpose annotation structures. We want to distinguish the annotation formalism itself from various enriched data structures with redundant encoding of hierarchical structure, just as we would distinguish it from various indices for convenient searching of labels and label sequences. In considering how to encode hierarchical information, we start from the premise that our representation will include some sort of graph structure, simply because this is the most fundamental and natural sort of linguistic annotation. Given this approach, hierarchical structure can often be read off the annotation graph structure, as was suggested informally above and will be discussed more thoroughly in 4. For many applications, this will be enough. For the residual cases, we might add either type-based or token- based encoding of hierarchical information (see 6.2). Based on the formal precedent of SGML, the model of how chart-like data structures are actually used in parsing, and the practical precedents of databases like TIMIT, it is tempting to consider adding a sort of grammar over arc labels as part of the formal deﬁnition of annotation graphs. However, in the absence of carefully-evaluated experience with circumstances in which this move is motivated, we prefer to leave this as something to be added by particular applications rather than incorporated into the formalism. In any case, we shall argue later (see 3.5) that we need a more general method to encode optional MS-CIS-99-01: Bird  Liberman 21 10 0.15 12 0.35 Lopen 20 0.16 21 0.24 Tclo 30 0.16 32 0.30 Vopen 22 0.35 Topen 14 0.52 Lclo 23 0.40 Tclo 25 0.58 Topen 33 0.41 35 0.56 Vopen 16 0.67 26 0.68 36 0.68 Lopen Tclo Vclo Vclo Figure 12: Gestural Score for the Phrase ten pin relationships among particular arcs. This method permits token-based marking of hierarchical structure as a special case. We also need to mention that particular applications in the areas of creation, query and display of annotations may be most naturally organized in ways that motivate a user interface based on a different sort of data structure than the one we are proposing. For instance, it may sometimes be easier to create annotations in terms of tree-like dominance relations rather than chart-like constituent extents, for instance in doing syntactic tree-banking [25]. It may likewise be easier in some cases to deﬁne queries explicitly in terms of tree structures. And ﬁnally, it may sometimes be more helpful to display trees rather than equivalent annotation graphs  the Festival example in 2.9 was a case in point. We believe that such user interface issues will vary from application to application, and may even depend on the tastes of individuals in some cases. In any case, decisions about such user interface issues are separable from decisions about the appropriate choice of basic database structures. 3.3 Gestural scores and multiple nodes at a time point In addition to the hierarchical and sequential structuring of information about linguistic signals, we also have parallel structuring. Nowhere is this clearer than in the gestural score notation used to describe the articulatory component of words and phrases (e.g. [8]). A gestural score maps out the time course of the gestural events created by the articulators of the vocal tract. This representation expresses the fact that the articulators move independently and that the segments we observe are the result of particular timing relationships between the gestures. Figure 12 gives the annotation graph for a gestural score. It shows the activity of the velum V, the tongue tip T and the lips L. This example stands in stark contrast to the hierarchical structures discussed in the previous section. Here there is no hierarchical relationship between the streams. Another important difference between hierarchical and parallel structures needs to be drawn here. Sup- pose that two labeled periods of an annotation begin (or end) at the same time. The alignment of two such boundaries might be necessary, or pure coincidence. As an example of necessary alignment, consider the case of phrase-initial words. Here, the left boundary of a phrase lines up with the left boundary of its initial word. Changing the time of the phrase boundary should change the time of the word boundary, and vice versa. In the general case, an update of this sort must propagate both upwards and downwards in the hierarchy. In fact, we argue that these two pieces of annotation actually share the same boundary: their arcs emanate from a single node. Changing the time reference of that node does not need to propagate anywhere, since the information is already shared by the relevant arcs. 22 A Formal Framework for Linguistic Annotation As an example of coincidental alignment, consider the case of gestural scores once more. In 100 annotated recordings of the same utterance we might ﬁnd that the boundaries of different gestures occasionally coincide. An example of this appears in Figure 12, where nodes 12 and 22 have the same time reference. However, this alignment is a contingent fact about a particular utterance token. An edit operation which changed the start time of one gesture would usually carry no implication for the start time of some other gesture. 3.4 Instants, overlap and gaps Even though a linguistic event might have duration, such as the attainment of a pitch target, the most perspicuous annotation may be tied to an instant rather than an interval. Some annotation formalisms (e.g. Emu, Festival, Partitur) provide a way to label instants. The alignment of these instants with respect to other instants or intervals can then be investigated or exploited. There are at least ﬁve conceivable approaches to labeled instants (note that this is not a mutually exclusive set): 1. nodes could be optionally labeled; or 2. an instant can be modeled as a self-loop on a node, and again labeled just like any other arc; or 3. instants can be treated as arcs between two nodes with the same time reference; or 4. instants can be treated as short periods, where these are labeled arcs just like any other; or 5. certain types of labels on periods could be interpreted as referring to the commencement or the culmination of that period. With little evidence on which to base a decision between these options we opt for the most conservative, which is the one embodied in the last two options. Thus with no extension to the ontology we already have two ways to model instants. As we have seen, annotations are often stratiﬁed, where each layer describes a different property of a signal. What are the possible temporal relationships between the pieces of a given layer? Some possibilities are diagrammed in Figure 13, where a point is represented as a vertical bar, and an interval is represented as a horizontal line between two points. In the ﬁrst row of Figure 13, we see a layer which exhaustively partitions the time-ﬂow into a sequence of non-overlapping intervals (or perhaps intervals which overlap just at their endpoints). In the second row we see a layer of discrete instants. The next two rows illustrate the notions of gaps and overlaps. Gaps might correspond to periods of silence, or to periods in between the salient events, or to periods which have yet to be annotated. Overlaps occur between speaker turns in discourse (see Figure 7) or even between adjacent words in a single speech stream (see Figure 14a). The ﬁfth row illustrates a hierarchical grouping of intervals within a layer (c.f. the Met arcs in Figure 11). The ﬁnal row contains an arbitrary set of intervals and instants. We adopt this last option (minus the instants) as the most general case for the layer of an annotation. As we shall see, layers themselves will not be treated specially; a layer can be thought of simply as the collection of arcs sharing the same type information. MS-CIS-99-01: Bird  Liberman 23 Gaps, overlaps structure Hierarchical and instants Partition into intervals Sequence of instants Intervals with gaps overlapping intervals Sequence of Figure 13: Possible Structures for a Single Layer 3.5 Multiple arcs and labels It is often the case that a given stretch of speech has multiple possible labels. For example, the region of speech corresponding to a monosyllabic word is both a syllable and a word, and in some cases it may also be a complete utterance. The combination of two independent annotations into a single annotation (through set union) may also result in two labels covering the same extent. In the general case, a label could be a (typed) attribute-value matrix, possibly incorporating nested structure, list- and set-valued attributes, and even disjunction. However, our hypothesis is that typed labels (with atomic types and labels) are sufﬁcient. Multiple labels spanning the same material reside on their own arcs. Their endpoints can be varied independently (see 3.3), and the combining and projection of annotations does not require the merging and splitting of arcs. An apparent weakness of this conception is that we have no way of individuating arcs, and it is not possible for arcs to reference each other. However, there are cases when such links between arcs are necessary. Three examples are displayed in Figure 14; we discuss each in turn. Recall from 3.3 that an annotation graph can contain several independent streams of information, where no nodes are shared between the streams. The temporal extents of the gestures in the different streams are almost entirely asynchronous; any equivalences are likely to be coincidences. However, these gestures may still have determinate abstract connections to elements of a phonological analysis. Thus a velar opening and closing gesture may be associated with a particular nasal feature, or with a set of nasal features, or with the sequence of changes from non-nasal to nasal and back again. But these associations cannot usually be established purely as a matter of temporal coincidence, since the phonological features involved are bundled together into other units (segments or syllables or whatever) containing other features that connect to other gestures whose temporal extents are all different. The rules of coordination for such gestures involve phase relations and physical spreading which are completely arbitrary from the perspective of the representational framework. A simpliﬁed example of the arbitrary relationship between the gestures comprising a word is illustrated in Figure 14a. We have the familiar annotation structure (taken from Figure 12), enriched with information 24 A Formal Framework for Linguistic Annotation Wpin Wten Lclo Topen Vclo Tclo Topen Vclo Vopen Tclo Lopen Vopen Lopen Tclo t f ts t s ate la7natshem pa siG nonotso nakpu ate la7natshem pa siG nonotso nakpu bois un jour du bois un jour du ate la7natshem pa siG nonotso nakpu on soeurs raconte que allerent deux chercher on soeurs raconte que allerent chercher deux on soeurs raconte que allerent deux chercher un jour du bois (c) (a) (c) (c) (b) Figure 14: Annotation Graphs Enriched with Inter-Arc Linkages about which words license which gestures. The words are shown as overlapping, although this is not crucially required. In the general case, the relationship between words and their gestures is not predictable from the temporal structure and the type structure alone. The example in Figure 14b shows a situation where we have multiple independent transcriptions of the same data. In this case, the purpose is to compare the performance of different transcribers on identical material. Although the intervals do not line up exactly, an obvious correspondence exists between the labels and it should be possible to navigate between corresponding labels, even though their precise temporal relationship is somewhat arbitrary. Observe that the cross references do not have equivalent status here; the relationship between ts and t is not the same as that between s and f. The ﬁnal example, Figure 14c, shows an annotation graph based on the Hayu example from Figure 5. We would like to be able to navigate between words of a phrasal translation and the corresponding Hayu words. This would be useful, for example, to study the various ways in which a particular Hayu word is idiomatically translated. Note that the temporal relationship between linked elements is much more chaotic here, and that there are examples of one-to-many and many-to-many mappings. The words being mapped do not even need to be contiguous subsequences. One obvious way to address these three examples is to permit arc labels to carry cross-references to other arc labels. The semantics of such cross-references might be left up to the individual case. This requires at least some arcs to be individuated (as all nodes are already). While it would be a simple matter to individuate arcs (c.f. 6.2), this step is not forced on us. There is another approach that stays more nearly within the conﬁnes of the existing formalism. In this approach, we treat all of the cases described above MS-CIS-99-01: Bird  Liberman 25 Vclo Vclo Vopen Vopen Topen Tclo Topen Tclo Tclo Wten Lclo Lopen Lopen Wpin t paras27 f paras28 (a) ts paras27 paras29 t s paras27 (b) ts t paras28 f (b) paras29 t s paras28 paras29 licensew35 licensew35 licensew35 licensew36 licensew36 licensew36 licensew35 licensew36 Figure 15: Inter-Arc Linkages for Parallel Transcriptions in terms of equivalence classes. One way to formalize a set of equivalence classes is as an ordered pair: class-type:identiﬁer. But this is just our label notation all over again  the only news is that for label types interpreted as denoting equivalence classes, different labels with the same identiﬁer are viewed as forming an equivalence class. Another way to put this is that two (or more) labels are connected not by referencing one another, but by jointly referencing a particular equivalence class. In the general case, we have n partially independent strands, where the material to be associated comes from some subset of the strands. Within a given strand, zero, one or more arcs may participate in a given association, and the arcs are not necessarily contiguous. For the gestural score in Figure 14a we augment each arc with a second arc having the same span. These additional arcs all carry the type license and the unique labels (say) w35 and w36, depending on which word they belong to. The word arcs are also supplemented: Wten with licensew35 and Wpin with licensew36. See Figure 15a. Now we can easily navigate around the set of gestures licensed by a word regardless of their temporal extent. We can use the type information on the existing labels in situations where we care about the directionality of the association. 26 A Formal Framework for Linguistic Annotation This approach can be applied to the other cases, with some further qualiﬁcations. For Figure 14b, there is more than one option, as shown in Figure 15b,b. In the ﬁrst option, we have a single cross-reference, while in the second option, we have two cross-references. We could combine both of these into a single graph containing three cross-references. The translation case of Figure 14c can be treated in the same way. If the phrasal translation of a word is a continuous stretch, it could be covered by multiple arcs (one for each existing arc), or it could be covered by just a single arc. If the phrasal translation of a word is not a contiguous stretch, we may be forced to attach more than one diacritic arc with a given label. We do not anticipate any adverse consequences of such a move. Incidentally, note that this linked multiple stream representation is employed in an actual machine translation system [9]. Observe that this construction involves assigning intervals (node-pairs) rather than arcs to equivalence classes. In cases where there are multiple independent cross references, it is conceivable that we might have distinct equivalence classes involving different arcs which span the same two nodes. So long as these arcs are distinguished by their types we do not foresee a problem. This section has described three situations where potentially complex relationships between arc labels are required. However, we have demonstrated that the existing formalism is sufﬁciently expressive to encompass such relationships, and so we are able to preserve the simplicity of the model. Despite this simplicity, there is one way in which the approach may seem proﬂigate. There are no less than three ways for a pair of arcs to be associated: temporal overlap, hierarchy, and equivalence-class linkages. Interestingly, this three-way possibility exactly mirrors the three ways that association is treated in the phonological literature. There, association is ﬁrst and foremost a graphical notion. From context it is usually possible to tell whether the line drawn between two items indicates temporal overlap, a hierarchical relationship, or some more abstract, logical relationship [6, 7, 4]. We have shown how all three uses are attested in the realm of linguistic annotation. The fact that the three conceptions of association are distinct and attested is sufﬁcient cause for us to include all three in the formalism, notwithstanding the fact that we get them for free. 3.6 Associations between annotations and ﬁles An annotated corpus is a set of annotation graphs and an associated body of time series data. The time series might comprise one or more audio tracks, one or more video streams, one or more streams of physiological data of various types, and so forth. The data might be sampled at a ﬁxed rate, or might consist of pairs of times and values, for irregularly spaced times. Different streams will typically have quite different sampling rates. Some streams might be deﬁned only intermittently, as in the case of a continuous audio recording with intermittent physiological or imaging data. This is not an imagined list of conceptually possible types of data  we are familiar with corpora with all of the properties cited. The time series data will be packaged into a set of one or more ﬁles. Depending on the application, these ﬁles may have some more or less complex internal structure, with headers or other associated information about type, layout and provenance of the data. These headers may correspond to some documented open standard, or they may be embedded in a proprietary system. The one thing that ties all of the time series data together is a shared time base. To use these arbitrarily diverse data streams, we need to be able to line them up time-wise. This shared time base is also the only pervasive and systematic connection such data is likely to have with annotations of the type we are discussing in this paper. MS-CIS-99-01: Bird  Liberman 27 It is not appropriate for an annotation framework to try to encompass the syntax and semantics of all existing time series ﬁle formats. They are simply too diverse and too far from being stable. However, we do need to be able to specify what time series data we are annotating, and how our annotations align with it, in a way that is clear and ﬂexible. An ambitious approach would be to specify a new universal framework for the representation of time series data, with a coherent and well-deﬁned semantics, and to insist that all annotated time series data should be translated into this framework. After all, we are doing the analogous thing for linguistic annotations: proposing a new, allegedly universal framework into which we argue that all annotations can be translated. Such an effort for all time series data, whether or not it is a reasonable thing to do, is far outside the scope of what we are attempting here. A much simpler and less ambitious way to connect annotation graphs to their associated time series is to introduce arcs that reference particular time-series ﬁles, or temporally contiguous sub-parts of such ﬁles. Each such arc speciﬁes that the cited portion of data in the cited time-function ﬁle lines up with the portion of the annotation graph speciﬁed by the time-marks on its source and sink nodes. Arbitrary additional information can be provided, such as an offset relative to the ﬁles intrinsic time base (if any), or a speciﬁcation selecting certain dimensions of vector-valued data. Taking this approach, a single annotation could reference multiple ﬁles  some parts of an annotation could refer speciﬁcally to a single ﬁle, while other parts of an annotation could be non-speciﬁc. In this way, events that are speciﬁc to a channel (like a particular speaker turn) can be marked as such. Equally, annotation content for an event which is not speciﬁc to a channel can be stored just once. These ﬁle-related labels, if properly designed and implemented, will permit an application to recover the time-series data that corresponds to a given piece of annotation  at least to the extent that the annotation is time-marked and that any time-function ﬁles have been speciﬁed for the cited subgraph(s). Thus if time-marking is provided at the speaker-turn level (as is often the case for published conversational data), then a search for all the instances of a speciﬁed word string will enable us to recover usable references to all available time-series data for the turn that contains each of these word strings. The information will be provided in the form of ﬁle names, time references, and perhaps time offsets; it will be the responsibility of the application (or the user) to resolve these references. If time-marking has been done at the word level, then the same query will enable us to recover a more exact set of temporal references in the same set of ﬁles. Our preference for the moment is to allow the details of how to deﬁne these ﬁle-references to fall outside the formalism we are deﬁning here. It should be clear that there are simple and natural ways to establish the sorts of linkages that are explicit in existing types of annotated linguistic database. After some practical experience, it may make sense to try to provide a more formal account of references to external time-series data. Spatial and image-plane references We would also like to point out a wider problem for which we do not have any general solution. Although it is not our primary focus, we would like the annotation formalism to be extensible to spatially-speciﬁc annotations of video signals and similar data, perhaps by enriching the temporal anchors with spatial andor image-plane information. Anthropologists, conversation analysts, and sign-language researchers are already producing annotations that are (at least conceptually) anchored not only to time spans but also to a particular spatial or image-plane trajectory through the corresponding series of video frames. 28 A Formal Framework for Linguistic Annotation In the case of simple time-series annotations, we are tagging nodes with absolute time references, perhaps offset by a single constant for a given recorded signal. However, if we are annotating a video recording, the additional anchoring used for annotating video sequences will mostly not be about absolute space, even with some arbitrary shift of coordinate origin, but rather will be coordinates in the image plane. If there are multiple cameras, then image coordinates for each will differ, in a way that time marks for multiple simultaneous recordings do not. In fact, there are some roughly similar cases in audio annotation, where an annotation might reference some speciﬁc two- or three-dimensional feature of (for instance) a time-series of short-time amplitude spectra (i.e. a spectrogram), in which case the quantitative details will depend on the analysis recipe. Our system allows such references (like any other information) to be encoded in arc labels, but does not provide any more speciﬁc support. Relationship to multimedia standards In this context we ought to raise the question of how annotation graphs relate to various multimedia standards like the Synchronized Multimedia Integration Language [www.w3.orgTRREC-smil] and MPEG-4 [drogo.cselt.itmpegstandardsmpeg-4mpeg-4.htm]. Since these provide ways to specify both temporal and spatial relationships among strings, audio clips, still pictures, video sequences, and so on, one hopes that they will offer support for linguistic annotation. It is hard to offer a conﬁdent evaluation, since MPEG-4 is still in development, and SMILs future as a standard is unclear. With respect to MPEG-4, we reserve judgment until its characteristics become clearer. Our preliminary assessment is that SMIL is not useful for purposes of linguistic annotation, because it is mainly focused on presentational issues (fonts, colors, screen locations, fades and animations, etc.) and does not in fact offer any natural ways to encode the sorts of annotations that we surveyed in the previous section. Thus it is easy to specify that a certain audio ﬁle is to be played while a certain caption fades in, moves across the screen, and fades out. It is not (at least straightforwardly) possible to specify that a certain audio ﬁle consists of a certain sequence of conversational turns, temporally aligned in a certain way, which consist in turn of certain sequences of words, etc. 3.7 Node references versus byte offsets The Tipster Architecture for linguistic annotation of text [19] is based on the concept of a fundamental, immutable textual foundation, with all annotations expressed in terms of byte offsets into this text. This is a reasonable solution for cases where the text is a published given, not subject to revision by annotators. However, it is not a good solution for speech transcriptions, which are typically volatile entities, constantly up for revision both by their original authors and by others. In the case of speech transcriptions, it is more appropriate to treat the basic orthographic transcription as just another annotation, no more formally privileged than a discourse analysis or a translation. Then we are in a much better position to deal with the common practical situation, in which an initial orthographic transcription of speech recordings is repeatedly corrected by independent users, who may also go on to add new types of annotation of their own, and sometimes also adopt new formatting conventions to suit their own display needs. Those who wish to reconcile these independent corrections, and also combine MS-CIS-99-01: Bird  Liberman 29 the independent additional annotations, face a daunting task. In this case, having annotations reference byte offsets into transcriptional texts is almost the worst imaginable solution. Although nothing will make it trivial to untangle this situation, we believe our approach comes close. As we shall see in 4.5, our use of a ﬂat, unordered ﬁle structure incorporating node identiﬁers and time references means that edits are as strictly local as they possibly can be, and connections among various types of annotation are as durable as they possibly can be. Some changes are almost completely transparent (e.g. changing the spelling of a name). Many other changes will turn out not to interact at all with other types of annotation. When there is an interaction, it is usually the absolute minimum that is necessary. Therefore, keeping track of what corresponds to what, across generations of distributed annotation and revision, is as simple as one can hope to make it. Therefore we conclude that Tipster-style byte offsets are an inappropriate choice for use as references to audio transcriptions, except for cases where such transcriptions are immutable in principle. In the other direction, there are several ways to translate Tipster-style annotations into our terms. The most direct way would be to treat Tipster byte offsets exactly as analogous to time references  since the only formal requirement on our time references is that they can be ordered. This method has the disadvantage that the underlying text could not be searched or displayed in the same way that a speech transcription normally could. A simple solution would be to add an arc for each of the lexical tokens in the original text, retaining the byte offsets on the corresponding nodes for translation back into Tipster- architecture terms. 3.8 What is time? TIMIT and some other extant databases denominate signal time in sample numbers (relative to a desig- nated signal ﬁle, with a known sampling rate). Other databases use ﬂoating-point numbers, representing time in seconds relative to some ﬁxed offset, or other representations of time such as centiseconds or milliseconds. In our formalization of annotation graphs, the only thing that really matters about time references is that they deﬁne an ordering. However, for comparability across signal types, time references need to be intertranslatable. We feel that time in seconds is generally preferable to sample or frame counts, simply because it is more general and easier to translate across signal representations. However, there may be circumstances in which exact identiﬁcation of sample or frame numbers is crucial, and some users may prefer to specify these directly to avoid any possibility of confusion. Technically, sampled data points (such as audio samples or video frames) may be said to denote time intervals rather than time points, and the translation between counts and times may therefore become ambiguous. For instance, suppose we have video data at 30 Hz. Should we take the 30th video frame (counting from one) to cover the time period from 2930 to 1 second or from 29.530 to 30.530 second? In either case, how should the endpoints of the interval be assigned? Different choices may shift the correspondence between times and frame numbers slightly. Also, when we have signals at very different sampling rates, a single sampling interval in one signal can correspond to a long sequence of intervals in another signal. With video at 30 Hz and audio at 44.1 kHz, each video frame corresponds to 1,470 audio samples. Suppose we have a time reference of .9833 seconds. A user might want to know whether this was created because some event was ﬂagged in the 29th video frame, for which we take the mean time point to be 29.530 seconds, or because some event 30 A Formal Framework for Linguistic Annotation was ﬂagged at the 43,365th audio sample, for which we take the central time point to be 43365.544100 seconds. For reasons like these, some users might want the freedom to specify references explicitly in terms of sample or frame numbers, rather than relying on an implicit method of translation to and from time in seconds. Several ways to accommodate this within our framework come to mind, but we prefer to leave this open, as we have no experience with applications in which this might be an issue. In our initial explorations, we are simply using time in seconds as the basis. 4 A Formal Framework 4.1 Background Looking at the practice of speech transcription and annotation across many existing communities of practice, we see commonality of abstract form along with diversity of concrete format. All annotations of recorded linguistic signals require one unavoidable basic action: to associate a label, or an ordered sequence of labels, with a stretch of time in the recording(s). Such annotations also typically distinguish labels of different types, such as spoken words vs. non-speech noises. Different types of annotation often span different-sized stretches of recorded time, without necessarily forming a strict hierarchy: thus a conversation contains (perhaps overlapping) conversational turns, turns contain (perhaps interrupted) words, and words contain (perhaps shared) phonetic segments. A minimal formalization of this basic set of practices is a directed graph with typed labels on the arcs and optional time references on the nodes. We believe that this minimal formalization in fact has sufﬁcient expressive capacity to encode, in a reasonably intuitive way, all of the kinds of linguistic annotations in use today. We also believe that this minimal formalization has good properties with respect to creation, maintenance and searching of annotations. Our strategy is to see how far this simple conception can go, resisting where possible the temptation to enrich its ontology of formal devices, or to establish label types with special syntax or semantics as part of the formalism. See section 6.2 for a perspective on how to introduce formal and substantive extensions into practical applications. We maintain that most, if not all, existing annotation formats can naturally be treated, without loss of generality, as directed acyclic graphs having typed labels on (some of) the edges and time-marks on (some of) the vertices. We call these annotation graphs. It is important to recognize that translation into annotation graphs does not magically create compatibility among systems whose semantics are different. For instance, there are many different approaches to transcribing ﬁlled pauses in English  each will translate easily into an annotation graph framework, but their semantic incompatibility is not thereby erased. It is not our intention here to specify annotations at the level of permissible tags, attributes, and values, as was done by many of the models surveyed in 2. This is an application-speciﬁc issue which does not belong in the formalism. The need for this distinction can be brought into sharp focus by analogy with database systems. Consider the relationship between the abstract notion of a relational algebra, the features of a relational database system, and the characteristics of a particular database. For example, the MS-CIS-99-01: Bird  Liberman 31 deﬁnition of substantive notions like date does not belong in the relational algebra, though there is good reason for a database system to have a special data type for dates. Moreover, a particular database may incorporate all manner of restrictions on dates and relations among them. The formalization presented here is targeted at the most abstract level: we want to get the annotation formalism right. We assume that system implementations will add all kinds of special-case data types (i.e. types of labels with specialized syntax and semantics). We further assume that particular databases will want to introduce additional speciﬁcations. Our current strategy  given the relative lack of experience of the ﬁeld in dealing with such matters  is to start with a general model with very few special label types, and an open mechanism for allowing users to impose essentially arbitrary interpretations. This is how we deal with instants (c.f. 3.4), associations between annotations and ﬁles (c.f. 3.6) and coindexing of arcs (c.f. 3.5). 4.2 Annotation graphs Let T be a set of types, where each type in T has a (possibly open) set of contentful elements. The label space L is the union of all these sets. We write each label as a typecontent pair, allowing the same contentful element to occur in different types. (So, for example, the phoneme a and the phonetic segment [a] can be distinguished as Pa vs Sa.) Annotation graphs are now deﬁned as follows: Deﬁnition 1 An annotation graph G over a label set L and a node set N is a set of triples having the form n1, l, n2, l  L, n1, n2  N, which satisﬁes the following conditions: 1. N, {n1, n2  n1, l, n2  G} is a directed acyclic graph. 2. τ : N  ℜ is an order-preserving map assigning times to some of the nodes. There is no requirement that annotation graphs be connected or rooted, or that they cover the whole time course of the linguistic signal they describe. The set of annotation graphs is closed under union, intersection and relative complement. For convenience, we shall refer to nodes which have a time reference (i.e. dom(τ)) as anchored nodes. It will also be useful to talk about annotation graphs which are minimally anchored, in the sense deﬁned below: Deﬁnition 2 An anchored annotation graph G over a label set L and a node set N is an annotation graph satisfying two additional conditions: 1. If n  N is such that n, l, n  G for any l  L, n  N, then τ : n  r  ℜ; 2. If n  N is such that n, l, n  G for any l  L, n  N, then τ : n  r  ℜ. Anchored annotation graphs have no dangling arcs (or chains) leading to an indeterminate time point. It follows from this deﬁnition that, for any unanchored node, we can reach an anchored node by following a chain of arcs. In fact every path from an unanchored node will ﬁnally take us to an anchored node. Likewise, an unanchored node can be reached from an anchored node. A key property of anchored 32 A Formal Framework for Linguistic Annotation annotation graphs is that we are guaranteed to have some information about the temporal locus of every node. This property will be made explicit in 5.1. An examination of the annotation graphs in 2 will reveal that they are all anchored annotation graphs. Note that the set of anchored annotation graphs is closed under union, but not under intersection or relative complement. We can also deﬁne a totally-anchored annotation graph as one in which τ is a total function. The annotation graphs in Figures 1, 2, 3 and 9 are all totally-anchored. Equipped with this three-element hierarchy, we will insist that the annotation graphs that are the primary objects in linguistic databases are anchored annotation graphs. For the sake of a clean algebraic seman- tics for the query language, we will permit queries and the results of queries to be (sets of) arbitrary annotation graphs. 4.3 Relations on nodes and arcs The following deﬁnition lets us talk about two kinds of precedence relation on nodes in the graph structure. The ﬁrst kind respects the graph structure (ignoring the time references), and is called structure precedence, or simply s-precedence. The second kind respects the temporal structure (ignoring the graph structure), and is called temporal precedence, or simply t-precedence. Deﬁnition 3 A node n1 s-precedes a node n2, written n1 s n2, if there is a chain from n1 to n2. A node n1 t-precedes a node n2, written n1 t n2, if τ(n1)  τ(n2). Observe that both these relations are transitive. There is a more general notion of precedence which mixes both relations. For example, we can infer that node n1 precedes node n2 if we can use a mixture of structural and temporal information to get from n1 to n2. This idea is formalized in the next deﬁnition. Deﬁnition 4 Precedence is a binary relation on nodes, written , which is the transitive closure of the union of the s-precedes and the t-precedes relations. Armed with these deﬁnitions we can now deﬁne some useful inclusion relations on arcs. The ﬁrst kind of inclusion respects the graph structure, so it is called s-inclusion. The second kind, t-inclusion, respects the temporal structure. Deﬁnition 5 An arc p  n1, n4 s-includes an arc q  n2, n3, written p s q, if n1 s n2 and n3 s n4. p t-includes q, written p t q, if n1 t n2 and n3 t n4. As with node precedence, we deﬁne a general notion of inclusion which generalizes over these two types: Deﬁnition 6 Inclusion is a binary relation on arcs, written , which is the transitive closure of the union of the s-inclusion and the t-inclusion relations. Note that all three inclusion relations are reﬂexive and transitive. We assume the existence of non-strict precedence and inclusion relations, deﬁned in the obvious way. MS-CIS-99-01: Bird  Liberman 33 4.4 Visualization It is convenient to have a variety of ways of visualizing annotation graphs. Most of the systems we surveyed in 2 come with visualization components, whether tree-based, extent-based, or some combination of these. We would endorse the use of any descriptively adequate visual notation in concert with the annotation graph formalism, so long as the notation can be endowed with an explicit formal semantics in terms of annotation graphs. Note, however, that not all such visual notations can represent everything an annotation graph contains, so we still need one or more general-purpose visualizations for annotation graphs. The primary visualization chosen for annotation graphs in this paper uses networks of nodes and arcs to make the point that the mathematical objects we are dealing with are graphs. In most practical situations, this mode of visualization is cumbersome to the point of being useless. Visualization techniques should be optimized for each type of data and for each application, but there are some general techniques that can be cited. Observe that the direction of time-ﬂow can be inferred from the left-to-right layout of annotation graphs, and so the arrow-heads are redundant. For simple connected sequences (e.g. of words) the linear structure of nodes and arcs is not especially informative; it is better to write the labels in sequence and omit the graph structure. The ubiquitous node identiﬁers should not be displayed unless there is accompanying text that refers to speciﬁc nodes. Label types can be effectively distinguished with colors, typefaces or vertical position. We will usually need to break an annotation graph into chunks which can be presented line-by-line (much like interlinear text) in order to ﬁt on a screen or a page. The applicability of these techniques depends on the fact that annotation graphs have a number of properties that do not follow automatically from a graphical notation. In other words, many directed acyclic graphs are not well-formed annotation graphs. Two properties are of particular interest here. First, as noted in 4.2, all the annotation graphs we have surveyed are actually anchored annotation graphs. This means that every arc lies on a path of arcs that is bounded at both ends by time references. So, even when most nodes lack a time reference, we can still associate such chains with an interval of time. A second property, more contingent but equally convenient, is that annotation graphs appear to be rightward planar, i.e. they can be drawn in such a way that no arcs cross and each arc is monotonically increasing in the rightwards direction (c.f. the deﬁnition of upward planarity in [12]). These properties are put to good use in Figure 16, which employs a score notation (c.f. [8, 11, 14, 27]). The conventions employed by these diagrams are as follows. An arc is represented by a shaded rectangle, where the shading (or color, if available) represents the type information. Where possible, arcs having the same type are displayed on the same level. Arcs are labeled, but the type information is omitted. Inter-arc linkages (see 3.5) are represented using coindexing. The ends of arcs are represented using short vertical lines having the same width as the rectangles. These may be omitted if the tokenization of a string is predictable. If two arcs are incident on the same node but their corresponding rectangles appear on different levels of the diagram, then the relevant endpoints are connected by a solid line. For ease of external reference, these lines can be decorated with a node identiﬁer. Anchored nodes are connected to the timeline with dotted lines. The point of intersection is labeled with a time reference. If necessary, multiple timelines may be used. Nodes sharing a time reference are connected with a dotted line. In order to ﬁt on a page, these diagrams may be cut at any point, with any partial rectangles labeled on both parts. 34 A Formal Framework for Linguistic Annotation speaker .46 .65 .21 .51 .59 .40 .61 W speaker W .19 995 yeah and .173 .077 she right um 0 h sh iy hv ae dcl y axr 2 4 6 8 12 10 .0 .360 .270 .200 .160 .720 .680 had said , B your P W so 994 996 997 helpful whatevers um he he . so right . A A A B Figure 16: Visualizations for the TIMIT and LDC Telephone Speech Examples Unlike some other conceivable visualizations (such as the tree diagrams and autosegmental diagrams used by Festival and Emu), this scheme emphasizes the fact that each component of an annotation has temporal extent. The scheme neatly handles the cases where temporal information is partial. 4.5 File Encodings As stated at the outset, we believe that the standardization of ﬁle formats is a secondary issue. The identiﬁcation of a common conceptual framework underlying all work in this area is an earlier milestone along any path to standardization of formats and tools. That said, we believe that ﬁle formats should be transparent encodings of the annotation structure. The ﬂattest data structure we can imagine for an annotation graph is a set of 3-tuples, one per arc, consisting of a node identiﬁer, a label, and another node identiﬁer. This data structure has a transparent relationship to our deﬁnition of annotation graphs, and we shall refer to it as the basic encoding. Node identiﬁers are supplemented with time values, where available, and are wrapped with angle brackets. A ﬁle encoding for the UTF example (Figure 8) is given below. MS-CIS-99-01: Bird  Liberman 35 213291.29 speakerGloria-Allred 252439.82 132391.11 Wcountry 142391.60 112348.81 spkrtypemale 142391.60 213291.29 spkrtypefemale 252439.82 22 Wi 232391.60 232391.60 Wthink 24 112348.81 speakerRoger-Hedgecock 142391.60 12 Wthis 132391.11 213291.29 Wwell 22 We make no ordering requirement, thus any reordering of these lines is taken to be equivalent. Equally, any subset of the tuples comprising an annotation graph (perhaps determined by matching a grep like pattern) is a well-formed annotation graph. Accordingly, a basic query operation on an annotation graph can be viewed as asking for subgraphs that satisfy some predicate, and each such subgraph will itself be an annotation graph. Any union of the tuples comprising annotation graphs is a well-formed annotation graph, and this can be implemented by simple concatenation of the tuples (ignoring any repeats). This format obviously encodes redundant information, in that nodes and their time references may be mentioned more than once. However, we believe this is a small price to pay for having a maximally simple ﬁle structure. Let us consider the implications of various kinds of annotation updates for the ﬁle encoding. The addition of new nodes and arcs simply involves concatenation to the basic encoding (recall that the basic encoding is an unordered list of arcs). The same goes for the addition of new arcs between existing nodes. For the user adding new annotation data to an existing read-only corpus  a widespread mode of operation  the new data can reside in one or more separate ﬁles, to be concatenated at load time. The insertion and modiﬁcation of labels for existing arcs involves changing one line of the basic encoding. Adding, changing or deleting a time reference may involve non-local change to the basic encoding of an annotation. This can be done in either of two ways: a linear scan through the basic encoding, searching for all instances of the node identiﬁer; or indexing into the basic encoding using the time-local index to ﬁnd the relevant lines. Of course, the time reference could be localized in the basic encoding by having a separate node set, referenced by the arc set. This would permit the time reference of a node to be stored just once. However, we prefer to keep the basic encoding as simple as possible. Maintaining consistency of the temporal and hierarchical structure of an annotation under updates requires further consideration. In the worst case, an entire annotation structure would have to be validated after each update. To the extent that information can be localized, it is to be expected that incremental validation will be possible. This might apply after each and every update, or after a collection of updates in case there is a sequence of elementary updates which unavoidably takes us to an invalid structure along the way to a ﬁnal, valid structure. Our approach to the ﬁle encoding has some interesting implications in the area of so-called standoff markup [37]. Under our proposed scheme, a readonly ﬁle containing a reference annotation can be concatenated with a ﬁle containing additional annotation material. In order for the new material to be linked to the existing material, it simply has to reuse the same node identiﬁers andor have nodes anchored to the same time base. Annotation deltas can employ a diff method operating at the level of individual arcs. Since the ﬁle contains one line per arc and since arcs are unordered, no context needs to be speciﬁed other than the line which is being replaced or modiﬁed. A consequence of our approach is that all speech annotation (in the broad sense) can be construed as standoff description. 36 A Formal Framework for Linguistic Annotation 5 Indexing Corpora of annotated texts and recorded signals may range in size from a few thousand words up into the billions. The data may be in the form of a monolithic ﬁle, or it may be cut up into word-size pieces, or anything in between. The annotation might be dense as in phonetic markup or sparse as in discourse markup, and the information may be uniformly or sporadically distributed through the data. At present, the annotational components of most speech databases are still relatively small objects. Only the largest annotations would cover a whole hour of speech (or 12,000 words at 200 words per minute), and even then, a dense annotation of this much material would only occupy a few hundred kilobytes. In most cases, serial search of such annotations will sufﬁce. Ultimately, however, it will be necessary to devise indexing schemes; these will necessarily be application-speciﬁc, depending on the nature of the corpus and of the queries to be expressed. The indexing method is not a property of the query language but a way to make certain kinds of query run efﬁciently. For large corpora, certain kinds of query might be essentially useless without such indexing. At the level of individual arc labels, we envision three simple indexes, corresponding to the three obvious dimensions of an annotation graph: a time-local index, a type-local index and a hierarchy-local index. These are discussed below. More sophisticated indexing schemes could surely be devised, for instance to support proximity search on node labels. We also assume the existence of an index for node identiﬁers; a simple approach would be to sort the lines of the annotation ﬁle with respect to an ordering on the node identiﬁers. Note that, since we wish to index linguistic databases, and not queries or query results, the indexes will assume that annotation graphs are anchored. 5.1 A time-local index We index the annotation graph in terms of the intervals it employs. Let ri  R be the sequence of time references used by the annotation. We form the intervals [ri, ri1). Next, we assign each arc to a contiguous set of these intervals. Suppose that an arc is incident on nodes which are anchored to time points rp and rq, where rp  rq. Then we assign the arc to the following set of intervals: {[rp, rp1), [rp1, rp2), . . . , [rq1, rq)} Now we generalize this construction to work when a time reference is missing from either or both of the nodes. First we deﬁne the greatest lower bound (glb) and the least upper bound (lub) of an arc. Deﬁnition 7 Let a  n1, l, n2 be an arc. glb(a) is the greatest time reference r  R such that there is some node n with τ(n)  r and n s n1. lub(a) is the least time reference r  R such that there is some node n with τ(n)  r and n2 s n. According to this deﬁnition, the glb of an arc is the time mark of the greatest anchored node from which the arc is reachable. Similarly, the lub of an arc is the time mark of the least anchored node reachable from that arc. If a  n1, l, n2 is anchored at both ends then glb(a)  n1 and lub(a)  n2. The glb and lub are guaranteed to exist for anchored annotation graphs (but not for annotation graphs in general). The glb and lub are guaranteed to be unique since R is a total ordering. We can take the potential temporal span of an arc a to be [glb(a), lub(a)). We then assign the arc to a set of intervals as before. Below we give an example time-local index for the UTF annotation from Figure 8. MS-CIS-99-01: Bird  Liberman 37 2348.81 2391.11 12 Wthis 132391.11 112348.81 speakerRoger-Hedgecock 142391.60 112348.81 spkrtypemale 142391.60 2391.11 2391.29 132391.11 Wcountry 142391.60 112348.81 speakerRoger-Hedgecock 142391.60 112348.81 spkrtypemale 142391.60 2391.29 2391.60 132391.11 Wcountry 142391.60 22 Wi 232391.60 213291.29 Wwell 22 213291.29 speakerGloria-Allred 252439.82 112348.81 speakerRoger-Hedgecock 142391.60 213291.29 spkrtypefemale 252439.82 112348.81 spkrtypemale 142391.60 2391.60 2439.82 213291.29 speakerGloria-Allred 252439.82 213291.29 spkrtypefemale 252439.82 232391.60 Wthink 24 The index is built on a sequence of four temporal intervals which are derived from the ﬁve time references used in Figure 8. Observe that the right hand side of the index is made up of fully-ﬂedged arcs (sorted lexicographically), rather than references to arcs. Using the longer, fully-ﬂedged arcs has two beneﬁts. First, it localizes the arc information on disk for fast access. Second, the right hand side is a well- formed annotation graph which can be directly processed by the same tools used by the rest of any implementation, or used as a citation. This time-local index can be used for computing general overlap and inclusion relations. To ﬁnd all arcs overlapping a given arc p, we iterate through the list of time-intervals comprising p and collect up the arcs found in the time-local index for each such interval. Additional checks can be performed to see if a candidate arc is s-overlapped or t-overlapped. This process, or parts of it, could be done ofﬂine. To ﬁnd all arcs included in a given arc p, we can ﬁnd the overlapping arcs and perform the obvious tests for s-inclusion or t-inclusion. Again, this process could be done ofﬂine. An interesting property of the time-local index is that it is well-behaved when time information is partial. 5.2 Type-local indexes Continuing in the same vein as the time-local index we propose a set of self-indexing structures for the types  one for each type. The arcs of an annotation graph are then partitioned into types. The index for each type is a list of arcs, sorted as follows (c.f. [29]): 1. of two arcs, the one bearing the lexicographically earlier label appears ﬁrst; 2. if two arcs share the same label, the one having the least glb appears ﬁrst; 3. if two arcs share the same label and have the same glb, then the one with the larger lub appears ﬁrst. 38 A Formal Framework for Linguistic Annotation W country 132391.11 Wcountry 142391.60 i 22 Wi 232391.60 think 232391.60 Wthink 24 this 12 Wthis 132391.11 well 213291.29 Wwell 22 speaker Gloria-Allred 213291.29 speakerGloria-Allred 252439.82 Roger-Hedgecock 112348.81 speakerRoger-Hedgecock 142391.60 spkrtype female 213291.29 spkrtypefemale 252439.82 male 112348.81 spkrtypemale 142391.60 5.3 A hierarchy-local index Annotations also need to be indexed with respect to their implicit hierarchical structure (c.f. 3.2). Recall that we have two kinds of inclusion relation, s-inclusion (respecting graph structure) and t-inclusion (respecting temporal structure). We reﬁne these relations to be sensitive to an ordering on our set of types T. This ordering has been left external to the formalism, since it does not ﬁt easily into the ﬂat structure described in 4.5. We assume the existence of a function type(p) returning the type of an arc p. Deﬁnition 8 An arc p s-dominates an arc q, written n1 s n2, if type(p)  type(q) and p s q. An arc p t-dominates an arc q, written n1 t n2, if type(p)  type(q) and p t q. Again, we can deﬁne a dominance relation which is neutral between these two, as follows: Deﬁnition 9 An arc p dominates an arc q, written n1  n2, if type(p)  type(q) and p  q. In our current conception, s-dominance will be the most useful. (The three kinds of dominance were included for generality and consistency with the preceding discussion.) We now illustrate an index for s-dominance. Suppose the ordering on types is: speaker  W and spkrtype  W. We could index the UTF example as follows, ordering the arcs using the method described in 5.2, and using indentation to distinguish the dominating arcs from the dominated arcs. 112348.81 speakerRoger-Hedgecock 142391.60 112348.81 spkrtypemale 142391.60 213291.29 Wwell 22 22 Wi 232391.60 232391.60 Wthink 24 213291.29 speakerGloria-Allred 252439.82 213291.29 spkrtypefemale 252439.82 12 Wthis 132391.11 132391.11 Wcountry 142391.60 This concludes the discussion of proposed indexes. We have been deliberately schematic, aiming to demonstrate a range of possibilities which can be reﬁned and extended later. Note that the various indexing schemes described above just work for a single annotation. We would need to enrich the node-id and time reference information in order for this to work for whole databases of annotations (see 6.2). It could then be generalized further, permitting search across multiple databases  e.g. to ﬁnd all instances of a particular word in both the Switchboard and CallHome English databases (c.f. 2.6). MS-CIS-99-01: Bird  Liberman 39 Many details about indexes could be application speciﬁc. Under the approach described here, we can have several copies of an annotation where each is self-indexing in a way that localizes different kinds of information. A different approach would be to provide three categories of iterators, each of which takes an arc and returns the next arc with respect to the temporal, sortal and hierarchical structure of an annotation. It would be the task of any implementation to make sure that the basic encoding is consistent with itself, and that the conglomerate structure (basic encoding plus indexes) is consistent. More broadly, the design of an application-speciﬁc indexing scheme will have to consider what kinds of sequences or connections among tokens are indexed. In general, the indexing method should be based on the same elementary structures from which queries are constructed. Indices will specify where particular elementary annotation graphs are to be found, so a complex search expression can be limited to those regions for which these graphs are necessary parts. 6 Conclusions and Future Work 6.1 Evaluation criteria There are many existing approaches to linguistic annotation, and many options for future approaches. Any evaluation of proposed frameworks, including ours, depends on the costs and beneﬁts incurred in a range of expected applications. Our explorations have presupposed a particular set of ideas about applications, and therefore a particular set of goals. We think that these ideas are widely shared, but it seems useful to make them explicit. Here we are using framework as a neutral term to encompass both the deﬁnition of the logical structure of annotations, as discussed in this paper, as well as various further speciﬁcations of e.g. annotation conventions and ﬁle formats. Generality, speciﬁcity, simplicity Annotations should be publishable (and will often be published), and thus should be mutually intelligible across laboratories, disciplines, computer systems, and the passage of time. Therefore, an annotation framework should be sufﬁciently expressive to encompass all commonly used kinds of linguistic annotation, including sensible variants and extensions. It should be capable of managing a variety of (partial) information about labels, timing, and hierarchy. The framework should also be formally well-deﬁned, and as simple as possible, so that researchers can easily build special-purpose tools for unforeseen applications as well as current ones, using future technology as well as current technology. Searchability and browsability Automatic extraction of information from large annotation databases, both for scientiﬁc research and for technological development, is a key application. Therefore, annotations should be conveniently and efﬁciently searchable, regardless of their size and content. It should be possible to search across annotations of different material produced by 40 A Formal Framework for Linguistic Annotation different groups at different times  if the content permits it  without having to write special programs. Partial annotations should be searchable in the same way as complete ones. This implies that there should be an efﬁcient algebraic query formalism, whereby complex queries can be composed out of well-deﬁned combinations of simple ones, and that the result of querying a set of annotations should be just another set of annotations. This also implies that (for simple queries) there should be efﬁcient indexing schemes, providing near constant-time access into arbitrarily large annotation databases. The framework should also support easy projection of natural sub-parts or dimensions of anno- tations, both for searching and for display purposes. Thus a user might want to browse a complex multidimensional annotation database  or the results of a preliminary search on one  as if it contained only an orthographic transcription. Maintainability and durability Large-scale annotations are both expensive to produce and valuable to retain. However, there are always errors to be ﬁxed, and the annotation process is in principle open-ended, as new properties can be annotated, or old ones re-done according to new principles. Experience suggests that maintenance of linguistic annotations, especially across distributed edits and additions, can be a vexing and expensive task. Therefore, any framework should facilitate maintenance of coherence in the face of distributed development and correction of annotations. Different dimensions of annotation should therefore be orthogonal, in the sense that changes in one dimension (e.g. phonetic transcription) do not entail any change in others (e.g. discourse transcription), except insofar as the content necessarily overlaps. Annotations of temporally separated material should likewise be modular, so that revisions to one section of an annotation do not entail global modiﬁcation. Queries not affected by corrections or additions should return the same thing before and after an update. In order to facilitate use in scientiﬁc discourse, it should be possible to deﬁne durable references which remain valid wherever possible, and produce the same results unless the referenced material itself has changed. Note that it is easy enough to deﬁne an invertible sequence of editing operations for any way of representing linguistic annotations  e.g. by means of Unix diff  but what we need in this case is also a way to specify the correspondence (wherever it remains deﬁned) between arbitrary bits of annotation before and after the edit. Furthermore, we do not want to impose any additional burden on human editors  ideally, the work minimally needed to implement a change should also provide any bookkeeping needed to maintain correspondences. How well does our proposal satisfy these criteria? We have tried to demonstrate generality, and to provide an adequate formal foundation, which is also ontologically parsimonious (if not positively miserly!). Although we have not deﬁned a query system, we have indicated the basis on which one can be constructed: (tuple sets constituting) annotation graphs are closed under union, intersection and relative complementation; the set of subgraphs of an annotation graph is simply the power set of its constituent tuples; simple pattern matching on an annotation graph can be deﬁned to produce a set of annotation MS-CIS-99-01: Bird  Liberman 41 subgraphs; etc. Obvious sorts of simple predicates on temporal relations, graphical relations, label types, and label contents will clearly ﬁt into this framework. The foundation for maintainability is present: fully orthogonal annotations (those involving different label types and time points) do not interact at all, while linked annotations (such as those that share time points) are linked only to the point that their content requires. New layers of annotation can be added monotonically, without any modiﬁcation whatsoever in the representation of existing layers. Corrections to existing annotations are as representationally local as they can be, given their content. Although we have not provided a recipe for durable citations (or for maintenance of trees of invertible modiﬁcations), the properties just cited will make it easier to develop practical approaches. In particular, the relationship between any two stages in the development or correction of an annotation will always be easy to compute as a set of basic operations on the tuples that express an annotation graph. This makes it easy to calculate just the aspects of a tree or graph of modiﬁcations that are relevant to resolving a particular citation. 6.2 Future work Interactions with relational data Linguistic databases typically include important bodies of information whose structure has nothing to do with the passage of time in any particular recording, nor with the sequence of characters in any particular text. For instance, the Switchboard corpus includes tables of information about callers (including date of birth, dialect area, educational level, and sex), conversations (including the speakers involved, the date, and the assigned topic), and so on. This side information is usually well expressed as a set of relational tables. There also may be bodies of relevant information concerning a language as a whole rather than any particular speech or text database: lexicons and grammars of various sorts are the most obvious examples. The relevant aspects of these kinds of information also often ﬁnd natural expression in relational terms. Users will commonly want to frame queries that combine information of these kinds with predicates deﬁned on annotation graphs: ﬁnd me all the phrases ﬂagged as questions produced by South Midland speakers under the age of 30. The simplest way to permit this is simply to identify (some of the) items in a relational database with (some of the) labels in an annotation. This provides a limited, but useful, method for using the results of certain relational queries in posing an annotational query, or vice versa. More complex modes of interaction are also possible, as are connections to other sorts of databases; we regard this as a fruitful area for further research. Generalizing time marks to an arbitrary ordering We have focused on the case of audio or video recordings, where a time base is available as a natural way to anchor annotations. This role of time can obviously be reassigned to any other well-ordered single dimension. The most obvious case is that of character- or byte-offsets into an invariant text ﬁle. This is the principle used in the so-called Tipster Architecture [19], where all annotations are associated with stretches of an underlying text, identiﬁed via byte offsets into a ﬁxed ﬁle. We do not think that this 42 A Formal Framework for Linguistic Annotation method is normally appropriate for use with audio transcriptions, because they are so often subject to revision. Generalizing node identiﬁers and arc labels As far as the annotation graph formalism is concerned, node identiﬁers, arc types, and arc labels are just sets. As a practical matter, members of each set would obviously be individuated as strings. This opens the door to applications which encode arbitrary information in these strings. Indeed, the notion that arc labels encode external information is fundamental to the enterprise. The whole point of the annotations is to include strings interpreted as orthographic words, speaker names, phonetic segments, ﬁle references, or whatever. These interpretations are not built into the formalism, however, and this is an equally important trait, since it determines the simplicity and generality of the framework. In the current formalization, arcs are decorated with pairs consisting of a type and a label. This structure already contains a certain amount of complexity, since the simplest kind of arc decoration would be purely atomic. In this case, we are convinced that the added value provided by label types is well worth the cost: all the bodies of annotation practice that we surveyed had some structure that was naturally expressed in terms of atomic label types, and therefore a framework in which arc decorations were just single uninterpreted strings  zeroth order labels  would not be expressively adequate. A ﬁrst-order approach is to allow arcs to carry multiple attributes and values  what amounts to a ﬁelded record. The current formalization can be seen as providing records with just two ﬁelds. It is easy to imagine a wealth of other possible ﬁelds. Such ﬁelds could identify the original annotator and the creation date of the arc. They could represent the conﬁdence level of some other ﬁeld. They could encode a complete history of successive modiﬁcations. They could provide hyperlinks to supporting material (e.g. chapter and verse in the annotators manual for a difﬁcult decision). They could provide equivalence class identiﬁers (as a ﬁrst-class part of the formalism rather than by the external convention as in 3.5). And they could include an arbitrarily-long SGML-structured commentary. In principle, we could go still further, and decorate arcs with arbitrarily nested attribute-value matrices (AVMs) endowed with a type system [10]  a second-order approach. These AVMs could contain references to other parts of the annotation, and multiple AVMs could contain shared substructures. Substructures could be disjoined to represent the existence of more than one choice, and where sepa- rate choices are correlated the disjunctions could be coindexed (i.e. parallel disjunction). Appropriate attributes could depend on the local type information. A DTD-like label grammar could specify available label types, their attributes and the type ordering discussed in 5.3. We believe that this is a bad idea: it negates the effort that we made to provide a simple formalism expressing the essential contents of linguistic annotations in a natural and consistent way. Typed feature structures are also very general and powerful devices, and entail corresponding costs in algorithmic and implementational complexity. Therefore, we wind up with a less useful representation that is much harder to compute with. Consider some of the effort that we have put into establishing a simple and consistent ontology for annotation. In the CHILDES case (2.3), we split a sentence-level annotation into a string of word-level annotations for the sake of simplifying word-level searches. In the Festival case (2.9) we modeled hierarchical information using the syntactic chart construction. Because of these choices, CHILDES and Festival annotations become formally commensurate  they can be searched or displayed in exactly the same terms. With labels as typed feature structures, whole sentences, whole tree structures, and MS-CIS-99-01: Bird  Liberman 43 indeed whole databases could be packed into single labels. We could therefore have chosen to translate CHILDES and Festival formats directly into typed feature structures. If we had done this, however, the relationship between simple concepts shared by the two formats  such as lexical tokens and time references  would remain opaque. For these reasons, we would like to remain cautious about adding to the ontology of our formalism. However, several simple extensions seem well worth considering. Perhaps the simplest one is to add a single additional ﬁeld to arc decorations, called the comment, which would be formally uninterpreted, but could be used in arbitrary (and perhaps temporary) ways by implementations. It could be used to add commentary, or to encode the authorship of the label, or indicate who has permission to edit it, or in whatever other way. Another possibility would be to add a ﬁeld for encoding equivalence classes of arcs directly, rather than by the indirect means speciﬁed earlier. Our preference is to extend the formalism cautiously, where it seems that many applications will want a particular capability, and to offer a simple mechanism to permit local or experimental extensions, while advising that it be used sparingly. Finally, we note in passing that the same freedom for enriching arc labels applies to node identiﬁers. We have not given any examples in which node identiﬁers are anything other than digit strings. However, as with labels, in the general case a node identiﬁer could encode an arbitrarily complex data structure. For instance, it could be used to encode the source of a time reference, or to give a variant reference (such as a video frame number, c.f. 3.8), or to specify whether a time reference is missing because it is simply not known or it is inappropriate (c.f. 2.3, 2.4). Unlike the situation with arc labels, this step is always harmless (except that implementations that do not understand it will be left in the dark). Only string identity matters to the formalism, and node identiﬁers do not (in our work so far) have any standard interpretation outside the formalism. 6.3 Software We have claimed that annotation graphs can provide an interlingua for varied annotation databases, a formal foundation for queries on such databases, and a route to easier development and maintenance of such databases. Delivering on these promises will require software. Since we have made only some preliminary explorations so far, it would be best to remain silent on the question until we have some experience to report. However, for those readers who agree with us that this is an essential point, we will sketch our current perspective. As our catalogue of examples indicated, it is fairly easy to translate between other speech database formats and annotation graphs, and we have already built translators in several cases. We are also experimenting with simple software for creation, visualization, editing, validation, indexing, and search. Our ﬁrst goal is an open collection of relatively simple tools that are easy to prototype and to modify, in preference to a monolithic annotation graph environment. However, we are also committed to the idea that tools for creating and using linguistic annotations should be widely accessible to computationally unsophisticated users, which implies that eventually such tools need to be encapsulated in reliable and simple interactive form. Other researchers have also begun to experiment with the annotation graph concept as a basis for their software tools, and a key index of the ideas success will of course be the extent to which tools are provided by others. 44 A Formal Framework for Linguistic Annotation Visualization, creation, editing Existing open-source software such as Transcriber [3], Snack [32], and the ISIP transcriber tool [www. isip.msstate.eduresourcessoftware], whose user interfaces are all implemented in Tcltk, make it easy to create interactive tools for creation, visualization, and editing of annotation graphs. For instance, Transcriber can be used without any changes to produce transcriptions in the LDC Broad- cast News format, which can then be translated into annotation graphs. Provision of simple inputoutput functions enables the program to read and write annotation graphs directly. The architecture of the cur- rent tool is not capable of dealing with arbitrary annotation graphs, but generalizations in that direction are planned. Validation An annotation may need to be submitted to a variety of validation checks, for basic syntax, content and larger-scale structure. First, we need to be able to tokenize and parse an annotation, without having to write new tokenizers and parsers for each new task. We also need to undertake some superﬁcial syntax checking, to make sure that brackets and quotes balance, and so on. In the SGML realm, this need is partially met by DTDs. We propose to meet the same need by developing conversion and creation tools that read and write well-formed graphs, and by inputoutput modules that can be used in the further forms of validation cited below. Second, various content checks need to be performed. For instance, are purported phonetic segment labels actually members of a designated class of phonetic symbols or strings? Are things marked as non-lexemic vocalizations drawn from the ofﬁcially approved list? Do regular words appear in the spell-check dictionary? Do capital letters occur in legal positions? These checks are not difﬁcult to implement, e.g. as Perl scripts, especially given a module for handling basic operations correctly. Finally, we need to check for correctness of hierarchies of arcs. Are phonetic segments all inside words, which are all inside phrases, which are all inside conversational turns, which are all inside conversations? Again, it is easy to deﬁne such checks in a software environment that has appropriately expressive primitives (e.g. a Perl annotation graph module). Indexing and Search Indexing of the types discussed earlier (5), is well deﬁned, algorithmically simple, and easy to imple- ment in a general way. Construction of general query systems, however, is a matter that needs to be explored more fully in order to decide on the details of the query primitives and the methods for building complex queries, and also to try out different ways to express queries. Among the many questions to be explored are: 1. how to express general graph- and time-relations; 2. how to integrate regular expression matching over labels; 3. how to integrate annotation-graph queries and relational queries; MS-CIS-99-01: Bird  Liberman 45 4. how to integrate lexicons and other external resources; 5. how to model sets of databases, each of which contains sets of annotation graphs, signals and perhaps relational side-information. It is easy to come up with answers to each of these questions, and it is also easy to try the answers out, for instance in the context of a collection of Perl modules providing the needed primitive operations. We regard it as an open research problem to ﬁnd good answers that interact well, and also to ﬁnd good ways to express queries in the system that those answers will deﬁne. 6.4 Envoi Whether or not our ideas are accepted by the various research communities who create and use linguistic annotations, we hope to foster discussion and cooperation among members of these communities. A focal point of this effort is the Linguistic Annotation Page at [www.ldc.upenn.eduannotation]. When we look at the numerous and diverse forms of linguistic annotation documented on that page, we see underlying similarities that have led us to imagine general methods for access and search, and shared tools for creation and maintenance. We hope that this discussion will move others in the same direction. 7 Acknowledgements An earlier version of this paper was presented at ICSLP-98. We are grateful to the following people for discussions which have helped clarify our ideas about annotations, and for comments on earlier drafts: Peter Buneman, Steve Cassidy, Chris Cieri, Hamish Cunningham, David Graff, Ewan Klein, Brian MacWhinney, Boyd Michailovsky, Florian Schiel, Richard Sproat, Paul Taylor, Henry Thompson, Peter Wittenburg, Jonathan Wright, and participants of the COCOSDA workshop at ICSLP-98. 46 A Formal Framework for Linguistic Annotation References [1] T. Altosaar, M. Karjalainen, M. Vainio, and E. Meister. Finnish and Estonian speech applications developed on an object-oriented speech processing and database system. In Proceedings of the First International Conference on Language Resources and Evaluation Workshop: Speech Database Development for Central and Eastern European Languages, 1998. Granada, Spain, May 1998. [2] A. Anderson, M. Bader, E. Bard, E. Boyle, G. M. Doherty, S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo, H. Thompson, and R. Weinert. The HCRC Map Task corpus. Language and Speech, 34:35166, 1991. [3] Claude Barras, Edouard Geoffrois, Zhibiao Wu, and Mark Liberman. Transcriber: a free tool for segmenting, labelling and transcribing speech. In Proceedings of the First International Conference on Language Resources and Evaluation, 1998. [4] Steven Bird. Computational Phonology: A Constraint-Based Approach. Studies in Natural Language Processing. Cambridge University Press, 1995. [5] Steven Bird. A lexical database tool for quantitative phonological research. In Proceedings of the Third Meeting of the ACL Special Interest Group in Computational Phonology. Association for Computational Linguistics, 1997. [6] Steven Bird and Ewan Klein. Phonological events. Journal of Linguistics, 26:3356, 1990. [7] Steven Bird and D. Robert Ladd. Presenting autosegmental phonology. Journal of Linguistics, 27:193210, 1991. [8] Catherine Browman and Louis Goldstein. Articulatory gestures as phonological units. Phonology, 6:20151, 1989. [9] Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, Robert L. Mercer, and Paul S. Roossin. A statistical approach to machine translation. Computational Linguistics, 16:7985, 1990. [10] Bob Carpenter. The Logic of Typed Feature Structures, volume 32 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1992. [11] Steve Cassidy and Jonathan Harrington. Emu: An enhanced hierarchical speech data management system. In Proceedings of the Sixth Australian International Conference on Speech Science and Technology, 1996. [www.shlrc.mq.edu.auemu]. [12] Giuseppe Di Battista, Peter Eades, Roberto Tamassia, and Ioannis G. Tollis. Algorithms for drawing graphs: an annotated bibliography. [wilma.cs.brownedupubpaperscompgeogdbiblio.ps.gz], 1994. [13] Laila Dybkjær, Niels Ole Bernsen, Hans Dybkjær, David McKelvie, and Andreas Mengel. The mate markup framework. MATE Deliverable D1.2, Odense University, 1998. MS-CIS-99-01: Bird  Liberman 47 [14] Konrad Ehlich. HIAT  a transcription system for discourse data. In Jane A. Edwards and Martin D. Lampert, editors, Talking Data: Transcription and Coding in Discourse Research, pages 12348. Hillsdale, NJ: Erlbaum, 1992. [15] John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathon G. Fiscus, David S. Pallett, and Nancy L. Dahlgren. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM. NIST, 1986. [www.ldc.upenn.eduloldocsTIMIT.html]. [16] Gerald Gazdar and Chris Mellish. Natural Language Processing in Prolog: An Introduction to Computational Linguistics. Addison-Wesley, 1989. [17] J. J. Godfrey, E. C. Holliman, and J. McDaniel. Switchboard: A telephone speech corpus for research and develpment. In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, volume I, pages 51720, 1992. [18] S. Greenberg. The switchboard transcription project. LVCSR Summer Research Workshop, Johns Hopkins University, 1996. [19] R. Grishman. TIPSTER Architecture Design Document Version 2.3. Technical report, DARPA, 1997. [www.nist.govitldiv894894.02related projectstipster]. [20] Jonathan Harrington, Steve Cassidy, Janet Fletcher, and A. McVeigh. The Mu speech database system. Computer Speech and Language, 7:30531, 1993. [21] Susan R. Hertz. The delta programming language: an integrated approach to nonlinear phonology, phonetics, and speech synthesis. In John Kingston and Mary E. Beckman, editors, Papers in Laboratory Phonology I: Between the Grammar and Physics of Speech, chapter 13, pages 21557. Cambridge University Press, 1990. [22] Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg, Andreas Stolcke, Paul Taylor, and Carol Van Ess-Dykema. Automatic detection of discourse structure for speech recognition and understanding. In Proceedings of the 1997 IEEE Workshop on Speech Recognition and Understanding, pages 8895, Santa Barbara, 1997. [23] Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca. Switchboard SWBD-DAMSL Labeling Project Coders Manual, Draft 13. Technical Report 97-02, University of Colorado Institute of Cognitive Science, 1997. [stripe.colorado.edujurafskymanual.august1.html]. [24] Brian MacWhinney. The CHILDES Project: Tools for Analyzing Talk. Mahwah, NJ: Lawrence Erlbaum., second edition, 1995. [poppy.psy.cmu.educhildes]. [25] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):31330, 1993. www.cis.upenn.edu treebankhome.html. [26] Boyd Michailovsky, John B. Lowe, and Michel Jacobson. Linguistic data archiving project. [lacito.vjf.cnrs.frARCHIVAGENGLISH.htm]. [27] Carol Neidle and D. MacLaughlin. SignStreamTM: a tool for linguistic research on signed languages. Sign Language and Linguistics, 1:11114, 1998. [web.bu.eduasllrpSignStream]. 48 A Formal Framework for Linguistic Annotation [28] NIST. A universal transcription format (UTF) annotation speciﬁcation for evaluation of spoken language technology corpora. [www.nist.govspeechhub4 98utf-1.0-v2.ps], 1998. [29] Ron Sacks-Davis, Tuong Dao, James A. Thom, and Justin Zobel. Indexing documents for queries on structure, content and attributes. In International Symposium on Digital Media Information Base, pages 23645, 1997. [30] Emanuel Schegloff. Reﬂections on studying prosody in talk-in-interaction. Language and Speech, 41:23560, 1998. www.sscnet.ucla.edusocfacultyschegloffprosody. [31] Florian Schiel, Susanne Burger, Anja Geumann, and Karl Weilhammer. The Partitur format at BAS. In Proceedings of the First International Conference on Language Resources and Evaluation, 1998. [www.phonetik.uni-muenchen.deBasBasFormatseng.html]. [32] Kare Sjolander, Jonas Beskow, Joakim Gustafson, Erland Lewin, Rolf Carlson, and Bjorn Granstrom. Web-based educational tools for speech technology. In ICSLP-98, 1998. [33] Ann Taylor. Dysﬂuency Annotation Stylebook for the Switchboard Corpus. University of Pennsylvania, Department of Computer and Information Science, 1995. [ftp.cis.upenn.edupubtreebankswbddocDFL-book.ps]. [34] Paul A. Taylor, Alan W. Black, and Richard J. Caley. The architecture of the Festival speech synthesis system. In Third International Workshop on Speech Synthesis, Sydney, Australia, November 1998. [35] Paul A. Taylor, Alan W. Black, and Richard J. Caley. Heterogeneous relation graphs as a mechanism for representing linguistic information. [www.cstr.ed.ac.ukpublicationsnewdraftTaylor draft a.ps], 1999. [36] Text Encoding Initiative. Guidelines for Electronic Text Encoding and Interchange (TEI P3). Oxford University Computing Services, 1994. [www.uic.eduorgstei]. [37] Henry S. Thompson and David McKelvie. Hyperlink semantics for standoff markup of read-only documents. In SGML Europe 97, 1997. [www.ltg.ed.ac.ukhtsgmleu97.html].",
  "28.pdf": "arXiv:cs9903008v1 [cs.CL] 5 Mar 1999 Empirically Evaluating an Adaptable Spoken Dialogue System Diane J. Litman1 and Shimei Pan2 1 ATT Labs - Research, Florham Park, NJ, USA 2 Computer Science Department, Columbia University, New York, NY, USA Abstract. Recent technological advances have made it possible to build real-time, inter- active spoken dialogue systems for a wide variety of applications. However, when users do not respect the limitations of such systems, performance typically degrades. Although users differ with respect to their knowledge of system limitations, and although different dialogue strategies make system limitations more apparent to users, most current systems do not try to improve performance by adapting dialogue behavior to individual users. This paper presents an empirical evaluation of TOOT, an adaptable spoken dialogue system for retrieving train schedules on the web. We conduct an experiment in which 20 users carry out 4 tasks with both adaptable and non-adaptable versions of TOOT, resulting in a corpus of 80 dialogues. The values for a wide range of evaluation measures are then extracted from this corpus. Our results show that adaptable TOOT generally outperforms non-adaptable TOOT, and that the utility of adaptation depends on TOOTs initial dialogue strategies. 1 Introduction Recent technological advances have made it possible to build real-time, interactive spoken di- alogue systems for a wide variety of applications. However, due to limitations in such areas as vocabulary, grammar, and application functionality, most deployed systems can only accept a small set of natural language inputs from users. When users do not respect the limitations of spoken dialogue systems, system performance typically degrades. For example, user utterances are more likely to be rejected or misunderstood, dialogue length typically increases, and users are less likely to achieve their task goals. Although individual users differ with respect to their knowledge of system limitations (Kamm et al., 1998), and although different dialogue strate- gies make system limitations more apparent to users (Walker et al., 1997a; Danieli and Gerbino, 1995), current spoken dialogue systems typically do not try to adapt their dialogue strategies to individual users. A major obstacle to having a spoken dialogue system perform such adaptation dynamically is that there is currently little understanding of the type of properties that would need to be represented in a user model for this task, let alone automatically acquired during the course of a dialogue. Nevertheless, users do seem to intuitively know when and how their in- teractions with a spoken dialogue system could be changed, in order to improve overall system performance. This observation has led us to explore the utility of an adaptable spoken dialogue system, where the user rather than the system controls the adaptation of the systems dialogue strategies. In particular, we hypothesize that if a user can effectively adapt the systems behavior, so that the system will use the dialogue strategies that best match the users needs at any point in a dialogue, then the overall performance of the system should improve.  We thank J. Chu-Carroll, C. Kamm, D. Lewis, M. Walker, and S. Whittaker for helpful comments. Figure 1 shows an example dialogue with an adaptable train-timetable spoken dialogue sys- tem called TOOT.1 In this excerpt, TOOT initially uses a set of dialogue strategies often found U1: Id like to go to Chicago from Baltimore on Saturday at 8 am. T1: Do you want to leave in the morning or in the evening? U2: Uh morning. T2: Sorry, I cant understand you. Please repeat your answer. U3: In the morning. T3: Do you want me to ﬁnd the trains from Baltimore to Chicago on Saturday around 10 morning now? U4: No. T4: How may I help you? U5: I would like to go from Baltimore to Chicago on Saturday at 8 oclock. T5: Do you want me to ﬁnd the trains from Baltimore to DC on Tuesday around 11:45 am now? U6: No. T6: How may I help you? U7: Change strategy. T7: You are using the user initiative. Which initiative do you want to change to? System  U8: System initiative please. T8: You are using the no conﬁrmation strategy. Which conﬁrmation strategy do you want to change to? No conﬁrmation, implicit conﬁrmation, or explicit conﬁrmation? U9: Implicit please. T9: I heard you say go to DC on Tuesday around 11:45 am. Do you want me to ﬁnd the trains from Baltimore to DC on Tuesday around 11:45 am now? U10: No. T10: Which city do you want to go to? U11: Chicago. T11: I heard you say go to Chicago. Which city do you want to leave from? U12: Baltimore. T12: I heard you say leave from Baltimore. On which day of the week do you want to leave? U13: Saturday. T13: I heard you say leave on Saturday. What time do you want to leave? U14: 8:00 in the morning. T14: Sorry, I cant understand you. Please repeat your answer. U15: 8:00 in the morning. T15: I heard you say leave around 8:00 in the morning. Do you want me to ﬁnd the trains from Baltimore to Chicago on Saturday around 8:00 in the morning now? U16: Yes. Figure 1. Example dialogue with an adaptable spoken dialogue system. in human-human interactions. However, the strategies do not work particularly well for this user. Utterances U1 and U5 are misunderstood by TOOT, and U2 is not understood at all. Recog- nizing a mismatch, the user changes the strategies used by TOOT (U7 - U9). As a result, the open ended prompt that TOOT used earlier in the dialogue (T4) is now replaced with a series of speciﬁc questions (T10, T11, T12, and T13). TOOT also now highlights any potential misrecog- 1 This excerpt is taken from the experimental corpus described below. nitions by prefacing its utterances with I heard you say .... Earlier in the dialogue, it took 5 utterances before the user was aware that TOOT had misrecognized the 8 in U1 as 10. As a result of these changes, the dialogue proceeds more smoothly after the adaptation (e.g., TOOTs misrecognition rate is reduced), and a correct database query is soon generated (T15-U16). In this paper, we present an evaluation of adaptability in TOOT. We conduct an experiment in which 20 novice users carry out 4 tasks with one of two versions of TOOT (adaptable and non- adaptable), resulting in a corpus of 80 dialogues. The values for a range of evaluation measures are then extracted from this corpus. Hypothesis testing shows that a variety of differences depend on the users ability to adapt the system. A PARADISE assessment of the contribution of each evaluation measure to overall performance (Walker et al., 1997b) shows that the phenomena inﬂuenced by adaptation are also the major phenomena that signiﬁcantly inﬂuence performance. Our results show that adaptable TOOT generally outperforms non-adaptable TOOT, and that the utility of adaptation depends on the initial conﬁguration of dialogue strategies. 2 TOOT TOOT is a voice-enabled dialogue system for accessing train schedules from the web via a tele- phone conversation. TOOT is implemented using a spoken dialogue system platform (Kamm et al., 1997) that combines automatic speech recognition (ASR), text-to-speech (TTS), a phone in- terface, and modules for specifying a dialogue manager and application functions. ASR in our platform is speaker-independent, grammar-based and supports barge-in (which allows users to interrupt TOOT when it is speaking, as in utterances T7 and U8 in Figure 1). The dialogue man- ager uses a ﬁnite state machine to control the interaction, based on the current system state and ASR results. TOOTs application functions access train schedules available at www.amtrak.com. Given a set of constraints, the functions return a table listing all matching trains in a speciﬁed temporal interval, or within an hour of a speciﬁed timepoint. This table is converted to a natural language response which can be realized by TTS through the use of templates.2 Depending on the users needs during the dialogue, TOOT can use one of three dialogue strategies for managing initiative (system, mixed or user), and one of three strategies for managing conﬁrmation (explicit, implicit, or no). TOOTs initiative strategy speci- ﬁes who has control of the dialogue, while TOOTs conﬁrmation strategy speciﬁes how and whether TOOT lets the user know what it just understood. In Figure 1, TOOT initially used user initiative and no conﬁrmation, then later used system initiative and implicit conﬁrmation. The following fragments provide additional illustrations of how dialogues vary with strategy: System Initiative, Explicit Conﬁrmation User Initiative, No Conﬁrmation T: Which city do you want to go to? T: How may I help you? U: Chicago. U: I want to go to Chicago from Baltimore. T: Do you want to go to Chicago? T: On which day of the week do you want to leave? U: Yes. U: I want a train at 8:00. 2 The current version of TOOT uses a literal response strategy (Litman et al., 1998). Informally, if the returned table contains 1-3 trains, TOOT lists the trains; if the table contains greater than 4 trains, TOOT lists the trains 3 at a time; if the table is empty, TOOT reports that no trains satisfy the constraints. TOOT then asks the user if she wants to continue and ﬁnd a new set of trains. Although system initiative with explicit conﬁrmation is the most cumbersome approach, it can help improve some aspects of performance for users who do not have a good understanding of the systems limitations. The use of system initiative helps reduce ASR misrecognitions and rejections (Walker et al., 1997a), by helping to keep the users utterances within the systems vocabulary and grammar. The use of explicit conﬁrmation helps increase the users task suc- cess (Danieli and Gerbino, 1995), by making the user more aware of any ASR misrecognitions and making it easier for users to correct misrecognitions when they occur. On the other hand, system initiative and explicit conﬁrmation typically increase total dialogue length (Walker et al., 1997a; Danieli and Gerbino, 1995). For users whose utterances are generally understood, other strategies might be more effective. Consider the use of user initiative with no conﬁrmation, the most human-like approach. In user (as well as in mixed) initiative mode, TOOT can still ask the user speciﬁc questions, but can also ask open-ended questions such as How may I help you?. Furthermore, in user (but not in mixed) initiative mode, TOOT even lets the user ignore TOOTs questions (as in the last user utterance in the example above). By allowing users to specify multi- ple attributes in a single utterance, and by not informing users of every potential misrecognition, this approach can lead to very short dialogues when ASR performance is not a problem. In an earlier implementation of TOOT (as well as in other spoken dialogue systems that we have studied (Walker et al., 1997a; Kamm et al., 1998)), a set of initial dialogue strategies was assigned to the system as a default for each user, and could not be changed if inappropriate.3 As discussed above, however, we hypothesize that we can improve TOOTs performance by dynam- ically adapting the choice of dialogue strategies, based on the circumstances at hand. Although one of our long-term goals is to have TOOT automatically control the adaptation process, this would require that we ﬁrst solve several open research topics. For example, TOOT would need to be able to detect, in real time, dialogue situations suggesting system adaptation. As a result, our initial research has instead focused on giving users the ability to dynamically adapt TOOTs dialogue behaviors. For example, if a users utterances are not being understood, the user could try to reduce the number of ASR rejections and misrecognitions by changing the strategies so that TOOT would take more initiative. Conversely, if a users utterances are being correctly un- derstood, the user could try to decrease the dialogue length by having TOOT perform less conﬁr- mations. To allow us to test whether such an adaptable system does indeed increase performance, we have created both adaptable and non-adaptable versions of TOOT. In adaptable TOOT, users are allowed to say change strategy at any point(s) in the dialogue. TOOT then asks the user to specify new initiative and conﬁrmation strategies, as in utterances U7-U9 in Figure 1. In non-adaptable TOOT, the default dialogue strategies can not be changed. 3 Experimental Design Our experiment was designed to test if adaptable TOOT performed better than non-adaptable TOOT, and whether any differences depended on TOOTs initial dialogue strategies andor the users task. Our design thus consisted of three factors: adaptability, initial dialogue strategy, and task scenario. Subjects were 20 ATT technical summer employees not involved with the design or implementation of TOOT, who were also novice users of spoken dialogue systems in 3 In particular, the previous version of TOOT always used system initiative with implicit conﬁrmation (Lit- man et al., 1998). general. 10 users were randomly assigned to adaptable TOOT and 10 to non-adaptable TOOT. For each of these groups, 5 users were randomly assigned to a version of TOOT with the initial dialogue strategies set to system initiative and explicit conﬁrmation (SystemExplicit TOOT); the remaining 5 users were assigned to a version of TOOT with the initial dialogue strategies set to user initiative and no conﬁrmation (UserNo TOOT). Each user performed the same 4 tasks in sequence. Our experiment yielded a corpus of 80 dialogues (2633 turns; 5.4 hours of speech). Users used the web to read a set of experimental instructions in their ofﬁce, then called TOOT from their phone. The experimental instructions consisted of a description of TOOTs functionality, hints for talking to TOOT, and links to 4 task scenarios. An example task scenario is as follows: Try to ﬁnd a train going to Chicago from Baltimore on Saturday at 8 oclock am. If you cannot ﬁnd an exact match, ﬁnd the one with the closest departure time. Please write down the exact departure time of the train you found as well as the total travel time. The instructions for adaptable TOOT also contained a brief tutorial explaining how to use change strategy, and guidelines for doing so (e.g.,  if you dont know what to do or say, try system initiative). We collected three types of data to compute a number of measures relevant for spoken dia- logue evaluation (Walker et al., 1997a). First, all dialogues were recorded. The recordings were used to calculate the total time of each dialogue (the evaluation measure Elapsed Time), and to (manually) count how many times per dialogue each user interrupted TOOT (Barge Ins). Second, the dialogue managers behavior on entering and exiting each state in the ﬁnite state machine was logged. This log was used to calculate the total number of System Turns and User Turns, Timeouts (when the user doesnt say anything within a speciﬁed time frame, TOOT pro- vides suggestions about what to say ), Helps (when the user says help, TOOT provides a context-sensitive help message), Cancels (when the user says cancel, TOOT undoes its pre- vious action), and ASR Rejections (when the conﬁdence level of ASR is too low, TOOT asks the user to repeat the utterance). In addition, by listening to the recordings and comparing them to the logged ASR results, we calculated the concept accuracy (intuitively, semantic interpreta- tion accuracy) for each utterance. This was then used, in combination with ASR rejections, to compute a Mean Recognition score per dialogue. Third, users ﬁlled out a web survey after each dialogue. Users speciﬁed the departure and travel times that they obtained via the dialogue. Given that there was a single correct train to be retrieved for each task scenario, this allowed us to determine whether users successfully achieved their task goal or not (Task Success). Users also responded to the following questionnaire:  Was the system easy to understand? (TTS Performance)  Did the system understand what you said? (ASR Performance)  Was it easy to ﬁnd the schedule you wanted? (Task Ease)  Was the pace of interaction with the system appropriate? (Interaction Pace)  Did you know what you could say at each point of the dialogue? (User Expertise)  How often was the system sluggish and slow to reply to you? (System Response)  Did the system work the way you expected it to? (Expected Behavior)  From your current experience with using our system, do you think youd use this regularly to access train schedules when you are away from your desk? (Future Use) Each question measured a particular usability factor, e.g., TTS Performance. Responses ranged over n pre-deﬁned values (e.g., almost never, rarely, sometimes, often, almost always), and were mapped to an integer in 1 . . . 5 (with 5 representing optimal performance). User Satisfaction was computed by summing each questions score, and thus ranged in value from 8 to 40. 4 Results We use analysis of variance (ANOVA) (Cohen, 1995) to determine whether the adaptability of TOOT produces signiﬁcant differences in any of the evaluation measures for our experiment. We also use the PARADISE evaluation framework (Walker et al., 1997b) to understand which of our evaluation measures best predicts overall performance in TOOT. Following PARADISE, we organize our evaluation measures along the following four performance dimensions:  task success: Task Success  dialogue quality: Helps, ASR Rejections, Timeouts, Mean Recognition, Barge Ins, Cancels  dialogue efﬁciency: System Turns, User Turns, Elapsed Time  system usability: User Satisfaction (based on TTS Performance, ASR Performance, Task Ease, Interaction Pace, User Expertise, System Response, Expected Behavior, Future Use) 4.1 Adaptability Effects Recall that our mixed4 experimental design consisted of three factors: adaptability, initial dia- logue strategy, and task scenario. Each of our evaluation measures is analyzed using a three-way ANOVA for these three factors. The ANOVAs demonstrate a main effect of adaptability for the task success and system usability dimensions of performance. These main adaptability effects are independent of TOOTs initial dialogue strategy as well as of the task scenario being executed by the user. The ANOVAs also demonstrate interaction effects of adaptability and initial dialogue strategy for the dialogue quality and system usability performance dimensions. In contrast to the main effects, these adaptability effects are not independent of TOOTs initial dialogue strategy (i.e., the effects of adaptability and initial strategy are not additive).5 Table 1 summarizes the means for each evaluation measure that shows a main effect of adapt- ability, and that cannot be further explained by any interaction effects. The ﬁrst row in the table indicates that Task Success is signiﬁcantly higher for adaptable TOOT than for non-adaptable TOOT. Users successfully achieve the goals speciﬁed in the task scenario in 80 of the dialogues with adaptable TOOT, but in only 55 of the dialogues with non-adaptable TOOT. The probabil- ity p.03 indicates that the difference is statistically signiﬁcance (the standard upper bound for calling a result statistically signiﬁcant is p.05 (Cohen, 1995)). The second row indicates that with respect to User Satisfaction, users also rate adaptable TOOT more highly than non-adaptable TOOT. Recall that User Satisfaction takes all of the factors in the usability questionnaire into ac- count. As will be discussed below, PARADISE correlates overall system performance with this measure. In sum, our ANOVAs indicate that making TOOT adaptable increases users rates of task success as well as users perceptions of overall system usability. Table 2 summarizes the means for each evaluation measure that shows an interaction effect of adaptability and initial dialogue strategy. A similar pattern of interaction emerges in the ﬁrst and 4 Task scenario is between-groups and initial dialogue strategy and adaptability are within-group. 5 Effects of initial dialogue strategy and task scenario are beyond the scope of this paper. Table 1. Main effects of adaptability. Measure Non-Adaptable (n40) Adaptable (n40) Task Success () (p.03) 55.00 80.00 User Satisfaction (p.03) 26.68 31.60 Table 2. Interaction effects of adaptability and initial dialogue strategy. Non-Adaptable (n40) Adaptable (n40) Measure SystemExplicit UserNo SystemExplicit UserNo Mean Recognition () (p.01) 88.44 57.94 82.55 75.85 User Expertise (p.05) 4.69 3.01 4.45 3.85 Future Use (p.02) 3.50 1.70 3.60 3.80 second rows of the table. When users are given the capability to adapt TOOT, Mean Recognition decreases for SystemExplicit TOOT (88.44 versus 82.55) but increases for UserNo TOOT (57.94 versus 75.85). Perceptions of User Expertise also decrease for adaptable SystemEx- plicit TOOT (4.69 versus 4.45) but increase for adaptable UserNo TOOT (3.01 versus 3.85). In contrast, Future Use is higher for adaptable TOOT than for non-adaptable TOOT, for both initial strategies. Thus, users of adaptable TOOT are more likely than users of non-adaptable TOOT to think that they would use TOOT on a regular basis. However, the increase in Future Use is smaller for SystemExplicit TOOT (3.5 to 3.6) than for UserNo TOOT (1.7 to 3.8). In sum, Table 2 in- dicates that differences reﬂecting both dialogue quality and system usability are an effect of the interaction of the adaptability of TOOT and TOOTs initial dialogue strategy. For the UserNo version of TOOT, making TOOT adaptable increases the means for all of the measures shown in Table 2. For the SystemExplicit version of TOOT, despite the Mean Recognition and User Expertise results in Table 2, users are nevertheless at least if not more likely to use adaptable System Explicit TOOT in the future. We speculate that users are willing to tolerate minor levels of particular types of performance degradations in SystemExplicit TOOT, in order to obtain the sense of control provided by adaptability. We also speculate that the utility of adaptable Syste- mExplicit TOOT would increase for expert users. In conjunction with Table 1, our results with novice users suggest that adaptability is an extremely useful capability to add to UserNo TOOT, and a capability that is still worth adding to SystemExplicit TOOT. It is interesting to also examine the way in which adaptation is performed for each initial dialogue strategy. Of the 20 dialogues with adaptable SystemExplicit TOOT, 5 dialogues con- tained 1 adaptation and a 6th dialogue contained 2 adaptations. 3 of the 5 users adapted at least 1 dialogue, and overall, conﬁrmation was changed more times than initiative. Of the 20 dialogues with adaptable UserNo TOOT, 10 dialogues contained 1 adaptation and an 11th dialogue con- tained 2 adaptations. All 5 users of UserNo TOOT adapted at least 1 dialogue. Users of UserNo TOOT changed initiative more than they changed conﬁrmation, and also changed initiative more drastically. In conjunction with our ANOVA results, these observations lead us to speculate that adapting a poorly performing system is both more feasible and more important for novice users than adapting a reasonably performing system. 4.2 Contributors to Performance To quantify the relative importance of our multiple evaluation measures to performance, we use the PARADISE evaluation framework to derive a performance function from our data. The PAR- ADISE model posits that performance can be correlated with a meaningful external criterion of usability such as User Satisfaction. PARADISE then uses stepwise multiple linear regression to model User Satisfaction from measures representing the performance dimensions of task success, dialogue quality, and dialogue efﬁciency: User Satisfaction  n  i1 wi  N(measurei) Linear regression produces coefﬁcients (i.e., weights wi) describing the relative contribution of predictor factors in accounting for the variance in a predicted factor. In PARADISE, the task success and dialogue cost measures are predictors, while User Satisfaction is predicted. The nor- malization function N guarantees that the coefﬁcients directly indicate the relative contributions. The application of PARADISE to the TOOT data shows that the most signiﬁcant contributors to User Satisfaction are Mean Recognition, Task Success, and Elapsed Time, respectively. In addition, PARADISE shows that the following performance function provides the best ﬁt to our data, accounting for 55 of the variance in User Satisfaction:6 User Satisfaction  .45N(Mean Recognition)  .33N(Task Success)  .14N(Elapsed Time) Our performance function demonstrates that TOOT performance (estimated using subjective us- ability ratings) can be best predicted using a weighted combination of objective measures of dialogue quality, task success, and dialogue efﬁciency. In particular, more accurate speech recog- nition, more success in achieving task goals, and shorter dialogues all contribute to increasing perceived performance in TOOT. Our performance equation helps explain the main effect of adaptability for User Satisfaction that was shown in Table 1. Recall that our ANOVAs for both Mean Recognition and Task Suc- cess showed adaptability effects (Tables 2 and 1, respectively). Our PARADISE analysis showed that these measures were also the most important measures in explaining the variance in User Satisfaction. It is thus not surprising that User Satisfaction shows an effect of adaptability, with users rating the performance of adaptable TOOT more highly than non-adaptable TOOT. A result that was not apparent from the analysis of variance is that Elapsed Time is a perfor- mance predictor. However, the weighting of the measures in our performance function suggests that Mean Recognition and Task Success are more important measures of overall performance than Elapsed Time. These ﬁndings are consistent with our previous PARADISE evaluations, where measures of task success and dialogue quality were also the most important performance predictors (Litman et al., 1998; Walker et al., 1998; Kamm et al., 1998). Our ﬁndings draw into question a frequently made assumption in the ﬁeld regarding the centrality of efﬁciency to per- formance, and like other recent work, demonstrates that there are important tradeoffs between efﬁciency and other performance dimensions (Danieli and Gerbino, 1995; Walker et al., 1997a). 6 Linear regression assumes that predictors are not highly correlated (e.g., because correlations above .70 can affect the coefﬁcients, deletion of redundant predictors is advised (Monge and Cappella, 1980)). There is only 1 positive correlation among our predictors (between Mean Recognition and Task Success), and it is well below .70. 5 Related Work In the area of spoken dialogue, van Zanten (1998) has proposed a method for adapting initiative in form-ﬁlling dialogues. Whenever the system rejects a users utterance, the system takes more initiative; whenever the user gives an over-informative answer, the system yields some initia- tive. While this method has the potential of being automated, the method has been neither fully implemented nor empirically evaluated. Smith (1998) has evaluated strategies for dynamically deciding whether to conﬁrm each user utterance during a task-oriented dialogue. Simulation re- sults suggest that context-dependent adaptation strategies can improve performance, especially when the system has greater initiative. Walker et al. (1998) and Levin and Pieraccini (1997) have used reinforcement learning to adapt dialogue behavior over time such that system performance improves. We have instead focused on optimizing performance during a single dialogue. The empirical evaluation of an adaptive interface in a commercial software system (Strachan et al., 1997) is also similar to our work. Analysis of variance demonstrated that an adaptive interface based on minimal user modeling improved subjective user satisfaction ratings. 6 Conclusion We have presented an empirical evaluation of adaptability in TOOT, a spoken dialogue system that retrieves train schedules from the web. Our results suggest that adaptable TOOT generally outperforms non-adaptable TOOT for novice users, and that the utility of adaptation is greater for UserNo TOOT than for SystemExplicit TOOT. By using analysis of variance to examine how a set of evaluation measures differ as a function of adaptability, we elaborate the conditions under which adaptability leads to greater performance. When users interact with adaptable rather than non-adaptable TOOT, User Satisfaction and Task Success are signiﬁcantly higher. These results are independent of TOOTs initial dialogue strategy and task scenario. In contrast, Mean Recog- nition, User Expertise, and Future Use illustrate an interaction between initial dialogue strategy and adaptability. For SystemExplicit TOOT, the adaptable version does not outperform the non- adaptable version, or does not outperform the non-adaptable version very strongly. For UserNo TOOT, the adaptable version outperforms the non-adaptable version on all three measures. By using PARADISE to derive a performance function from data, we show that Mean Recog- nition, Task Success, and Elapsed Time best predict a users overall satisfaction with TOOT. These results help explain why adaptability in TOOT leads to overall greater performance, and allow us to make predictions about future performance. For example, we predict that a Sys- temImplicit strategy is likely to outperform our SystemExplicit strategy, since we expect that Mean Recognition and Task Success will remain constant but that Elapsed Time will decrease. Currently, we are extending our results along two dimensions. First, we have made a ﬁrst step towards automating the adaptation process in TOOT, by using machine learning to develop a classiﬁer for detecting dialogues with poor speech recognition (Litman et al., 1999). (Recall that our PARADISE evaluation suggested that recognition accuracy was our best performance predictor.) We hope to use this classiﬁer to determine the features that need to be represented in a user model, and to tell us when the user model indicates the need for adaptation. Guided by our empirical results, we can then develop an initial adaptation algorithm that takes dialogue strategy into account. For example, based on our experiment, we would like UserNo TOOT to adapt itself fairly aggressively when it recognizes that the user is having a problem. Second, the experiments reported here considered only our two most extreme initial dialogue strategy conﬁgurations. To generalize our results, we are currently experimenting with other dialogue strategies. To date we have collected 40 dialogues using a mixed initiative, implicit conﬁrmation version of TOOT, with initial promising results. For example, user satisfaction continues to exhibit the same main effect of adaptability when our corpus is augmented with these new dialogues. References [1995]Cohen, P. (1995). Empirical Methods for Artiﬁcial Intelligence. MIT Press, Boston. [1995]Danieli, M., and Gerbino, E. (1995). Metrics for evaluating dialogue strategies in a spoken lan- guage system. In Proc. AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, 3439. [1997]Kamm, C., Narayanan, S., Dutton, D., and Ritenour, R. (1997). Evaluating spoken dialog systems for telecommunication services. In Proc. 5th European Conf. on Speech Communication and Technology. [1998]Kamm, C., Litman, D., and Walker, M. (1998). From novice to expert: The effect of tutorials on user expertise with spoken dialogue systems. In Proc. 5th International Conf. on Spoken Language Processing, 12111214. [1997]Levin, E., and Pieraccini, R. (1997). A stochastic model of computer-human interaction for learning dialogue strategies. In Proc. 5th European Conf. on Speech Communication and Technology. [1998]Litman, D., Pan, S., and Walker, M. (1998). Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent. In Proc. 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conf. on Computational Linguistics, 780786. [1999]Litman, D., Walker, M., and Kearns, M. (1999). Automatic detection of poor speech recognition at the dialogue level. Manuscript submitted for publication. [1980]Monge, P., and Cappella, J., eds. (1980). Multivariate Techniques in Human Communication Re- search. Academic Press, New York. [1998]Smith, R. W. (1998). An evaluation of strategies for selectively verifying utterance meanings in spoken natural language dialog. International Journal of Human-Computer Studies 48:627647. [1997]Strachan, L., Anderson, J., Sneesby, M., and Evans, M. (1997). Pragmatic user modelling in a commercial software system. In Proc. 6th International Conf. on User Modeling, 189200. [1998]van Zanten, G. V. (1998). Adaptive mixed-initiative dialogue management. Technical Report 52, IPO, Center for Research on User-System Interaction. [1997a]Walker, M., Hindle, D., Fromer, J., Fabbrizio, G. D., and Mestel, C. (1997a). Evaluating competing agent strategies for a voice email agent. In Proc. 5th European Conf. on Speech Communication and Technology. [1997b]Walker, M., Litman, D., Kamm, C., and Abella, A. (1997b). PARADISE: A general framework for evaluating spoken dialogue agents. In Proc. 35th Annual Meeting of the Association for Computational Linguistics and 8th Conf. of the European Chapter of the Association for Computational Linguistics, 271280. [1998]Walker, M., Fromer, J., and Narayanan, S. (1998). Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email. In Proc. 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conf. on Computational Linguistics, 13451352.",
  "29.pdf": "arXiv:cs9904008v1 [cs.CL] 15 Apr 1999 Transducers from Rewrite Rules with Backreferences Dale Gerdemann Gertjan van Noord University of Tuebingen Groningen University Kl. Wilhelmstr. 113 PO Box 716 D-72074 Tuebingen NL 9700 AS Groningen dgsfs.nphil.uni-tuebingen.de vannoordlet.rug.nl September 25, 2018 Abstract Context sensitive rewrite rules have been widely used in several areas of natural language processing, including syntax, morphology, phonology and speech processing. Kaplan and Kay, Karttunen, and Mohri  Sproat have given various algorithms to compile such rewrite rules into ﬁnite-state transducers. The present paper extends this work by allowing a limited form of backreferencing in such rules. The explicit use of backreferencing leads to more elegant and general solutions. 1 Introduction Context sensitive rewrite rules have been widely used in several areas of natural language process- ing. Johnson [4] has shown that such rewrite rules are equivalent to ﬁnite state transducers in the special case that they are not allowed to rewrite their own output. An algorithm for compilation into transducers was provided by [5]. Improvements and extensions to this algorithm have been provided by [7], [9], [8] and [12]. In this paper, the algorithm will be extended to provide a limited form of backreferencing. Backreferencing has been implicit in previous research, such as in the batch rules of [5], bracketing transducers for ﬁnite-state parsing [8], and the LocalExtension operation of [13]. The explicit use of backreferencing leads to more elegant and general solutions. Backreferencing is widely used in editors, scripting languages and other tools employing regular expressions [3]. For example, Emacs uses the special brackets ( and ) to capture strings along with the notation n to recall the nth such string. The expression (a)b1 matches strings of the form anban. Unrestricted use of backreferencing thus can introduce non-regular languages. For NLP ﬁnite state calculi [6, 16] this is unacceptable. The form of backreferences introduced in this paper will therefore be restricted. The central case of an allowable backreference is: x  T (x)λ ρ (1) This says that each string x preceded by λ and followed by ρ is replaced by T (x), where λ and ρ are arbitrary regular expressions, and T is a transducer.1 This contrasts sharply with the rewriting rules that follow the tradition of Kaplan  Kay: 1The syntax at this point is merely suggestive. As an example, suppose that Tacr transduces phrases into acronyms. Then x  Tacr(x)abbr abbr would transduce abbrnon-deterministic finite automatonabbr into abbrNDFAabbr. To compare this with a backreference in Perl, suppose that Tacr is a subroutine that converts phrases into acronyms and that Racr is a regular expression matching phrases that can be converted into acronyms. Then (ignoring the left context) one can write something like: s(Racr)(?ABBR)Tacr(1)ge;. The backreference variable, 1, will be set to whatever string Racr matches. 1 φ  ψλ ρ (2) In this case, any string from the language φ is replaced by any string independently chosen from the language ψ. We also allow multiple (non-permuting) backreferences of the form: x1x2 . . . xn  T1(x1)T2(x2) . . . Tn(xn)λ ρ (3) Since transducers are closed under concatenation, handling multiple backreferences reduces to the problem of handling a single backreference: x  (T1  T2  . . .  Tn)(x)λ ρ (4) A problem arises if we want capturing to follow the POSIX standard requiring a longest- capture strategy. Friedl [3] (p. 117), for example, discusses matching the regular expression (totop)(opolo)?(gicalo?logical) against the word: topological. The desired result is that (once an overall match is established) the ﬁrst set of parentheses should capture the longest string pos- sible (top); the second set should then match the longest string possible from whats left (o), and so on. Such a left-most longest match concatenation operation is described in 3. In the following section, we initially concentrate on the simple case in (1) and show how (1) may be compiled assuming left-to-right processing along with the overall longest match strategy described by [8]. The major components of the algorithm are not new, but straightforward modiﬁcations of components presented in [8] and [12]. We improve upon existing approaches because we solve a problem concerning the use of special marker symbols (2.1.2). A further contribution is that all steps are implemented in a freely available system, the FSA Utilities of [16] (2.1.1). 2 The Algorithm 2.1 Preliminary Considerations Before presenting the algorithm proper, we will deal with a couple of meta issues. First, we introduce our version of the ﬁnite state calculus in 2.1.1. The treatment of special marker symbols is discussed in 2.1.2. Then in 2.1.3, we discuss various utilities that will be essential for the algorithm. 2.1.1 FSA Utilities The algorithm is implemented in the FSA Utilities [16]. We use the notation provided by the toolbox throughout this paper. Table 1 lists the relevant regular expression operators. FSA Utilities oﬀers the possibility to deﬁne new regular expression operators. For example, consider the deﬁnition of the nullary operator vowel as the union of the ﬁve vowels: macro(vowel,{a,e,i,o,u}). In such macro deﬁnitions, Prolog variables can be used in order to deﬁne new n-ary regular expression operators in terms of existing operators. For instance, the lenient composition operator [10] is deﬁned by: macro(priority_union(Q,R), {Q, domain(Q) o R}). macro(lenient_composition(R,C), priority_union(R o C,R)). 2 [] empty string [E1,...En] concatenation of E1 ...En {} empty language {E1,...En} union of E1,...En E Kleene closure E optionality E complement E1-E2 diﬀerence  E containment E1  E2 intersection ? any symbol A:B pair E1 x E2 cross-product A o B composition domain(E) domain of a transduction range(E) range of a transduction identity(E) identity transduction inverse(E) inverse transduction Table 1: Regular expression operators. Here, priority union of two regular expressions Q and R is deﬁned as the union of Q and the composition of the complement of the domain of Q with R. Lenient composition of R and C is deﬁned as the priority union of the composition of R and C (on the one hand) and R (on the other hand). Some operators, however, require something more than simple macro expansion for their def- inition. For example, suppose a user wanted to match n occurrences of some pattern. The FSA Utilities already has the  and  quantiﬁers, but any other operators like this need to be user deﬁned. For this purpose, the FSA Utilities supplies simple Prolog hooks allowing this general quantiﬁer to be deﬁned as: macro(match_n(N,X),Regex) :- match_n(N,X,Regex). match_n(0,_X,[]). match_n(N,X,[XRest]) :- N  0, N1 is N-1, match_n(N1,X,Rest). For example: match_n(3,a) is equivalent to the ordinary ﬁnite state calculus expression [a,a,a]. Finally, regular expression operators can be deﬁned in terms of operations on the underlying automaton. In such cases, Prolog hooks for manipulating states and transitions may be used. This functionality has been used in [17] to provide an implementation of the algorithm in [12]. 2.1.2 Treatment of Markers Previous algorithms for compiling rewrite rules into transducers have followed [5] by introducing special marker symbols (markers) into strings in order to mark oﬀ candidate regions for replace- ment. The assumption is that these markers are outside the resulting transducers alphabets. But previous algorithms have not ensured that the assumption holds. This problem was recognized by [8], whose algorithm starts with a ﬁlter transducer which ﬁlters out any string containing a marker. This is problematic for two reasons. First, when applied to a string that does happen to contain a marker, the algorithm will simply fail. Second, it leads 3 to logical problems in the interpretation of complementation. Since the complement of a regular expression R is deﬁned as Σ  R, one needs to know whether the marker symbols are in Σ or not. This has not been clearly addressed in previous literature. We have taken a diﬀerent approach by providing a contextual way of distinguishing markers from non-markers. Every symbol used in the algorithm is replaced by a pair of symbols, where the second member of the pair is either a 0 or a 1 depending on whether the ﬁrst member is a marker or not.2 As the ﬁrst step in the algorithm, 0s are inserted after every symbol in the input string to indicate that initially every symbol is a non-marker. This is deﬁned as: macro(non_markers,[?,[]:0]). Similarly, the following macro can be used to insert a 0 after every symbol in an arbitrary expression E. macro(non_markers(E), range(E o non_markers)). Since E is a recognizer, it is ﬁrst coerced to identity(E). This form of implicit conversion is standard in the ﬁnite state calculus. Note that 0 and 1 are perfectly ordinary alphabet symbols, which may also be used within a replacement. For example, the sequence [1,0] represents a non-marker use of the symbol 1. 2.1.3 Utilities Before describing the algorithm, it will be helpful to have at our disposal a few general tools, most of which were described already in [5]. These tools, however, have been modiﬁed so that they work with our approach of distinguishing markers from ordinary symbols. So to begin with, we provide macros to describe the alphabet and the alphabet extended with marker symbols: macro(sig,[?,0]). macro(xsig,[?,{0,1}]). The macro xsig is useful for deﬁning a specialized version of complementation and contain- ment: macro(not(X),xsig - X). macro((X),[xsig,X,xsig]). The algorithm uses four kinds of brackets, so it will be convenient to deﬁne macros for each of these brackets, and for a few disjunctions. macro(lb1,[1,1]). macro(lb2,[2,1]). macro(rb2,[2,1]). macro(rb1,[1,1]). macro(lb,{lb1,lb2}). macro(rb,{rb1,rb2}). macro(b1,{lb1,rb1}). macro(b2,{lb2,rb2}). macro(brack,{lb,rb}). 2This approach is similar to the idea of laying down tracks as in the compilation of monadic second-order logic into automata Klarlund (1997, p. 5). In fact, this technique could possibly be used for a more eﬃcient implementation of our algorithm: instead of adding transitions over 0 and 1, one could represent the alphabet as bit sequences and then add a ﬁnal 0 bit for any ordinary symbol and a ﬁnal 1 bit for a marker symbol. 4 As in Kaplan  Kay, we deﬁne an Intro(S) operator that produces a transducer that freely introduces instances of S into an input string. We extend this idea to create a family of Intro operators. It is often the case that we want to freely introduce marker symbols into a string at any position except the beginning or the end.  Free introduction macro(intro(S),{xsig-S,[] x S}).  Introduction, except at begin macro(xintro(S),{[],[xsig-S,intro(S)]}).  Introduction, except at end macro(introx(S),{[],[intro(S),xsig-S]}).  Introduction, except at begin  end macro(xintrox(S),{[],[xsig-S], [xsig-S,intro(S),xsig-S]}). This family of Intro operators is useful for deﬁning a family of Ignore operators: macro( ign( E1,S),range(E1 o intro( S))). macro(xign( E1,S),range(E1 o xintro( S))). macro( ignx(E1,S),range(E1 o introx(S))). macro(xignx(E1,S),range(E1 o xintrox(S))). In order to create ﬁlter transducers to ensure that markers are placed in the correct positions, Kaplan  Kay introduce the operator P-iff-S(L1,L2). A string is described by this expression iﬀ each preﬁx in L1 is followed by a suﬃx in L2 and each suﬃx in L2 is preceded by a preﬁx in L1. In our approach, this is deﬁned as: macro(if_p_then_s(L1,L2), not([L1,not(L2)])). macro(if_s_then_p(L1,L2), not([not(L1),L2])). macro(p_iff_s(L1,L2), if_p_then_s(L1,L2)  if_s_then_p(L1,L2)). To make the use of p iff s more convenient, we introduce a new operator l iff r(L,R), which describes strings where every string position is preceded by a string in L just in case it is followed by a string in R: macro(l_iff_r(L,R), p_iff_s([xsig,L],[R,xsig])). Finally, we introduce a new operator if(Condition,Then,Else) for conditionals. This oper- ator is extremely useful, but in order for it to work within the ﬁnite state calculus, one needs a convention as to what counts as a boolean true or false for the condition argument. It is possible to deﬁne true as the universal language and false as the empty language: macro(true,? ). macro(false,{}). With these deﬁnitions, we can use the complement operator as negation, the intersection operator as conjunction and the union operator as disjunction. Arbitrary expressions may be coerced to booleans using the following macro: 5 macro(replace(T,Left,Right), non_markers  introduce 0 after every symbol o  (a b c  a 0 b 0 c 0). r(Right)  introduce rb2 before any string o  in Right. f(domain(T))  introduce lb2 before any string in o  domain(T) followed by rb2. left_to_right(domain(T))  lb2 ... rb2 around domain(T) optionally o  replaced by lb1 ... rb1 longest_match(domain(T))  filter out non-longest matches marked o  in previous step. aux_replace(T)  perform Ts transduction on regions marked o  off by b1s. l1(Left)  ensure that lb1 must be preceded o  by a string in Left. l2(Left)  ensure that lb2 must not occur preceded o  by a string in Left. inverse(non_markers)).  remove the auxiliary 0s. Figure 1: Deﬁnition of replace operator. macro(coerce_to_boolean(E), range(E o (true x true))). Here, E should describe a recognizer. E is composed with the universal transducer, which transduces from anything (?) to anything (?). Now with this background, we can deﬁne the conditional: macro(if(Cond,Then,Else), { coerce_to_boolean(Cond) o Then, coerce_to_boolean(Cond) o Else }). 2.2 Implementation A rule of the form x  T (x)λ ρ will be written as replace(T,Lambda,Rho). Rules of the more general form x1 . . . xn  T1(x1) . . . Tn(xn)λ ρ will be discussed in 3. The algorithm consists of nine steps composed as in ﬁgure 1. The names of these steps are mostly derived from [7] and [12] even though the transductions involved are not exactly the same. In particular, the steps derived from Mohri  Sproat (r, f, l1 and l2) will all be deﬁned in terms of the ﬁnite state calculus as opposed to Mohri  Sproats approach of using low-level manipulation of states and transitions.3 The ﬁrst step, non markers, was already deﬁned above. For the second step, we ﬁrst consider a simple special case. If the empty string is in the language described by Right, then r(Right) should insert an rb2 in every string position. The deﬁnition of r(Right) is both simpler and more eﬃcient if this is treated as a special case. To insert a bracket in every possible string position, we use: [[[] x rb2,sig],[] x rb2] If the empty string is not in Right, then we must use intro(rb2) to introduce the marker rb2, followed by l iff r to ensure that such markers are immediately followed by a string in Right, or more precisely a string in Right where additional instances of rb2 are freely inserted in any position other than the beginning. This expression is written as: 3The alternative implementation is provided in [17]. 6 intro(rb2) o l_iff_r(rb2,xign(non_markers(Right),rb2)) Putting these two pieces together with the conditional yields: macro(r(R), if([]  R,  If: [] is in R: [[[] x rb2,sig],[] x rb2], intro(rb2)  Else: o l_iff_r(rb2,xign(non_markers(R),rb2)))). The third step, f(domain(T)) is implemented as: macro(f(Phi), intro(lb2) o l_iff_r(lb2,[xignx(non_markers(Phi),b2), lb2,rb2])). The lb2 is ﬁrst introduced and then, using l iff r, it is constrained to occur immediately before every instance of (ignoring complexities) Phi followed by an rb2. Phi needs to be marked as normal text using non markers and then xign x is used to allow freely inserted lb2 and rb2 anywhere except at the beginning and end. The following lb2 allows an optional lb2, which occurs when the empty string is in Phi. The fourth step is a guessing component which (ignoring complexities) looks for sequences of the form lb2 Phi rb2 and converts some of these into lb1 Phi rb1, where the b1 marking indicates that the sequence is a candidate for replacement. The complication is that Phi, as always, must be converted to non markers(Phi) and instances of b2 need to be ignored. Furthermore, between pairs of lb1 and rb1, instances of lb2 are deleted. These lb2 markers have done their job and are no longer needed. Putting this all together, the deﬁnition is: macro(left_to_right(Phi), [[xsig, [lb2 x lb1, (ign(non_markers(Phi),b2) o inverse(intro(lb2)) ), rb2 x rb1] ], xsig]). The ﬁfth step ﬁlters out non-longest matches produced in the previous step. For example (and simplifying a bit), if Phi is ab, then a string of the form . . . rb1 a b lb1 b . . . should be ruled out since there is an instance of Phi (ignoring brackets except at the end) where there is an internal lb1. This is implemented as:4 macro(longest_match(Phi), not(([lb1, (ignx(non_markers(Phi),brack)  (rb1) ),  longer match must be 4The line with (rb1) can be optimized a bit: Since we know that an rb1 must be preceded by Phi, we can write: [ign (non markers(Phi),brack),rb1,xsig]). This may lead to a more constrained (hence smaller) transducer. 7 rb  followed by an rb ]))  so context is ok o  done with rb2, throw away: inverse(intro(rb2))). The sixth step performs the transduction described by T. This step is straightforwardly imple- mented, where the main diﬃculty is getting T to apply to our specially marked string: macro(aux_replace(T), {{sig,lb2}, [lb1, inverse(non_markers) o T o non_markers, rb1 x [] ] }). The seventh step ensures that lb1 is preceded by a string in Left: macro(l1(L), ign(if_s_then_p( ignx([xsig,non_markers(L)],lb1), [lb1,xsig]), lb2) o inverse(intro(lb1))). The eighth step ensures that lb2 is not preceded by a string in Left. This is implemented similarly to the previous step: macro(l2(L), if_s_then_p( ignx(not([xsig,non_markers(L)]),lb2), [lb2,xsig]) o inverse(intro(lb2))). Finally the ninth step, inverse(non markers), removes the 0s so that the ﬁnal result in not marked up in any special way. 3 Longest Match Capturing As discussed in 1 the POSIX standard requires that multiple captures follow a longest match strategy. For multiple captures as in (3), one establishes ﬁrst a longest match for domain(T1)  . . .  domain(Tn). Then we ensure that each of domain(Ti) in turn is required to match as long as possible, with each one having priority over its rightward neighbors. To implement this, we deﬁne a macro lm concat(Ts) and use it as: replace(lm_concat(Ts),Left,Right) Ensuring the longest overall match is delegated to the replace macro, so lm concat(Ts) needs only ensure that each individual transducer within Ts gets its proper left-to-right longest matching priority. This problem is mostly solved by the same techniques used to ensure the longest match 8 macro(lm_concat(Ts),mark_boundaries(Domains) o ConcatTs):- domains(Ts,Domains), concatT(Ts,ConcatTs). domains([],[]). domains([FR0],[domain(F)R]):- domains(R0,R). concatT([],[]). concatT([TTs], [inverse(non_markers) o T,lb1 x []Rest]):- concatT(Ts,Rest).  macro(mark_boundaries(L),Exp): This is the central component of lm_concat. For our  \"toplological\" example we will have:  mark_boundaries([domain([{[t,o],[t,o,p]},[]: ]),  domain([{o,[p,o,l,o]},[]: ]),  domain({[g,i,c,a,l],[o,l,o,g,i,c,a,l]})])  which simplifies to:  mark_boundaries([{[t,o],[t,o,p]}, {o,[p,o,l,o]}, {[g,i,c,a,l],[o,l,o,g,i,c,a,l]}]).  Then by macro expansion, we get:  [{[t,o],[t,o,p]} o non_markers,[]x lb1,  {o,[p,o,l,o]} o non_markers,[]x lb1,  {[g,i,c,a,l],[o,l,o,g,i,c,a,l]} o non_markers,[]x lb1]  o   Filter 1: {[t,o],[t,o,p]} gets longest match   [ignx_1(non_markers({[t,o],[t,o,p]}),lb1),  ign(non_markers({o,[p,o,l,o]}),lb1),  ign(non_markers({[g,i,c,a,l],[o,l,o,g,i,c,a,l]}),lb1)]  o   Filter 2: {o,[p,o,l,o]} gets longest match   [non_markers({[t,o],[t,o,p]}),lb1,  ignx_1(non_markers({o,[p,o,l,o]}),lb1),  ign(non_markers({[g,i,c,a,l],[o,l,o,g,i,c,a,l]}),lb1)] macro(mark_boundaries(L),Exp):- boundaries(L,Exp0),  guess boundary positions greed(L,Exp0,Exp).  filter non-longest matches boundaries([],[]). boundaries([FR0],[F o non_markers, [] x lb1 R]):- boundaries(R0,R). greed(L,Composed0,Composed) :- aux_greed(L,[],Filters), compose_list(Filters,Composed0,Composed). aux_greed([HT],Front,Filters):- aux_greed(T,H,Front,Filters,_CurrentFilter). aux_greed([],F,_,[],[ign(non_markers(F),lb1)]). aux_greed([HR0],F,Front,[L1R],[ign(non_markers(F),lb1)R1]) :- append(Front,[ignx_1(non_markers(F),lb1)R1],L1), append(Front,[non_markers(F),lb1],NewFront), aux_greed(R0,H,NewFront,R,R1).  ignore at least one instance of E2 except at end macro(ignx_1(E1,E2), range(E1 o [[? ,[] x E2],? ])). compose_list([],SoFar,SoFar). compose_list([FR],SoFar,Composed):- compose_list(R,(SoFar o F),Composed). Figure 2: Deﬁnition of lm concat operator. 9 within the replace macro. The only complication here is that Ts can be of unbounded length. So it is not possible to have a single expression in the ﬁnite state calculus that applies to all possible lenghts. This means that we need something a little more powerful than mere macro expansion to construct the proper ﬁnite state calculus expression. The FSA Utilities provides a Prolog hook for this purpose. The resulting deﬁnition of lm concat is given in ﬁgure 2. Suppose (as in [3]), we want to match the following list of recognizers against the string topological and insert a marker in each boundary position. This reduces to applying: lm_concat([ [{[t,o],[t,o,p]},[]: ], [{o,[p,o,l,o]},[]: ], {[g,i,c,a,l],[o,l,o,g,i,c,a,l]} ]) This expression transduces the string topological only to the string topological.5 4 Conclusions The algorithm presented here has extended previous algorithms for rewrite rules by adding a limited version of backreferencing. This allows the output of rewriting to be dependent on the form of the strings which are rewritten. This new feature brings techniques used in Perl-like languages into the ﬁnite state calculus. Such an integration is needed in practical applications where simple text processing needs to be combined with more sophisticated computational linguistics techniques. One particularly interesting example where backreferences are essential is cascaded determin- istic (longest match) ﬁnite state parsing as described for example in Abney [2] and various papers in [14]. Clearly, the standard rewrite rules do not apply in this domain. If NP is an NP recognizer, it would not do to say NP  [NP]λ ρ. Nothing would force the string matched by the NP to the left of the arrow to be the same as the string matched by the NP to the right of the arrow. One advantage of using our algorithm for ﬁnite state parsing is that the left and right contexts may be used to bring in top-down ﬁltering.6 An often cited advantage of ﬁnite state parsing is ro- bustness. A constituent is found bottom up in an early level in the cascade even if that constituent does not ultimately contribute to an S in a later level of the cascade. While this is undoubtedly an advantage for certain applications, our approach would allow the introduction of some top- down ﬁltering while maintaining the robustness of a bottom-up approach. A second advantage for robust ﬁnite state parsing is that bracketing could also include the notion of repair as in [1]. One might, for example, want to say something like: xy  [NP RepairDet(x) RepairN(y) ]λ ρ7 so that an NP could be parsed as a slightly malformed Det followed by a slightly malformed N. RepairDet and RepairN, in this example, could be doing a variety of things such as: contextual- ized spelling correction, reordering of function words, replacement of phrases by acronyms, or any other operation implemented as a transducer. Finally, we should mention the problem of complexity. A critical reader might see the nine steps in our algorithm and conclude that the algorithm is overly complex. This would be a false conclusion. To begin with, the problem itself is complex. It is easy to create examples where the resulting transducer created by any algorithm would become unmanageably large. But there exist strategies for keeping the transducers smaller. For example, it is not necessary for all nine steps to be composed. They can also be cascaded. In that case it will be possible to implement diﬀerent 5An anonymous reviewer suggested that lm concat could be implemented in the framework of [8] as: [t o t o p  o  p o l o ]  ... ; Indeed the resulting transducer from this expression would transduce topological into topological. But unfor- tunately this transducer would also transduce polotopogical into polotopogical, since the notion of left-right ordering is lost in this expression. 6The bracketing operator of [8], on the other hand, does not provide for left and right contexts. 7The syntax here has been simpliﬁed. The rule should be understood as: replace(lm concat([[]:[np, repair det, repair n, []:]],lambda, rho). 10 steps by diﬀerent strategies, e.g. by deterministic or non-deterministic transducers or bimachines [15]. The range of possibilities leaves plenty of room for future research. References [1] Steve Abney. Rapid incremental parsing with repair. In Proceedings of the 6th New OED Conference: Electronic Text Rese arch, pages 19, 1990. [2] Steven Abney. Partial parsing via ﬁnite-state cascades. In Proceedings of the ESSLLI 96 Robust Parsing Workshop, 1996. [3] Jeﬀrey Friedl. Mastering Regular Expressions. OReilly  Associates, Inc., 1997. [4] C. Douglas Johnson. Formal Aspects of Phonological Descriptions. Mouton, The Hague, 1972. [5] Ronald Kaplan and Martin Kay. Regular models of phonological rule systems. Computational Linguistics, 20(3):331379, 1994. [6] L. Karttunen, J-P. Chanod, G. Grefenstette, and A. Schiller. Regular expressions for language engineering. Natural Language Engineering, 2(4):305238, 1996. [7] Lauri Karttunen. The replace operator. In 33th Annual Meeting of the Association for Computational Linguistics, M.I.T. Cambridge Mass., 1995. [8] Lauri Karttunen. Directed replacement. In 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, 1996. [9] Lauri Karttunen. The replace operator. In Emannual Roche and Yves Schabes, editors, Finite-State Language Processing, pages 117147. Bradford, MIT Press, 1997. [10] Lauri Karttunen. The proper treatment of optimality theory in computational phonology. In Finite-state Methods in Natural Language Processing, pages 112, Ankara, June 1998. [11] Nils Klarlund. Mona  Fido: The logic automaton connection in practice. In CSL 97, 1997. [12] Mehryar Mohri and Richard Sproat. An eﬃcient compiler for weighted rewrite rules. In 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, 1996. [13] Emmanuel Roche and Yves Schabes. Deterministic part-of-speech tagging with ﬁnite-state transducers. Computational Linguistics, 21:227263, 1995. Reprinted in Roche  Schabes (1997). [14] Emmanuel Roche and Yves Schabes, editors. Finite-State Language Processing. MIT Press, Cambridge, 1997. [15] Emmanuel Roche and Yves Schabes. Introduction. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing. MIT Press, Cambridge, Mass, 1997. [16] Gertjan van Noord. Fsa utilities, 1997. The FSA Utilities toolbox is available free of charge under Gnu General Public License at http:www.let.rug.nlvannoordFsa. [17] Gertjan van Noord and Dale Gerdemann. An extendible regular expression compiler for ﬁnite- state approaches in natural language processing. In Workshop on Implementing Automata 99, Potsdam Germany, 1999. 11",
  "30.pdf": "An ascription-based approach to Speech Acts Mark Lee and Yorick Wilks Department of Computer Science University of Shefﬁeld Regent Court, 211 Portobello Street Shefﬁeld S1 4DP, UK M.Leedcs.shef.ac.uk Y.Wilksdcs.shef.ac.uk Abstract: The two principal areas of natural language processing research in pragmatics are belief modelling and speech act processing. Belief modelling is the development of techniques to represent the mental attitudes of a dia- logue participant. The latter approach, speech act processing, based on speech act theory, involves view- ing dialogue in planning terms. Utterances in a dia- logue are modelled as steps in a plan where understanding an utterance involves deriving the com- plete plan a speaker is attempting to achieve. How- ever, previous speech act based approaches have been limited by a reliance upon relatively simplistic belief modelling techniques and their relationship to plan- ning and plan recognition. In particular, such tech- niques assume precomputed nested belief structures. In this paper, we will present an approach to speech act processing based on novel belief modelling tech- niques where nested beliefs are propagated on demand. 1. Introduction The use of simplistic belief models has accompanied complex accounts of speech acts where highly nested belief sets accompany any speech act. We believe that by utilising a more sophisticated view of mental attitudes, a simpler and more elegant theory of speech acts can be constructed. Also, as previous work has pointed out (Wilks et al, 1991) past models have failed to differentiate explicitly between the speakers and hearers belief sets. Such a failure causes problems in dealing with misconceptions and badly formed plans (Pollack, 1990). This paper augments ViewGen, a computer program originally developed by Ballim and Wilks (1991) to model the beliefs and meta-beliefs of a system using nested belief structures. ViewGen is able to reason about its own and other agents beliefs using belief ascription and inference techniques. The current version of ViewGen is implemented in Quintus Prolog. The structure of this paper is as follows: in Section 2, we review and discuss previous speech act approaches and their representation of mental attitudes. We argue that precom- puted highly nested belief structures arent necessary. In Section 3, we describe how ViewGen represents mental atti- tudes and computes nested structures by a process of ascrip- tion and in Section 4, show how such techniques can be used to represent speech acts for use in planning and plan recog- nition. Finally, in Section 5, we discuss some implications and future directions of our work 2. Speech acts and mental attitudes It is clear that any understanding of an utterance must involve reference to the attitudes of the speaker. For exam- ple, the full understanding of the utterance Do you know where Thomas is? depends upon whether the speaker already knows where Thomas is and whether he or she believes the hearer knows. Speech act based AI approaches normally make refer- ence to mental attitudes and often provide links between the surface form of the utterance and the mental attitudes of both the speaker and hearer. For example, Appelt (1985) describes a system which generates discourse from an inten- sional logic representation of a set of beliefs. However, as pointed out by Pollack (1990), they have typically used rela- tively simple models of such attitudes. In particular, previ- ous approaches have lacked any way to model the propagation of belief within the system itself and instead have made use of precomputed and ﬁxed nestings of mental attitudes. One widely used concept in speech act accounts is mutual belief. Following work in philosophy by Lewis (1969), Clark and Marshall (1981) introduced the notion of mutual belief to account for hearer attitudes. A proposition P is a mutual belief if shared by two agents A and B such that: A believes P B believes P A believes B believes P B believes A believes P etc., ad inﬁnitum There cannot be a logical limit to the number of levels of regression since, as Schiffer (1972) argued, for any level of nested belief, a dialogue example can be constructed which requires an additional level of belief nesting. Because of this potentially inﬁnite regression, it has proven difﬁcult to use an axiomatic deﬁnition of mutual belief based in terms of simple belief in computational implementations. Alternative approaches have either avoided deﬁning axioms for mutual belief, e.g. Taylor and Whitehill (1981) or deﬁned it as a primitive operator without reference to simple beliefs, e.g. Cohen and Levesque (1985). Appeared in the Proceedings of COLING-96, Copenhagen. Despite such work, it appears that the mutual belief hypothesis, i.e. that agents compute potentially inﬁnite nest- ings of belief in comprehension, appears to be too strong a hypothesis to be realistic. It is impossible that agents per- form this kind of potentially inﬁnite nesting during real dia- logue and no clear constraint can be given on how many iterations would be necessary in a real dialogue situation. Though examples can be artiﬁcially created which require n levels of nesting for large n, during a study of dialogue cor- pora, Lee (1994) found no need for highly nested belief models. In fact, it appears that no dialogue exchange required more than a two level belief nesting. Also, mistakes in assuming what was common to both agents in a dialogue occurred but were quickly repaired through the use of cor- rections and repetitions and other dialogue control acts. Sim- ilar results have been reported by Taylor and Carletta (1994) in analysing the HCRC Map Task corpus. Rather than compute nested beliefs to some ﬁxed level during comprehension. It is far more plausible that agents compute nested representations on so that highly nested belief representations are only constructed if required in the dialogue. This is the basic principle behind ViewGen. 3. The ViewGen system ViewGen is a nested attitude model which constructs intensional environments to model the attitudes of other agents. Previous work on ViewGen has been concerned with only modelling belief attitudes (Wilks, Barnden and Ballim, 1991). We have extended ViewGen to model and represent, in addition, goals and intentions. In this section, we brieﬂy describe ViewGens operation. 3.1 ViewGen representations of mental attitudes ViewGen assumes that each agent in a dialogue has a belief environment which includes attitudes about what other agents believe, want, and intend. Such attitudes are represented in a nested structure. Each nesting is an environ- ment which contains propositions which may be grouped by a particular topic or stereotype. The particular topic is given on the top left corner of the environment while the holder of a belief is given at the bottom of the environment. ViewGen represents all attitudes in environments with the attitude type labelled on the far right bottom of the box. Though different attitude types are separated by environ- ments, they can be nested so that agents can have beliefs, goals, and intentions about these attitudes. For example, suppose the System believes that John intends to buy a car, but wants to convince him otherwise by getting him to believe correctly that the car is a wreck. This is illustrated in Figure 1. In ViewGen, different attitudes have different types. Beliefs and goals refer to propositions which the agent either believes is true or wants to be true at some point in the future. Intentions, however, are represented as connected planning actions which represent the plans the agent intends to pursue to achieve his or her goals. 3.2 Ascription of attitudes As noted above, ViewGen avoids using a concept of shared or mutual beliefs. Rather, ViewGen attributes beliefs, goals and intentions to other agents as required. This process is termed ascription. There are two methods of ascription: default ascription and stereotypical ascription. Each method is brieﬂy described below. 3.2.1 Default Ascription Default ascription applies to common beliefs. Most beliefs in any set held by an agent are common beliefs about the world, and can be assumed to be common to any other rational agent unless marked otherwise. For example, an agent may believe that the world is round and therefore, without any evidence, guess that any other agent probably shares this belief. To model this, ViewGen uses a default ascription rule i.e. Default Ascription rule: Given a System belief, ascribe it to any other agent as required, unless there is contrary evidence. Such a rule results in beliefs being pushed from outer belief environments to inner belief environments. For exam- ple, Figure 2 illustrates ViewGen assuming that John shares its belief that the world is round. Evidence against ascription is normally an explicit belief Intention John Intention buy(John,Car) System John Belief isa(Car,Wreck) System Goal Belief System John Intention buy(John,Car) System John Belief isa(Car,Wreck) Figure 1: Meta-attitudes on other attitudes that an another agent believes the opposite of the ascribed belief. For example, an agent might already be believed by ViewGen to believe the world is ﬂat and thus block any ascription of ViewGens belief. It is important for ViewGen to reason about other agents beliefs about other agents. For example, it is plausible that an agent who believes the world is round may well also believe by default that other agents believe the same. Unlike beliefs, the assumption that other agents share similar goals and intentions cannot be made by default. Goals and intentions are more dynamic than beliefs in that an agent will try to achieve goals and carry out intentions in the future which once achieved are dropped from the agents attitude set. Also, goals and intentions are often highly stere- otypical. Therefore, a default rule of ascription cannot be applied to such attitudes. However, a combination of stereo- typical ascription and plan recognition can be used to pro- vide sensible ascriptions of goals and intentions. Stereotypical ascription is discussed next while plan recog- nition is discussed in 4.3. 3.2.2 Stereotypical Ascription A stereotype is a collection of attitudes which are gener- ally applicable to a particular class of agent. For example, Doctors tend to have expert medical knowledge and have goals to diagnose diseases and cure patients. To model this ViewGen uses a stereotypical ascription rule: Stereotypical Ascription rule: Given a System stereotypical belief, ascribe it to any other agent to which the stereotype applies as required, unless there is contrary evidence. In ViewGen, stereotypes consist of sets of attitudes which an agent who ﬁts a particular stereotype might typi- cally hold. Such stereotypical beliefs can be ascribed to an agent by default - i.e. unless there is explicit evidence that the agent holds a contrary belief. For example, in Figure 3, the System has a stereotypical set of beliefs for doctors and, since it believes John is a doctor, ascribes these to John. 4. Ascription based Speech act rep- resentation In this section, we will outline our theory of speech acts. In 4.1, we outline a list of features which we believe any the- ory should possess and in 4.2 we describe a theory based on belief ascription. 4.1 Desideratum for a theory of speech acts We believe that a theory of speech acts should have at least the following features: 1, The theory should be solipsist The notion of mutual knowledge was introduced to pro- vide a realistic account of the effects of a speech act on a hearer. However, as has argued above and elsewhere (Ballim and Wilks, 1991), mutual belief is too strong a notion to be used. Instead, a theory of speech acts should be solipsistic in that it refers solely to ﬁnite belief representations of either the speaker or hearer of the dialogue act. System round(earth) round(earth) John Figure 2: ViewGen default ascription System round(world) round(world) Belief System John Belief Figure 3: Stereotypical ascription of medical knowledge System Belief System isa(John,Doctor) Doctor Stereotype Belief John Belief isa(pneumonia, bacteria)) treatment(bacteria, anti-biotics) isa(pneumonia, bacteria) treatment(bacteria, anti-biotics) 2, The theory must provide separate interpretations for the speaker and hearer A theory must, however, take into account the attitudes of both the speaker and hearer by allowing the separate deri- vation of the effects of a speech act from the speakers and the hearers points of view. 3, Speech acts should be minimalistic Any theory should assume only the minimal conditions for any utterance to be successful. This means avoiding the ascription of precomputed attitude nestings beyond the essential conditions of each act for the act to be achieved. 4, Speech acts should be extendable Despite assuming only the minimal conditions and effects for any speech act, they should in principle be extendable to deal with problematic examples involving high degrees of belief nesting proposed by work in philoso- phy. 5, The theory must provide a means to derive generalised effects from each acts conditions As argued by Searle (1969), any classiﬁcation of speech acts must be based on the conditions of each act and not its effects. However, we also want a principled way to derive the conventional effects of any act from its conditions. This is necessary so that we can then provide a clear distinction between an acts conventional illocutionary effect and its context-speciﬁc perlocutionary effect. We believe that our account of speech acts satisﬁes the above criteria. In the next two sections we will outline how we represent speech acts in terms of belief ascription and how we use these in planning and plan recognition. 4.2 An ascription based theory of speech acts We represent 20 different speech acts types in four classes: questions, answers, requests and inform acts. This set is partially based on Bunts taxonomy of 24 speech acts (1989). While not claiming that such a set of acts is com- plete, we have found it sufﬁcient for the dialogue corpora we have analysed. Every act is classiﬁed with respect to its pre- conditions which are the mental attitudes a speaker must adopt to felicitously perform the speech act. Acts are ordered by speciﬁcity: more speciﬁc speech acts inherit or strengthen the preconditions of more general ones. For example, an inform act requires that the speaker believes the proposition in question and has a goal that the hearer also believes the proposition, i.e.: A correction act is a more speciﬁc type of informing and, therefore, inherits the preconditions of informing plus the condition that the speaker believes that the hearer believes the opposition of the proposition, i.e.: Rather than specify individual effects for each dialogue act, we provide separate update rules based on belief ascrip- tion. Our update rule from the speakers point of view is: That is, for every condition in the speech act, the speaker must ascribe a belief to the hearer that the condition is satis- ﬁed. For example, Figure 4 shows the conditions for an inform act: the speaker believes the proposition to be com- municated and wants the hearer to believe it too. To achieve this goal, the speaker intends to use an inform speech act. After performing the inform act, the speaker can ascribe to the hearer the belief that each of the preconditions were met i.e. the speaker believes that the hearer believes the speaker believes the proposition and has the goal of getting the hearer to believe it too. The effects of the inform act on the speakers attitude set are shown in Figure 5. Note that after the inform act is performed, the intention to perform it is dropped. However, the speakers goal of getting the hearer to believe the proposition remains. This is because we assume only the minimal conditions for the act to be suc- cessful i.e. if the speaker can successfully ascribe each speech act precondition to the hearer. For the hearer to believe the proposition, he or she has to perform a mental Inform(Speaker,Hearer,Proposition) Preconditions: believe(Speaker,Proposition) goal(Speaker,believe(Hearer,Proposition) Correction(Speaker,Hearer,Proposition) Preconditions: believe(Speaker,Proposition) goal(Speaker,believe(Hearer,Proposition) believe(Speaker,believe(Hearer, not(Proposition))) For every condition C in dialogue act performed: default_ascribe(Speaker, Hearer, believe(C)) Update on the Speakers belief set Belief Intention Goal Belief on(coffee, stove) Speaker Speaker Speaker Hearer Speaker inform(Speaker,Hearer,on(coffee,stove)) Figure 4: Representation of a speakers attitudes before performing an inform act on(coffee, stove) act. Mental acts are detailed in the next section. The update rule for the hearer is the converse of the speakers: That is, given that the speaker has performed an inform act, the hearer can ascribe to the speaker the preconditions of the inform act assuming that the speaker is being coopera- tive. The effects of the inform act are shown in Figure 6. Note that the hearers update rule is one level less nested: the preconditions rather than beliefs about the preconditions are ascribed. 4.3 Planning and plan simulation in nested belief environments ViewGen uses a non-linear POCL planner (McAllester and Rosenblatt, 1991) to plan actions to achieve goals. Such a planner is provably correct and complete so that it is guar- Update on the Hearers belief set For every condition C in dialogue act performed: default_ascribe(Hearer,Speaker, C) anteed to ﬁnd a solution if one exists and only generates valid solutions. Since ViewGen represents the attitudes of agents in nested environments, it is able to use the planner to simulate other agents planning. This simulation can be applied to any depth of nested belief e.g. ViewGen can simulate John simu- lating Mary generating a plan to achieve a given goal by considering its beliefs of Johns beliefs of Marys beliefs, goals and intentions. Which plan is constructed depends on what the nested agent is believed to believe. Therefore, during nested plan- ning, ViewGen has to reason about which beliefs are held to be true at that level of nesting. However, as mentioned above, belief ascription only is performed as required: we cannot predict which beliefs will be relevant to a plan before the plan is constructed and therefore, ascription must be per- formed as the plan is generated. To achieve this, both types Belief on(coffee, stove) Speaker Speaker Belief Hearer Belief Hearer on(coffee, stove) Speaker Goal on(coffee, stove) Speaker Belief Goal Belief Speaker Hearer on(coffee, stove) Figure 5: Representation of a speakers attitudes after performing an inform act Belief Hearer Belief Speaker Belief Hearer on(coffee, stove) Goal on(coffee, stove) Speaker Hearer Figure 6: Representation of a hearers attitudes after an inform act of ascription are represented in plan operator notation as mental acts. For example, default belief ascription as detailed in section 3.2.1 is represented as: In addition to pushing outer nested beliefs into inner environments, we require a method of adopting other agents beliefs by pushing inner nested beliefs into outer environ- ments. For this, we have an accept-belief operator: That is, if an Agent2 has some belief and Agent1 doesnt hold a contrary belief and believes that Agent2 is trustwor- thy, then it is acceptable for Agent1 to also believe Agent2s belief. This plays a role in informing where a hearer must decide whether or not to believe the communicated proposi- tion. During planning, plans are constructed based on the beliefs, goals and intentions which are explicitly present at that level of nesting. However, if a proposition isnt repre- sented at this level of nesting, then the POCL planner must plan ascription actions to determine whether the simulated agent holds the relevant attitude. Therefore, simulated plan- ning involves two types of planning: planning by the agent simulated and planning by ViewGen itself to maintain its belief representation of the agent. In addition to plan simulation, we have extended the basic POCL algorithm to allow other agents plans to be rec- ognised. This involves inferring from an agents performed action, the agents set of goals he or she is trying to achieve and the plan he or she intends to follow to achieve these goals. This is achieved by collecting together the ascribable goals at the particular level of nesting and attempting to ﬁnd a plan which achieves at least one of the ascribable goals. Once a plan is generated, any goals achieved by the plan are ascribed. In both simulation and recognition, once an acceptable plan is generated, the actions and goals in the plan are ascribed to the agent at that level of nesting. 5. Conclusions and future work We have argued that the computation of highly nested belief structures during the performance or recognition of a speech act is implausible. In particular, the concept of mutual belief seems too strong. Instead, we have put forward a theory of speech acts where only the minimal set of beliefs is ascribed at the time of the utterance. If further belief nest- ings are required then they can be derived using belief ascription techniques as required. Default_belief_ascription(Agent1, Agent2, Proposition) Preconditions: belief(Agent1, Proposition) belief(Agent1, not(belief(Agent2, not(Proposition)))) Effects: belief(Agent1, belief(Agent2, Proposition)) Accept_belief(Agent1, Agent2, Proposition) Preconditions: belief(Agent1, belief(Agent2, Proposition)) not(belief(Agent1, not(Proposition))) belief(Agent1,trustworthy(Agent2)) Effects: belief(Agent1,Proposition) We believe that, for the most part, during normal dia- logue, the minimal effects of any speech act are all that are required. However, our approach allows highly nested belief structures to be computed on demand if required, for exam- ple, to understand non-conventional language use. Future work includes the attachment of a robust dia- logue parser. We also intend to link ViewGen to the LaSie information extraction platform (Gaizaukas et al, 1995) so as to develop a testable belief set empirically derived from a small medical domain corpus. References D. Appelt. (1985) Planning English Sentences. Cambridge University Press, Cambridge. A. Ballim and Y. Wilks (1991). Artiﬁcial Believers. Law- rence Erlbaum Associates, Hillsdale, New Jersey. H.C. Bunt (1989). Information dialogues as communicative action in relation to partner modelling and information processing. In M.M. Taylor, F. Neel, and D.G. Bouwhuis, editors, The Structure of Multimodal Dialogue. Elsevier Sci- ence Publishers, North Holland. H. Clark and C. Marshall (1981). Deﬁnite reference and mutual knowledge. In A. Joshi, B. Webber, and I. Sag, edi- tors, Elements of Discourse Understanding, pages 10  62. Cambridge University Press, Cambridge, UK. P.R. Cohen and H. Levesque (1985). Speech acts and ration- ality. In Proceedings of the 23erd Annual Meeting of the Association for Computational Linguistics. University of Chicago Press. R. Gaizauskas, T. Wakao, K. Humphreys, H. Cunningham, and Y. Wilks (1995). University of shefﬁeld: Description of lasie system as used. In Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, Maryland. Morgan Kaufman Publishers Inc. M. Lee (1994). Conjunctive goals as a cause of conversa- tional implicature. Technical Report 94-10-05, Dept. of Computer Science, University of Shefﬁeld. D.K. Lewis (1969). Convention. Harvard University Press, Cambridge, Mass. D.A. McAllester and D. Rosenblatt (1990). Systematic non- linear planning. In Proceedings of AAAI90, pages 634639. M.E. Pollack (1990) Plans as complex mental attitudes. In P.R. Cohon, J. Morgan, and M.E. Pollack, editors, Intentions in Communication. BradfordMIT press, Cambridge, Mass. S Schiffer (1972). Meaning. Clarendon Press, Oxford. J.R. Searle (1969) Speech acts. Cambridge University Press, Cambridge. G.B. Taylor and S.B. Whitehill (1981). A belief representa- tion for understanding deception. In Proceeding of IJCAI- 81, pages 475479, Los Altos, CA. J. Taylor and J. Carletta (1994). Limiting nested beliefs in cooperative dialogue. In A. Ram and K. Eiselt, editors, Pro- ceedings of the 16th annual conference of the cognitive sci- ence society, pages 858863, Hillsdale, New Jersey. Lawrence Erlbaum Associates. Y. Wilks, J. Barnden, and A. Ballim (1991). Belief ascrip- tion, metaphor and intensional identity. Cognitive Science, 15(1).",
  "31.pdf": "arXiv:cs9904018v1 [cs.CL] 24 Apr 1999 A COMPUTATIONAL MEMORY AND PROCESSING MODEL FOR PROSODY Janet E. Cahn Massachusetts Institute of Technology cahnmedia.mit.edu ABSTRACT This paper links prosody to the information in the text and how it is processed by the speaker. It describes the operation and output of Loq, a text-to-speech imple- mentation that includes a model of limited attention and working memory. Attentional limitations are key. Vary- ing the attentional parameter in the simulations varies in turn what counts as given and new in a text, and there- fore, the intonational contours with which it is uttered. Currently, the system produces prosody in three diﬀer- ent styles: child-like, adult expressive, and knowledgeable. This prosody also exhibits diﬀerences within each style  no two simulations are alike. The limited resource ap- proach captures some of the stylistic and individual variety found in natural prosody. 1. INTRODUCTION Ask any lay person to imitate computer speech and you will be treated to an utterance delivered in melodic and rhythmic monotone, possibly accompanied by choppy ar- ticulation and a voice quality that is nasal and strained. In fact, current synthesized speech is far superior. Yet few would argue that synthetic and natural speech are indistin- guishable. The diﬀerence, as popular impression suggests, is the relative lack of interesting and natural variability in the synthetic version. It may be traced in part to the lack of a common causal account of pitch, timing, articulation and voice quality. Intonation and stress are usually linked to the linguistic and information structure of text. Fea- tures such as pause location and word duration are linked mainly to the speakers cognitive and expressive capacities, and pitch range, intensity, voice quality and articulation to her physiological and aﬀective state. In this paper, I describe a production model that at- tributes pitch and timing to the essential operations of a speakers working memory  the storage and retrieval of in- formation. Simulations with this model produce synthetic speech in three of the prosodic styles likely to be associ- ated with attentional and memory diﬀerences: a child-like exaggerated prosody for limited recall; a more adult but still expressive style for mid-range capacities; and a knowl- edgeable style for maximum recall. The same model also produces individual diﬀerences within each style, owing to its stochastic storage algorithm. The ability to produce both individual and genre variations supports its eventual use in prosthetic, entertainment and information applica- tions, especially in the production of reading materials for the blind and the use of computer-based autonomous and communicative agents. 2. A MEMORY MODEL FOR PROSODY Prosody organizes spoken text into phrases, and high- lights its most salient components with pitch accents, dis- tinctive pitch contours applied to the word. Pitch accents are both attentional and propositional. Their very use indicates salience; their particular form conveys a propo- sition about the words they mark. For example, speakers typically use a high pitch accent (denoted as H) to mark salient information that they believe to be new to the ad- dressee. Conversely, when they believe the addressee is already aware of the information, they will typically de- accent it[2] or, if it is salient, apply a low pitch accent (L)[7]. Re-stated as a commentary on working mem- ory, the H accent conveys the speakers belief that the addressee can not retrieve the accented information from working memory. De-accenting implicitly conveys the op- posite expectation. The L accent does so explicitly. This view predicts diﬀerent speaking styles as a consequence of the speakers beliefs about an addressees storage and retrieval capacities. For example, it ascribes the exagger- ated intonation that adults use with infants and young children[4], to the adults belief that the childs knowledge and attention are extremely limited; therefore, he needs clear and explicit prosodic instructions as to how to pro- cess language and interaction. The model of working memory I use shows how retrieval limits can determine the information status of an item as either given or new, and therefore, its corresponding prosody. It was developed and implemented by Thomas Landauer[5] and models working memory as a periodic three dimensional Cartesian space, the focus of attention via a moving search and storage pointer that traverses the space in a slow random walk, and retrieval ability via a search radius that deﬁnes the size of a region whose center is the pointers current location. Search for familiar items proceeds outward from the pointer, one city block per time step, up to the distance speciﬁed by the search radius. As a consequence of the random walk, incoming stim- uli are stored in a spatial pattern that is locally random search region, radius  1 search  storage pointer the pointer's random walk Figure 1: Using AWM, stimuli are classed as given if they have counterparts within the the search radius. New items have no such counterparts because they are either not in working memory, or are stored outside the radius. but globally coherent. That is, temporal proximity in the stimuli begets spatial proximity in the model. It contrasts with stack models of memory that are strictly chronolog- ical, and semantic spaces in which distance is conceptual rather than temporal. Most importantly, it is a valid computational model of attention and working memory (AWM, from here on). Landauer used it to reproduce the well-known learning phenomena of recency and frequency, in which subjects tend to recall stimuli encountered most recently or most frequently[5]. It has since been used by Walker[9] to show that resource-bound dialog partners will make a proposition explicit when it is not retrievable or in- ferable, despite having been previously mentioned. Retrieval in AWM is the process of matching the current stimulus to the contents of the region centered around the pointer. The search radius determines the size of this re- gion and therefore is the main AWM simulation parameter. If a match is found within the search region, the stimulus is classiﬁed as given, otherwise, it is new. Figure 1 illus- trates this with the simple example of ﬁlled and unﬁlled circles, a 4x4 AWM space, and a search radius of one. At the center of the search region is the current stimulus, a ﬁlled circle. Because the region contains no other ﬁlled circles, the stimulus is classed as new. Had the stimulus been an unﬁlled circle, it would have instead been classed as given because a match is retrievable within the search radius. Or, alternatively, had the search radius been two instead of one, a matching ﬁlled circle would have been found, and the stimulus again classed as given. The ability to identify given and new items makes AWM a useful producer of prosody based on this distinction. Os- tensibly, it shows how a speakers processing aﬀects her prosody. However, although the working memory belongs to the speaker, its operation and determinations may re- ﬂect the speakers own retrieval capacities, her estimate of those of the addressee, or a mixture of both. That is, a speaker can always adapt her style (prosodic and lexi- cal) to the needs of a less knowledgeable or capable ad- dressee. A cooperative and communicative speaker will usually do this. However, she cannot model a retrieval capacity greater than her own  her own knowledge and attentional limits always constitute the upper bounds on her performance. 3. SYSTEM DESIGN The AWM component is embedded in a software im- plementation, Loq, that takes a text-to-speech approach. As shown Figure 2, the input to AWM is text, the out- Text Speech Linguistic Analysis AWM: Search  Storage Map to Pitch and Timing Synthesizer Figure 2: AWM as a component of the Loq system. put is speech. Therefore, Loq models read rather than spontaneous speech. Text comprehension is the process of searching for a match. Uttering the text is a question of mapping the search process and its results to prosodic fea- tures and sending the prosodically annotated text to the synthesizer. Like many commercial text-to-speech synthesizers, the text structure is analyzed before prosody is assigned. How- ever, the Loq analysis is richer. It takes advantage of on-line linguistic databases to approximate the speakers knowledge of English semantics, pronunciation and usage. The structural analysis is richer as well, providing both grammatical structure (subject, verb, object), empty cate- gories (ellipses, for example) and information about clausal attachment. The main qualitative diﬀerence is that Loq interposes a model of limited attention and working mem- ory between the text analysis and prosodic mapping com- ponents. 3.1. Matching For the example in Figure 1, the matching criterion is binary and simple  a circle is either ﬁlled or unﬁlled. How- ever, language is many more times complex, and matches may occur for a variety of features, some of which are more informative than others. The matching criteria used in Loq attempt to distill from the literature (e.g., [6, 3]) the most relevant and prevalent ways that items in mem- ory prime for the current stimulus, and by the same token, the ways in which the current stimulus can function as a retrieval cue. In other words, they gauge the mutual infor- mation between the current stimulus and previously stored items. Altogether, Loq tests for matches on twenty-four se- mantic, syntactic, collocation, grammatical and acoustical features. Each test contributes to the total match score, which is then compared to a threshold. If it is below, the search continues; if above, it stops. As shown in Fig- ure 3, matches on any criterion express priming, and scores above the threshold constitute a match suﬃcient to stop the search even before it reaches the edge of the search region. Because some tests are more informative than oth- ers, a high score can reﬂect the positive outcome of many  0 priming stimulus previously stored item Compare score  threshold no priming match beyond search limit? yes no yes no yes no Figure 3: Loq matching algorithm. un-informative tests, or of one that is deﬁnitive. Thus, in the current ordering, co-reference ensures a match, while structural parallelism in and of itself does not. 3.2. Input The matching criteria determine the form and kind of information in the text input. As with commercial synthe- sizers, this includes part of speech tagging. Loq uses the output of Lingsofts ENGCG (English Constraint Gram- mar) software which provides both tags and phrase struc- ture information. However, reliable automatic means for identifying other information, such as grammatical clauses, empty categories, attachment and co-reference do not yet exist. Therefore, this information was entered by hand. The Loq software turns the parsed and annotated text into a sequence of tokens that assembles clauses in a bot- tom up fashion, starting with the word and followed by the syntactic and grammatical clauses to which it belongs. This models the readers assembly of the words into mean- ingful syntactic and grammatical groupings.1 To facilitate the matching process, the text is also aug- mented with information from the WordNet database for semantic comparisons, a pronunciation database for acous- tical comparisons and the Thorndike-Lorge and Kucera- Francis for word frequency counts2 to scale the match score by the prior probability for the language. The WordNet synonym indices were assigned by hand. However, all sub- sequent semantic comparisons using WordNet are auto- matic as required by the matching process. 3.3. Mapping I have described how AWM produces the L accent (or none) for retrievable items, and H for new ones. However, there are more than two pitch accents  Pierrehumbert et al.[1] identify six3  and more components to prosody. Ob- taining them from one model ﬁrst requires an adjustment such that given or new status is determined from the ef- fect of the stimulus on the region as a whole, as follows: The result of any one comparison aﬀects the state of the item to which the stimulus is compared. State is simply deﬁned  a L annotation records a match most any crite- rion,4 and a H annotation records a match score of zero. 1Adapting this for a spontaneous speaker would proceed in reverse, from the concept, to grammatical roles, syntactic phrases and ﬁnally, the words. 2As provided in the Oxford Psycholinguistic Database. 3L, H, LH, LH, HL, HL. 4Some criteria are parasitic and only contribute to the score in combination with other criteria. L H L L H After comparisons H L L H H LH Before comparisons LH L H L H H L H L L H Figure 4: In Loq, bitonals occur when the LH counts diﬀer before and after the matching process. The main tone of a bitonal is treated as a categorical indicator of the magnitude of the eﬀect of the stimulus on the context. Thus, the comparison process registers both priming and a true match. Both receive L annotations, but only a match whose score exceeds the threshold stops the search. A pitch accent is then derived by comparing the con- tents of the search radius before and after the matching process. Majority rules apply such that the annotation with the higher count becomes the deﬁning tone. If both the before and after conﬁgurations are composed mainly of L annotations, the accent form is LL, which becomes the L accent. However, if there is a change, for example, from a L to H majority, the accent form is LH. The in- terpretation of LL is, roughly, that a familiar item was expected and provided. Likewise, the interpretation for LH is that a familiar item was expected but an unex- pected one provided. To complete the bitonal derivation, Loq treats the lo- cation of the main tone as a categorical reﬂection of the magnitude of the eﬀect of the stimulus. If the stimulus changes the annotations for the majority of items in the search region, the second tone is the main tone. Other- wise, it is the ﬁrst. This schema produces the six pitch accents identiﬁed by Pierrehumbert et al. More generally, the annotation schema provides the model with a simple form of feedback  the results of prior processing persist and contribute to a bias that aﬀects future processing. The pitch accent mapping illustrates the main features of the prosodic mapping in general. First, all mappings reﬂect the activity and state within the region deﬁned by the search radius. Second, they express some aspects of prosody as a plausible consequence of search and storage. For example, storage and search times are mapped to word and pause duration. However, others  for example, the bitonal derivation are, at best, coherent with the opera- tion and purpose of the model and not contradicted by the current (sparse) data on the relation of cognitive capacity to the prosody of read speech. In all, the mapping from AWM activity and state produces intonational categories (pitch accent, phrase accent and boundary tone) and their prominence, word duration, pause duration and the pitch range of an intonational phrase. 5 10 15 20 0 20 40 60 H unaccented search radius words Figure 5: Mean unaccented vs. H accented word counts as a function of search radius. 0 5 10 15 20 25 0 0.2 0.4 0.6 0.8 search radius prominence Figure 6: Mean pitch accent prominence for all six accent types, as a function of search radius. 4. RESULTS Although simulations were run using text from three diﬀerent genres (ﬁction, radio broadcast, rhymed poetry), two and three dimensional AWM spaces and three memory sizes (small, mid-range and large), most of the prosodic output was correlated with the search radius. There- fore, the results reported here are for the mid-range two- dimensional memory (22x22) and for the news report text only (one paragraph, 68 words.) Five simulations were run for each radius. True to the attentional predictions, Figure 5 shows that as the search radius increases, the mean number of unac- cented words increases as well, while the number of H ac- cents decreases. Under the current mapping, pitch accent prominence is a function the distance at which the search stops and the number of comparisons performed prior to stopping. This produces a decrease in the mean promi- nence as the search radius increases (Figure 6). These patterns contribute to the lively and child-like intonation produced for the smallest radii (1 and 2), the expressive but more subdued intonation for the mid-range radii (3-8) and ﬂatter intonation of the higher radii. The naturalness of synthetic prosody is diﬃcult to evalu- ate via in perceptual tests[8]. However, informal comments from listeners revealed that while the three styles were rec- ognizable and the prosody more natural-sounding than the commercial default, it was best for shorter sections rather than for the passages as a whole. A comparison with the natural prosody for the same text (the BU Corpus radio newscasts) showed that when the simulations agreed on pitch accent location and type, they tended to disagree on boundary location and type, mostly because the Loq simulations produced many more phrase breaks than the natural speaker. 5. CONCLUSION AND FUTURE DIRECTIONS Loq is a production model. It produces prosody as the consequence of cognitive processing as modeled by the AWM component. Its focus on retrieval makes it a perfor- mance model as well, demonstrating that prosody is not determined solely by the text. It produces three recogniz- able styles that appear to correlate with retrieval capac- ities as deﬁned by the search radius: child-like (for radii of 1 and2), adult expressive (for radii between 3 and 8) and knowledgeable (for radii higher than 8). This is a step towards producing prosody that is both expressive and natural and, in addition, speciﬁc to the speaker, Currently, the main problem is that the prosody is not entirely cohesive within one text. Therefore, one next step is to explore variations on the mapping of AWM activity and state to prosodic features. More distant work includes extending the model to incorporate other inﬂuences, espe- cially the inﬂuence of physiology. This may be the key to producing more than three styles, and to incorporat- ing both the dynamics and constraints that will produce consistently natural-sounding speech. 6. REFERENCES 1. Mary E. Beckman and Janet B. Pierrehumbert. Into- national structure in Japanese and English. In Colin Ewen and John Anderson, editors, Phonology Yearbook 3, pages 255309. Cambridge University Press, 1986. 2. Gillian Brown. Prosodic Structure and the GivenNew Distinction. In A. Cutler and D. R. Ladd, editors, Prosody: Models and Measurements, chapter 6, pages 6777. Springer Verlag, 1983. 3. David Fay and Anne Cutler. Malapropisms and the Structure of the Mental Lexicon. Linguistic Inquiry, 8(3):505520, Summer 1977. 4. A. Fernald and T. Simon. Expanded Intonation Con- tours in Mothers Speech to Newborns. Developmental Psychology, 20:104113, 1984. 5. Thomas K. Landauer. Memory Without Organiza- tion: Properties of a Model with Random Storage and Undirected Retrieval. Cognitive Psychology, 7:495531, 1975. 6. S. G. Nooteboom and J. M. B. Terken. What Makes Speakers Omit Pitch Accents? An Experiment. Pho- netica, 39:317336, 1982. 7. Janet B. Pierrehumbert and Julia Hirschberg. The Meaning of Intonation Contours in the Interpretation of Discourse. In Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, editors, Intentions in Communica- tion, pages 271311. MIT Press, 1990. 8. K. Ross and M. Ostendorf. Prediction of abstract prosodic labels for speech synthesis. Computer Speech and Language, 10:155185, 1996. 9. Marilyn A. Walker. Testing collaborative strategies by computational simulations: cognitive and task eﬀects. Knowledge-Based Systems, 8(2-3):105116, April-June 1995.",
  "32.pdf": "arXiv:cs9905001v1 [cs.CL] 2 May 1999 Supervised Grammar Induction using Training Data with Limited Constituent Information  Rebecca Hwa Division of Engineering and Applied Sciences Harvard University Cambridge, MA 02138 USA rebeccaeecs.harvard.edu Abstract Corpus-based grammar induction generally re- lies on hand-parsed training data to learn the structure of the language. Unfortunately, the cost of building large annotated corpora is pro- hibitively expensive. This work aims to improve the induction strategy when there are few labels in the training data. We show that the most in- formative linguistic constituents are the higher nodes in the parse trees, typically denoting com- plex noun phrases and sentential clauses. They account for only 20 of all constituents. For in- ducing grammars from sparsely labeled training data (e.g., only higher-level constituent labels), we propose an adaptation strategy, which pro- duces grammars that parse almost as well as grammars induced from fully labeled corpora. Our results suggest that for a partial parser to replace human annotators, it must be able to automatically extract higher-level constituents rather than base noun phrases. 1 Introduction The availability of large hand-parsed corpora such as the Penn Treebank Project has made high-quality statistical parsers possible. How- ever, the parsers risk becoming too tailored to these labeled training data that they cannot re- liably process sentences from an arbitrary do- main. Thus, while a parser trained on the Wall Street Journal corpus can fairly accurately parse a new Wall Street Journal article, it may not perform as well on a New Yorker article. To parse sentences from a new domain, one would normally directly induce a new grammar  This material is based upon work supported by the Na- tional Science Foundation under Grant No. IRI 9712068. We thank Stuart Shieber for his guidance, and Lillian Lee, Ric Crabbe, and the three anonymous reviewers for their comments on the paper. from that domain, in which the training pro- cess would require hand-parsed sentences from the new domain. Because parsing a large cor- pus by hand is a labor-intensive task, it would be beneﬁcial to minimize the number of labels needed to induce the new grammar. We propose to adapt a grammar already trained on an old domain to the new domain. Adaptation can exploit the structural similar- ity between the two domains so that fewer la- beled data might be needed to update the gram- mar to reﬂect the structure of the new domain. This paper presents a quantitative study com- paring direct induction and adaptation under diﬀerent training conditions. Our goal is to un- derstand the eﬀect of the amounts and types of labeled data on the training process for both induction strategies. For example, how much training data need to be hand-labeled? Must the parse trees for each sentence be fully spec- iﬁed? Are some linguistic constituents in the parse more informative than others? To answer these questions, we have performed experiments that compare the parsing quali- ties of grammars induced under diﬀerent train- ing conditions using both adaptation and di- rect induction. We vary the number of labeled brackets and the linguistic classes of the labeled brackets. The study is conducted on both a sim- ple Air Travel Information System (ATIS) cor- pus (Hemphill et al., 1990) and the more com- plex Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Our results show that the training examples do not need to be fully parsed for either strat- egy, but adaptation produces better grammars than direct induction under the conditions of minimally labeled training data. For instance, the most informative brackets, which label con- stituents higher up in the parse trees, typically identifying complex noun phrases and senten- tial clauses, account for only 17 of all con- stituents in ATIS and 21 in WSJ. Trained on this type of label, the adapted grammars parse better than the directly induced grammars and almost as well as those trained on fully labeled data. Training on ATIS sentences labeled with higher-level constituent brackets, a directly in- duced grammar parses test sentences with 66 accuracy, whereas an adapted grammar parses with 91 accuracy, which is only 2 lower than the score of a grammar induced from fully la- beled training data. Training on WSJ sentences labeled with higher-level constituent brackets, a directly induced grammar parses with 70 accuracy, whereas an adapted grammar parses with 72 accuracy, which is 6 lower than the score of a grammar induced from fully labeled training data. That the most informative brackets are higher-level constituents and make up only one- ﬁfth of all the labels in the corpus has two impli- cations. First, it shows that there is potential reduction of labor for the human annotators. Although the annotator still must process an entire sentence mentally, the task of identifying higher-level structures such as sentential clauses and complex nouns should be less tedious than to fully specify the complete parse tree for each sentence. Second, one might speculate the pos- sibilities of replacing human supervision alto- gether with a partial parser that locates con- stituent chunks within a sentence. However, as our results indicate that the most informa- tive constituents are higher-level phrases, the parser would have to identify sentential clauses and complex noun phrases rather than low-level base noun phrases. 2 Related Work on Grammar Induction Grammar induction is the process of inferring the structure of a language by learning from ex- ample sentences drawn from the language. The degree of diﬃculty in this task depends on three factors. First, it depends on the amount of supervision provided. Charniak (1996), for in- stance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. On the other hand, if the examples consist of raw sentences with no extra struc- tural information, grammar induction is very diﬃcult, even theoretically impossible (Gold, 1967). One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979), which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992). Part of our work explores the in-between case, when only some constituent la- bels are available. Section 3 deﬁnes the diﬀerent types of annotation we examine. Second, as supervision decreases, the learning process relies more on search. The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. For ﬁnding a good initial parameter set, Lari and Young (1990) suggested ﬁrst estimating the probabili- ties with a set of regular grammar rules. Their experiments, however, indicated that the main beneﬁt from this type of pretraining is one of run-time eﬃciency; the improvement in the quality of the induced grammar was minimal. Briscoe and Waegner (1992) argued that one should ﬁrst hand-design the grammar to en- code some linguistic notions and then use the re- estimation procedure to ﬁne-tune the parame- ters, substituting the cost of hand-labeled train- ing data with that of hand-coded grammar. Our idea of grammar adaptation can be seen as a form of initialization. It attempts to seed the grammar in a favorable search space by ﬁrst training it with data from an existing corpus. Section 4 discusses the induction strategies in more detail. A third factor that aﬀects the learning pro- cess is the complexity of the data. In their study of parsing the WSJ, Schabes et al. (1993) have shown that a grammar trained on the Inside- Outside re-estimation algorithm can perform quite well on short simple sentences but falters as the sentence length increases. To take this factor into account, we perform our experiments Categories Labeled Sentence ATIS WSJ HighP (I want (to take (the ﬂight with at most one stop))) 17 21 BaseNP (I) want to take (the ﬂight) with (at most one stop) 27 29 BaseP (I) want to take (the ﬂight) with (at most one) stop 32 30 AllNP (I) want to take ((the ﬂight) with (at most one stop)) 37 43 NotBaseP (I (want (to (take (the ﬂight (with (at most one stop))))))) 68 70 Table 1: The second column shows how the example sentence ((I) (want (to (take ((the flight) (with ((at most one) stop))))))) is labeled under each category. The third and fourth columns list the percentage break-down of brackets in each category for ATIS and WSJ respectively. on both a simple domain (ATIS) and a complex one (WSJ). In Section 5, we describe the exper- iments and report the results. 3 Training Data Annotation The training sets are annotated in multiple ways, falling into two categories. First, we con- struct training sets annotated with random sub- sets of constituents consisting 0, 25, 50, 75 and 100 of the brackets in the fully an- notated corpus. Second, we construct sets train- ing in which only a certain type of constituent is annotated. We study ﬁve linguistic categories. Table 1 summarizes the annotation diﬀerences between the ﬁve classes and lists the percent- age of brackets in each class with respect to the total number of constituents1 for ATIS and WSJ. In an AllNP training set, all and only the noun phrases in the sentences are labeled. For the BaseNP class, we label only simple noun phrases that contain no embedded noun phrases. Similarly for a BaseP set, all sim- ple phrases made up of only lexical items are labeled. Although there is a high intersection between the set of BaseP labels and the set of BaseNP labels, the two classes are not identical. A BaseNP may contain a BaseP. For the exam- ple in Table 1, the phrase at most one stop is a BaseNP that contains a quantiﬁer BaseP at most one. NotBaseP is the complement of BaseP. The majority of the constituents in a sentence belongs to this category, in which at least one of the constituents sub-constituents is not a simple lexical item. Finally, in a HighP set, we label only complex phrases that decom- 1For computing the percentage of brackets, the outer- most bracket around the entire sentence and the brack- ets around singleton phrases (e.g., the pronoun I as a BaseNP) are excluded because they do not contribute to the pruning of parses. pose into sub-phrases that may be either an- other HighP or a BaseP. That is, a HighP con- stituent does not directly subsume any lexical word. A typical HighP is a sentential clause or a complex noun phrase. The example sentence in Table 1 contains 3 HighP constituents: a com- plex noun phrase made up of a BaseNP and a prepositional phrase; a sentential clause with an omitted subject NP; and the full sentence. 4 Induction Strategies To induce a grammar from the sparsely brack- eted training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992). The inferred grammars are repre- sented in the Probabilistic Lexicalized Tree In- sertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a), which is lexical- ized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b)). Second, its lexicalized representation makes the training process more eﬃcient than a traditional PCFG while main- taining comparable parsing qualities. Two training strategies are considered: di- rect induction, in which a grammar is induced from scratch, learning from only the sparsely la- beled training data; and adaptation, a two-stage learning process that ﬁrst uses direct induction to train the grammar on an existing fully la- beled corpus before retraining it on the new cor- pus. During the retraining phase, the probabil- ities of the grammars are re-estimated based on the new training data. We expect the adaptive method to induce better grammars than direct induction when the new corpus is only partially annotated because the adapted grammars have collected better statistics from the fully labeled data of another corpus. 5 Experiments and Results We perform two experiments. The ﬁrst uses ATIS as the corpus from which the diﬀerent types of partially labeled training sets are gener- ated. Both induction strategies train from these data, but the adaptive strategy pretrains its grammars with fully labeled data drawn from the WSJ corpus. The trained grammars are scored on their parsing abilities on unseen ATIS test sets. We use the non-crossing bracket mea- surement as the parsing metric. This experi- ment will show whether annotations of a partic- ular linguistic category may be more useful for training grammars than others. It will also in- dicate the comparative merits of the two induc- tion strategies trained on data annotated with these linguistic categories. However, pretrain- ing on the much more complex WSJ corpus may be too much of an advantage for the adaptive strategy. Therefore, we reverse the roles of the corpus in the second experiment. The partially labeled data are from the WSJ corpus, and the adaptive strategy is pretrained on fully labeled ATIS data. In both cases, part-of-speech(POS) tags are used as the lexical items of the sen- tences. Backing oﬀ to POS tags is necessary because the tags provide a considerable inter- section in the vocabulary sets of the two cor- pora. 5.1 Experiment 1: Learning ATIS The easier learning task is to induce grammars to parse ATIS sentences. The ATIS corpus con- sists of 577 short sentences with simple struc- tures, and the vocabulary set is made up of 32 POS tags, a subset of the 47 tags used for the WSJ. Due to the limited size of this corpus, ten sets of randomly partitioned train-test-held-out triples are generated to ensure the statistical signiﬁcance of our results. We use 80 sentences for testing, 90 sentences for held-out data, and the rest for training. Before proceeding with the main discussion on training from the ATIS, we brieﬂy describe the pretraining stage of the adaptive strategy. 5.1.1 Pretraining with WSJ The idea behind the adaptive method is simply to make use of any existing labeled data. We hope that pretraining the grammars on these data might place them in a better position to learn from the new, sparsely labeled data. In the pretraining stage for this experiment, a grammar is directly induced from 3600 fully labeled WSJ sentences. Without any further training on ATIS data, this grammar achieves a parsing score of 87.3 on ATIS test sentences. The relatively high parsing score suggests that pretraining with WSJ has successfully placed the grammar in a good position to begin train- ing with the ATIS data. 5.1.2 Partially Supervised Training on ATIS We now return to the main focus of this experi- ment: learning from sparsely annotated ATIS training data. To verify whether some con- stituent classes are more informative than oth- ers, we could compare the parsing scores of the grammars trained using diﬀerent constituent class labels. But this evaluation method does not take into account that the distribution of the constituent classes is not uniform. To nor- malize for this inequity, we compare the parsing scores to a baseline that characterizes the rela- tionship between the performance of the trained grammar and the number of bracketed con- stituents in the training data. To generate the baseline, we create training data in which 0, 25, 50, 75, and 100 of the constituent brackets are randomly chosen to be included. One class of linguistic labels is better than an- other if its resulting parsing improvement over the baseline is higher than that of the other. The test results of the grammars induced from these diﬀerent training data are summa- rized in Figure 1. Graph (a) plots the outcome of using the direct induction strategy, and graph (b) plots the outcome of the adaptive strat- egy. In each graph, the baseline of random con- stituent brackets is shown as a solid line. Scores of grammars trained from constituent type spe- ciﬁc data sets are plotted as labeled dots. The dotted horizontal line in graph (b) indicates the ATIS parsing score of the grammar trained on WSJ alone. Comparing the ﬁve constituent types, we see that the HighP class is the most informative 50 55 60 65 70 75 80 85 90 95 0 200 400 600 800 1000 1200 1400 1600 Parsing accuracies of directly induced grammars on ATIS test set Number of brackets in the ATIS training data HighP BaseNP BaseP AllNP NotBaseP Rand-100 Rand-75 Rand-50 Rand-25 50 55 60 65 70 75 80 85 90 95 0 200 400 600 800 1000 1200 1400 1600 Parsing accuracies of adapted grammars on ATIS test set Number of brackets in the ATIS training data HighP BaseNP BaseP AllNP NotBaseP Rand-100 Rand-75 Rand-50 Rand-25 WSJ only (a) (b) Figure 1: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as a function of the number of brackets present in the training corpus. There are 1595 brackets in the training corpus all together. for the adaptive strategy, resulting in a gram- mar that scored better than the baseline. The grammars trained on the AllNP annotation per- formed as well as the baseline for both strate- gies. Grammars trained under all the other training conditions scored below the baseline. Our results suggest that while an ideal train- ing condition would include annotations of both higher-level phrases and simple phrases, com- plex clauses are more informative. This inter- pretation explains the large gap between the parsing scores of the directly induced grammar and the adapted grammar trained on the same HighP data. The directly induced grammar performed poorly because it has never seen a labeled example of simple phrases. In contrast, the adapted grammar was already exposed to labeled WSJ simple phrases, so that it success- fully adapted to the new corpus from annotated examples of higher-level phrases. On the other hand, training the adapted grammar on anno- tated ATIS simple phrases is not successful even though it has seen examples of WSJ higher- level phrases. This also explains why gram- mars trained on the conglomerate class Not- BaseP performed on the same level as those trained on the AllNP class. Although the Not- BaseP set contains the most brackets, most of the brackets are irrelevant to the training pro- cess, as they are neither higher-level phrases nor simple phrases. Our experiment also indicates that induction strategies exhibit diﬀerent learning characteris- tics under partially supervised training condi- tions. A side by side comparison of Figure 1 (a) and (b) shows that the adapted grammars perform signiﬁcantly better than the directly induced grammars as the level of supervision decreases. This supports our hypothesis that pretraining on a diﬀerent corpus can place the grammar in a good initial search space for learn- ing the new domain. Unfortunately, a good ini- tial state does not obviate the need for super- vised training. We see from Figure 1(b) that retraining with unlabeled ATIS sentences actu- ally lowers the grammars parsing accuracy. 5.2 Experiment 2: Learning WSJ In the previous section, we have seen that anno- tations of complex clauses are the most helpful for inducing ATIS-style grammars. One of the goals of this experiment is to verify whether the result also holds for the WSJ corpus, which is structurally very diﬀerent from ATIS. The WSJ corpus uses 47 POS tags, and its sentences are longer and have more embedded clauses. As in the previous experiment, we construct training sets with annotations of diﬀerent con- stituent types and of diﬀerent numbers of ran- domly chosen labels. Each training set consists of 3600 sentences, and 1780 sentences are used as held-out data. The trained grammars are tested on a set of 2245 sentences. Figure 2 (a) and (b) summarize the outcomes 35 40 45 50 55 60 65 70 75 80 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 Parsing accuracies of directly induced grammars on WSJ test set Number of brackets in the WSJ training data HighP BaseNP BaseP AllNP NotBaseP Rand-100 Rand-75 Rand-50 Rand-25 35 40 45 50 55 60 65 70 75 80 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 Parsing accuracies of adapted grammars on WSJ test set number of brackets in the WSJ training data HighP BaseNP BaseP AllNP NotBaseP ATIS only Rand-100 Rand-75 Rand-50 Rand-25 (a) (b) Figure 2: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as a function of the number of brackets present in the training corpus. There is a total of 46463 brackets in the training corpus. of this experiment. Many results of this section are similar to the ATIS experiment. Higher- level phrases still provide the most information; the grammars trained on the HighP labels are the only ones that scored as well as the baseline. Labels of simple phrases still seem the least in- formative; scores of grammars trained on BaseP and BaseNP remained far below the baseline. Diﬀerent from the previous experiment, how- ever, the AllNP training sets do not seem to provide as much information for this learning task. This may be due to the increase in the sentence complexity of the WSJ, which further de-emphasized the role of the simple phrases. Thus, grammars trained on AllNP labels have comparable parsing scores to those trained on HighP labels. Also, we do not see as big a gap between the scores of the two induction strate- gies in the HighP case because the adapted grammars advantage of having seen annotated ATIS base nouns is reduced. Nonetheless, the adapted grammars still perform 2 better than the directly induced grammars, and this im- provement is statistically signiﬁcant.2 Furthermore, grammars trained on NotBaseP do not fall as far below the baseline and have higher parsing scores than those trained on HighP and AllNP. This suggests that for more complex domains, other linguistic constituents 2A pair-wise t-test comparing the parsing scores of the ten test sets for the two strategies shows 99 conﬁ- dence in the diﬀerence. such as verb phrases3 become more informative. A second goal of this experiment is to test the adaptive strategy under more stringent condi- tions. In the previous experiment, a WSJ-style grammar was retrained for the simpler ATIS corpus. Now, we reverse the roles of the cor- pora to see whether the adaptive strategy still oﬀers any advantage over direct induction. In the adaptive methods pretraining stage, a grammar is induced from 400 fully labeled ATIS sentences. Testing this ATIS-style gram- mar on the WSJ test set without further train- ing renders a parsing accuracy of 40. The low score suggests that fully labeled ATIS data does not teach the grammar as much about the structure of WSJ. Nonetheless, the adap- tive strategy proves to be beneﬁcial for learning WSJ from sparsely labeled training sets. The adapted grammars out-perform the directly in- duced grammars when more than 50 of the brackets are missing from the training data. The most signiﬁcant diﬀerence is when the training data contains no label information at all. The adapted grammar parses with 60.1 accuracy whereas the directly induced grammar parses with 49.8 accuracy. 3We have not experimented with training sets con- taining only verb phrases labels (i.e., setting a pair of bracket around the head verb and its modiﬁers). They are a subset of the NotBaseP class. 6 Conclusion and Future Work In this study, we have shown that the structure of a grammar can be reliably learned without having fully speciﬁed constituent information in the training sentences and that the most in- formative constituents of a sentence are higher- level phrases, which make up only a small per- centage of the total number of constituents. Moreover, we observe that grammar adaptation works particularly well with this type of sparse but informative training data. An adapted grammar consistently outperforms a directly in- duced grammar even when adapting from a sim- pler corpus to a more complex one. These results point us to three future di- rections. First, that the labels for some con- stituents are more informative than others im- plies that sentences containing more of these in- formative constituents make better training ex- amples. It may be beneﬁcial to estimate the informational content of potential training (un- marked) sentences. The training set should only include sentences that are predicted to have high information values. Filtering out unhelpful sentences from the training set reduces unnec- essary work for the human annotators. Second, although our experiments show that a sparsely labeled training set is more of an obstacle for the direct induction approach than for the grammar adaptation approach, the direct induction strat- egy might also beneﬁt from a two stage learning process similar to that used for grammar adap- tation. Instead of training on a diﬀerent corpus in each stage, the grammar can be trained on a small but fully labeled portion of the corpus in its ﬁrst stage and the sparsely labeled por- tion in the second stage. Finally, higher-level constituents have proved to be the most infor- mative linguistic units. To relieve humans from labeling any training data, we should consider using partial parsers that can automatically de- tect complex nouns and sentential clauses. References J.K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547550, Boston, MA, June. E.J. Briscoe and N. Waegner. 1992. Robust stochastic parsing using the inside-outside al- gorithm. In Proceedings of the AAAI Work- shop on Probabilistically-Based NLP Tech- niques, pages 3953. E. Charniak. 1996. Treebank grammars. In Proceedings of the Thirteenth National Con- ference on Artiﬁcial Intelligence, pages 1031 1036. E. Mark Gold. 1967. Language identiﬁcation in the limit. Information Control, 10(5):447 474. C.T. Hemphill, J.J. Godfrey, and G.R. Dod- dington. 1990. The ATIS spoken language systems pilot corpus. In DARPA Speech and Natural Language Workshop, Hidden Valley, Pennsylvania, June. Morgan Kaufmann. R. Hwa. 1998a. An empirical evaluation of probabilistic lexicalized tree insertion gram- mars. In Proceedings of COLING-ACL, vol- ume 1, pages 557563. R. Hwa. 1998b. An empirical evaluation of probabilistic lexicalized tree insertion gram- mars. Technical Report 06-98, Harvard Uni- versity. Available as cmp-lg9808001. K. Lari and S.J. Young. 1990. The estima- tion of stochastic context-free grammars us- ing the inside-outside algorithm. Computer Speech and Language, 4:3556. M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annontated corpus of english: the penn treebank. Computational Linguistics, 19(2):313330. F. Pereira and Y. Schabes. 1992. Inside- Outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the ACL, pages 128135, Newark, Delaware. Y. Schabes and R. Waters. 1993. Stochastic lexicalized context-free grammar. In Proceed- ings of the Third International Workshop on Parsing Technologies, pages 257266. Y. Schabes, M. Roth, and R. Osborne. 1993. Parsing the Wall Street Journal with the Inside-Outside algorithm. In Proceedings of the Sixth Conference of the European Chap- ter of the ACL, pages 341347.",
  "33.pdf": "arXiv:cs9906003v1 [cs.CL] 2 Jun 1999 THE SYNTACTIC PROCESSING OF PARTICLES IN JAPANESE SPOKEN LANGUAGE Melanie Siegel Department of Computational Linguistics University of the Saarland Postfach 151150 D-66041 Saarbrucken, Germany siegeldfki.de Abstract Particles fullﬁll several distinct central roles in the Japanese language. They can mark arguments as well as adjuncts, can be functional or have semantic funtions. There is, however, no straightforward matching from particles to functions, as, e.g., ga can mark the subject, the object or an adjunct of a sentence. Particles can cooccur. Verbal arguments that could be identiﬁed by particles can be eliminated in the Japanese sentence. And ﬁnally, in spoken language particles are often omitted. A proper treatment of particles is thus necessary to make an analysis of Japanese sentences possible. Our treatment is based on an empirical investigation of 800 dialogues. We set up a type hierarchy of particles motivated by their subcategorizational and modiﬁcational behaviour. This type hierarchy is part of the Japanese syntax in VERBMOBIL. 1 Introduction The treatment of particles is essential for the processing of the Japanese language for two reasons. The ﬁrst reason is that these are the words that occur most frequently. The second reason is that particles have various central functions in the Japanese syntax: case particles mark subcategorized verbal arguments, postpositions mark adjuncts and have semantic attributes, topic particles mark topicalized phrases and no marks an attributive nominal adjunct. Their treatment is diﬃcult for three reasons: 1) despite their central position in Japanese syntax the omission of particles occurs quite often in spoken language. 2) One particle can fulﬁll more than one function. 3) Particles can cooccur, but not in an arbitrary way. In order to set up a grammar that accounts for a larger amount of spoken language, a comprehensive investigation of Japanese particles is thus necessary. Such a comprehensive investigation of Japanese particles was missing up to now. Two kinds of solutions have previously been proposed: (1) the particles are divided into case particles and postpositions. The latter build the heads of their phrases, while the former do not (cf. [6], [12]). (2) All kinds of particles build the head of their phrases and have the same lexical structure (cf. [1]). Both kinds of analyses lead to problems: if postpositions are heads, while case particles are nonheads, a suﬃcient treatment of those cases where two or three particles occur sequentially is not possible, as we will show. If on the other hand there is no distinction of particles, it is not possible to encode their diﬀerent behaviour in subcategorization and modiﬁcation. We carried out an empirical investigation of cooccurrences of particles in Japanese spoken language. As a result, we could set up restrictions for 25 particles. We show that the problem is essentially based at the lexical level. Instead of assuming diﬀerent phrase structure rules we state a type hierarchy of Japanese particles. This makes a uniform treatment of phrase structure as well as a diﬀerentiation of subcategorization patterns possible. We therefore adopt the all-head analysis, but extend it by a type hierarchy in order to be able to diﬀerentiate between the particles. Our analysis is based on 800 Japanese dialogues of the VERBMOBIL data concerning appointment scheduling. 2 The Type Hierarchy of Japanese Particles Japanese noun phrases can be modiﬁed by more than one particle at a time. There are many examples in our data where two or three particles occur sequentially. On the one hand, this phenomenon must be accounted for in order to attain a correct processing of the data. On the other hand, the discrimination of particles is Particle case-particle wo ga ni-case complementizer to modifying particle noun-modifying particle no verb-modifying particle topic-particle wa ga-top mo koso adverbial particle ni-adv-p to-adv-p de postpositions Figure 1: Type Hierarchy of Japanese Particles. Postpositions include e, naNka, sonota, tomo, kara, made, soshite, nado, bakari, igai, yori, toshite, toshimashite, nitsuite, nikaNshite and nikakete. motivated by their modiﬁcational and subcategorizational behaviour. We carried out an empirical analysis, based on our dialogue data. Table 1 shows the frequency of cooccurrence of two particles in the dialogue data. There is a tendency to avoid the cooccurrence of particles with the same phonology, even if it is possible in principal in some cases. The reason is obvious: such sentences are diﬃcult to understand. leftright ga wo ni de e kara made no wa mo naNka to ga 0 0 0 0 0 0 0 0 0 0 0 0 wo 0 0 0 0 0 0 0 0 0 0 0 3 ni 0 0 0 19 0 0 0 0 137 49 0 15 de 2 0 0 0 0 0 0 14 158 241 0 30 e 0 0 0 1 0 0 0 4 0 0 0 0 kara 23 0 30 81 0 0 0 34 69 12 0 123 made 17 1 66 32 0 0 0 40 63 1 0 79 no 64 9 1 2249 0 0 0 0 287 11 0 4 wa 0 0 0 2 0 0 0 0 0 0 1 3 mo 0 0 0 0 0 0 0 0 0 0 0 0 naNka 3 0 0 1 0 0 0 0 30 0 0 0 to 0 3 0 1 0 0 0 14 17 58 0 0 toshite 0 0 0 0 0 0 0 0 36 15 0 0 toshimashite 0 0 0 0 0 0 0 0 15 0 0 0 Table 1: Cooccurrence of 2 Particles in the 800 Dialogues [4] treats wa, ga, wo, ni, de, to, made, kara and ya as particles. They are divided into those that are in the deep structure and those that are introduced through transformations. An example for the former is kara, examples for the latter are ga(SBJ), wo(OBJ), ga(OBJ) and ni(OBJ2). [1] assigns all particles the part-of- speech P. Examples are ga, wo, ni, no, de, e, kara and made. All particles are heads of their phrases. Verbal arguments get a grammatical relation [GR OBJSBJ]. In [2] the part-of-speech class P contains only ga, wo and ni. [12] deﬁnes postpositions and case particles such that postpositions are the Japanese counterpart of prepositions in English and cannot stand independently, while case particles assign case and can follow postpositions. Her case particles include ga, wo, ni, no and wa. [7] divides case markers (ga, wo, ni and wa) from copula forms (ni, de, na and no). He argues that ni, de, na and no are the inﬁnitive, gerund and adnominal forms of the copula. In the class of particles, we include case particles, complementizers, modifying particles and conjunctional particles. We thus assume a common class of the several kinds of particles introduced by the other authors. But they are further divided into subclasses, as can be seen in ﬁgure 1. We assume not only a diﬀerentiation between case particles and postpositions, but a ﬁner graded distinction that includes diﬀerent kinds of particles not mentioned by the other authors. de is assumed to be a particle and not a copula, as [7] proposes. It belongs to the class of adverbial particles. One major motivation for the type hierarchy is the observation we made of the cooccurrence of particles. Case particles (ga, wo, ni) are those that attach to verbal arguments. A complementizer marks complement sentences. Modifying particles attach to adjuncts. They are further divided into noun-modifying particles and verb-modifying particles. Verb modifying particles can be topic particles, adverbial particles, or postpositions. Some particles can have more than one function, as for example ni has the function of a case particle and an adverbial particle. Figure 1 shows the type hierarchy of Japanese particles. The next sections examine the individual types of particles. 2.1 Case Particles There is no number nor gender agreement between noun phrase and verb. The verbs assign case to the noun phrases. This is marked by the case particles. Therefore these have a syntactic function, but not a semantic one. Unlike in English, the grammatical functions cannot be assigned through positions in the sentence or c-command-relations, since Japanese exhibits no ﬁxed word position for verbal arguments. The assignment of the grammatical function is not expressed by the case particle alone but only in connection with the verbal valency. There are verbs that require ga-marked objects, while in most cases the ga-marked argument is the subject: (1) nantoka somehow yotei time ga GA toreru can take N desu COP ga SAP (Somehow (I) can ﬁnd some time.) Japanese is described as a head-ﬁnal language. [1] therefore assumes only one phrase structure rule: Mother  Daughter Head. However, research literature questions whether this also applies to nominal phrases and their case particles. [9]:45 assume Japanese case particles to be markers. On the one hand, there are several reasons to distinguish case particles and modifying particles. On the other hand, I doubt whether it is reasonable to assume diﬀerent phrase structures for NPcase particle and NPmodifying particle. The phrase-structural distinction of case particles and postpositions leads to problems, when more than one particle occur. The following example comes from the Verbmobil corpus: (2) naNji what time kara from ga GA yoroshii good desu COP ka QUE (At what time would you like to start?) If one now assumes that the modifying particle kara is head of naNji as well as of the case particle ga, the result for naNji kara ga with the head-marker structure described in [9]1 would be as shown in ﬁgure 2. The case particle ga would have to allow nouns and modifying particles in SPEC. The latter are however normally adjuncts that modify verbal projections. Therefore the head of kara entails the information that it can modify a verb. This information is inherited to the head of the whole phrase by the Head-Feature Principle as is to be seen in the tree above. As a result, this is also admitted as an adjunct to a verb, which leads to wrong analyses for sentences like the following one: (3) naNji what time kara from ga GA sochira you ga GA jikaN time ga GA toremasu can take ka QUE If, on the other hand, case particles and topic markers are heads, one receives a consistent and correct processing of this kind of example too. This is because the head information [MOD none] is given from the particle ga to the head of the phrase naNji kara ga. Thus this phrase is not admitted as an adjunct. Instead of assuming diﬀerent phrase structure rules, a distinction of the kinds of particles can be based on lexical types. HPSG oﬀers the possibility to deﬁne a common type and to set up speciﬁcations for the diﬀerent types of particles. We assume Japanese to be head-ﬁnal in this respect. All kinds of particles are analysed 1The Marking Principle says: In a headed phrase, the MARKING value is token-identical with that of the MARKER- DAUGHTER if any, and with that of the HEAD-DAUGHTER otherwise[9]. MARKING unmarked COMPLEMENT HEAD MARKING unmarked naNji kara MARKER HEAD ga SUBCAT  SUBCAT  MARKING unmarked HEAD [3] MARKING [1]ga HEAD [4] [2] SUBCAT  MARKING [1]ga HEAD.SPEC [2] HEAD [3] HEAD [3] SUBCAT  [5] SUBCAT [5] Fi 2 N N k i h H d M k S as heads of their phrases. The relation between case particle and nominal phrase is a Complement-Head relation. The complement is obligatory and adjacent2. Normally the case particle ga marks the subject, the case particle wo the direct object and the case particle ni the indirect object. There are, however, many exceptions. We therefore use predicate-argument-structures instead of a direct assignment of grammatical functions by the particles (and possibly transformations). The valency information of the Japanese verbs does not only contain the syntactic category and the semantic restrictions of the subcategorized arguments, but also the case particles they must be annotated with3. In most cases the ga-marked noun phrase is the subject of the sentence. However, this is not always the case. Notably stative verbs subcategorize for ga-marked objects. An example is the stative verb dekimasu4: (4) kanojo she ga GA oyogi swimming ga GA dekimasu can (She can swim.) These and other cases are sometimes called double-subject constructions in the literature. But these ga- marked noun phrases do not behave like subjects. They are neither subject to restrictions on subject hon- oriﬁcation nor subject to reﬂexive binding by the subject. This can be shown by the following example: (5) gogo afternoon no NO hou side ga GA yukkuri at ease hanashi talking ga GA dekimasu can ne SAP (We can talk at ease in the afternoon.) hanashi does not meet the semantic restriction [animate] stated by the verb dekimasu for its subject. There are even ga-marked adjuncts. [5] assumes these double-subject constructions to be derived from genitive relations. But this analysis seems not to be true for example 5), because the following sentence is wrong: (6) gogo afternoon no NO hou side no NO yukkuri at ease hanashi talk ga GA dekimasu can ne SAP The case particle wo normally marks the direct object of the sentence. In contrast to ga, no two phrases in one clause may be marked by wo. This restriction is called double-wo constraint in research literature (see, for example, [12]:249ﬀ.). Object positions with wo-marking as well as subject positions with ga-marking can be saturated only once. There are neither double subjects nor double objects. This restriction is also valid for indirect objects. Arguments found must be assigned a saturated status in the subcategorization frame, so that they cannot be saturated again (as in English). The verbs subcategorize for at most one subject, object and indirect object. Only one of these arguments may be marked by wo, while a subject and an object may both be marked by ga. These attributes are determined by the verbal valency. The wo-marked argument is not required to be adjacent to the verb. It is possible to reverse NP-ga and NP-wo as well as to insert adjuncts between the arguments and the verb. The particle ni can have the function of a case particle as well as that of an adjunct particle modifying the predicate. [10] also identify homophoneous ni that can mark adjuncts or complements. They use the notion of aﬀectedness to distinguish them. This is however not useful in our domain. [8] suggest testing the possibility of passivization. Some verbs subcategorize for a ni-marked object, as for example naru: (7) raigetsu next month ni NI naru become N desu COP ga SAP (It will be next month.) ni-marked objects cannot occur twice in the same clause, just as ga-marked subjects and wo-marked objects. The double-wo constraint is neither a speciﬁc Japanese restriction nor a speciﬁc peculiarity of the Japanese direct object. It is based on the wrong assumption that grammatical functions are assigned by case particles. There are a lot of examples with double NP-ni, but these are adjuncts. The lexical entries of case particles get a case entry in the HEAD. Possible values are ga, wo, ni and to. They are neither adjuncts nor speciﬁers and thus get the entries [MOD none] and [SPEC none]. They subcategorize for an adjacent object. This can be a noun, a postposition or an adverbial particle5. 2Obligatory Japanese arguments are always adjacent, and vice versa. 3[8] investigates the particles ni, ga and wo and also states that grammatical functions must be clearly distinguished from surface cases 4see [4] for a semantic classiﬁcation of verbs that take ga-objects 5A fundamental diﬀerence between Japanese grammar and English grammar is the fact that verbal arguments can be optional. For example, subjects and objects that refer to the speaker are omitted in most cases in spoken language. The verbal arguments can freely scramble. Additionally, there exist adjacent verbal arguments. To account for this, our subcategorization contains the attributes SAT and VAL. In SAT it is noted, whether a verbal argument is already saturated (such that it cannot be saturated again), optional or adjacent. VAL contains the agreement information for the verbal argument. Adjacency must be checked in every rule that combines heads and arguments or adjuncts. HEAD POS p CASE case MOD none SPEC none SUBCAT SAT.OBJ adjacent VAL.OBJ.LOCAL.CAT.HEAD noun or postposition or adv-p Figure 3: Head and Subcat of Case Particles 2.2 The Complementizer to to marks adjacent complement sentences that are subcategorized for by verbs like omou, iu or kaku. (8) sochira you ni NI ukagaitai visit to TO omoimasu think node SAP (I would like to visit you.) Some verbs subcategorize for a to marked object. This object can be optional or obligatory with verbs like kuraberu. (9) kono that hi day mo too chotto somewhat hito people to TO au meet yotei plan ga GA gozaimasu exist (That day too, there is a plan to meet some people.) to in these cases is categorized as a complementizer. Another possibility is that to marks an adjunct to a predicate, which qualiﬁes to as a verb modifying particle: (10) shimizu Shimizu seNsei Prof. to TO teNjikai exhibition wo WO go-issho together sasete do itadaku HON (I would like to organize an exhibition with Prof. Shimizu.) Finally, the complementizer to can be an NP conjunction (which will not be considered at the moment, see [4]). The complementizer gets a case entry, because its head is a subtype of case-particle-head. It subcategorizes for a noun, a verb, an utterance, an adverbial particle or a postposition. 2.3 Modifying Particles An essential problem is to ﬁnd criteria for the distinction of case particles and modifying particles. On the semantic level they can be distinguished in that modifying particles introduce semantics, while case particles have a functional meaning. According to this, the particle no is a modifying one, because it introduces attributive meaning, as opposed to ([12]:134), who classiﬁes it as a case particle. Another distinctive criterion that is introduced by [12]:135 says that modifying particles6 are obligatory in spoken language, while case particles can be omitted. Case particles are indeed suppressed more often, but there are also cases of suppressed modifying particles. These occur mainly in temporal expressions in our dialogue data: (11) soredewa then juuyokka 14th no NO gogo afternoon Ø Ø niji 2 oclock Ø Ø robii lobby no NO hou side de DE o machi HON-wait shite do orimasu AUX-HON (I will then wait in the lobby at 2 oclock on the 14th.) Finally [12] gives the criterion that case particles can follow modifying particles while modifying particles cannot follow case particles. This criterion in particular implies that a ﬁner distinction is necessary, as we have shown that it is not that easy. This can be realized with HPSG types. According to this criterion, no behaves like a modifying particle, while according to the criterion on meaning, it behaves like a case particle. Our ﬁrst distinction is thus a functional one: modifying particles diﬀer from case particles in that their marked entities are not subcategorized for by the verb. Case particles get the head information [CASE case] that controls agreement between verbs and their arguments. Modifying particles do not get this entry. They get the information in MOD that they can become adjuncts to verbs (verb modifying particles) or nouns (the noun modifying particle no) and semantic information. They subcategorize for a noun, as all particles do. The modifying particles share the following features in their lexical entries. 6He calls them postpositions. HEAD POS p MOD synsem SPEC none SUBCAT SAT.OBJ adjacent Figure 4: Head and Subcat of Modifying Particles 2.3.1 Verb Modifying Particles The verb modifying particles specify the modiﬁcation of the verb in MOD. The postpositions modify a (nonauxiliary) verb as an adjunct and subcategorize for a nominal object. [7] treats ni and de as the inﬁnitive and the gerund form of the copula. ni is similar to the inﬁnitive form to the extend that it can take an adverb as its argument (gogo wa furii ni nat-te i-masu  afternoon - WA - free - become). But the inﬁnitive is clearly distinct from the characteristics of ni, that cannot be used with N desu, cannot mark a relative sentence (John ga furii ni koto) and cannot be marked with the complementizer to (John ga furii ni to omou). The adjunctive form de has both qualities of a gerundive copula and qualities of a particle. But there is some data that shows diﬀerent behaviour of de and other gerundives. Firstly, it concerns the cooccurrence possibilities of de and other particles, compared to gerundive forms and particles: de wa - V-te wa de mo - V-te mo de no - V-te no de ni - V-te ni de ga - V-te ga de wo - V-te wo de de - V-te de Secondly, a gerund may modify auxiliaries, e.g. shite kudasai, shite orimasu, but de may not. Additionally there is something which distinguishes de of a copula: it may not subcategorize for a subject. A word that is an adjunct to verbs, subcategorizes for an unmarked noun or a postpositional phrase and is subcategorized for by several particles (see above) ﬁts well into our description of a verb modifying particle. The adverbial particles ni, de and to subcategorize for a noun or a postposition. As already described, to behaves like an adverbial particle, too. 2.3.2 The Noun Modifying Particle NO no is a particle that modiﬁes nominal phrases. This is an attributive modiﬁcation and has a wide range of meanings.7 [12]:134ﬀ. assigns no to the class of case particles. However, the criteria she sets up to distin- guish between case particles and postpositions do not apply to this classiﬁcation of no: ﬁrstly, Tsujimuras postpositions have their own semantic meaning. Case particles have a functional meaning. no however has a semantic, namely attributive meaning. Secondly, Tsujimuras postpositions are obligatory in spoken lan- guage, case particles are optional. no is as obligatory as kara and made. Finally, Case particles can - as Tsujimura states - follow postpositions, but postpositions cannot follow case particles. According to this criterion, no behaves like a case particle. no combines qualities of case particles with those of modifying particles (which Tsujimura calls postpositions). This means that a special treatment of this particle is necessary. The particle no subcategorizes for a noun, as the other particles do. It also modiﬁes a noun. This separates it from the other modifying particles. The particle no modiﬁes a noun phrase and occurs after a noun or a verb modifying particle. 2.3.3 Particles of Topicalization The topic particle wa can mark arguments as well as adjuncts. In the case of argument marking it replaces the case particle. In the case of adjunct marking it can replace the verb modifying particle or it can occur after it. On the syntactic level, it has to be decided, whether the topic particle marks an argument or an adjunct, when it occurs without a verb modifying particle. This is diﬃcult because of the optionality of verbal arguments in Japanese. If it marks an argument, it has to be decided which grammatical function this argument has. This problem can often not be solved on the purely syntactic level. Semantic restrictions for verbal arguments are necessary: (12) basho place no NO hou side wa WA dou how shimashou shall do ka QUE (How shall we resolve the problem of the place?) Subject and object of the verb shimashou are suppressed in this example. The sentence can be interpreted as having a topic adjunct, but no surface subject and object, when using semantic restrictions for the subject (agentive) and the object (situation). 7See also [11] HEAD POS p MOD.LOCAL.CAT.HEAD nonaux_verb IC  SPEC none SUBCAT SAT.OBJ adjacent VAL.OBJ.LOCAL.CAT.HEAD noun or vmod-p or comp or verb[te] or idiom Figure 5: Topic Particle AVM [2] analyses Japanese topicalization with a trace that introduces a value in SLASH and the Binding Fea- ture Principle that uniﬁes the value of SLASH with a wa-marked element8. This treatment is similar to the one introduced by [9] for the treatment of English topicalization. However, Japanese topicalization is fundamentally diﬀerent from English one. Firstly, it occurs more frequently. Up to 50 of the sentences are concerned ([15]). Secondly, there are examples where the topic occurs in the middle of the sentence, unlike the English topics that occur sentence-initially. Thirdly, suppressing of verbal arguments in Japanese could be called more a rule than an exception in spoken language. The SLASH approach would introduce traces in almost every sentence. This, in connection with scrambling and suppressed particles, could not be restricted in a reasonable way. If one follows Gunjis interpretation of those cases, where the topic-NP can be interpreted as a noun modifying phrase, a genitive gap has to be assumed. But this leads to assuming a genitive gap for every NP that is not modiﬁed. Further, genitive modiﬁcation can be iterated. Finally, two or three occurences of NP-wa are possible in one utterance. Thus, we decided to assign topicalized sentences the same syntactic structure as non-topicalized sentences and to resolve the problem on the lexical level. The topic particle is, on the syntactic level, interpreted as a verbal adjunct. The binding to verbal arguments is left to the semantic interpretation module in VERBMOBIL, see ﬁgure 5. mo is similar to wa in some aspects. It can mark a predicative adjunct and can follow de and ni. But it can also follow wa, an adjective and a sentence with question mark: (13) dekiru can ka QUE mo MO shiremaseN do not know (I dont know if I can) mo is a particle that has the head of a topic-adverbial particle, but a diﬀerent subcategorization frame than wa. koso is another topic particle that can occur after nouns, postpositions or adverbial particles. 2.4 Omitted Particles Some particles can be omitted in Japanese spoken language. Here is an example from the Verbmobil corpus: (14) rokugatsu June Ø Ø juusaNnichi 13th no NO kayoubi Tuesday Ø Ø gogo afternoon kara KARA wa WA ikaga good deshou COP ka QUE (Would the 13th of June suit you?) This phenomenon can be found frequently in connection with pronouns and temporal expressions in the domain of appointment scheduling. [3] assumes that exclusively wa can be suppressed. [14] however shows that there are contexts, where ga, wo or even e can be omitted. He assigns it as phonological deletion. [5] analyses omitted wo particles and explains these with linearization: a particle wo can only be omitted, when it occurs directly before a verb. [14] however gives examples to prove the opposite. It can be observed that NPs without particles can fulﬁll the functions of a verbal argument or of a verbal adjunct (ex. 14). We decided to interpret these NPs as verbal adjuncts and to leave the binding to argument positions to the semantic interpretation. NPs thus get a MOD value that allows them to modify nonauxiliary verbs. 2.5 ga-Adjuncts One can ﬁnd several examples with ga marked adjuncts in the Verbmobil data. On the level of information structure it is said that ga marks neutral descriptions or exhaustive descriptions (c.f. [1], [4]). Gunji analyses these exhaustive descriptions syntactically in the same way as he analyses his type-I topicalization. They build adjuncts that control gaps or reﬂexives in the sentence. He views ga marked adjuncts without control 8The Binding Feature Principle says: The value of a binding feature of the mother is identical to the union of the values of the binding feature of the daughters minus the category bound in the branching. [2] relations as relying on a very specialized context. However, his treatment leads to problems. Firstly, in all cases, where ga marks a constituent that is subcategorized as ga-marked by the verb, a second reading is analysed that contains a ga marked adjunct controlling a gap. This is not reasonable. The treatment of the diﬀerent meaning of ga marking and wa marking belongs to the semantics and not into the phrase structure. Secondly, this treatment assumes gaps. We already criticized this in connection with topicalization. Therefore, we do not need reﬂexive control at the moment. However, it contains mostly examples with ga marked adjuncts without syntactic control relation to the rest of the sentences. At the level of syntax, we do not decide whether a ga-marked subject or object is a neutral description or an exhaustive listing. This decision must be based on context information, where it can be ascertained whether the noun phrase is generic, anaphoric or new. We distinguish occurrences of NPga that are verbal arguments from those that are adjuncts. The examples for ga-marked adjuncts in the Verbmobil dialogues either describe a temporal entity or a human. All cases found are predicate modifying. To further restrict exhaustive interpretations, we introduced selectional restrictions for the marked NP, based on observations in the data. 3 Conclusion The syntactic behaviour of Japanese particles has been analysed based on the Verbmobil dialogue data. We observed 25 diﬀerent particles in 800 dialogues on appointment scheduling. It has been possible to set up a type hierarchy of Japanese particles. We have therefore adopted a lexical treatment instead of a syntactic treatment based on phrase structure. This is based on the diﬀerent kinds of modiﬁcation and subcategorization that occur with the particles. We analysed the Japanese particles according to their cooccurrence potential, their modiﬁcational behaviour and their occurrence in verbal arguments. We clariﬁed the question which common characteristics and diﬀerences between the individual particles exist. A classiﬁcation in categories was carried out. After that a model hierarchy could be set up for an HPSG grammar. The simple distinction into case particles and postpositions was proved to be insuﬃcient. The assignment of the grammatical function is done by the verbal valency and not directly by the case particles. The topic particle is ambiguous. Its binding is done by ambiguity and underspeciﬁcation in the lexicon and not by the Head-Filler Rule as in the HPSG for English ([9]). The approach presented here is part of the syntactic analysis of Japanese in the Verbmobil machine translation system. It is implemented in the PAGE parsing system [13]. It has been proved to be essential for the processing of a large amount of Japanese dialogue data. Further research concerning coordinating particles (to, ya, toka, yara, ka etc.) and sentence end particles (ka, node, yo, ne etc.) is necessary. References [1] Takao Gunji. Japanese Phrase Structure Grammar. Dordrecht: Reidel., 1987. [2] Takao Gunji. An overview of JPSG: A constraint-based descriptive theory for Japanese. In Proceedings of Japanese Syntactic Processing Workshop. Duke University, 1991. [3] John Hinds. Particle deletion in Japanese and Korean. Linguistic Inquiry, 8(4):602604, 1977. [4] Susumo Kuno. The Structure of Japanese Language. Cambridge, Mass.: MIT Press., 1973. [5] S.-Y. Kuroda. Japanese Syntax and Semantics. Collected Papers., volume 22 of Studies in Natural Language and Linguistic Theory. Dordrecht: Kluwer Academic Publishers, 1992. [6] Shigeru Miyagawa. Predication and numeral quantiﬁers. In William J. Poser, editor, Papers from the Second International Workshop on Japanese Syntax, pages 157191. CSLI, 1986. [7] Stephen Nightingale. An HPSG Account of the Japanese Copula and Related Phenomena. PhD thesis, University of Edinburgh, 1996. [8] Kiyoharu Ono. Annularity in the distribution of the case particles ga, o and ni in Japanese. Theoretical Linguistics, 20(1):7193, 1994. [9] C. Pollard and I.A. Sag. Head-Driven Phrase Structure Grammar. Chicago: University of Chicago Press., 1994. [10] Kumi Sadakane and Masatoshi Koizumi. On the nature of the dative particle ni in Japanese. Linguistics, 33:533, 1995. [11] Hiroshi Tsuda and Yasunari Harada. Semantics and pragmatics of adnominal particle no in Quixote. In Takao Gunji, editor, Studies in the Universality of Constraint-Based Structure Grammars. Osaka., 1996. [12] Natsuko Tsujimura. An Introduction to Japanese Linguistics. Blackwell, Cambridge, 1996. [13] Hans Uszkoreit, Rolf Backofen, Stephan Busemann, Abdel Kader Diagne, Elizabeth A. Hinkelman, Walter Kasper, Bernd Kiefer, Hans-Ulrich Krieger, Klaus Netter, Gunter Neumann, Stephan Oepen, and Stephen P. Spackman. DISCOan HPSG-based NLP system and its application for appointment scheduling. In Proceedings of COLING-94, pages 436440, 1994. [14] Shoichi Yatabe. Scrambling and Japanese Phrase Structure. PhD thesis, Stanford University., 1993. [15] Kei Yoshimoto. Tense and Aspect in Japanese and English. PhD thesis, Universitat Stuttgart, 1997.",
  "34.pdf": "arXiv:cs9906009v1 [cs.CL] 6 Jun 1999 Cascaded Markov Models Thorsten Brants Universitat des Saarlandes, Computerlinguistik D-66041 Saarbrucken, Germany thorstencoli.uni-sb.de In Proceedings of 9th Conference of the European Chapter of the Association for Computational Linguistics EACL-99. Bergen, Norway, 1999 Abstract This paper presents a new approach to partial parsing of context-free structures. The approach is based on Markov Mod- els. Each layer of the resulting structure is represented by its own Markov Model, and output of a lower layer is passed as input to the next higher layer. An em- pirical evaluation of the method yields very good results for NPPP chunking of German newspaper texts. 1 Introduction Partial parsing, often referred to as chunking, is used as a pre-processing step before deep analysis or as shallow processing for applications like in- formation retrieval, messsage extraction and text summarization. Chunking concentrates on con- structs that can be recognized with a high degree of certainty. For several applications, this type of information with high accuracy is more valuable than deep analysis with lower accuracy. We will present a new approach to partial pars- ing that uses Markov Models. The presented models are extensions of the part-of-speech tag- ging technique and are capable of emitting struc- ture. They utilize context-free grammar rules and add left-to-right transitional context information. This type of model is used to facilitate the syntac- tic annotation of the NEGRA corpus of German newspaper texts (Skut et al., 1997). Part-of-speech tagging is the assignment of syn- tactic categories (tags) to words that occur in the processed text. Among others, this task is ef- ﬁciently solved with Markov Models. States of a Markov Model represent syntactic categories (or tuples of syntactic categories), and outputs represent words and punctuation (Church, 1988; DeRose, 1988, and others). This technique of sta- tistical part-of-speech tagging operates very suc- cessfully, and usually accuracy rates between 96 and 97 are reported for new, unseen text. Brants et al. (1997) showed that the technique of statistical tagging can be shifted to the next level of syntactic processing and is capable of as- signing grammatical functions. These are func- tions like subject, direct object, head, etc. They mark the function of a child node within its par- ent phrase. Figure 1 shows an example sentence and its structure. The terminal sequence is complemen- ted by tags (Stuttgart-Tubingen-Tagset, Thielen and Schiller, 1995). Non-terminal nodes are la- beled with phrase categories, edges are labeled with grammatical functions (NEGRA tagset). In this paper, we will show that Markov Mod- els are not restricted to the labeling task (i.e., the assignment of part-of-speech labels, phrase labels, or labels for grammatical functions), but are also capable of generating structural elements. We will use cascades of Markov Models. Starting with the part-of-speech layer, each layer of the result- ing structure is represented by its own Markov Model. A lower layer passes its output as input to the next higher layer. The output of a layer can be ambiguous and it is complemented by a probability distribution for the alternatives. This type of parsing is inspired by ﬁnite state cascades which are presented by several authors. CASS (Abney, 1991; Abney, 1996) is a partial parser that recognizes non-recursive basic phrases (chunks) with ﬁnite state transducers. Each transducer emits a single best analysis (a longest match) that serves as input for the transducer at the next higher level. CASS needs a special gram- mar for which rules are manually coded. Each layer creates a particular subset of phrase types. FASTUS (Appelt et al., 1993) is heavily based on pattern matching. Each pattern is associated with one or more trigger words. It uses a series of non-deterministic ﬁnite-state transducers to build chunks; the output of one transducer is passed Ein ART An enormer ADJA enormous Posten NN amount an APPR of Arbeit NN work und KON and Geld NN money wird VAFIN is von APPR by den ART the 37 CARD 37 beteiligten ADJA involved Vereinen NN organizations aufgebracht VVPP CJ CD CJ AC NK NK NK NK AC CNP NK PP SBP HD NK NK NK PP MNR NP SB HD VP OC 0 1 2 3 4 5 6 7 8 9 10 11 12 13 500 503 501 504 502 505 S raised A large amount of money and work was raised by the involved organizations Figure 1: Example sentence and annotation. The structure consists of terminal nodes (words and their parts-of-speech), non-terminal nodes (phrases) and edges (labeled with grammatical functions). as input to the next transducer. (Roche, 1994) uses the ﬁx point of a ﬁnite-state transducer. The transducer is iteratively applied to its own out- put until it remains identical to the input. The method is successfully used for eﬃcient processing with large grammars. (Cardie and Pierce, 1998) present an approach to chunking based on a mix- ture of ﬁnite state and context-free techniques. They use NP rules of a pruned treebank grammar. For processing, each point of a text is matched against the treebank rules and the longest match is chosen. Cascades of automata and transducers can also be found in speech processing, see e.g. (Pereira et al., 1994; Mohri, 1997). Contrary to ﬁnite-state transducers, Cascaded Markov Models exploit probabilities when pro- cessing layers of a syntactic structure. They do not generate longest matches but most-probable sequences. Furthermore, a higher layer sees dif- ferent alternatives and their probabilities for the same span. It can choose a lower ranked alterna- tive if it ﬁts better into the context of the higher layer. An additional advantage is that Cascaded Markov Models do not need a stratiﬁed gram- mar (i.e., each layer encodes a disjoint subset of phrases). Instead the system can be immediately trained on existing treebank data. The rest of this paper is structured as follows. Section 2 addresses the encoding of parsing pro- cesses as Markov Models. Section 3 presents Cas- caded Markov Models. Section 4 reports on the evaluation of Cascaded Markov Models using tree- bank data. Finally, section 5 will give conclusions. 2 Encoding of Syntactical Information as Markov Models When encoding a part-of-speech tagger as a Markov Model, states represent syntactic cate- gories1 and outputs represent words. Contex- tual probabilities of tags are encoded as transi- tion probabilities of tags, and lexical probabilities of the Markov Model are encoded as output prob- abilities of words in states. We introduce a modiﬁcation to this encoding. States additionally may represent non-terminal categories (phrases). These new states emit par- tial parse trees (cf. ﬁgure 2). This can be seen as collapsing a sequence of terminals into one non- terminal. Transitions into and out of the new states are performed in the same way as for words and parts-of-speech. Transitional probabilities for this new type of Markov Models can be estimated from annotated data in a way very similar to estimating proba- bilities for a part-of-speech tagger. The only dif- ference is that sequences of terminals may be re- placed by one non-terminal. Lexical probabilities need a new estimation method. We use probabilities of context-free par- tial parse trees. Thus, the lexical probability of the state NP in ﬁgure 2 is determined by P(NP  ART ADJA NN, ART  ein, ADJA  enormer, NN  Posten)  P(NP  ART ADJA NN) P(ART  ein)  P(ADJA  enormer) P(NN  Posten) Note that the last three probabilities are the same as for the part-of-speech model. 1Categories and states directly correspond in bi- gram models. For higher order models, tuples of cat- egories are combined to one state. NP APPR CNP VAFIN PP VVPP an wird aufgebracht end start P(NP,) P(APPR,NP) P(CNPNP,APPR) P(VAFINAPPR,CNP) P(PPCNP,VAFIN) P(VVPPVAFIN,PP) P(PP,VVPP) ART ADJA NN NN KON NN APPR ART CARD ADJA NN P( NP) P( CNP) P( PP) P(anAPPR) P(wird VAFIN) P(aufgebrachtVVPP) ein enormer Posten Arbeit und Geld von den 37 beteiligten Vereinen Figure 2: Part of the Markov Models for layer 1 that is used to process the sentence of ﬁgure 1. Contrary to part-of-speech tagging, outputs of states may consist of structures with probabilities according to a stochastic context-free grammar. 3 Cascaded Markov Models The basic idea of Cascaded Markov Models is to construct the parse tree layer by layer, ﬁrst struc- tures of depth one, then structures of depth two, and so forth. For each layer, a Markov Model de- termines the best set of phrases. These phrases are used as input for the next layer, which adds one more layer. Phrase hypotheses at each layer are generated according to stochastic context-free grammar rules (the outputs of the Markov Model) and subsequently ﬁltered from left to right by Markov Models. Figure 3 gives an overview of the parsing model. Starting with part-of-speech tagging, new phrases are created at higher layers and ﬁltered by Markov Models operating from left to right. 3.1 Tagging Lattices The processing example in ﬁgure 3 only shows the best hypothesis at each layer. But there are alter- native phrase hypotheses and we need to deter- mine the best one during the parsing process. All rules of the generated context-free grammar with right sides that are compatible with part of the sequence are added to the search space. Fig- ure 4 shows an example for hypotheses at the ﬁrst layer when processing the sentence of ﬁgure 1. Each bar represents one hypothesis. The position of the bar indicates the covered words. It is la- beled with the type of the hypothetical phrase, an index in the upper left corner for later ref- erence, the negative logarithm of the probability that this phrase generates the terminal yield (i.e., the smaller the better; probabilities for part-of- speech tags are omitted for clarity). This part is very similar to chart entries of a chart parser. All phrases that are newly introduced at this layer are marked with an asterisk (). They are produced according to context-free rules, based on the elements passed from the next lower layer. The layer below layer 1 is the part-of-speech layer. The hypotheses form a lattice, with the word boundaries being states and the phrases being edges. Selecting the best hypothesis means to ﬁnd the best path from node 0 to the last node (node 14 in the example). The best path can be eﬃ- ciently found with the Viterbi algorithm (Viterbi, 1967), which runs in time linear to the length of the word sequence. Having this view of ﬁnding the best hypothesis, processing of a layer is similar to word lattice processing in speech recognition (cf. Samuelsson, 1997). Two types of probabilities are important when searching for the best path in a lattice. First, these are probabilities of the hypotheses (phrases) generating the underlying terminal nodes (words). They are calculated according to a stochastic context-free grammar and given in ﬁgure 4. The second type are context probabilities, i.e., the probability that some type of phrase follows or precedes another. The two types of probabilities coincide with lexical and contextual probabilities of a Markov Model, respectively. According to a trigram model (generated from a corpus), the path in ﬁgure 4 that is marked grey is the best path in the lattice. Its probability is composed of Pbest  P(NP, )P(NP  ein enormer Posten) P(APPR, NP)P(APPR  an) P(CNPNP, APPR)P(CNP  Arbeit und Geld) P(VAFINAPPR, CNP)P(VAFIN  wird) NE VAFIN APPR PPOSAT NN NN VVPP PP NE VAFIN NN NE VAFIN VP S 0 1 2 3 Tagging Grammatical Functions NK MO OA HD SB HD OC Kronos haben mit ihrer Musik Brücken geschlagen NE VAFIN APPR PPOSAT NN NN VVPP AC NK PP VP S NK AC MO OA HD HD SB OC NK PartofSpeech Tagging Cascaded Markov Models  VVPP Layer Kronos have with their bridges music built Kronos built bridges with their music Kronos haben mit ihrer Musik Brücken geschlagen Input Figure 3: The combined, layered processing model. Starting with part-of-speech tagging (layer 0), pos- sibly ambiguous output together with probabilities is passed to higher layers (only the best hypotheses are shown for clarity). At each layer, new phrases and grammatical functions are added. P(PPCNP, VAFIN) P(PP  von den 37 beteiligten Vereinen) P(VVPPVAFIN, PP)P(VVPP  aufgebracht) P(PP, VVPP). Start and end of the path are indicated by a dollar sign (). This path is very close to the cor- rect structure for layer 1. The CNP and PP are correctly recognized. Additionally, the best path correctly predicts that APPR, VAFIN and VVPP should not be attached in layer 1. The only error is the NP ein enormer Posten. Although this is on its own a perfect NP, it is not complete because the PP an Arbeit und Geld is missing. ART, ADJA and NN should be left unattached in this layer in order to be able to create the correct structure at higher layers. The presented Markov Models act as ﬁlters. The probability of a connected structure is de- termined only based on a stochastic context-free grammar. The joint probabilities of unconnected partial structures are determined by additionally using Markov Models. While building the struc- ture bottom up, parses that are unlikely according to the Markov Models are pruned. 3.2 The Method The standard Viterbi algorithm is modiﬁed in or- der to process Markov Models operating on lat- tices. In part-of-speech tagging, each hypothesis (a tag) spans exactly one word. Now, a hypothesis can span an arbitrary number of words, and the same span can be covered by an arbitrary num- ber of alternative word or phrase hypotheses. Us- ing terms of a Markov Model, a state is allowed to emit a context-free partial parse tree, starting with the represented non-terminal symbol, yield- ing part of the sequence of words. This is in con- trast to standard Markov Models. There, states emit atomic symbols. Note that an edge in the lat- tice is represented by a state in the corresponding Markov Model. Figure 2 shows the part of the Markov Model that represents the best path in the lattice of ﬁgure 4. The equations of the Viterbi algorithm are adapted to process a language model operating on a lattice. Instead of the words, the gaps be- tween the words are enumerated (see ﬁgure 4), and an edge between two states can span one or more words, such that an edge is represented by a triple t, t, q, starting at time t, ending at time t and representing state q. We introduce accumulators t,t(q) that col- lect the maximum probability of state q covering words from position t to t. We use δi,j(q) to de- note the probability of the deriviation emitted by state q having a terminal yield that spans posi- tions i to j. These are needed here as part of the accumulators . Initialization: 0,t(q)  P(qqs)δ0,t(q) (1) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Layer 1 Ein enor- mer Po- sten an Ar- beit und Geld wird von den 37 betei- ligten Ver- einen aufge- bracht 1ART 16NP 6.60 17NP 9.68 2ADJA 18AP 10.28 19NP 7.69 3NN 4APPR 20AP 9.25 21PP 6.38 5NN 22CNP 9.05 6KON 7NN 23VP 9.00 8VAFIN 9APPR 24PP 6.22 25AVP 6.88 26PP 10.23 27PP 17.96 10ART 28NP 8.63 29NM 9.23 11CARD 30AP 11.55 31NP 12.24 12ADJA 32NP 11.51 13NN 14VVPP Figure 4: Phrase hypotheses according to a context-free grammar for the ﬁrst layer. Hypotheses marked with an asterisk () are newly generated at this layer, the others are passed from the next lower layer (layer 0: part-of-speech tagging). The best path in the lattice is marked grey. Recursion: t,t(q)  max t,t,qLattice t,t(q)P(qq)δt,t(q), (2) for 1  t  T . Termination: max QQ P(Q, Lattice)  max t,T,qLattice t,T (q)P(qeq). (3) Additionally, it is necessary to keep track of the el- ements in the lattice that maximized each t,t(q). When reaching time T , we get the best last ele- ment in the lattice tm 1 , T, qm 1   argmax t,T,qLattice t,T (q)P(qeq). (4) Setting tm 0  T , we collect the arguments t, t, q  Lattice that maximized equation 2 by walking backwards in time: tm i1, tm i , qm i1  argmax t,tm i ,qLattice t,tm i (q)P(qm i q)δtm i ,tm i1(qi) (5) for i  1, until we reach tm k  0. Now, qm 1 . . . qm k is the best sequence of phrase hypotheses (read backwards). 3.3 Passing Ambiguity to the Next Layer The process can move on to layer 2 after the ﬁrst layer is computed. The results of the ﬁrst layer are taken as the base and all context-free rules that apply to the base are retrieved. These again form a lattice and we can calculate the best path for layer 2. The Markov Model for layer 1 operates on the output of the Markov Model for part-of-speech tagging, the model for layer 2 operates on the out- put of layer 1, and so on. Hence the name of the processing model: Cascaded Markov Models. Very often, it is not suﬃcient to calculate just the best sequences of wordstagsphrases. This may result in an error leading to subsequent er- rors at higher layers. Therefore, we not only cal- culate the best sequence but several top ranked sequences. The number of the passed hypotheses depends on a pre-deﬁned threshold θ  1. We se- lect all hypotheses with probabilities P  Pbestθ. These are passed to the next layer together with their probabilities. 3.4 Parameter Estimation Transitional parameters for Cascaded Markov Models are estimated separately for each layer. Output parameters are the same for all layers, they are taken from the stochastic context-free grammar that is read oﬀ the treebank. Training on annotated data is straight forward. First, we number the layers, starting with 0 for the part-of-speech layer. Subsequently, informa- tion for the diﬀerent layers is collected. Each sentence in the corpus represents one training sequence for each layer. This sequence consists of the tags or phrases at that layer. If a span is not covered by a phrase at a particular layer, we take the elements of the highest layer below the actual layer. Figure 5 shows the train- ing sequences for layers 0  4 generated from the sentence in ﬁgure 1. Each sentence gives rise to one training sequence for each layer. Contextual parameter estimation is done in analogy to models for part-of-speech tagging, and the same smooth- ing techniques can be applied. We use a linear interpolation of uni-, bi-, and trigram models. A stochastic context-free grammar is read oﬀ the corpus. The rules derived from the anno- tated sentence in ﬁgure 1 are also shown in ﬁgure 5. The grammar is used to estimate output pa- rameters for all Markov Models, i.e., they are the Layer Sequence 4 S 3 NP VAFIN VP 2 ART ADJA NN PP VAFIN VP 1 ART ADJA NN APPR CNP VAFIN PP VVPP 0 ART ADJA NN APPR NN KON NN VAFIN APPR ART CARD ADJA NN VVPP Context-free rules and their frequencies S  NP VAFIN VP (1) PP  APPR ART CARD ADJA NN (1) NP  ART ADJA NN PP (1) ART  Ein (1) PP  APPR CNP (1) ADJA  enormer (1) CNP  NN KON NN (1)       VP  PP VVPP (1) VVPP  aufgebracht (1) Figure 5: Training material generated from the sentence in ﬁgure 1. The sequences for layers 0  4 are used to estimate transition probabilities for the corresponding Markov Models. The context-free rules are used to estimate the SCFG, which determines the output probabilities of the Markov Models. same for all layers. We could estimate probabil- ities for rules separately for each layer, but this would worsen the sparse data problem. 4 Experiments This section reports on results of experiments with Cascaded Markov Models. We evaluate chunking precision and recall, i.e., the recognition of kernel NPs and PPs. These exclude prenominal adverbs and postnominal PPs and relative clauses, but in- clude all other prenominal modiﬁers, which can be fairly complex adjective phrases in German. Fig- ure 6 shows an example of a complex NP and the output of the parsing process. For our experiments, we use the NEGRA corpus (Skut et al., 1997). It consists of German news- paper texts (Frankfurter Rundschau) that are an- notated with predicate-argument structures. We extracted all structures for NPs, PPs, APs, AVPs (i.e., we mainly excluded sentences, VPs and co- ordinations). The version of the corpus used con- tains 17,000 sentences (300,000 tokens). The corpus was divided into training part (90) and test part (10). Experiments were repeated 10 times, results were averaged. Cross-evaluation was done in order to obtain more reliable perfor- mance estimates than by just one test run. Input of the process is a sequence of words (divided into sentences), output are part-of-speech tags and structures like the one indicated in ﬁgure 6. Figure 7 presents results of the chunking task using Cascaded Markov Models for diﬀerent num- bers of layers.2 Percentages are slightly below those presented by (Skut and Brants, 1998). But 2The ﬁgure indicates unlabeled recall and preci- sion. Diﬀerences to labeled recallprecision are small, since the number of diﬀerent non-terminal categories is very restricted. they started with correctly tagged data, so our task is harder since it includes the process of part- of-speech tagging. Recall increases with the number of layers. It ranges from 54.0 for 1 layer to 84.8 for 9 lay- ers. This could be expected, because the num- ber of layers determines the number of phrases that can be parsed by the model. The additional line for topline recall indicates the percentage of phrases that can be parsed by Cascaded Markov Models with the given number of layers. All nodes that belong to higher layers cannot be recognized. Precision slightly decreases with the number of layers. It ranges from 91.4 for 1 layer to 88.3 for 9 layers. The F-score is a weighted combination of recall R and precision P and deﬁned as follows: F  (β2  1)PR β2P  R (6) β is a parameter encoding the importance of recall and precision. Using an equal weight for both (β  1), the maximum F-score is reached for 7 layers (F 86.5). The part-of-speech tagging accuracy slightly in- creases with the number of Markov Model layers (bottom line in ﬁgure 7). This can be explained by top-down decisions of Cascaded Markov Models. A model at a higher layer can select a tag with a lower probability if this increases the probability at that layer. Thereby some errors made at lower layers can be corrected. This leads to the increase of up to 0.3 in accuracy. Results for chunking Penn Treebank data were previously presented by several authors (Ramshaw and Marcus, 1995; Argamon et al., 1998; Veenstra, 1998; Cardie and Pierce, 1998). These are not directly comparable to our results, die ART the von APPR by der ART the Bundesregierung NN government angestrebte ADJA intended Entlassung NN dismissal des ART (of) the Bundes NN federation aus APPR from einzelnen ADJA several Bereichen NN areas AC NK NK AC NK NK PP NK NK NP PP SBP HD NK AP NK NK 0 1 2 3 4 5 6 7 8 9 10 500 501 502 503 504 NP the dismissal of the federation from several areas that was intended by the government Figure 6: Complex German NP and chunker output (postnominal genitive and PP are not attached). NEGRA Corpus: Chunking Results 1 2 3 4 5 6 7 8 9 60 70 80 90 100 RecallPrecision  Layers 96.2 96.3 96.4 96.4 96.5 96.5 96.5 96.5 96.5  POS accuracy Topline Recall min  72.6 max 100.0  Recall min  54.0 max 84.8  Precision min  88.3 max 91.4                  Figure 7: NPPP chunking results for the NEGRA Corpus. The diagram shows recall and precision depending on the number of layers that are used for parsing. Layer 0 is used for part-of-speech tagging, for which tagging accuracies are given at the bottom line. Topline recall is the maximum recall possible for that number of layers. because they processed a diﬀerent language and generated only one layer of structure (the chunk boundaries), while our algorithm also generates the internal structure of chunks. But generally, Cascaded Markov Models can be reduced to gen- erating just one layer and can be trained on Penn Treebank data. 5 Conclusion and Future Work We have presented a new parsing model for shal- low processing. The model parses by represent- ing each layer of the resulting structure as a sep- arate Markov Model. States represent categories of words and phrases, outputs consist of partial parse trees. Starting with the layer for part-of- speech tags, the output of lower layers is passed as input to higher layers. This type of model is restricted to a ﬁxed maximum number of layers in the parsed structure, since the number of Markov Models is determined before parsing. While the eﬀects of these restrictions on the parsing of sen- tences and VPs are still to be investigated, we ob- tain excellent results for the chunking task, i.e., the recognition of kernel NPs and PPs. It will be interesting to see in future work if Cas- caded Markov Models can be extended to parsing sentences and VPs. The average number of lay- ers per sentence in the NEGRA corpus is only 5; 99.9 of all sentences have 10 or less layers, thus a very limited number of Markov Models would be suﬃcient. Cascaded Markov Models add left-to-right context-information to context-free parsing. This contextualization is orthogonal to another impor- tant trend in language processing: lexicalization. We expect that the combination of these tech- niques results in improved models. We presented the generation of parameters from annotated corpora and used linear interpolation for smoothing. While we do not expect im- provements by re-estimation on raw data, other smoothing methods may result in better accura- cies, e.g. the maximum entropy framework. Yet, the high complexity of maximum entropy parame- ter estimation requires careful pre-selection of rel- evant linguistic features. The presented Markov Models act as ﬁlters. The probability of the resulting structure is de- termined only based on a stochastic context-free grammar. While building the structure bottom up, parses that are unlikely according to the Markov Models are pruned. We think that a combined probability measure would improve the model. For this, a mathematically motivated com- bination needs to be determined. Acknowledgements I would like to thank Hans Uszkoreit, Yves Schabes, Wojciech Skut, and Matthew Crocker for fruitful discussions and valuable comments on the work presented here. And I am grateful to Sabine Kramp for proof-reading this paper. This research was funded by the Deutsche Forschungsgemeinschaft in the Sonderforschungs- bereich 378, Project C3 NEGRA. References [Abney1991] Steven Abney. 1991. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-Based Parsing, Dordrecht. Kluwer Academic Publishers. [Abney1996] Steven Abney. 1996. Partial parsing via ﬁnite-state cascades. In Proceedings of the ESSLLI Workshop on Robust Parsing, Prague, Czech Republic. [Appelt et al.1993] D. Appelt, J. Hobbs, J. Bear, D. J. Israel, and M. Tyson. 1993. FASTUS: a ﬁnite-state processor for information extraction from real-world text. In Proceedings of IJCAI- 93, Washington, DC. [Argamon et al.1998] Shlomo Argamon, Ido Da- gan, and Yuval Krymolowski. 1998. A memory- based approach to learning shallow natural lan- guage patterns. In Proceedings of the 17th In- ternational Conference on Computational Lin- guistics COLING-ACL-98), Montreal, Canada. [Brants et al.1997] Thorsten Brants, Wojciech Skut, and Brigitte Krenn. 1997. Tagging grammatical functions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP-97, Providence, RI, USA. [Cardie and Pierce1998] Claire Cardie and David Pierce. 1998. Error-driven pruning of treebank grammars for base noun phrase identiﬁcation. In Proceedings of the 17th International Confer- ence on Computational Linguistics COLING- ACL-98), Montreal, Canada. [Church1988] Kenneth Ward Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing ANLP-88, pages 136143, Austin, Texas, USA. [DeRose1988] Steven J. DeRose. 1988. Grammat- ical category disambiguation by statistical opti- mization. Computational Linguistics, 14(1):31 39. [Mohri1997] Mehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23(2). [Pereira et al.1994] Fernando Pereira, Michael Ri- ley, and Richard Sproat. 1994. Weighted ratio- nal transductions and their application to hu- man language processing. In Proceedings of the Workshop on Human Language Technology, San Francisco, CA. Morgan Kaufmann. [Ramshaw and Marcus1995] Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Pro- ceedings of the third Workshop on Very Large Corpora, Dublin, Ireland. [Roche1994] Emmanuel Roche. 1994. Two parsing algorithms by means of ﬁnite state transducers. In Proceedings of the 15th International Confer- ence on Computational Linguistics COLING- 94, pages 431435, Kyoto, Japan. [Samuelsson1997] Christer Samuelsson. 1997. Ex- tending n-gram tagging to word graphs. In Pro- ceedings of the 2nd International Conference on Recent Advances in Natural Language Process- ing RANLP-97, Tzigov Chark, Bulgaria. [Skut and Brants1998] Wojciech Skut and Thorsten Brants. 1998. A maximum-entropy partial parser for unrestricted text. In Sixth Workshop on Very Large Corpora, Montreal, Canada. [Skut et al.1997] Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order lan- guages. In Proceedings of the Fifth Confer- ence on Applied Natural Language Processing ANLP-97, Washington, DC. [Thielen and Schiller1995] Christine Thielen and Anne Schiller. 1995. Ein kleines und erweitertes Tagset furs Deutsche. In Tagungsberichte des Arbeitstreﬀens Lexikon  Text 17.18. Februar 1994, Schloß Hohentubingen. Lexicographica Series Maior, Tubingen. Niemeyer. [Veenstra1998] Jorn Veenstra. 1998. Fast NP chunking using memory-based learning tech- niques. In Proceedings of the Eighth Belgian- Dutch Conference on Machine Learning, Wa- geningen. [Viterbi1967] A. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically op- timum decoding algorithm. In IEEE Transac- tions on Information Theory, pages 260269.",
  "35.pdf": "arXiv:cs9906014v1 [cs.CL] 14 Jun 1999 Evaluation of the NLP Components of the OVIS2 Spoken Dialogue System Gert Veldhuijzen van Zanten1 Gosse Bouma2 Khalil Simaan3 Gertjan van Noord2 Remko Bonnema4 1IPO Eindhoven 2Rijksuniversiteit Groningen 3Universiteit Utrecht 4Universiteit van Amsterdam Abstract The NWO Priority Programme Language and Speech Technology is a 5-year research programme aiming at the development of spoken lan- guage information systems. In the Programme, two alternative natural language processing (NLP) modules are developed in parallel: a grammar- based (conventional, rule-based) module and a data-oriented (memory- based, stochastic, DOP) module. In order to compare the NLP modules, a formal evaluation has been carried out three years after the start of the Programme. This paper describes the evaluation procedure and the evaluation results. The grammar-based component performs much better than the data-oriented one in this comparison. 1 Introduction The NWO Priority Programme Language and Speech Technology is a 5-year research programme aiming at the development of spoken language information systems. Its immediate goal is to develop a demonstrator of a public transport information system, which operates over ordinary telephone lines. This demon- strator is called OVIS, Openbaar Vervoer Informatie Systeem (Public Transport Information System). The language of the system is Dutch. In this Programme, two alternative NLP modules are developed in paral- lel: a grammar-based (conventional, rule-based) module and a data-oriented 1 (memory-based, stochastic, DOP) module. Both of these modules ﬁt into the system architecture of OVIS. They accept as their input word graphs produced by the automatic speech recognition component, and produce updates which are passed on to the pragmatic analysis component and dialogue manager. A word graph (Oerder and Ney, 1993) is a compact representation for all se- quences of words that the speech recogniser hypothesises for a spoken utterance. The states of the graph represent points in time, and a transition between two states represents a word that may have been uttered between the corresponding points in time. Each transition is associated with an acoustic score representing a measure of conﬁdence that the word perceived there was actually uttered. The dialogue manager maintains an information state to keep track of the information provided by the user. An update expression is an instruction for updating the information state. The syntax and semantics of such updates are deﬁned in Veldhuijzen van Zanten (1996). The sentence: (1) Ik I wil want op on 4 4 februari February van from Amsterdam Amsterdam naar to Groningen Groningen I want to travel from Amsterdam to Groningen on February 4th is translated into the update expression: (2) (user.wants. (destination.(place.groningen); (origin.(place.amsterdam)); (moment.at.(date.(month.february;day.4)))) which indicates that the destination and origin slots can be ﬁlled in, as well as the moment.at slot. In order to compare the NLP modules, a formal evaluation has been carried out three years after the start of the Programme. In this paper, we ﬁrst shortly describe the two competing NLP components in section 2. The evaluation mea- sures string accuracy, semantic accuracy and computational resources. This is described in more detail in section 3. The evaluation results are presented in section 4. On the basis of these results some conclusions are drawn in section 5. 2 Two NLP Components For detailed descriptions of the NLP components, the reader is referred to van Noord et al. (1996a), van Noord et al. (1996b) and van Noord et al. (1999) for the grammar-based NLP module. The data-oriented approach is documented in Scha et al. (1996), Simaan (1997) and Bod  Scha (1997). 2.1 Data Oriented Parsing Research in the Data Oriented Parsing framework explores the hypothesis that humans analyse new input by drawing analogies with concrete past language experiences, rather than by applying abstract rules (Scha, 1990). 2 In developing computational models embodying this idea, we have so far focused on one particularly straightforward instantiation of it: our algorithms analyse new input by considering the various ways in which this input might be generated by a stochastic process which combines fragments of trees from an annotated corpus of previously encountered utterances. Formally, these models may be viewed as implementing extremely redundant Stochastic Tree Substitu- tion Grammars (STSGs); the grammar rules are the subtrees extracted from the annotated corpus (Bod, 1993). An important parameter of models of this sort is the way in which the subtree-substitution probabilities in the stochastic process are computed on the basis of the subtree frequencies in the annotated corpus. All current models follow (Bod, 1993) in deﬁning the probability of substituting a subtree t on a speciﬁc node as the probability of selecting t among all subtrees in the corpus that could be substituted on that node, i.e., as the number of occurrences of t divided by the total number of occurrences of subtrees t with the same root node label as t: (3) P(t)  t  t:root(t)root(t)t Given these subtree substitution probabilities, the probability of a deriva- tion t1      tn can be computed as the product of the probabilities of the substitutions that it consists of (4) P(t1      tn)   iP(ti) The probability of a parse tree is equal to the probability that any of its distinct derivations is generated, which is the sum of the probabilities of all derivations of that parse tree. Let tid be the i-th subtree in the derivation d that yields tree T , then the probability of T is given by: (5) P(T )   d  iP(tid) An eﬃcient polynomial algorithm for computing the Most Probable Deriva- tion is given in Simaan (1996a). From a theoretical point of view we might expect the computation of the Most Probable Parse to yield better disambigua- tion accuracies than the Most Probable Derivation, and this expectation was conﬁrmed by certain experiments. However, Simaan (1996b) has shown that the problem of computing the Most Probable Parse is not solvable by determin- istic polynomial time algorithms. For reasons regarding time-complexity, the most probable derivation (MPD) is still the method of choice for a real-world application. The algorithm presented in Simaan (1996a) is implemented in the Data Oriented Parsing and DIsambiguation System (dopdis). The algorithm ex- tends a well-known CFG parsing algorithm (the CKY algorithm) in a suitable way in order to deal with STSGs. The extension makes use of the fact that the paths in a parse-tree, which is generated from an STSG derivation for a given sentence, form a regular set that can be easily computed. By computing such sets, dopdis generates exactly the necessary trees which the STSG dictates. 3 On top of this mechanism, dopdis computes both the most probable derivation and the probability of an input sentence. The construction operates in time- complexity which is cubic in sentence length and linear in STSG size, which is a good achievement in parsing tree grammars (existing tree-parsing techniques have complexity which is square in grammar size). The extension to parsing and disambiguating word graphs maintains the same time and space complexity (where instead of sentence length here the complexity concerns the numbers of nodes i.e. states the word graph contains). dopdis computes the so called Most Probable intersection-Derivation of a word graph and the DOP STSG. An intersection-derivation (i-derivation) is a pair string, derivation where string is the string of words on a path in the input word graph, and derivation is an STSG derivation for that string. The probabil- ity of the i-derivation is computed through a weighted product of the probabil- ities on the path in the word graph and the STSG-derivation probability. The probabilities of the word graph paths are obtained from the speech-recognisers likelihoods by a normalisation heuristic. The probabilities resulting from this heuristic combine better with the DOP probabilities than raw speech recogniser likelihoods. The issue of scaling the likelihoods of the speech-recogniser in a well-founded way is still under study. The current method divides the likeli- hood of every transition by a normalisation factor that is computed from the word graph. An extension to semantic interpretation. In van den Berg, Bod and Scha (1994), an extension of the model to semantic interpretation is presented. A ﬁrst implementation of this extension was described in Bonnema (1996). A DOP model as described above can be extended from just syntactic, to semantic analysis, by augmenting the trees in the tree-bank with semantic rules. These rules indicate, for each individual analysis in the tree-bank, how its meaning is constructed out of the meaning of its parts. Just as in the purely syntactic ver- sion of DOP, we extract all possible fragments from these syntacticsemantic analyses. We then use these fragments to build an STSG. We alter the con- straints on tree substitution, by demanding that both the syntactic category and the semantic type of the root node of a subtree match with those at the substitution site. Note that the semantic types restrict the possibility of substi- tution. The language generated by an STSG created on the basis of a semanti- cally enriched tree-bank, is therefore a subset of the language generated by an STSG created on the basis of the same tree-bank without the semantic annota- tions. Ideally, the former STSG would exclude exactly the set of semantically ill-formed sentences. The preferred analysis of an utterance will now provide us with both a syntactic and a semantic interpretation. In the current implemen- tation the most probable analysis is taken to be the interpretation given by the most probable derivation. A corpus of syntactic and semantic analyses of transcribed utterances from the OVIS domain was created to test this model. The OVIS tree-bank currently contains 10.000 analyzed utterances. The top-node semantics of each annotated 4 utterance is an update-expression that conforms to the formalism described in Veldhuijzen van Zanten (1996). The semantic label of a node N in an analysis consists of a rule, that indicates how the meaning of N (the update) is built-up out of the meanings of Ns daughter nodes. This semantic rule is typed. Its type follows from both the rule itself, and from the types of the semantic labels of the daughter-nodes of N, given the deﬁnition of the logical language used. In the present case, the type of an expression is a pair of integers, its meet and join. The meet and join correspond to the least upper bound and the greatest lower bound of the expression in the type-hierarchy, as described in Veldhuijzen van Zanten (1996). A semantically enriched STSG as described above, must fulﬁl an important property. It has to be possible to deﬁne a function from derivations to logi- cal formulas, that is deﬁned for every derivation that can be produced by the grammar. In other words, the information provided by semantic types and syn- tactic categories in an analysis must be suﬃcient. Because the set of subtrees is closed under the operation of subtree extraction, i.e., all subtrees T  that can be extracted from another subtree T , belong to the same set as T , it is easy to establish this property, even for a very large grammar. We only need to look at the subtrees of depth one. If there is a unique semantic rule associated with the root-node of all subtrees of depth one, given the syntactic categories and semantic types of its nodes, it follows that we know the semantic rule at the nonterminal-nodes of every subtree. Fortunately, the nature of the annotated tree-bank is such, that in about 99.9  of cases we can indeed establish the semantic rule at the root-node of a subtree in this way. The few exceptions are assigned an exception-type, to reduce the uncertainty to zero. We exploit the property described above, to construct a rewrite system for the semantic STSG. This rewrite system applies the semantic rules associated with every node in a derivation in a bottom up fashion, and arrives at the complete logical formula. Methods for word graphs. The evaluation experiments were performed using just the semantic DOP-model as it was described above. For every word graph the most probable intersection derivation was deter- mined. The leaf-nodes of this derivation constitute the best path through the word graph. The probability of a derivation is calculated on the basis of both the probabilities of the subtrees extracted from the OVIS tree-bank, and the acoustic likelihoods of the words in the word graph. We created several instances of the semantic DOP-parser, with diﬀering con- straints on the form of possible subtrees. Four parameters can be distinguished, whose values determine the constraints on subtrees. Below the parameters are given, with the letters we commonly use to refer to each parameter. d The maximal depth of subtrees. l The maximal number of lexical items in a subtree. L The maximal number of consecutive lexical items in a subtree. 5 n The maximal number of substitution sites in a subtree. Obviously, the number of possible combinations is huge. We chose to use the parameter settings for which previous experiments yielded the best results. The results presented in this document are all obtained using the following settings: l9, L3, n2. For the maximum depth we used d2 and d4. No constraints were applied to subtrees of depth 1. If a word graph contains more than 350 transitions, the maximum depth of subtrees is automatically limited to two, to avoid excessive memory require- ments. About 3 of word graphs in the testing material do contain more than 350 transitions. Methods for test sentences. For sentences, we used the same parameter settings, but added an experiment with d5. In this document, some results on sentences are given with the extra indica- tion group. We will now brieﬂy explain what this means. Non-terminals in the semantic DOP-model consist of a syntactic-category  semantic-type pair. Such a non-terminal imposes a rather rigid constraint on substitution. For the parsing of word graphs, this constraint seems to be bene- ﬁciary. For the parsing of sentences, on the other hand, these constraints could be too rigid. A greater degree of freedom results in over-generation, which in turn may lead to better statistics. An algorithm was designed to group semantic types that have a comparable distribution. This results in fewer non-terminals in the tree-bank, and has been shown to lead to a higher semantic accuracy for sentences. The results marked with group indicate that this grouping algorithm has been employed. 2.2 Grammar-based NLP The grammar-based NLP component developed in Groningen is based on a de- tailed computational grammar for Dutch, and a robust parsing algorithm which incorporates this grammatical knowledge as well as other knowledge sources, such as the acoustic evidence (present in the word graph) and Ngram statistics (collected from a large set of user utterances). It has been argued (van Noord et al. 1999) that robust parsing can be based on sophisticated grammatical analysis. In particular, the grammar describes full sentences, but in doing so, also describes the grammar of temporal expressions and locative phrases which are the crucial concepts for the timetable information application. Robustness is achieved by taking these phrases into consideration, if a full parse of an ut- terance is not available. Computational Grammar for Dutch. In developing the grammar the short-term goal of developing a grammar which meets the requirements imposed by the application (i.e. robust processing of the output of the speech recogniser, extensive coverage of locative phrases and temporal expressions, and the con- struction of ﬁne-grained semantic representations) was combined with the long- 6 term goal of developing a general, computational, grammar which covers all the major constructions of Dutch. The design and organisation of the grammar, as well as many aspects of the particular grammatical analyses, are based on Head-driven Phrase Structure Grammar (Pollard and Sag 1994). The grammar is compiled into a restricted kind of deﬁnite clause grammar for which eﬃcient processing is feasible. The semantic component follows the approach to monotonic semantic interpretation using quasi-logical forms presented originally in Alshawi (1992). The grammar currently covers the majority of verbal subcategorisation types (intransitives, transitives, verbs selecting a pp, and modal and auxiliary verbs), np-syntax (including pre- and post-nominal modiﬁcation, with the exception of relative clauses), pp-syntax, the distribution of vp-modiﬁers, various clausal types (declaratives, yesno and wh-questions, and subordinate clauses), all tem- poral expressions and locative phrases relevant to the domain, and various typ- ical spoken-language constructs. Due to restrictions imposed by the speech recogniser, the lexicon is relatively small (3200 word forms, many of which are names of stations and cities). Robust and Eﬃcient Parsing. Parsing algorithms for strings can be gener- alised to parse word graphs (van Noord 1995). In the ideal case, the parser will ﬁnd a path in the word graph that can be assigned an analysis according to the grammar, such that the path covers the complete time span of the utterance, i.e. the path leads from the start state to a ﬁnal state. The analysis gives rise to an update of the dialogue state, which is then passed on to the dialogue manager. However, often no such paths can be found in the word graph, due to:  errors made by the speech recogniser,  linguistic constructions not covered in the grammar, and  irregularities in the spoken utterance. Even if no full analysis of the word graph is possible, it is usually the case that useful information can be extracted from the word graph. Consider for example the utterance: (6) Ik I wil want van from van from Assen Assen naar to Amsterdam Amsterdam I want to travel from Assen to Amsterdam The grammar will not assign an analysis to this utterance due to the re- peated preposition. However, it would be useful if the parser would discover the prepositional phrases van Assen and naar Amsterdam since in that case the important information contained in the utterance can still be recovered. Thus, in cases where no full analysis is possible the system should fall back on an approach reminiscent of concept spotting. In van Noord et al. (1999) a general algorithm is proposed which achieves this. 7 The ﬁrst ingredient to a solution is that the parser is required to discover all occurrences of major syntactic categories (such as noun phrase, prepositional phrase, subordinate sentence, root sentence) anywhere in the word graph. Con- ceptually, one can think of these categories as edges which are added to the word graph in addition to the transitions produced by the speech recogniser. For such word graphs annotated with additional category edges, a path can be deﬁned as a sequence of steps where each step is either a transition or a category edge. A transition step is called a skip. For a given annotated word graph many paths are possible. On the basis of an appropriate weight function on such paths, it is possible to search for the best path. The search algorithm is a straightforward generalisation of the dag-shortest-path algorithm (Cormen et al. 1990). The weight function is sensitive to the following factors:  Acoustic score. Obviously, the acoustic score present in the word graph is an important factor.  The number of skips is minimised in order to obtain a preference for the maximal projections found by the parser.  Number of maximal projections. The number of maximal projections is minimised in order to obtain a preference for more extended linguistic analyses over a series of smaller ones.  Ngram statistics. The grammar-based NLP component is implemented in SICStus Prolog. Below we report on a number of diﬀerent methods which are all variations with respect to this weight function. Variants of Grammar-based NLP. The grammar-based NLP methods that have taken part in the evaluation are of two types. The ﬁrst type, b(B,N), consists of two phases. In the ﬁrst phase the word graph is made smaller by selecting the N-best paths from the word graph, using the acoustic scores and a language model consisting of bigrams (Bbi) or trigrams (Btri) (with bigrams for backing-oﬀ). Only those transitions of the word graph remain which are part of at least one of those N-best paths. In the second phase the parser is applied, using acoustic scores and a language model of trigrams (again with bigrams for backing-oﬀ). The second type of method is f(B,N). In this case, if the word graph contains less than N transitions, then the full word graph is input to the parser, and acoustic scores and a language model of trigrams (bigrams for backing-oﬀ) is applied to select the best analysis. If the word graph contains more than N transitions, then method b(B,1) is applied. 8 3 Evaluation Procedure and Criteria 3.1 Procedure An experimental version of the system has been available to the general public for almost a year. From a large set of more recent dialogues a subset was selected randomly for testing. Many of the other dialogues were available for training purposes. Both the training and test dialogues are therefore dialogues with normal users. In particular, a training set of 10K richly annotated word graphs was avail- able. The 10K training corpus is annotated with the user utterance, a syntactic tree and an update. This training set was used to train the DOP system. It was also used by the grammar-based component for reasons of grammar main- tenance and grammar testing. A further training set of about 90K annotated user utterances was available as well. It was primarily used for constructing the Ngram models incorporated in the grammar-based component. The NLP components of OVIS2 have been evaluated on 1000 unseen user utterances. The latest version of the speech recogniser produced 1000 word graphs on the basis of these 1000 user utterances. For these word graphs, annotations consisting of the actual sentence (test sentence), and an update (test update) were assigned semi-automatically, without taking into account the dialogue context in which the sentences were uttered. These annotations were unknown to both NLP groups. The annotation tools are described in Bonnema (1996). After both NLP components had produced the results on word graphs, the test sentences were made available. Both NLP components were then applied to these test utterances as well, to mimic a situation in which speech recognition is perfect. The test updates were available for inspection by the NLP groups only after both modules completed processing the test material. A small number of errors was encountered in these test updates. These errors were corrected before the accuracy scores were computed. The accuracy scores presented below were all obtained using the same evaluation software. 3.2 Criteria The NLP components were compared with respect to the following two tasks. Note that in each task, analysis proceeds in isolation from the dialogue context. The ﬁrst task is to provide an update for the test sentence (in this report we refer to this update as the best update). The second task is to provide an update and a sentence for the word graph (best update and best sentence). The quality of the NLP components will be expressed in terms of string accuracy (comparison of the best sentences with the test sentences), semantic accuracy (comparison of the best updates with the test updates) and computational resources. Each of these criteria is now explained in more detail. 9 String accuracy. String accuracy measures the distance of the test sentence and the best sentence. String accuracy is expressed in terms of sentence accuracy (SA, the proportion of cases in which the test sentence coincides with the best sentence), and word accuracy (WA). The string comparison on which word accuracy is based is deﬁned by the minimal number of substitutions, deletions and insertions of words that is required to turn the best sentence into the test sentence (Levenshtein distance). Word accuracy is deﬁned as (7) WA  1  d n where n is the length of the actual utterance and d is the Levenshtein distance. For example, if the analysis gives a b a c d for the utterance a a c e, then the Levenshtein distance is 2, hence the WA is 1-24 is 50. Semantic accuracy. An update is a logical formula which can be evaluated against an information state and which gives rise to a new, updated information state. The most straightforward method for evaluating concept accuracy in this setting is to compare the update produced by the grammar with the annotated update. One problem with this approach is the fact that the update language does not provide a simple way to compute equivalence of updates (there is no notion of normal form for update expressions). A further obstacle is the fact that very ﬁne-grained semantic distinctions can be made in the update- language. While these distinctions are relevant semantically (i.e. in certain cases they may lead to slightly diﬀerent updates of an information state), they often can be ignored by a dialogue manager. For instance, the updates below are semantically not equivalent, as the ground-focus distinction is slightly diﬀerent. In the ﬁrst update the feature place is supposed to be ground, whereas in the second update, it is part of the focus. (8) user.wants.travel.destination.place ([ town.leiden];[! town.abcoude]) (9) user.wants.travel.destination. ([ place.town.leiden];[! place.town.abcoude]) However, the dialogue manager will decide in both cases that this is a correction of the destination town. Since semantic analysis is the input for the dialogue manager, we have there- fore measured concept accuracy in terms of a simpliﬁed version of the update language. Following a somewhat similar proposal in Boros et al. (1996), we translate each update into a set of semantic units, were a unit in our case is a triple CommunicativeFunction Slot Value. For instance, the examples above translate as (10) denial destination_town leiden correction destination_town abcoude Both the updates in the annotated corpus and the updates produced by the system are translated into semantic units of the form given above. The syntax 10 of the semantic unit language and the translation of updates to semantic units is deﬁned in van Noord (1997), but note that the translation of updates to semantic units is relatively straightforward and is not expected to be a source of discussion, because the relation is many to one. Semantic accuracy can now be deﬁned as follows. Firstly, we list the propor- tion of utterances for which the corresponding semantic units exactly match the semantic units of the annotation (exact match). Furthermore we calculate pre- cision (the number of correct semantic units divided by the number of semantic units which were produced) and recall (the number of correct semantic units divided by the number of semantic units of the annotation). Finally, following Boros et al. (1996) we also present concept accuracy as (11) CA   1  SUSSUI SUD SU  where SU is the total number of semantic units in the corpus annotation, and SU S, SU I , and SU D are the number of substitutions, insertions, and deletions that are necessary to make the (translated) update of the analysis equivalent to (the translation of) the corpus update. Computational Resources. In order to measure computational eﬃciency, the total amount of CPU-time, the maximum amount of CPU-time per input, and the total memory requirements will be measured. Due to diﬀerences in hardware, details diﬀer between the two NLP components. For the data-oriented methods, the CPU-time given is the user-time of the parsing process, in seconds. This measure excludes the time used for system calls made on behalf of the process (this can be ignored). Time was measured on a Silicon Graphics Indigo, with a MIPS R10000 processor, running IRIX 6.2. Memory usage is the maximum number of mega-bytes required, to interpret the 1000 utterances. Regrettably, for a very small percentage (0.02) of word graphs, the process ran out of memory. This means that the ﬁgures for word graph parsing indicate the size of the jobs at the moment the system gave up, which is generally when the physical memory is ﬁlled. On the other hand, we should acknowledge the fact that some large word graphs that did receive an interpretation, also approached this limit. For the grammar-based methods, CPU-time is measured in milliseconds on a HP 9000780 (running HP-UX 10.20). The system uses SICStus Prolog 3 3. CPU-time include all phases of processing, but does not contain the time required for system calls (can be ignored) and garbage collection (adds at most 15 for a given run). The memory requirements are given as the increase of the UNIX process size to complete the full run of 1000 inputs. At start-up the process size can be as large as 30 megabytes, so this number has been added in order to estimate total memory requirements. 3.3 Test Set Some indication of the diﬃculty of the set of 1000 word graphs is presented in table 1. A further indication of the diﬃculty of this set of word graphs is 11 graphs trans states words tw max(t) max(s) input 1000 48215 16181 3229 14.9 793 151 normalised 1000 73502 11056 3229 22.8 2943 128 Table 1: Characterisation of test set (1). This table lists the number of transitions, the number of states, the number of words of the actual utterances, the average number of transitions per word, the maximum number of transitions, and the maximum number of states. The ﬁrst row provides those statistics for the input word graph; the second row for the so-called normalised word graph in which all ǫ-transitions (to model the absence of sound) are removed. The number of transitions per word is an indication of the extra ambiguity for the parser introduced by the word graphs in comparison with parsing of an ordinary string. method WA SA speech 69.8 56.0 possible 90.5 83.7 speech bigram 81.1 73.6 speech trigram 83.9 76.2 Table 2: Characterisation of test set (2). Word accuracy and sentence accuracy based on acoustic score only (speech); using the best possible path through the word graph, i.e. based on acoustic scores only (possible); and using a combination of bigram (resp. trigram) scores and acoustic scores. obtained if we look at the word and sentence accuracy obtained by a number of simple methods. The method speech only takes into account the acoustic scores found in the word graph. No language model is taken into account. The method possible assumes that there is an oracle which chooses a path such that it turns out to be the best possible path. This method can be seen as a natural upper bound of what can be achieved. The methods speech bigram and speech trigram use a combination of bigram (resp. trigram) statistics and the speech score. In the latter four cases, a language model was computed from about 50K utterances (not containing the utterances from the test set). The results are summarised in table 2. During the development of the NLP components of OVIS2, word graphs were typically small: about 4 transitions per word on average. During the evaluation, however, the number of transitions per word for the test set was much larger. It turned out that the NLP components had trouble with very large word graphs (both memory and CPU-time requirements increase rapidly). Recently, improvements have already been obtained to treat such large word graphs. For example, the grammar-based NLP component has been extended with a heuristic version of the search algorithm which is not guaranteed to ﬁnd the best path. In practice this implementation returns the same answers as the 12 Method Site String Acc Semantic Accuracy CPU Mem WA SA match prec recall ca total max max d2 Adam 76.8 69.3 74.9 80.1 78.8 75.5 7011 648 619 d4 Adam 77.2 69.4 74.9 79.1 78.8 75.1 32798 2023 621 f(bi,50) Gron 81.3 74.6 79.5 82.9 83.8 79.9 215 16 37 f(bi,100) Gron 82.3 75.8 80.9 83.6 84.8 80.9 297 15 37 f(bi,125) Gron 82.3 75.9 81.3 83.9 85.2 81.3 340 24 38 b(bi,1) Gron 81.1 73.6 78.5 82.1 83.1 78.9 175 16 31 b(bi,2) Gron 82.3 75.7 80.8 83.9 84.8 81.1 255 20 32 b(bi,4) Gron 82.8 76.0 80.8 83.8 85.0 81.3 479 115 34 b(bi,8) Gron 83.4 76.5 81.6 84.6 85.6 82.2 780 276 43 b(bi,16) Gron 83.8 76.4 81.7 84.9 86.0 82.6 1659 757 60 f(tr,50) Gron 83.9 76.2 81.8 84.9 85.9 82.5 1399 607 64 f(tr,100) Gron 84.2 76.6 82.0 85.0 86.0 82.6 1614 690 64 f(tr,125) Gron 84.2 76.5 82.1 85.3 86.3 82.8 1723 755 64 b(tr,1) Gron 83.9 76.2 81.5 84.5 85.7 82.2 1420 603 64 b(tr,2) Gron 84.1 76.4 81.8 85.3 86.4 83.0 2802 1405 101 b(tr,4) Gron 84.3 76.4 82.0 85.4 86.4 83.0 5524 2791 177 Table 3: Accuracy and Computational Resources for 1000 word graphs. String Accuracy and Semantic Accuracy is given as percentages; total and max- imum CPU-time in seconds, maximum memory requirements in Megabytes. original search algorithm, but much more quickly so (two orders of magnitude faster). 4 Results of the Evaluation This section lists the results for word graphs. In table 3 we list the results in terms of string accuracy, semantic accuracy and the computational resources required to complete the test. The total amount of CPU-time is somewhat misleading because typically many word graphs can be treated very eﬃciently, whereas only a few word graphs require very much CPU-time. In table 4 we indicate the semantic accu- racy (concept accuracy) that is obtained if a time-out is assumed (in such cases we assume that the system does not provide an update). We also present the results for test sentences (rather than word graphs). Such a test indicates what the results are if the speech recogniser would perform perfectly. Obviously, it does not make sense to measure string accuracy in such a set-up. Semantic accuracy and computational resources is presented in table 5. Because the average sentence length is very small, we present the results for concept accuracy versus the length of the input sentence in table 6. 5 Conclusions The grammar-based methods developed in Groningen perform much better than the data-oriented methods developed in Amsterdam. For word graphs, the best 13 Method Site 100 500 1000 5000 10000  d2 Adam 37.0 53.0 58.1 68.1 70.4 75.5 d4 Adam 24.6 34.5 38.2 50.4 57.3 75.1 f(bi,50) Gron 46.0 73.7 76.9 80.3 80.3 79.9 f(bi,100) Gron 44.4 67.9 75.3 81.1 81.2 80.9 f(bi,125) Gron 44.6 64.9 73.3 81.3 81.7 81.3 b(bi,1) Gron 58.2 73.1 76.6 79.3 79.2 78.9 b(bi,2) Gron 54.7 74.1 77.6 81.1 81.5 81.1 b(bi,4) Gron 49.6 72.3 75.6 80.3 80.5 81.3 b(bi,8) Gron 45.9 70.2 74.4 80.9 81.5 82.2 b(bi,16) Gron 42.2 65.5 72.5 78.0 81.0 82.6 f(tr,50) Gron 45.5 71.2 75.4 81.0 81.7 82.6 f(tr,100) Gron 44.5 64.2 71.9 80.5 81.8 82.6 f(tr,125) Gron 44.1 62.2 70.2 80.6 81.9 82.8 b(tr,1) Gron 52.7 70.9 74.7 80.7 81.2 82.2 b(tr,2) Gron 49.6 68.8 72.7 79.1 81.4 83.0 b(tr,4) Gron 48.0 66.6 71.6 78.2 79.5 83.0 Table 4: Concept accuracy for 1000 word graphs (percentages), if all results are disregarded with a time-out of respectively 100, 500, 1000, 5000, 10000 milliseconds of CPU-time. The last column repeats the results if no time-out is assumed. Method Site Semantic Accuracy CPU Mem match prec recall ca total max max d4 Adam 92.2 93.8 91.2 90.4 856 14 21 group.d2 Adam 93.0 94.0 92.5 91.6 91 9 14 group.d4 Adam 92.7 93.8 91.8 91.0 1614 174 48 group.d5 Adam 92.6 93.7 92.3 91.4 3159 337 78 nlp Gron 95.7 95.7 96.4 95.0 27 1 31 Table 5: Semantic Accuracy and Computational Resources for 1000 test sentences. Total and maximum CPU-time in seconds; memory in Megabytes. Method site all  2  4  6  8  10  instances 1000 601 344 160 74 38 d4 Adam 90.4 87.4 84.7 75.9 69.8 65.2 group.d2 Adam 91.6 89.0 86.7 78.7 69.8 68.3 group.d4 Adam 91.0 88.2 85.8 77.3 69.4 64.6 group.d5 Adam 91.4 88.7 86.6 78.9 71.8 67.1 nlp Gron 95.0 93.4 93.0 88.1 85.9 87.0 Table 6: Concept Accuracy versus Sentence Length for 1000 test sen- tences. The third column repeats the results for the full test set. The remaining columns list the results for the subset of the test set containing the sentences with at least 2 (4, 6, 8, 10) words. 14 data-oriented method obtains an error-rate for concept accuracy of 24.5. The best grammar-based method performs more than 30 better: an error-rate for concept accuracy of 17.0. For sentences, a similar diﬀerence can be observed. The best data-oriented method obtains an error rate for concept accuracy of 8.5 whereas the grammar-based method performs more than 40 better with a 5.0 error rate. The diﬀerences increase with increasing sentence length. The grammar-based methods require less computational resources than the data-oriented methods. However, the CPU-time requirements are still out- rageous for a small number of very large word graphs1. For sentences, the grammar-based component performs satisfactorily (with a maximum CPU-time of 610 milliseconds). The by far most important problem for the application consists of disam- biguation of the word graph. The evaluation shows that NLP hardly helps here: a combination of speech scores and trigram scores performs much better in terms of string accuracy than the data-oriented methods. The grammar-based methods have incorporated the insight that Ngrams are good at disambiguating word graphs; by incorporating Ngram statistics similar results for string accu- racy are obtained. In order to see whether NLP helps at all, we could compare the b(tr,1) method (which simply uses the best path in the word graph as input for the parser) with any of the other grammar-based methods. For instance, the method b(tr,4) performs somewhat better than b(tr,1) (83.0 vs. 82.2 concept accuracy). This shows that in fact NLP is helpful in choosing the best path2. If it were feasible to use methods b(tr,N) or f(tr,N) with larger values of N, further improvements might be possible. Once a given word graph has been disambiguated, then both NLP com- ponents work reasonably well: this can be concluded based upon the concept accuracy obtained for sentences. In those cases the grammar-based NLP com- ponent also performs better than the data-oriented parser; this indicates that the diﬀerence in performance between the two components is not (only) due to the introduction of Ngram statistics in the grammar-based NLP component. The current evaluation has brought some important shortcomings of the DOP approach to light. Two important problems, for which solutions are in the making, are brieﬂy discussed below. The ﬁrst one is the inadequacy of the deﬁnition of subtree probability. It turns out that Bods equation (3) given on page 3 shows a bias toward analyses derived by subtrees from large corpus trees. The error lies in viewing an anno- tated corpus as the ﬂat collection of all its subtrees. Information is lost when the distribution of the analyses that supply the subtrees is ignored. The eﬀect is that a large part of the probability mass is consumed by subtrees stemming from relatively rare, large trees in the tree-bank. A better model has been designed, that provides a more reliable way of estimating subtree probabilities. 1As mentioned before, a dramatic reduction has been obtained by a heuristic search algorithm. 2This result is (just) statistically signiﬁcant. We performed a paired T-test on the number of wrong semantic units per graph. This results in a t score of 2.0 (with 999 degrees of freedom). 15 The second shortcoming we will discuss is the fact that existing DOP algo- rithms are unable to generalise over the syntactic structures in the data. Corpus- based methods such as the current implementation of DOP, assume that the tree-bank which they employ for acquiring the parser, constitutes a rich enough sample of the domain. It is assumed that the part of the annotation scheme that is actually instantiated in the tree-bank does not under-generate on sentences of the domain. This assumption is not met by our current tree-bank. It turned out that one can expect the tree-bank grammar to generate a parse-space contain- ing the right syntacticsemantic tree only for approximately 90-91 of unseen domain utterances. This ﬁgure constitutes an upper bound on the accuracy for any probabilistic model. Enlarging the tree-bank does not guarantee a good coverage, however. The tree-bank will always represent only a sample of the domain. A solution for this problem is the development of automatic methods for generalising grammars, to enhance their coverage. The goal is to improve both accuracy and coverage by generalising over the structures encountered in the tree-bank. Acknowledgements This research was carried out within the framework of the Priority Programme Language and Speech Technology (TST). The TST-Programme is sponsored by NWO (Dutch Organisation for Scientiﬁc Research). References Alshawi, Hiyan, editor. 1992. The Core Language Engine. ACL-MIT press, Cambridge Mass. van den Berg, M., R. Bod, and R. Scha. 1994. A Corpus-Based Approach to Semantic Interpretation. In Proceedings Ninth Amsterdam Colloquium. ILLC,University of Amsterdam. Bod, Rens. 1993. Using an annotated corpus as a stochastic grammar. In Sixth Conference of the European Chapter of the Association for Computational Linguistics, Utrecht. Bod, Rens and Remko Scha. 1997. Data-oriented language processing: An overview. Technical Report 38, NWO Priority Programme Language and Speech Technology. Bonnema, R. 1996. Data oriented semantics. Masters thesis, Department of Computational Linguistics, University of Amsterdam. Boros, M., W. Eckert, F. Gallwitz, G. Gorz, G. Hanrieder, and H. Niemann. 1996. Towards understanding spontaneous speech: Word accuracy vs. con- cept accuracy. In Proceedings of the Fourth International Conference on Spoken Language Processing (ICSLP 96), Philadelphia. 16 Cormen, Leiserson, and Rivest. 1990. Introduction to Algorithms. MIT Press, Cambridge Mass. van Noord, Gertjan. 1995. The intersection of ﬁnite state automata and deﬁnite clause grammars. In 33th Annual Meeting of the Association for Computa- tional Linguistics, pages 159165, MIT Cambridge Mass. cmp-lg9504026. van Noord, Gertjan. 1997. Evaluation of OVIS2 NLP components. Technical Report 46, NWO Priority Programme Language and Speech Technology. van Noord, Gertjan, Gosse Bouma, Rob Koeling, and Mark-Jan Nederhof. 1996. Conventional natural language processing in the NWO priority programme on language and speech technology. October 1996 Deliverables. Technical Report 28, NWO Priority Programme Language and Speech Technology. van Noord, Gertjan, Gosse Bouma, Rob Koeling, and Mark-Jan Nederhof. 1999. Robust grammatical analysis for spoken dialogue systems. Journal of Nat- ural Language Engineering. To appear; 48 pages. van Noord, Gertjan, Mark-Jan Nederhof, Rob Koeling, and Gosse Bouma. 1996. Conventional natural language processing in the NWO priority programme on language and speech technology. January 1996 Deliverables. Technical Report 22, NWO Priority Programme Language and Speech Technology. Oerder, Martin and Hermann Ney. 1993. Word graphs: An eﬃcient inter- face between continuous-speech recognition and language understanding. In ICASSP Volume 2, pages 119122. Pollard, Carl and Ivan Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago  CSLI. Scha, R. 1990. Language Theory and Language Technology; Competence and Performance (in Dutch). In Q.A.M. de Kort and G.L.J. Leerdam, editors, Computertoepassingen in de Neerlandistiek, Almere: Landelijke Vereniging van Neerlandici (LVVN-jaarboek). Scha, Remko, Remko Bonnema, Rens Bod, and Khalil Simaan. 1996. Dis- ambiguation and interpretation of wordgraphs using data oriented parsing. Technical Report 31, NWO Priority Programme Language and Speech Tech- nology. Simaan, K. 1996a. An optimized algorithm for Data Oriented Parsing. In R. Mitkov and N. Nicolov, editors, Recent Advances in Natural Language Processing 1995, volume 136 of Current Issues in Linguistic Theory. John Benjamins, Amsterdam. Simaan, K. 1996b. Computational Complexity of Probabilistic Disambigua- tion by means of Tree-Grammars. In Proceedings COLING96, Copenhagen, Denmark, August. 17 Simaan, Khalil. 1997. Learning eﬃcient parsing. Technical Report 35, NWO Priority Programme Language and Speech Technology. Veldhuijzen van Zanten, Gert. 1996. Semantics of update expressions. Technical Report 24, NWO Priority Programme Language and Speech Technology. 18",
  "36.pdf": "arXiv:cs9906015v1 [cs.CL] 14 Jun 1999 Learning Transformation Rules to Find Grammatical Relations Lisa Ferro and Marc Vilain and Alexander Yeh The MITRE Corporation 202 Burlington Rd. Bedford, MA 01730 USA {lferro,mbv,asy}mitre.org Abstract Appears in Computational Natural Language Learning (CoNLL-99), pages 43-52. A workshop at the 9th Conf. of the European Chapter of the Assoc. for Computational Linguistics (EACL-99). Bergen, Norway, June, 1999. cs.CL9906015 Grammatical relationships are an important level of natural language processing. We present a trainable approach to ﬁnd these relationships through transfor- mation sequences and error-driven learning. Our ap- proach ﬁnds grammatical relationships between core syntax groups and bypasses much of the parsing phase. On our training and test set, our procedure achieves 63.6 recall and 77.3 precision (f-score  69.8). Introduction An important level of natural language process- ing is the ﬁnding of grammatical relationships such as subject, object, modiﬁer, etc. Such relation- ships are the objects of study in relational grammar [Perlmutter, 1983]. Many systems (e.g., the KERNEL system [Palmer et al., 1993]) use these relationships as an intermediate form when determining the semantics of syntactically parsed text. In the SPARKLE project [Carroll et al., 1997a], grammatical relations form the layer above the phrasal-level in a three layer syntax scheme. Grammatical relationships are often stored in some type of structure like the F-structures of lexical- functional grammar [Kaplan, 1994]. Our own interest in grammatical relations is as a se- mantic basis for information extraction in the Alembic system. The extraction approach we are currently in- vestigating exploits grammatical relations as an inter- mediary between surface syntactic phrases and proposi- tional semantic interpretations. By directly associating syntactic heads with their arguments and modiﬁers, we are hoping that these grammatical relations will provide a high degree of generality and reliability to the process of composing semantic representations. This ability to  This paper reports on work performed at the MITRE Corporation under the support of the MITRE Sponsored Research Program. Helpful assistance has been given by Yuval Krymolowski, Lynette Hirschman and an anonymous reviewer. Copyright c1999 The MITRE Corporation. All rights reserved. parse into a semantic representation is according to Charniak [Charniak, 1997, p. 42], the most important task to be tackled now. In this paper, we describe a system to learn rules for ﬁnding grammatical relationships when just given a partial parse with entities like names, core noun and verb phrases (noun and verb groups) and semi-accurate estimates of the attachments of prepositions and subor- dinate conjunctions. In our system, the diﬀerent enti- ties, attachments and relationships are found using rule sequence processors that are cascaded together. Each processor can be thought of as approximating some as- pect of the underlying grammar by ﬁnite-state trans- duction. We present the problem scope of interest to us, as well as the data annotations required to support our investi- gation. We also present a decision procedure for ﬁnding grammatical relationships. In brief, on our training and test set, our procedure achieves 63.6 recall and 77.3 precision, for an f-score of 69.8. Phrase Structure and Grammatical Relations In standard derivational approaches to syntax, start- ing as early as 1965 [Chomsky, 1965], the notion of grammatical relationship is typically parasitic on that of phrase structure. That is to say, the primary vehicles of syntactic analysis are phrase structure trees; gram- matical relationships, if they are to be considered at all, are given as a secondary analysis deﬁned in terms of phrase structure. The surface subject of a sentence, for example, is thus no more than the NP attached by the production S  NP VP; i.e., it is the left-most NP daughter of an S node. The present paper takes an alternate outlook. In our current work, grammatical relationships play a central role, to the extent even of replacing phrase structure as the descriptive vehicle for many syntactic phenom- ena. To be speciﬁc, our approach to syntax operates at two levels: (1) that of core phrases, which are an- alyzed through standard derivational syntax, and (2) that of argument and modiﬁer attachments, which are analyzed through grammatical relations. These two levels roughly correspond to the top and bottom lay- ers of the three layer syntax annotation scheme in the SPARKLE project [Carroll et al., 1997a]. Core syntactic phrases In recent years, a consensus of sorts has emerged that postulates some core level of phrase analy- sis. By this we mean the kind of non-recursive simpliﬁcations of the NP and VP that in the lit- erature go by names such as nounverb groups [Appelt et al., 1993], chunks [Abney, 1996], or base NPs [Ramshaw and Marcus, 1995]. The common thread between these approaches and ours is to approximate full noun phrases or verb phrases by only parsing their non-recursive core, and thus not attaching modiﬁers or arguments. For English noun phrases, this amounts to roughly the span between the determiner and the head noun; for English verb phrases, the span runs roughly from the auxiliary to the head verb. We call such simpliﬁed syntactic categories groups, and consider in particular, noun, verb, adverb, adjective, and IN groups.1 An IN group2 contains a preposition or subordinate conjunction (including wh- words and that). For example, for I saw the cat that ran., we have the following core phrase analysis: [I]ng [saw]vg [the cat]ng [that]ig [ran]vg. where [...]ng indicates a noun group, [...]vg a verb group, and [...]ig an IN group. In English and other languages where core phrases (groups) can be analyzed by head-out (island-like) pars- ing, the group head-words are basically a by-product of the core phrase analysis. Distinguishing core syntax groups from traditional syntactic phrases (such as NPs) is of interest because it singles out what is usually thought of as easy to parse, and allows that piece of the parsing problem to be ad- dressed by such comparatively simple means as ﬁnite- state machines or transformation sequences. What is then left of the parsing problem is the diﬃcult stuﬀ: namely the attachment of prepositional phrases, rela- tive clauses, and other constructs that serve in modiﬁ- cation, adjunctive, or argument-passing roles. 1In addition, for the noun group, our deﬁnition encom- passes the named entity task, familiar from information ex- traction [Def, 1995]. Named entities include among others the names of people, places, and organizations, as well as dates, expressions of money, and (in an idiosyncratic exten- sion) titles, job descriptions, and honoriﬁcs. 2The name comes from the Penn Treebank part-of-speech label for prepositions and subordinate conjunctions. Grammatical relations In the present work, we encode this hard stuﬀ through a small repertoire of grammatical relations. These re- lations hold directly between constituents, and as such deﬁne a graph, with core constituents as nodes in the graph, and relations as labeled arcs. Our previous ex- ample, for instance, generates the following grammati- cal relations graph (head words underlined): SUBJect   OBJect SUBJect  [I] [saw] [the cat] [that] [ran]  MODiﬁer Our grammatical relations eﬀectively replace the re- cursive X analysis of traditional phrase structure gram- mar. In this respect, the approach bears resemblance to a dependency grammar, in that it has no notion of a spanning S node, or of intermediate constituents cor- responding to argument and modiﬁer attachments. One major point of departure from dependency gram- mar, however, is that these grammatical relation graphs can generally not be reduced to labeled trees. This hap- pens as a result of argument passing, as in [Fred] [promised] [to help] [John] where [Fred] is both the subject of [promised] and [to help]. This also happens as a result of argument- modiﬁer cycles, as in [I] [saw] [the cat] [that] [ran] where the relationships between [the cat] and [ran] form a cycle: [the cat] has a subject relationshipdependency to [ran], and [ran] has a modiﬁer dependency to [the cat], since [ran] helps indicate (modiﬁes) which cat is seen. There has been some work at making additions to extract grammatical relationships from a dependency tree structure [Broker, 1998, Lai and Huang, 1998] so that one ﬁrst produces a surface structure dependency tree with a syntactic parse and then extracts grammat- ical relationships from that tree. In contrast, we skip trying to ﬁnd a surface structure tree and just proceed to more directly ﬁnding the grammatical relationships, which are the relationships of interest to us. A reason for skipping the tree stage is that extracting grammatical relations from a surface structure tree is often a nontrivial task by itself. For instance, the pre- cise relationship holding between two constituents in a surface structure tree cannot be derived unambiguously from their relative attachments. Contrast, for example the attack on the military base with the attack on March 24. Both of these have the same underlying surface structure (a PP attached to an NP), but the 44 former encodes the direct object of a verb nominaliza- tion, while the latter encodes a time modiﬁer. Also, in a surface structure tree, long-distance dependencies between heads and arguments are not explicitly indi- cated by attachments between the appropriate parts of the text. For instance in Fred promised to help John, no direct attachment exists between the Fred in the text and the help in the text, despite the fact that the former is the subject of the latter. For our purposes, we have delineated approximately a dozen head-to-argument relationships as well as a commensurate number of modiﬁcation relationships. Among the head-to-argument relationships, we have the deep subject and object (SUBJ and OBJ respectively), and also include the surface subject and object of cop- ulas (COP-SUBJ and the various COP-OBJ forms). In addition, we include a number of relationships (e.g., PP-SUBJ, PP-OBJ) for arguments that are mediated by prepositional phrases. An example is in  PP-OBJect  OBJect [the attack] [on] [the military base] where [the attack], a noun group with a verb nominal- ization, has its object [the military base] passed to it via the preposition in [on]. Among modiﬁer relationships, we designate both generic modiﬁcation and some spe- cializations like locational and temporal modiﬁcation. A complete deﬁnition of all the grammatical relations is beyond the scope of this paper, but we give a summary of usage in Table 1. An earlier version of the deﬁnitions can be found in our annotation guidelines [Ferro, 1998]. The appendix shows some examples of grammatical re- lationship labeling from our experiments. Our set of relationships is similar to the set used in the SPARKLE project [Carroll et al., 1997a] [Carroll et al., 1998a]. One diﬀerence is that we make many semantically-based distinctions between what SPARKLE calls a modiﬁer, such as time and location modiﬁers, and the various arguments of event nouns. Semantic interpretation A major motivation for this approach is that it sup- ports a direct mapping into semantic interpretations. In our framework, semantic interpretations are given in a neo-Davidsonian propositional logic. Grammati- cal relations are thus interpreted in terms of mappings and relationships between the constants and variables of the propositional language. For instance, the deep subject relation (SUBJ) maps to the ﬁrst position of a predicates argument list, the deep object (OBJ) to the second such position, and so forth. Our example sentence, I saw the cat that ran thus translates directly to the following: Proposition Comment saw(x1 x2) SUBJ and OBJ relations I(x1) cat(x2) ran(x2)e3 SUBJ relation (e3 is the event variable) mod(e3 x2) MOD relation We do not have an explicit level for clauses between our core phrase and grammatical relations levels. How- ever, we do have a set of implicit clauses in that each verb (event) and its arguments can be deemed a base level clause. In our example I saw the cat that ran, we have two such base level clauses. saw and its argu- ments form the clause I saw the cat. ran and its ar- gument form the clause the cat ran. Each noun with a possible semantic class of act or process in Wordnet [Miller, 1990] (and that nouns arguments) can likewise be deemed a base level clause. The Processing Model Our system uses transformation-based error-driven learning to automatically learn rules from training ex- amples [Brill and Resnik, 1994]. One ﬁrst runs the system on a training set, which starts with no grammatical relations marked. This training run moves in iterations, with each iteration producing the next rule that yields the best net gain in the training set (number of matching relationships found minus the number of spurious relationships intro- duced). On ties, rules with less conditions are favored over rules with more conditions. The training run ends when the next rule found produces a net gain below a given threshold. The rules are then run in the same order on the test set to see how well they do. The rules are conditionaction pairs that are tried on each syntax group. The actions in our system are limited to attaching (or unattaching) a relationship of a particular type from the group under consideration to that groups neighbor a certain number of groups away in a particular direction (left or right). A sample action would be to attach a SUBJ relation from the group under consideration to the group two groups away to the right. A rule only applies to a syntax group when that group and its neighbors meet the rules conditions. Each con- dition tests the group in a particular position relative to the group under consideration (e.g., two groups away to the left). All tests can be negated. Table 2 shows the possible tests. A sample rule is when a noun group ns  immediate group to the right has some form of the verb be as the head-word, 45 RELATION EXAMPLE(s) in the format Name Description [source]  [target] in text subj subject [I]  [promised] in I promised to help  subject of a verb [I]  [to help] in I promised to help [the cat]  [ran] in the cat that ran  link a copula subject and object [You]  [happy] in You are happy [You]  [a runner] in You are a runner  link a state with the item in that state [you]  [happy] in They made you happy  link a place with the item moving [I]  [home] in I went home to or from that place obj object [saw]  [the cat] in I saw the cat  object of a verb [promised]  [to help] in I promised to help you  object of an adjective [happy]  [to help] in I was happy to help  surface subject in passives [I]  [was seen] in I was seen by a cat  object of a preposition, [by]  [the tree] in I was by the tree not for partitives or subsets  object of [After]  [left] in After I left, I ate an adverbial clause complementizer loc-obj location object [went]  [home] in I went home link a movement verb with a place [went]  [in] in I went in the house where entities are moving to or from indobj indirect object [gave]  [you] in I gave you a cake empty use instead of subj relation when subject [There]  [trees] in There are trees is an expletive (existential) it or there pp-subj genitive functional ofs [name]  [of] in name of the building use instead of subj relation when the [was seen]  [by] in I was seen by a cat subject is linked via a preposition, links preposition to its head pp-obj nongenitive functional ofs [age]  [of] in age of 12 use in place of obj relation when the [the attack]  [on] in the attack on the base object is linked via a preposition, links preposition to its head pp-io use in place of indobj relation when the [gave]  [to] in gave a cake to them indirect object is linked via a preposition, links preposition to its head cop-subj surface subject for a copula [You]  [are] in You are happy n-cop-obj surface nominative object for a copula [is]  [a rock] in It is a rock p-cop-obj surface predicate object for a copula [are]  [happy] in You are happy subset subset [ﬁve]  [the kids] in ﬁve of the kids mod generic modiﬁer (use when [the cat]  [ran] in the cat that ran modiﬁer does not ﬁt in a case below) [ran]  [with] in I ran with new shoes mod-loc location modiﬁer [ate]  [at] in I ate at home mod-time time modiﬁer [ate]  [at] in I ate at midnight [Yesterday]  [ate] in Yesterday, I ate mod-poss possessive modiﬁer [the cat]  [toy] in the cats toy mod-quant quantity modiﬁer (partitive) [hundreds]  [people] in hundreds of people mod-ident identity modiﬁer (names) [a cat]  [Fuzzy] in a cat named Fuzzy [the winner]  [Pat Kay] in the winner, Pat Kay, is mod-scalar scalar modiﬁer [16 years]  [ago] in 16 years ago Table 1: Summary of grammatical relationships 46 Test Type Example, Sample Value(s) group type noun, verb verb group property passive, inﬁnitival, unconjugated present participle end group in a sentence ﬁrst, last pp-attachment Is a preposition or subordinate conjunction attached to the group under consideration? group contains a particular lexeme or part-of-speech between two groups, there is a particular lexeme or part-of-speech groups head (main) word cat head word part-of-speech common plural noun head word within a named entity person, organization head word subcategorization and complement categories intransitive verbs (from Comlex [Wolﬀ et al., 1995], over 100 categories) head word semantic classes process, communication (from Wordnet [Miller, 1990], 25 noun and 15 verb classes) punctuation or coordinating conjunction exist between two groups? head word in a word list? list of relative pronouns, list of partitive quantities (e.g., some) Table 2: Possible tests  immediate group to the left is not an IN group (preposition, wh-word, etc.) and  ns head-word is not an existential there make n a SUBJ of the group two groups over to ns right. When applied to the group [The cat] (head words are underlined) in the sentence [The cat] [was] [very happy]. this rule makes [The cat] a SUBJect of [very happy]. Searching over the space of possible rules is very com- putationally expensive. Our system has features to make it easier to perform searching in parallel and to minimize the amount of work that needs to be undone once a rule is selected. With these features, rules that (un)attach diﬀerent types of relationships or relation- ships at diﬀerent distances can be searched indepen- dently of each other in parallel. One feature is that the action of any rule only aﬀects the applicability of rules with either the exact same or opposite action. For example, selecting and running a rule which attaches a MOD relationship to the group that is two groups to the right only can aﬀect the ap- plicability of other rules that either attach or unattach a MOD relationship to the group that is two groups to the right. Another feature is the use of net gain as a proxy measure during training. The actual measure by which we judge the systems performance is called an f-score. This f-score is a type of harmonic mean of the precision (p) and recall (r) and is given by 2pr(p  r). Unfor- tunately, this measure is nonlinear, and the application of a new rule can alter the eﬀects of all other possible rules on the f-score. To enable the described parallel search to take place, we need a measure in which how a rule aﬀects that measure only depends on other rules with either the exact same or opposite action. The net gain measure has this trait, so we use it as a proxy for the f-score during training. Another way to increase the learning speed is to re- strict the number of possible combinations of condi- tionsconstraints or actions to search over. Each rule is automatically limited to only considering one type of syntactic group. Then when searching over possible conditions to add to that rule, the system only needs to consider the parts-of-speech, semantic classes, etc. applicable to that type of group. Many other restrictions are possible. One can esti- mate which restrictions to try by making some train- ing and test runs with preliminary data sets and seeing what restrictions seem to have no eﬀect on performance, etc. The restrictions used in our experiments are de- scribed below. 47 Experiments The Data Our data consists of bodies of some elementary school reading comprehension tests. For our purposes, these tests have the advantage of having a fairly predictable size (each body has about 100 relationships and syntax groups) and a consistent style of writing. The tests are also on a wide range of topics, so we avoid a narrow specialized vocabulary. Our training set has 1963 re- lationships (2153 syntax groups, 3299 words) and our test set has 748 relationships (830 syntax groups, 1151 words). We prepared the data by ﬁrst manually removing the headers and the questions at the end for each test. We then manually annotated the remainder for named entities, syntax groups and relationships. As the system reads in our data, it automatically breaks the data into lexemes and sentences, tags the lexemes for part-of-speech and estimates the attachments of prepositions and subordinate conjunctions. The part- of-speech tagging uses a high-performance tagger based on [Brill, 1993]. The attachment estimation uses a pro- cedure described in [Yeh and Vilain, 1998] when mul- tiple left attachment possibilities exist and four simple rules when no or only one left attachment possibility exists. Previous testing indicates that the estimation procedure is about 75 accurate. Parameter Settings for Training As described earlier, a training run uses many param- eter settings. Examples include where to look for rela- tionships and to test conditions, the maximum number of constraints allowed in a rule, etc. Based on the observation that 95 of the relation- ships are to at most three groups away in the training set, we decided to limit the search for relationships to at most three groups in length. To keep the number of possible constraints down, we disallowed the negations of most tests for the presence of a particular lexeme or lexeme stem. To help determine many of the settings, we made some preliminary runs using diﬀerent subsets of our ﬁ- nal training set as the preliminary training and test sets. This kept the ﬁnal test set unexamined during develop- ment. From these preliminary runs, we decided to limit a rule to at most three constraints3 in order to keep the training time reasonable. We found a number of limita- tions that help speed up training and seemed to have no eﬀect on the preliminary test runs. A threshold of four was set to end a training run. So training ends when it can no longer ﬁnd a rule that produces at least a net 3In addition to the constraint on the relationships source group type. gain of four in the score. Only syntax groups spanned by the relationship being attached or unattached and those groups immediate neighbors were allowed to be mentioned in a rules conditions. Each condition test- ing a head-word had to test a head-word of a diﬀerent group. Except for the lexemes of, ? and a few determiners like the, tests for single lexemes were re- moved. Also disallowed were negations of tests for the presence of a particular part-of-speech anywhere within a syntax group. In our preliminary runs, lowering the threshold tended to raise recall and lower precision. The Results Training produced a sequence of 95 rules which had 63.6 recall and 77.3 precision for an f-score of 69.8 when run on the test set. In our test set, the key re- lationships, SUBJ and OBJ, formed the bulk of the relationships (61). Both recall and precision for both SUBJ and OBJ were above 70, which pleased us. Be- cause of their relative abundance in the test set, these two relationships also had the most number of errors in absolute terms. Combined, the two accounted for 45 of the recall errors and 66 of the precision errors. In terms of percentages, recall was low for many of the less common relationships, such as generic, time and loca- tion modiﬁcation relationships. In addition, the relative precision was low for those modiﬁcation relationships. The appendix shows some examples of our system re- sponding to the test set. To see how well the rules, which were trained on reading comprehension test bodies, would carry over to other texts of non-specialized domains, we examined a set of six broadcast news stories. This set had 525 re- lationships (585 syntax groups, 1129 words). By some measures, this set was fairly similar to our training and test sets. In all three sets, 3334 of the relationships were OBJ and 2628 were SUBJ. The broadcast news set did tend to have relationships between groups that were slightly further apart: Percent of Relations with Length Set  1  2  3 training 66 87 95 test 68 89 96 broadcast news 65 84 90 This tendency, plus diﬀerences in the relative propor- tions of various modiﬁcation relationships are probably what produced the drop in results when we tested the rules against this news set: recall at 54.6, precision at 70.5 (f-score at 61.6). To estimate how fast the results would improve by adding more training data, we had the system learn rules on a new smaller training set and then tested 48 against the regular test set. Recall dropped to 57.8, precision to 76.2. The smaller training set had 981 relationships (50 of the original training set). So dou- bling the training data here (going from the smaller to the regular training set) reduced the smaller training sets recall error of 42.2 by 14 and the precision er- ror of 23.8 by 5. Using the broadcast news set as a test produced similar error reduction results. One complication of our current scoring scheme is that identifying a modiﬁcation relationship and mis- typing it is more harshly penalized than not ﬁnding a modiﬁcation relationship at all. For example, ﬁnd- ing a modiﬁcation relationship, but mistakingly calling it a generic modiﬁer instead of a time modiﬁer pro- duces both a missed key error (not ﬁnding a time mod- iﬁer) and a spurious response error (responding with a generic modiﬁer where none exists). Not ﬁnding that modiﬁcation relationship at all just produces a missed key error (not ﬁnding a time modiﬁer). This compli- cation, coupled with the fact that generic, time and location modiﬁers often have a similar surface appear- ance (all are often headed by a preposition or a comple- mentizer) may have been responsible for the low recall and precision scores for these types of modiﬁers. Even the training scores for these types of modiﬁers were particularly low. To test how well our system ﬁnds these three types of modiﬁcation when one does not care about specifying the sub-type, we reran the origi- nal training and test with the three sub-types merged into one sub-type in the annotation. With the merging, recall of these modiﬁcation relationships jumped from 27.8 to 48.9. Precision rose from 52.1 to 67.7. Since these modiﬁcation relationships are only about 20 of all the relationships, the overall improvement is more modest. Recall rises to 67.7, precision to 78.6 (f-score to 72.6). Taking this one step further, the LOC-OBJ and var- ious PP-x arguments also all have both a low recall (below 35) in the test and a similar surface structure to that of generic, time and location modiﬁers. When these argument types were merged with the three modi- ﬁer types into one combined type, their combined recall was 60.4 and precision was 81.1. The corresponding overall test recall and precision were 70.7 and 80.5, respectively. Comparison with Other Work At one level, computing grammatical relationships can be seen as a parsing task, and the question naturally arises as to how well this approach compares to current state-of-the-art parsers. Direct performance compar- isons, however, are elusive, since parsers are evaluated on an incommensurate tree bracketing task. For exam- ple, the SPARKLE project [Carroll et al., 1997a] puts tree bracketing and grammatical relations in two dif- ferent layers of syntax. Even if we disregard the ques- tionable aspects of comparing tree bracketing apples to grammatical relation oranges, an additional compli- cation is the fact that our approach divides the pars- ing task into an easy piece (core phrase boundaries) and a hard one (grammatical relations). The results we have presented here are given solely for this harder part, which may explain why at roughly 70 points of f-score, they are lower than those reported for current state-of-the-art parsers (e.g., Collins [Collins, 1997]). More comparable to our approach are some other grammatical relation ﬁnders. Some examples for En- glish include the English parser used in the SPARKLE project [Briscoe et al., ] [Carroll et al., 1997b] [Carroll et al., 1998b] and the ﬁnder built with a memory-based approach [Argamon et al., 1998]. These relation ﬁnders make use of large annotated training data sets andor manually generated grammars and rules. Both techniques take much eﬀort and time. At ﬁrst glance both of these ﬁnders perform better than our approach. Except for the object precision score of 77 in [Argamon et al., 1998], both ﬁnders have gram- matical relation recall and precision scores in the 80s. But a closer examination reveals that these results are not quite comparable with ours. 1. Each system is recovering a diﬀerent variation of grammatical relations. As mentioned earlier, one diﬀerence between us and the SPARKLE project is that the latter ignores many of distinctions that we make for diﬀerent types of modiﬁers. The system in [Argamon et al., 1998] only ﬁnds a subset of the surface subjects and objects. 2. In addition, the evaluations of these two ﬁnders produced more complications. In an illustration of the time consuming nature of annotating or reanno- tating a large corpus, the SPARKLE project orig- inally did not have time to annotate the English test data for modiﬁer relationships. As a result, the SPARKLE English parser was originally not eval- uated on how well it found modiﬁer relationships [Carroll et al., 1997b] [Carroll et al., 1998b]. The re- ported results as of 1998 only apply to the argument (subject, object, etc.) relationships. Later on, a test corpus with modiﬁer relationship annotation was pro- duced. Testing the parser against this corpus pro- duced generally lower results, with an overall recall, precision and f-score of 75 [Carroll et al., 1999]. This is still better than our f-score of 70, but not by nearly as much. This comparison ignores the fact that the results are for diﬀerent versions of grammat- 49 ical relationships and for diﬀerent test corpora. The ﬁgures given above were the original (1998) re- sults for the system in [Argamon et al., 1998], which came from training and testing on data derived from the Penn Treebank corpus [Marcus et al., 1993] in which the added null elements (like null subjects) were left in. These null elements, which were given a -NONE- part-of-speech, do not appear in raw text. Later (1999 results), the system was re-evaluated on the data with the added null elements removed. The subject results declined a little. The object results de- clined more, with the precision now lower than ours (73.6 versus 80.3) and the f-score not much higher (80.6 versus 77.8). This comparison is also be- tween results with diﬀerent test corpora and slightly diﬀerent notions of what an object is. Summary, Discussion, and Speculation In this paper, we have presented a system for ﬁnd- ing grammatical relationships that operates on easy- to-ﬁnd constructs like noun groups. The approach is guided by a variety of knowledge sources, such as read- ily available lexica4, and relies to some degree on well- understood computational infrastructure: a p-o-s tag- ger and an attachment procedure for preposition and subordinate conjunctions. In sample text, our system achieves 63.6 recall and 77.3 precision (f-score  69.8) on our repertory of grammatical relationships. This work is admittedly still in relatively early stages. Our training and test corpora, for instance, are less- than-gargantuan compared to such collections as the Penn Treebank [Marcus et al., 1993]. However, the fact that we have obtained an f-score of 70 from such sparse training materials is encouraging. The recent imple- mentation of rapid annotation tools should speed up further annotation of our own native corpus. Another task that awaits us is a careful measurement of interannotator agreement on our version the gram- matical relationships. We are also keenly interested in applying a wider range of learning procedures to the task of identify- ing these grammatical relations. Indeed, a ﬁne-grained analysis of our development test data has identiﬁed some recurring errors related to the rule sequence ap- proach. A hypothesis for further experimentation is that these errors might productively be addressed by revisiting the way we exploit and learn rule sequences, or by some hybrid approach blending rules and statisti- cal computations. In addition, since generic, time and location modiﬁers, and LOC-OBJ and various PP-x ar- guments often have a similar surface appearance, one 4Resources to ﬁnd a words possible stem(s), semantic class(es) and subcategorization category(ies). might ﬁrst just try to locate all such entities and then in a later phase try to classify them by type. Diﬀerent applications will need to deal with diﬀerent styles of text (e.g., journalistic text versus narratives) and diﬀerent standards of grammatical relationships. An additional item of experimentation is to use our sys- tem to adapt other systems, including earlier versions of our system, to these diﬀering styles and standards. Like other Brill transformation rule sys- tems [Brill and Resnik, 1994], our system can take in the output of another system and try to improve on it. This suggests a relatively low expense method to adapt a hard-to-alter system that performs well on a slightly diﬀerent style or standard. Our training approach ac- cepts as a starting point an initial labeling of the data. So far, we have used an empty labeling. However, our system could just as easily start from a labeling pro- duced as the output of the hard-to-alter system. The learning would then not be reducing the error between an empty labeling and the key annotations, but between the hard-to-alter systems output and the key anno- tations. By using our system in this post-processing manner, we could use a relatively small retraining set to adapt, for example, the SPARKLE English parser, to our standard of grammatical relationships without having reengineer that parser. Palmer [Palmer, 1997] used a similar approach to improve on existing word segmenters for Chinese. Trying this suggestion out is also something for us to do. This discussion of training set size brings up perhaps the most obvious possible improvement. Namely, en- larging our very small training set. As has been men- tioned, we have recently improved our annotation envi- ronment and look forward to working with more data. Clearly we have many experiments ahead of us. But we believe that the results obtained so far are a promis- ing start, and the potential rewards of the approach are very signiﬁcant indeed. Appendix: Examples from Test Results Figure 1 shows some example sentences from the test results of our main experiment.5  marks the relation- ship that our system missed.  marks the relationship that our system wrongly hypothesized. In these ex- amples, our system handled a number of phenomena correctly, including:  The coordination conjunction of the objects [cars] and [trucks] 5The material came from level 2 of The 5 Ws written by Linda Miller. It is available from Remedia Publications, 10135 E. Via Linda D124, Scottsdale, AZ 85258, USA. 50 SUBJ   OBJ  MOD  OBJ [The ship] [was carrying] [oil] [for] [cars] and [trucks].  OBJ SUBJ  OBJ [That] [means] [the same word] [might have] [two or three spellings]. SUBJ  OBJ SUBJ   OBJ  OBJ [He] [loves] [to work] [with] [words]. SUBJ  PP-OBJ SUBJ  OBJ  OBJ SUBJ   OBJ [A man] [named] [Noah] [wrote] [this book]. MOD MOD-IDENT Figure 1: Example test responses from our system.  marks the missed key.  marks the spurious response.  The verb group [might have] being an object of an- other verb.  The noun group [He] being the subject of two verbs.  The relationships within the reduced relative clause [A man] [named] [Noah], which makes one noun group a name or label for another noun group. Our system misses a PP-OBJ relationship, which is a low occurrence relationship. Our system also acciden- tally make both [A man] and [Noah] subjects of the group [wrote] when only the former should be. References [Abney, 1996] S. Abney. Partial parsing via ﬁnite-state cascades. In Proc. of ESSLI96 Workshop on Robust Parsing, 1996. [Appelt et al., 1993] D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. Fastus: A ﬁnite-state pro- cessor for information extraction. In 13th Intl. Conf. On Artiﬁcial Intelligence (IJCAI-93), 1993. [Argamon et al., 1998] S. Argamon, I. Dagan, and Y. Krymolowski. A memory-based approach to learn- ing shallow natural language patterns. In COLING- ACL98, pages 6773, Montreal, Canada, 1998. An expanded 1999 version will appear in JETAI. [Brill and Resnik, 1994] E. Brill and P. Resnik. A rule- based approach to prepositional phrase attachment disambiguation. In 15th International Conf. on Com- putational Linguistics (COLING), 1994. [Brill, 1993] E. Brill. A Corpus-based Approach to Lan- guage Learning. PhD thesis, U. Pennsylvania, 1993. [Briscoe et al., ] T. Briscoe, J. Carroll, G. Car- roll, S. Federici, G. Grefenstette, S. Montemagni, V. Pirrelli, I. Prodanof, M. Rooth, and M. Van- nocchi. Phrasal parser software  deliverable 3.1. Available at http:www.ilc.pi.cnr.it- sparklesparkle.htm. [Broker, 1998] N. Broker. Separating surface order and syntactic relations in a dependency gram- mar. In COLING-ACL98, pages 174180, Montreal, Canada, 1998. 51 [Carroll et al., 1997a] J. Carroll, T. Briscoe, N. Cal- zolari, S. Federici, S. Montemagni, V. Pir- relli, G. Grefenstette, A. Sanﬁlippo, G. Car- roll, and M. Rooth. Sparkle work pack- age 1, speciﬁcation of phrasal parsing, ﬁnal re- port. Available at http:www.ilc.pi.cnr.it- sparklesparkle.htm, November 1997. [Carroll et al., 1997b] J. Carroll, T. Briscoe, G. Car- roll, M. Light, D. Prescher, M. Rooth, S. Federici, S. Montemagni, V. Pirrelli, I. Prodanof, and M. Vannocchi. Sparkle work package 3, phrasal parsing software, deliverable d3.2. Available at http:www.ilc.pi.cnr.it- sparklesparkle.htm, November 1997. [Carroll et al., 1998a] J. Carroll, T. Briscoe, and A. Sanﬁlippo. Parser evaluation: a survey and a new proposal. In 1st Intl. Conf. on Language Resources and Evaluation (LREC), pages 447454, Granada, Spain, 1998. [Carroll et al., 1998b] J. Carroll, G. Minnen, and T. Briscoe. Can subcategorisation probabilities help a statistical parser? In 6th ACLSIGDAT workshop on Very Large Corpora, Montreal, Canada, 1998. [Carroll et al., 1999] J. Carroll, G. Minnen, and T. Briscoe. Corpus annotation for parser evaluation. In To appear in the EACL99 workshop on Linguisti- cally Interpreted Corpora (LINC99), 1999. [Charniak, 1997] E. Charniak. Statistical techniques for natural language parsing. AI magazine, 18(4):33 43, 1997. [Chomsky, 1965] N. Chomsky. Aspects of the Theory of Syntax. Massachusetts Institute of Technology, 1965. [Collins, 1997] M. Collins. Three generative, lexical- ized models for statistical parsing. In Proceedings of ACLEACL97, 1997. [Def, 1995] Defense Advanced Research Projects Agency. Proc. 6th Message Understanding Confer- ence (MUC-6), November 1995. [Ferro, 1998] L. Ferro. Guidelines for annotating gram- matical relations. Unpublished annotation guide- lines, 1998. [Kaplan, 1994] R. Kaplan. The formal architecture of lexical-functional grammar. In M. Dalrymple, R. Ka- plan, J. Maxwell III, and A. Zaenen, editors, Formal issues in lexical-functional grammar. Stanford Uni- versity, 1994. [Lai and Huang, 1998] T. B.Y. Lai and C. Huang. Complements and adjuncts in dependency grammar parsing emulated by a constrained context-free gram- mar. In COLING-ACL98 Workshop: Processing of Dependency-based Grammars, Montreal, Canada, 1998. [Marcus et al., 1993] M. Marcus, B. Santorini, and M. Marcinkiewicz. Building a large annotated cor- pus of english: the penn treebank. Computational Linguistics, 19(2), 1993. [Miller, 1990] G. Miller. Wordnet: an on-line lexical database. Intl. J. of Lexicography, 3(4), 1990. [Palmer et al., 1993] M. Palmer, R. Passonneau, C. Weir, and T. Finin. The kernel text understanding system. Artiﬁcial In- telligence, 63:1768, 1993. [Palmer, 1997] D. Palmer. A trainable rule-based al- gorithm for word segmentation. In Proceedings of ACLEACL97, 1997. [Perlmutter, 1983] D. Perlmutter. Studies in Relational Grammar 1. U. Chicago Press, 1983. [Ramshaw and Marcus, 1995] L. Ramshaw and M. Marcus. Text chunking using transformation- based learning. In Proc. of the 3rd Workshop on Very Large Corpora, pages 8294, Cambridge, MA, USA, 1995. [Wolﬀ et al., 1995] S. Wolﬀ, C. Macleod, and A. Mey- ers. Comlex Word Classes. C.S. Dept., New York U., Feb. 1995. prepared for the Linguistic Data Consor- tium, U. Pennsylvania. [Yeh and Vilain, 1998] A. Yeh and M. Vilain. Some properties of preposition and subordinate conjunc- tion attachments. In COLING-ACL98, pages 1436 1442, Montreal, Canada, 1998. 52",
  "37.pdf": "arXiv:cs9906020v1 [cs.CL] 22 Jun 1999 Temporal Meaning Representations in a Natural Language Front-End Ion Androutsopoulos Software and Knowledge Engineering Laboratory Institute of Informatics and Telecommunications National Centre for Scientiﬁc Research Demokritos 153 10 Ag. Paraskevi, Athens, Greece e-mail: ionandriit.demokritos.gr Abstract Previous work in the context of natural language querying of temporal databases has established a method to map automatically from a large subset of English time- related questions to suitable expressions of a temporal logic-like language, called top. An algorithm to translate from top to the tsql2 temporal database language has also been deﬁned. This paper shows how top expressions could be translated into a simpler logic-like language, called bot. bot is very close to traditional ﬁrst-order predicate logic (fopl), and hence existing methods to manipulate fopl expressions can be exploited to interface to time-sensitive applications other than tsql2 databases, maintaining the existing English-to-top mapping. 1 Introduction Time is an important research issue in linguistics (e.g. [7], [8], [19]), logics (e.g. [12], [28]), and computer systems (e.g. temporal databases [26] [27]). In [3] and [4] a framework that integrates ideas from these three areas was proposed in the context of natural language querying of temporal databases. This framework consists of: (i) a formally deﬁned logic- like language, dubbed top, (ii) a systematic mapping from a large and rich in temporal phenomena subset of English to top, based on the widely used hpsg grammar theory [20] [21], and (iii) an algorithm to translate from top to tsql2, tsql2 being a recent temporal extension of the sql database language that has been proposed by the temporal databases community [24]. The framework allows written time-related English questions to be answered automatically, by converting them into top and then tsql2 expressions, and executing the resulting tsql2 queries. The framework improves on previous approaches to natural language querying of temporal databases (e.g. [6], [10]), mainly in terms of linguistic coverage, existence of formal deﬁnitions, and implementation (see [3] and [4] for details).1 This paper shows how top expressions can be translated automatically into a simpler logic-like representation language, called bot. bot is very close to traditional ﬁrst-order predicate logic (fopl). Hence, existing methods to manipulate fopl expressions can be ex- ploited, to interface to time-sensitive applications other than tsql2 databases (e.g. hybrids 1A prototype implementation of this framework is freely available from http:www.dai.ed.ac.uk groupsnlp. of standard sql and Prolog databases [5] [11] [15], or planners [9]), maintaining the existing English-to-top linguistic front-end. The mapping from top to bot is also expected to make the linguistic front-end easier to interface to forthcoming new temporal sql versions [25], as bot is much simpler than top, and hence establishing a mapping from bot to a new database language is easier than from top. This paper focuses on top, bot, and the mapping from top to bot. Information about other aspects of the work mentioned above, including the English-to-top mapping, can be found in [3] and [4]. The remainder of this paper is organised as follows: Section 2 introduces top, Section 3 presents bot, Section 4 describes the top-to-bot mapping, and Section 5 concludes. Formal deﬁnitions of top and bot can be found in Appendices A and B respectively. Appendix C provides a full list of the translation rules that are used in the top-to-bot mapping. 2 The TOP language This section introduces informally top, the logic-like language English questions are initially translated into. A formal deﬁnition of the syntax and semantics of top can be found in Appendix A. top was designed to support the systematic representation of English time- related semantics, rather than inferencing (contrary to the logics of e.g. [1], [14], [16], [17]). Hence, although in many ways similar to traditional temporal logics, top is not a full logic, as it provides no proof theory. top atomic formulae are constructed by applying predicate symbols to constants and variables. More complex formulae are constructed using conjunctions and temporal oper- ators (top is an acronym for Temporal OPerators). For example, (1) is represented by the top formula (2). The v suﬃx marks variables, and free variables (e.g. the ev in (2)) are treated as quantiﬁed by an implicit existential quantiﬁer with scope over the entire formula. top currently provides no disjunction, negation, or explicit quantiﬁcation mecha- nisms, as these were not needed for the linguistic phenomena that the work being reported here focused on. Such mechanisms can be added easily in future top versions. (1) Was tank 5 empty on 1198? (2) At[1198, Past[ev, empty(tank5)]] Roughly speaking, the verb tense in (1) introduces a Past operator, which requires empty(tank5) to have been empty at some past time ev, and the at 1198 adverbial introduces an At operator, which requires that past time to fall within 1198. (Unlike Priorean operators [22], tops Past and At operators do not shift the time where their argument is expected to hold. They simply accumulate restrictions on what this time can be. This is explained further below.) Assuming that 1198 falls in the past, the Past operator of (2) is actually redundant, since any time that falls within 1198 will also belong to the past. It is important to realise, however, that the mapping from English to top is carried out automatically. This mapping introduces a Past operator when encountering the past tense, to ensure that a sentence like (3), where no adverbial is present, is represented correctly (as in (4)). (3) Tank 5 was empty. (4) Past[ev, empty(tank5)]] The combination of At and Past operators in (2) also accounts for the oddity of (1) when uttered before 1198. The oddity of (1) can be attributed to the fact that in this case the At and Past operators introduce incompatible restrictions (no past time can fall within 1198 if the question is uttered before 1198). Temporal operators are used in top (much as in [9]) to introduce compact chunks of semantics, in a manner that makes it easier to track the semantic contribution of each linguistic mechanism. No claim is made that top is more expressive than (or even as expressive as) other temporal representation formalisms (e.g. [22]), though it should be noted that top is part of a complete path from English to an application formalism (tsql2), which is not available with most other temporal representation formalisms. Time in top is linear, discrete and bounded [12] [28]. Following Reichenbach [23], formulae are evaluated with respect to three times: speech time (st), event time (et), and localisation time (lt). Intuitively, st is the time when the question is submitted, et is the time when the situation of the formula holds, and lt is a temporal window that contains et. (tops lt is diﬀerent from Reichenbachs reference time, and closer to the location time of [13].) st is a time-point, while et and lt are generally periods. Period is used here to refer to what logicians usually call intervals, i.e. convex sets of time-points. In (1), the answer will be aﬃrmative if (2) evaluates to true. When evaluating a formula, lt initially covers the entire time, but it can be narrowed down by temporal operators. In (2), the At and Past operators narrow lt to its intersection with 1198 and [tfirst, st) respectively, where tfirst is the beginning of time. Assuming that 1198 lies entirely in the past, the resulting lt is 1198. The formula evaluates to true iﬀ there is an et where empty(tank5) is true, and et  lt. (p1 is a subperiod of p2, written p1  p2 iﬀ p1, p2 are periods and p1  p2.) The semantics of top guarantee that top predicates are always homogeneous, meaning that if a predicate is true at some et, it will also be true at any other et  et. (A similar notion of homogeneity is used in [2].) In (2), if tank 5 was empty from 301297 to 10198 (dates are shown in the ddmmyy format), empty(tank5) will be true at any et that is a subperiod of that period. Hence, there will be an et that is a subperiod of 1198 (the lt) where empty(tank5) holds, and (2) will evaluate to true. The reading of (1) that requires the tank to have been empty throughout 1198, which is easier to grasp in the aﬃrmative (5), is expressed as (6). The Fills operator requires et to cover the entire lt. (5) Tank 5 was empty on 1198. (6) At[1198, Past[ev, Fills[empty(tank5)]]] The remainder of this section illustrates the use of some of tops temporal operators, narrowing the discussion to the representation of yesno single-clause questions. To save space, some of tops mechanisms, including those that are used to represent wh-questions (e.g. Which tanks were empty on 1198?) and multiple clauses (e.g. Which ﬂights were circling while BA737 was landing?), are not covered (see [3] and [4] for the full details). With verbs that refer to situations with inherent climaxes [18] [29], non-progressive tenses introduce an additional Culm operator, which requires et to be the period from the point where the situation ﬁrst started to the point where the situation last stopped, and the situation to reach its climax at the end of et. For example, (7) and (9) are mapped to (8) and (10) respectively. (10) requires the building to have been completed, and the entire building to have taken place within 1997. In contrast, (8) simply requires part of the bulding to have been ongoing in 1997. (7) Was HouseCorp building bridge 2 in 1997? (8) At[1997, Past[ev, buiding(housecorp, bridge2)]] (9) Did HouseCorp build bridge 2 in 1997? (10) At[1997, Past[ev, Culm[building(housecorp, bridge2)]]] Questions referring to present situations are represented using the Pres operator, which simply requires st to fall within et. (11), for example, is represented as (12). (11) Is tank 5 empty? (12) Pres[empty(tank5)] The Perf operator is used to express the perfective aspect of questions like (13). The Perf operator introduces a new event time (denoted by e2v in (14)) that must precede the original one (e1v). In (14), the inspection time (e2v) must precede another past time (e1v). The latter corresponds to Reichenbachs reference time [23], a time that serves as a view-point. (13) Had J. Adams inspected BA737? (14) Past[e1v, Perf [e2v, Culm[inspecting(jadams, ba737)]]] In (15), the on 1195 may refer to either the inspection time or the reference time. The two readings are represented by (16) and (17) respectively (the English to top mapping generates both). (15) Had J. Adams inspected BA737 on 1195? (16) Past[e1v, Perf [e2v, At[1195, Culm[inspecting(jadams, ba737)]]]] (17) At[1195, Past[e1v, Perf [e2v, Culm[inspecting(jadams, ba737)]]]] The Ntense operator (borrowed from [9]) is useful in questions like (18), where the president may refer to either the present or the 1995 president. The two readings are captured by (19) and (20) respectively. In (20), the ev arguments of the Past and Ntense operators are used to ensure that both president(pv) and visiting(pv, athens) hold at the same time.2 (18) Did the president visit Athens in 1995? (19) Ntense[now, president(pv)]  At[1995, Past[ev, visiting(pv, athens)]] (20) Ntense[ev, president(pv)]  At[1995, Past[ev, visiting(pv, athens)]] The reading of (21) that asks if tank 5 was empty at some time after 5:00 pm is rep- resented as (22). The Part operator forces f v to range over 5:00pm-times, and the After operator requires the past et where tank 5 is empty to follow f v. (21) Was tank 5 empty after 5:00pm? 2The ev arguments of the Past and Perf operators are also used in time-asking questions. Consult [3] and [4] for related discussion. (22) Part[5:00pm, f v]  After[f v, Past[ev, empty(tank5)]] In practice, (21) would be uttered in a context where previous discourse has established a temporal window that contains a single 5:00pm-time, and at 5:00pm would refer to that time. This anaphoric use of at 5:00pm can be captured by setting the initial value of lt to the discourse-deﬁned window, rather than the entire time-axis. Finally, durations can be speciﬁed with the For operator. (23) is mapped to (24), which requires 45 consecutive minute-periods to exist, and the ﬂight to have been circling throughout the concatenation of these periods. (23) Was BA737 circling for 45 minutes? (24) For[minute, 45, Past[ev, circling(ba737)]] 3 The BOT language Let us now turn to bot, the simpler formal language top expressions are subsequently translated into. bot is essentially the traditional ﬁrst-order predicate logic (fopl), with some special terms and predicates to refer to time-points and periods. As in top, bot assumes that time is discrete, linear, and bounded. For simplicity, the same constant and predicate symbols are used as in the corresponding top expressions. It is assumed, however, that bot predicates that correspond to top predicates have an additional argument, whose denotations range over the maximal event- time periods where the corresponding top predicates hold. For example, (1) could be represented in bot as (25) (cf. (2)). (25) empty(tank5, pv)  subper(ev, pv)  subper(ev, intersect(intersect([beg, end], 1198), [beg, now))) (25) requires pv to denote a maximal period where the tank was empty, and ev to be a subperiod of both pv and the intersection of 1198 with the past. ev corresponds to the event time of (2), and intersect(intersect([beg, end], 1198), [beg, now)) emulates tops localisation time, initially the entire time-axis, which has been narrowed to cover past points within 1198. (beg, end, and now denote the beginning of time, end of time, and speech time respectively, intersect denotes set intersection, and square and round brackets are used to specify the boundaries of periods in the usual manner.) As in top, free variables are treated as existentially quantiﬁed. A special bot predicate symbol part, similar to tops Part operator, allows variables to range over families of periods. In (27), for example, m1v and m2v range over minute- periods. earliest and latest are used to refer to the earliest and latest time-points of a period, succ steps forward one-time point, and eq requires the denotations of its arguments to be identical. (27) requires a 2-minute long ev period to exist, and ev to fall within the past and be a subperiod of a maximal period pv where tank 5 was empty. As in (25), intersect pred- icates are used to emulate tops localisation time (here, intersect([beg, end], [beg, now))). (27) represents (26). (26) Was tank 5 empty for two minutes? (27) part(minute, m1v)  part(minute, m2v)  eq(earliest(m1v), earliest(ev))  eq(succ(latest(m1v)), earliest(m2v))  eq(latest(m2v), latest(ev))  empty(tank5, pv)  subper(ev, pv)  subper(ev, intersect([beg, end], [beg, now))) The semantics of bot is much simpler than top, though bot formulae tend to be much longer, and hence diﬃcult to grasp, than the corresponding top ones. The syntax and semantics of bot are deﬁned formally in Appendix B. 4 Translating from TOP to BOT top formulae are translated systematically into bot using a set of rewrite rules of the form: trans(φ1, ε, λ)  φ2 where φ1 is a top formula, φ2 is a bot formula (possibly containing recursive invocations of other translation rules), and ε, λ are bot expressions representing tops event and lo- cation times respectively. There are base (non-recursive) translation rules for atomic top formulae, and recursive translation rules for conjunctions and each one of tops operators. For example, the translation rule for tops At operator is (28). (28) trans(At[τ, φ], ε, λ)  period(τ)  trans(φ, ε, intersect(λ, τ)) The translation rules essentially express in terms of bot constructs the semantics of the corresponding top constructs. (28), for example, narrows the localisation time to the intersection of its original value with the denotation of τ, which must be a period, mirroring the semantics of tops At operator (see Appendix A). φ is then translated into bot using the new value of the localisation time. When translating from top to bot, λ is initially set to [beg, end], which corresponds to the initial value of tops localisation time. ε is set to a new variable, a variable that has not been used in any other expression. This reﬂects the fact that tops event time is initially allowed to be any period (see the deﬁnition of denotation w.r.t. M, st in Appendix A). For example, to compute the bot translation of (1), one would invoke (28) as in (29), where etv is a new variable that stands for the event time. (29) trans(At[1198, Past[ev, empty(tank5)]], etv, [beg, end])  period(1198)  trans(Past[ev, empty(tank5)], etv, intersect([beg, end], 1198) The translation rules for tops Past operator and predicates are shown in (30) and (31) respectively. The rule for Past narrows the localisation time to the past, and requires β to point to the event time. (The β argument of tops Past operator is useful in time-asking questions, which are not covered in this paper.) The rule for predicates requires the event time to be a subperiod of both the localisation time and of a maximal period where the predicate holds (here β is a new variable). These are, again, in accordance with tops semantics. The reader is reminded that bot predicates that correspond to predicates in the top formula have an additional argument (β in (31)), which ranges over the maximal event-time periods where the corresponding top predicate holds. (30) trans(Past[β, φ], ε, λ)  eq(β, ε)  trans(φ, ε, intersect(λ, [beg, now))) (31) trans(π(τ1, . . . , τn), ε, λ)  subper(ε, λ)  π(τ1, . . . , τn, β)  subper(ε, β) Using (30) and (31), the right-hand side of (29) becomes (32). The right-hand side of (32) is the ﬁnal result of the translation, which is essentially the same as the hand-crafted (25). (The additional etv variable and period predicate, do not contribute signiﬁcantly in this case, but they are needed to prove the correctness of the automatic translation.) (32) period(1198)  eq(ev, etv)  trans(empty(tank5), etv, intersect(intersect([beg, end], 1198), [beg, now)))  period(1198)  eq(ev, etv)  subper(etv, intersect(intersect([beg, end], 1198), [beg, now)))  empty(tank5, pv)  subper(etv, pv) As explained in section 2, tops Culm operator requires the event time to be the period from the point where the situation described by Culms argument ﬁrst started to the point where it last stopped, and the situation to reach its climax at the end of the event time. To be able to translate top formulae containing Culm operators, we assume two map- pings η1 and η2 from predicate functors to new (unused elsewhere) predicate functors. If π(τ1, . . . , τn) is a predicate in a top formula, η1(π)(τ1, . . . , τn) is a bot predicate intended to be true if the situation of π(τ1, . . . , τn) reached its climax at the point where it last stopped. η2(π)(τ1, . . . , τn, ε) is another bot predicate, where ε denotes the period from the ﬁrst to the last point where the situation of π(τ1, . . . , τn) was ongoing. The translation rule for Culm is (33). (33) trans(Culm[π(τ1, . . . , τn)], ε, λ)  subper(ε, λ)  η1(π)(τ1, . . . , τn)  η2(π)(τ1, . . . , τn, ε) Using (28), (30), (33), and assuming η1(building)  cmp building and η2(building)  max building, the top formula of (10) (which represents (9)) is translated into bot as in (34). (34) trans(At[1997, Past[ev, Culm[building(housecorp, bridge2)]]], etv, [beg, end])  period(1997)  trans(Past[ev, Culm[building(housecorp, bridge2)]], etv, intersect([beg, end], 1997))  period(1997)  eq(ev, etv)  trans(Culm[building(housecorp, bridge2)], etv, intersect(intersect([beg, end], 1997), [beg, now)))  period(1997)  eq(ev, etv)  subper(etv, intersect(intersect([beg, end], 1997), [beg, now)))  cmp building(housecorp, bridge2)  max building(housecorp, bridge2) Like (10), (34) requires the period from the point where the building ﬁrst started to the point where it last stopped to fall completely within the past and within 1997. Furthermore, the building must have reached its completion at the end of that period. A complete list of the top to bot translation rules can be found in Appendix C. Al- though beyond the scope of this paper, a formal proof of the correctness of the top to bot translation rules could be produced by showing that the denotations of the source top expressions are identical to the denotations of the resulting bot expressions by induction on the syntactic complexity of the top expressions. The same strategy was used in [3] to prove formally the correctness of the top-to-tsql2 translation rules (see [4] for a summary of the proof). 5 Conclusions This paper has shown how an existing mapping from English to a complex temporal mean- ing representation formalism (top) can be coupled with a mapping from that formalism to a simpler one (bot). The simpler formalism is very close to traditional ﬁrst-order predicate logic, making it possible to exploit existing techniques to interface to time-sensitive appli- cations other than tsql2 databases, while maintaining the existing linguistic front-end. Acknowledgements Part of the work reported here was carried out at the Division of Informatics of the Univer- sity of Edinburgh, with support from the Greek State Scholarships foundation, under the supervision of Graeme Ritchie and Peter Thanisch. References [1] J.F. Allen. Maintaining Knowledge about Temporal Intervals. Communications of the ACM, 26(11):832843, 1983. [2] J.F. Allen. Towards a General Theory of Action and Time. Artiﬁcial Intelligence, 23:123154, 1984. [3] I. Androutsopoulos. A Principled Framework for Constructing Natural Language In- terfaces to Temporal Databases. PhD thesis, Department of Artiﬁcial Intelligence, University of Edinburgh, 1996. [4] I. Androutsopoulos, G.D. Ritchie, and P. Thanisch. Time, Tense and Aspect in Natural Language Database Interfaces. Natural Language Engineering, 4(3):229276, 1998. [5] S. Ceri, G. Gottlob, and G. Wiederhold. Eﬃcient Database Access from Prolog. IEEE Transactions on Software Engineering, 15(2):153163, 1989. [6] J. Cliﬀord. Formal Semantics and Pragmatics for Natural Language Querying. Cam- bridge Tracts in Theoretical Computer Science, Cambridge University Press, 1990. [7] B. Comrie. Aspect. Cambridge University Press, 1976. [8] B. Comrie. Tense. Cambridge University Press, 1985. [9] R.S. Crouch and S.G. Pulman. Time and Modality in a Natural Language Interface to a Planning System. Artiﬁcial Intelligence, 63:265304, 1993. [10] S. De, S. Pan, and A.B. Whinston. Temporal Semantics and Natural Language Pro- cessing in a Decision Support System. Information Systems, 12(1):2947, 1987. [11] C. Draxler. Accessing Relational and Higher Databases through Database Set Predicates in Logic Programming Languages. PhD thesis, University of Zurich, 1992. [12] D.M. Gabbay, I. Hodkinson, and M. Reynolds. Temporal Logic: Mathematical Foun- dations and Computational Aspects. Oxford University Press, 1994. [13] H. Kamp and U. Reyle. From Discourse to Logic. Kluer Academic Publishers, 1993. [14] R. Kowalski and M. Sergot. A Logic-based Calculus of Events. New Generation Computing, 4:6795, 1986. [15] R. Lucas. Database Applications Using Prolog. Halsted Press, 1988. [16] J. McCarthy and P.J. Hayes. Some Philosophical Problems from the Standpoint of Artiﬁcial Intelligence. In Machine Intelligence 4, pages 463502. Edinburgh University Press, 1969. [17] D. McDermott. A Temporal Logic for Reasoning about Processes and Plans. Cognitive Science, 6:101155, 1982. [18] M. Moens and M. Steedman. Temporal Ontology and Temporal Reference. Computa- tional Linguistics, 14(2):1528, 1988. [19] T. Parsons. Events in the Semantics of English: A Study in Subatomic Semantics. MIT Press, 1990. [20] C. Pollard and I.A. Sag. Information-Based Syntax and Semantics  Volume 1, Fun- damentals. Center for the Study of Language and Information, Stanford, 1987. [21] C. Pollard and I.A. Sag. Head-Driven Phrase Structure Grammar. University of Chicago Press and Center for the Study of Language and Information, Stanford., 1994. [22] A. Prior. Past, Present and Future. Oxford University Press, 1967. [23] H. Reichenbach. Elements of Symbolic Logic. Collier-Macmillan, London, 1947. [24] R.T. Snodgrass, editor. The TSQL2 Temporal Query Language. Kluwer Academic Publishers, 1995. [25] R.T. Snodgrass, M. Boehlen, C.S. Jensen, and A. Steiner. Transitioning Temporal Support in TSQL2 to SQL3. In O. Etzion and S. Sripada, editors, Temporal Databases: Research and Practice, pages 150  194. Springer-Verlag, Berlin, 1998. [26] A. Tansel, J. Cliﬀord, S.K. Gadia, S. Jajodia, A. Segev, and R.T. Snodgrass. Temporal Databases  Theory, Design, and Implementation. BenjaminCummings, California, 1993. [27] V.J. Tsotras and A. Kumar. Temporal Database Bibliography Update. ACM SIGMOD Record, 25(1), 1996. [28] J.F.A.K. van Benthem. The Logic of Time. D. Reidel Publishing Company, Dordrecht, Holland, 1983. [29] Z. Vendler. Verbs and Times. In Linguistics in Philosophy, chapter 4, pages 97121. Cornell University Press, Ithaca, NY, 1967. Appendix A Deﬁnition of the TOP language This section deﬁnes the syntax and semantics of the subset of top that was introduced in this paper. See [3] and [4] for a full deﬁnition of top. A.1 Syntax of TOP The syntax of top is deﬁned below using bnf. Angle brackets are used to group bnf elements.  denotes zero or more repetitions.  denotes one or more repetitions. Terminal symbols are in lower case, possibly with an initial capital. Non-terminals are in upper case. The distinguished symbol is YNFORMS. YNFORMS  AFORMS  YNFORMS  YNFORMS  Pres[YNFORMS]  Past[VARS, YNFORMS]  Perf [VARS, YNFORMS]  Culm[LITERAL]  At[TERMS, YNFORMS]  Before[TERMS, YNFORMS]  After[TERMS, YNFORMS]  Ntense[VARS, YNFORMS]  Ntense[now, YNFORMS]  For[CPARTS, VQTY , YNFORMS]  Fills[YNFORMS] AFORMS  LITERAL  Part[PARTS, VARS] LITERAL  PFUNS({TERMS, }TERMS) TERMS  CONS  VARS PARTS  CPARTS  GPARTS VQTY  1  2  3  . . . PFUNS, CPARTS, GPARTS, CONS, and VARS are disjoint open classes of terminal symbols. A.2 Semantics of TOP Temporal ontology A point structure PTS,  is assumed, where PTS is the set of time-points, and  is a binary, transitive, irreﬂexive relation over PTS  PTS. Time is assumed to be discrete, bounded, and linear [12] [28]. tfirst and tlast are the earliest and latest time-points respec- tively. prev(t) and next(t) are used to refer to the immediately previous and following time-points of a t  PTS. For S  PTS, minpt(S) and maxpt(S) denote the earliest and latest time-points in S. A period p over PTS,  is a non-empty subset of PTS. Periods are convex, i.e. if t1, t2  p, t3  PTS, and t1  t3  t2, then t3  p. PERIODS is the set of all periods over PTS, . p1 is a subperiod of p2 (written p1  p2), iﬀ p1, p2  PERIODS and p1  p2. p1 is a proper subperiod of p2 (written p1  p2), iﬀ p1, p2  PERIODS and p1  p2. The usual notational conventions apply when specifying the boundaries of periods; e.g. (t1, t2] is an abbreviation for {t  PTS  t1  t  t2}. If S is a set of periods, then mxlpers(S) is the set of maximal periods of S. mxlpers(S)  {p  S  for no p  S is it true that p  p}. TOP model A top model M is an ordered 7-tuple: M  PTS, , OBJS, fcons, fpfuns, fculms, fgparts, fcparts where PTS,  is the point structure, PERIODS  OBJS, and fcons, fpfuns, fculms, fgparts, and fcparts are as speciﬁed below: OBJS is a set containing all the objects in the modelled world that can be denoted by top terms, and fcons is a function CONS  OBJS. Intuitively, fcons maps each constant to the object it denotes. fpfuns maps each π  PFUNS to a function (OBJS)n  pow(PERIODS). It is assumed that each predicate symbol π  PFUNS is used with a particular arity (number of argu- ments) n. pow(S) denotes the powerset (set of all subsets) of S. For every π  PFUNS and every o1, o2, . . . , on  (OBJS)n, it must be true that: if p1, p2  fpfuns(π)(o1, o2, . . . , on) and p1  p2  PERIODS, then p1  p2 Intuitively, fpfuns shows the maximal periods where the situation represented by π(τ1, . . . , τn) holds. fculms is a function that maps each π  PFUNS to a function (OBJS)n  {T, F}. Intuitively, fculms shows whether or not a situation reaches a climax at the latest time-point where it is ongoing. fgparts is a function that maps each element of GPARTS to a gappy partitioning. A gappy partitioning is a subset S of PERIODS, such that for every p1, p2  S, p1  p2  , and  pS p  PTS. fcparts is a function that maps each element of CPARTS to a complete partitioning. A complete partitioning is a subset S of PERIODS, such that for every p1, p2  S, p1  p2  , and  pS p  PTS. Variable assignment A variable assignment w.r.t. a top model M is a function g : VARS  OBJS. GM, or simply G, is the set of all possible variable assignments w.r.t. M. TOP denotation w.r.t. M, st, et, lt, g Non-terminal symbols of the top bnf are used here as names of sets that contain expressions which can be analysed syntactically as the corresponding non-terminals. An index of evaluation is an ordered 3-tuple st, et, lt, such that st  PTS, et  PERIODS, and lt  PERIODS  {}. The denotation of a top expression ξ w.r.t. a model M, an index of evaluation st, et, lt, and a variable assignment g, is written ξM,st,et,lt,g or simply ξst,et,lt,g. When the deno- tation of ξ does not depend on st, et, and lt, we may write ξM,g or simply ξg.  If κ  CONS, then κg  fcons(κ).  If β  VARS, then βg  g(β).  If φ  YNFORMS, then φst,et,lt,g  {T, F}.  If π(τ1, τ2, . . . , τn)  LITERAL, then π(τ1, τ2, . . . , τn)st,et,lt,g  T iﬀ et  lt and for some pmxl  fpfuns(π)(τ1g, τ2g, . . . , τng), et  pmxl.  If φ1, φ2  YNFORMS, then φ1  φ2st,et,lt,g  T iﬀ φ1st,et,lt,g  φ2st,et,lt,g  T.  Part[σ, β]g  T iﬀ g(β)  f(σ) (where f  fcparts if σ  CPARTS, and f  fgparts if σ  GPARTS).  Pres[φ]st,et,lt,g  T, iﬀ st  et and φst,et,lt,g  T.  Past[β, φ]st,et,lt,g  T, iﬀ g(β)  et and φst,et,lt[tfirst,st),g  T.  Culm[π(τ1, . . . , τn)]st,et,lt,g  T, iﬀ et  lt, fculms(π)(τ1g, . . . , τng)  T, S  , and et  [minpt(S), maxpt(S)], where: S   pfpfuns(π)(τ1g,...,τng) p  At[τ, φ]st,et,lt,g  T, iﬀ τg  PERIODS and φst,et,ltτg,g  T.  Before[τ, φ]st,et,lt,g  T, iﬀ τg  PERIODS and φst,et,lt[tfirst,minpt(τg)),g  T.  After[τ, φ]st,et,lt,g  T, iﬀ τg  PERIODS and φst,et,lt(maxpt(τg),tlast],g  T.  Fills[φ]st,et,lt,g  T, iﬀ et  lt and φst,et,lt,g  T.  Ntense[β, φ]st,et,lt,g  T, iﬀ for some et  PERIODS, g(β)  et and φst,et,PTS,g  T.  Ntense[now, φ]st,et,lt,g  T, iﬀ φst,{st},PTS,g  T.  For[σc, νqty, φ]st,et,lt,g  T, iﬀ φst,et,lt,g  T, and for some p1, p2, . . . , pνqty  fcparts(σc), it is true that minpt(p1)  minpt(et), next(maxpt(p1))  minpt(p2), next(maxpt(p2))  minpt(p3), . . . , next(maxpt(pνqty1))  minpt(pνqty), and maxpt(pνqty)  maxpt(et).  Perf [β, φ]st,et,lt,g  T, iﬀ et  lt, and for some et  PERIODS, it is true that g(β)  et, maxpt(et)  minpt(et), and φst,et,PTS,g  T. TOP denotation w.r.t. M, st The denotation of φ w.r.t. M, st, written φM,st or simply φst, is deﬁned only for φ  YNFORMS:  If φ  YNFORMS, then φst   T, if for some g  G and et  PERIODS, φst,et,PTS,g  T,  F, otherwise B Deﬁnition of the BOT language B.1 Syntax of BOT The syntax of bot is deﬁned using bnf, with the same conventions as in the deﬁnition of top. The distinguished symbol is YNFORMSB. YNFORMSB  AFORMSB  YNFORMSB  YNFORMSB AFORMS B  LITERALB  subper(PEREX, PEREX )  eq(TERMS B, TERMS B)  period(TERMS B)  part(PARTS, TERMS B) LITERALB  PFUNS({TERMS B, }TERMS B) TERMS B  CONS  VARS  PEREX  PTEX PARTS  CPARTS  GPARTS PTEX  beg  now  end  earliest(PEREX )  latest(PEREX )  succ(PTEX )  prec(PTEX , PTEX) PEREX  [PTEX, PTEX ]  [PTEX , PTEX )  (PTEX , PTEX ]  (PTEX, PTEX )  intersect(PEREX, PEREX ) PFUNS, CPARTS, GPARTS, CONS, and VARS are disjoint open classes of terminal symbols that do not contain any of the other bot terminal symbols. The same symbols for PFUNS, PARTS, CPARTS, GPARTS, CONS, VARS are used as in the deﬁnition of top, because these classes are the same in both languages. B.2 Semantics of BOT bot assumes the same temporal ontology as top. BOT model A bot model M is an ordered 6-tuple: MB  PTS, , OBJS, fcons, f B pfuns, fgparts, fcparts where PTS,  is the point structure, and OBJS, fcons, fgparts, and fcparts is are as in top. f B pfuns is as a function that maps every π  PFUNS to a function OBJS n  {T, F}, where n is the arity of π. Variable assignment A variable assignment for bot is a function g : VARS  OBJS, as in top. G has the same meaning as in top. BOT denotation w.r.t. M, st, g The denotation of a bot expression ξ w.r.t. a bot model MB, a speech time st  PTS, and a g  G, written ξMB,st,g or simply ξst,g, is deﬁned as follows:  If κ  CONS, then κst,g  fcons(κ).  begst,g  tfirst, nowst,g  st, endst,g  tlast.  earliest(τ)st,g  minpt(τst,g). latest(τ)st,g  maxpt(τst,g).  [ξ1, ξ2)st,g  {t  PTS  ξ1st,g  t  ξ2st,g}. The denotations of [ξ1, ξ2], (ξ1, ξ2], and (ξ1, ξ2) are deﬁned similarly.  intersect(ξ1, ξ2)st,g  ξ1st,g  ξ2st,g.  succ(ξ)st,g  next(ξst,g).  prec(ξ1, ξ2)st,g is T if ξ1st,g  ξ2st,g and F otherwise.  If β  VARS, then βst,g  g(β).  If φ  YNFORMSB, then φst,g  {T, F}.  If π(τ1, τ2, . . . , τn)  LITERALB, then π(τ1, τ2, . . . , τn)st,g  f B pfuns(π)(τ1st,g, τ2st,g, . . . , τnst,g).  If φ1, φ2  YNFORMSB, then φ1  φ2st,g  T iﬀ φ1st,g  T and φ2st,g  T.  part(σ, β)st,g  T iﬀ g(β)  f(σ) (where f  fcparts if σ  CPARTS, and f  fgparts if σ  GPARTS).  eq(τ1, τ2)st,g  T iﬀ τ1st,g  τ2st,g.  subper(ξ1, ξ2)st,g  T iﬀ ξ1st,g  ξ2st,g.  period(τ)st,g  T iﬀ τst,g  PERIODS. BOT denotation w.r.t. M, st The denotation of φ w.r.t. MB, st, written φMB,st or simply φst, is deﬁned only for φ  YNFORMSB:  If φ  YNFORMSB, then φst   T, if for some g  G, φst,g  T,  F, otherwise C TOP to BOT translation rules  If π  PFUNS and τ1, . . . , τn  TERMS, then: trans(π(τ1, . . . , τn), ε, λ)  subper(ε, λ)  π(τ1, . . . , τn, β)  subper(ε, β), where β is a new variable.  trans(φ1  φ2, ε, λ)  trans(φ1, ε, λ)  trans(φ2, ε, λ).  trans(Part[σ, β], ε, λ)  part(σ, β).  trans(Pres[φ], ε, λ)  subper([now, now], ε)  trans(φ, ε, λ).  trans(Past[β, φ], ε, λ)  eq(β, ε)  trans(φ, ε, intersect(λ, [beg, now))).  trans(Culm[π(τ1, . . . , τn)], ε, λ)  subper(ε, λ)η1(π)(τ1, . . . , τn)η2(π)(τ1, . . . , τn, ε), where η1, η2 are as in section 4.  trans(At[τ, φ], ε, λ)  period(τ)  trans(φ, ε, intersect(λ, τ)).  trans(Before[τ, φ], ε, λ)  period(τ)  trans(φ, ε, intersect(λ, [beg, minpt(τ)))).  trans(After[τ, φ], ε, λ)  period(τ)  trans(φ, ε, intersect(λ, (maxpt(τ), end])).  trans(Fills[φ], ε, λ)  eq(ε, λ)  trans(φ, ε, λ).  trans(Ntense[β, φ], ε, λ)  period(β)  trans(φ, β, [beg, end]).  trans(Ntense[now, φ], ε, λ)  trans(φ, [now, now], [beg, end]).  trans(For[σc, νqty, φ], ε, λ)  part(σc, β1)  part(σc, β2)      part(σc, βνqty) eq(earliest(β1), earliest(ε))  eq(succ(latest(β1)), earliest(β2))  eq(succ(latest(β2)), earliest(β3))      eq(succ(latest(βνqty1)), earliest(βνqty)  eq(latest(βνqty), latest(ε))  trans(φ, ε, λ), where β1, β2, . . . , βνqty are new variables.  trans(Perf [β, φ], ε, λ)  subper(ε, λ)  period(β)  prec(latest(β), earliest(ε))  trans(φ, β, [beg, end]).",
  "38.pdf": "arXiv:cs9906025v1 [cs.CL] 24 Jun 1999 Mapping Multilingual Hierarchies Using Relaxation Labeling J. Daude, L. Padro  G. Rigau TALP Research Center Departament de Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya. Barcelona {daude,padro,g.rigau}lsi.upc.es Abstract This paper explores the automatic construction of a multilingual Lexical Knowledge Base from pre-existing lexical resources. We present a new and robust approach for linking already existing lexicalsemantic hierarchies. We used a con- straint satisfaction algorithm (relaxation label- ing) to select among all the candidate trans- lations proposed by a bilingual dictionary the right English WordNet synset for each sense in a taxonomy automatically derived from a Span- ish monolingual dictionary. Although on aver- age, there are 15 possible WordNet connections for each sense in the taxonomy, the method achieves an accuracy over 80. Finally, we also propose several ways in which this technique could be applied to enrich and improve exist- ing lexical databases. 1 Introduction There is an increasing need of having available general, accurate and broad coverage multilin- gual lexicalsemantic resources for developing nl applications. Thus, a very active ﬁeld in- side nl during the last years has been the fast development of generic language resources. Several attempts have been performed to pro- duce multilingual ontologies. In (Ageno et al., 1994), a SpanishEnglish bilingual dictionary is used to (semi)automatically link Spanish and English taxonomies extracted from dgile (Al- var, 1987) and ldoce (Procter, 1987). Sim- ilarly, a simple automatic approach for link- ing Spanish taxonomies extracted from dgile to WordNet (Miller et al., 1991) synsets is proposed in (Rigau et al., 1995). The work reported in (Knight and Luk, 1994) focuses on the construction of Sensus, a large knowl- edge base for supporting the Pangloss machine translation system. In (Okumura and Hovy, 1994) (semi)automatic methods for associating a Japanese lexicon to an English ontology us- ing a bilingual dictionary are described. Sev- eral experiments aligning edr and WordNet on- tologies are described in (Utiyama and Hasida, 1997). Several lexical resources and techniques are combined in (Atserias et al., 1997) to map Spanish words from a bilingual dictionary to WordNet, and in (Farreres et al., 1998) the use of the taxonomic structure derived from a monolingual mrd is proposed as an aid to this mapping process. This paper presents a novel approach for merging already existing hierarchies. The method has been applied to attach substan- tial fragments of the Spanish taxonomy derived from dgile (Rigau et al., 1998) to the English WordNet using a bilingual dictionary for con- necting both hierarchies. This paper is organized as follows: In section 2 we describe the used technique (the relaxation labeling algorithm) and its application to hier- archy mapping. In section 3 we describe the constraints used in the relaxation process, and ﬁnally, after presenting some experiments and preliminary results, we oﬀer some conclusions and outline further lines of research. 2 Application of Relaxation Labeling to NLP Relaxation labeling (RL) is a generic name for a family of iterative algorithms which perform function optimization, based on local informa- tion. See (Torras, 1989) for a summary. Its most remarkable feature is that it can deal with any kind of constraints, thus, the model can be improved by adding any constraints available, and the algorithm is independent of the com- plexity of the model. That is, we can use more sophisticated constraints without changing the algorithm. The algorithm has been applied to pos tag- ging (Marquez and Padro, 1997), shallow pars- ing (Voutilainen and Padro, 1997) and to word sense disambiguation (Padro, 1998). Although other function optimization algo- rithms could have been used (e.g. genetic algo- rithms, simmulated annealing, etc.), we found RL to be suitable to our purposes, given its abil- ity to use models based on context constraints, and the existence of previous work on applying it to nlp tasks. Detailed explanation of the algorithm can be found in (Torras, 1989), while its application to NLP tasks, advantages and drawbacks are addressed in (Padro, 1998). 2.1 Algorithm Description The Relaxation Labeling algorithm deals with a set of variables (which may represent words, synsets, etc.), each of which may take one among several diﬀerent labels (pos tags, senses, mrd entries, etc.). There is also a set of con- straints which state compatibility or incompat- ibility of a combination of pairs variablelabel. The aim of the algorithm is to ﬁnd a weight assignment for each possible label for each vari- able, such that (a) the weights for the labels of the same variable add up to one, and (b) the weight assignation satisﬁes to the maximum possible extent the set of constraints. Summarizing, the algorithm performs con- straint satisfaction to solve a consistent labeling problem. The followed steps are: 1. Start with a random weight assignment. 2. Compute the support value for each label of each variable. Support is computed ac- cording to the constraint set and to the cur- rent weights for labels belonging to context variables. 3. Increase the weights of the labels more compatible with the context (larger sup- port) and decrease those of the less com- patible labels (smaller support). Weights are changed proportionally to the support received from the context. 4. If a stoppingconvergence criterion is sat- isﬁed, stop, otherwise go to step 2. We use the criterion of stopping when there are no more changes, although more sophisti- cated heuristic procedures may also be used to stop relaxation processes (Eklundh and Rosenfeld, 1978; Richards et al., 1981). The cost of the algorithm is proportional to the product of the number of variables by the number of constraints. 2.2 Application to taxonomy mapping As described in previous sections, the problem we are dealing with is to map two taxonomies. That is:  The starting point is a sense disam- biguated Spanish taxonomy automatically extracted from a monolingual dictionary (Rigau et al., 1998).  We have a conceptual taxonomy (e.g. WordNet (Miller et al., 1991)), in which the nodes represent concepts, organized as synsets.  We want to relate both taxonomies in order to have an assignation of each sense of the Spanish taxonomy to a WN synset. The modeling of the problem is the following:  Each sense in the Spanish taxonomy is a variable for the relaxation algorithm.  The possible labels for that variable, are all the WN synsets which contain a word that is a possible translation of the Span- ish sense. Thus, we will need a bilingual dictionary to know all the possible trans- lations for a given Spanish word. This has the eﬀect of losing the sense information we had in the Spanish taxonomy.  The algorithm will need constraints stating whether a synset is a suitable assignment for a sense. These constraints will rely on the taxonomy structure. Details are given in section 3. 3 The Constraints Constraints are used by relaxation labeling al- gorithm to increase or decrease the weight for a variable label. In our case, constraints increase the weights for the connections between a sense in the Spanish taxonomy and a WordNet synset. Increasing the weight for a connection implies decreasing the weights for all the other possi- ble connections for the same node. To increase the weight for a connection, constraints look for already connected nodes that have the same re- lationships in both taxonomies. Although there is a wide range of relation- ships between WordNet synsets which can be used to build constraints, we have focused on the hyperhyponym relationships. That is, we increase the weight for a connection when the involved nodes have hypernymshyponyms also connected. We consider hyperhyponym rela- tionships either directly or indirectly (i.e. an- cestors or descendants), depending on the kind of constraint used. Figure 1 shows an example of possible con- nections between two taxonomies. Connection C4 will have its weight increased due to C5, C6 and C1, while connections C2 and C3 will have their weights decreased. C1 C2 C3 C4 C5 C6 Figure 1: Example of connections between tax- onomies. Constraints are coded with three characters xyz, which are read as follows: The last char- acter, z, indicates whether the constraints re- quires the existence of a connected hypernym (e), hyponym (o), or both (b). The two ﬁrst characters indicate how the hyperhyponym re- lationship is considered in the Spanish taxon- omy (character x) and in WordNet (charac- ter y): (i) indicates that only immediate hy- perhyponym match, and (a) indicates that any ancestordescendant matches. Thus, we have constraints iieiio which in- crease the weight for a connection between a Spanish sense and a WordNet synset when there is a connection between their respective hyper- nymshyponyms. Constraint iib requires the si- multaneous satisfaction of iie and iio. Similarly, we have constraints iaeiao, which increase the weight for a connection between a Spanish sense and a WordNet synset when there is a connection between the immediate hyper- nymhyponym of the Spanish sense and any an- cestordescendant of the wn synset. Constraint iab requires the simultaneous satisfaction of iae and iao. Symmetrically, constraints aie, aio and aib, admit recursion on the Spanish taxon- omy, but not in WordNet. Finally, constraints aae, aao and aab, admit recursion on both sides. For instance, the following example shows a taxonomy in which the iie constraint would be enough to connect the Spanish node ra- paz to the bird of prey synset, given that there is a connection between ave (hypernym of rapaz) and animal bird (hypernym of bird of prey). animal (Tops animal,animate being,...) (person beast,brute,...) (person dunce,blockhead,...) ave (animal bird) (animal fowl,poultry,...) (artifact bird,shuttle,...) (food fowl,poultry,...) (person dame,doll,...) faisan (animal pheasant) (food pheasant) rapaz (animal bird of prey,...) (person cub,lad,...) (person chap,fellow,...) (person lass,young girl,...) Constraint iie would wrongly connect the Spanish sense faisan to the food pheasant synset, since there is a connection between its immediate hypernym (ave) and the immedi- ate hypernym food pheasant (which is food fowl,poultry,...), but the animal synsets for ave are nonimmediate ancestors of the animal synsets for pheasant. This would be rightly solved when using iae or aae constraints. More information on constraints and their ap- plication can be found in (Daude et al., 1999). 4 Experiments and Results In this section we will describe a set of experi- ments and the results obtained. A brief descrip- tion of the used resources is included to set the reader in the test environment. 4.1 Spanish Taxonomies We tested the relaxation labeling algorithm with the described constraints on a set of disambiguated Spanish taxonomies automat- ically acquired from monolingual dictionar- ies. These taxonomies were automatically as- signed to a WordNet semantic ﬁle (Rigau et al., 1997; Rigau et al., 1998). We tested the performance of the method on two dif- ferent kinds of taxonomies: those assigned to a well deﬁned and concrete semantic ﬁles (noun.animal, noun.food), and those assigned to more abstract and less structured ones (noun.cognition and noun.communication). We performed experiments directly on the taxonomies extracted by (Rigau et al., 1997), as well as on slight variations of them. Namely, we tested on the following modiﬁed taxonomies: top Add a new virtual top as an hypernym of all the top nodes of taxonomies belong- ing to the same semantic ﬁle. The virtual top is connected to the top synset of the WordNet semantic ﬁle. In this way, all the taxonomies assigned to a semantic ﬁle, are converted to a single one. no-senses The original taxonomies were built taking into account dictionary entries. Thus, the nodes are not words, but dictio- nary senses. This test consists of collaps- ing together all the sibling nodes that have the same word, regardless of the dictionary sense they came from. This is done as an attempt to minimize the noise introduced at the sense level by the taxonomy building procedure. 4.2 Bilingual dictionaries The possible connections between a node in the Spanish taxonomy and WN synsets were ex- tracted from bilingual dictionaries. Each node has as candidate connections all the synsets for all the words that are possible translations for the Spanish word, according to the bilingual dictionary. Although the Spanish taxonomy nodes are dictionary senses, bilingual dictionar- ies translate words. Thus, this step introduces noise in the form of irrelevant connections, since not all translations necessarily hold for a single dictionary sense. We used an integration of several bilingual sources available. This multisource dictionary contains 124,949 translations (between 53,830 English and 41,273 Spanish nouns). Since not all words in the taxonomy appear in our bilingual dictionaries, coverage will be par- tial. Table 1 shows the percentage of nodes in each taxonomy that appear in the dictionaries (and thus, that may be connected to WN). Among the words that appear in the bilingual dictionary, some have only one candidate con- nection i.e. are monosemous. Since selecting a connection for these cases is trivial, we will fo- cus on the polysemic nodes. Table 2 shows the percentage of polysemic nodes (over the num- ber of words with bilingual connection) in each test taxonomy. The average polysemy ratio (number of candidate connections per Spanish sense) is 15.8, ranging from 9.7 for taxonomies in noun.animal, to 20.1 for less structured do- mains such as noun.communication. 4.3 Results In the performed tests we used simultaneously all constraints with the same recursion pattern. This yields the packs: ii, ai, ia and aa, which were applied to all the taxonomies for the four test semantic ﬁles. Table 3 presents coverage ﬁgures for the dif- ferent test sets, computed as the amount of nodes for which some constraint is applied and thus their weight assignment is changed. Per- centage is given over the total amount of nodes with bilingual connections. To evaluate the precision of the algorithm, we hand checked the results for the original tax- onomies, using aa constraints. Precision re- sults can be divided in several cases, depending on the correctness of the Spanish taxonomies used as a starting point. TOK, FOK The Spanish taxonomy was well built and correctly assigned to the semantic ﬁle. TOK, FNOK The Spanish taxonomy was well built, but wrongly assigned to the semantic ﬁle. TNOK The Spanish taxonomy was wrongly built. In each case, the algorithm selects a connec- tion for each sense, we will count how many connections are rightwrong in the ﬁrst and sec- ond cases. In the third case the taxonomy was original top no-senses noun.animal 45 45 43 noun.food 55 56 52 noun.cognition 54 55 52 noun.communication 66 66 64 Table 1: Percentage of nodes with bilingual connection in each test taxonomy. original top no-senses noun.animal 77 77 75 noun.food 81 81 79 noun.cognition 74 74 72 noun.communication 87 87 86 Table 2: Percentage of nodes with more than one candidate connection. wrongly extracted and is nonsense, so the assig- nations cannot be evaluated. Note that we can distinguish rightwrong assignations in the second case because the con- nections are taken into account over the whole WN, not only on the semantic ﬁle being pro- cessed. So, the algorithm may end up correctly assigning the words of a hierarchy, even when it was assigned to the wrong semantic ﬁle. For instance, in the hierarchy piel (skin, fur, peel, pelt) marta (sable, marten, coal back) vison (mink, mink coat) all words may belong either to the semantic ﬁle noun.substance (senses related to fur, pelt) or to noun.animal (animal, animal part senses), among others. The right noun.substance synsets for each word are selected, since there was no synset for piel that was ancestor of the animal senses of marta and vison. In this case, the hierarchy was well built, and well solved by the algorithm. The only mistake was having assigned it to the noun.animal se- mantic ﬁle, so we will count it as a right choice of the relaxation labeling algorithm, but write it in a separate column. Tables 4 and 5 show the precision rates for each original taxonomy. In the former, ﬁg- ures are given over polysemic words (nodes with more than one candidate connection). In the later, ﬁgures are computed overall (nodes with at least one candidate connection). Accuracy is computed at the semantic ﬁle level, i.e., if a word is assigned a synset of the right semantic ﬁle, it is computed as right, oth- erwise, as wrong. To give an idea of the task complexity and the quality of the reported results, even with this simpliﬁed evaluation, consider the following:  Those nodes with only one possible synset for the right semantic ﬁle (30 in average, ranging from 22 in noun.communication to 45 in noun.animal) are not aﬀected by the evaluation at the semantic ﬁle level.  The remaining nodes have more than one possible synset in the right se- mantic ﬁle: 6.3 in average (ranging from 3.0 for noun.animal to 8.7 for noun.communication).  Thus, we can consider that we are eval- uating a task easier than the actual one (the actual evaluation would be performed at the synset level). This simpliﬁed task has an average polysemy of 6.7 possible choices per sense, while the actual task at the synset level would have 15.8. Although this situates the baseline of a random as- signment about 15 instead of 6, it is still a hard task. 5 Conclusions We have applied the relaxation labeling algo- rithm to assign an appropriate WN synset to each node of an automatically extracted tax- onomy. Results for two diﬀerent kinds of con- ceptual structures have been reported, and they point that this may be an accurate and robust method (not based on ad-hoc heuristics) to con- nect hierarchies (even in diﬀerent languages). WN ﬁle taxonomy ii ai ia aa original 134 (23) 135 (23) 357 (62) 365 (63) noun.animal top 138 (24) 143 (25) 375 (65) 454 (78) no-senses 118 (23) 119 (20) 311 (61) 319 (62) original 119 (36) 130 (39) 164 (49) 180 (63) noun.food top 134 (40) 158 (47) 194 (58) 259 (77) no-senses 102 (36) 111 (39) 153 (51) 156 (55) original 225 (37) 230 (38) 360 (60) 373 (62) noun.cognition top 230 (38) 240 (40) 395 (65) 509 (84) no-senses 192 (37) 197 (38) 306 (59) 318 (61) original 552 (43) 577 (45) 737 (57) 760 (59) noun.communication top 589 (46) 697 (54) 802 (62) 1136 (88) no-senses 485 (43) 509 (45) 645 (57) 668 (59) Table 3: Coverage of each constraint set for diﬀerent test sets. precision over precision over total precision number TOK, FOK TOK, FNOK over TOK of TNOK animal 279 (90) 30 (91) 309 (90) 23 food 166 (94) 3 (100) 169 (94) 2 cognition 198 (67) 27 (90) 225 (69) 49 communication 533 (77) 40 (97) 573 (78) 16 Table 4: Precision results over polysemic words for the test taxonomies. The experiments performed up to now seem to indicate that:  The relaxation labeling algorithm is a good technique to link two diﬀerent hierarchies. For each node with several possible connec- tions, the candidate that best matches the surrounding structure is selected.  The only information used by the algorithm are the hyperhyponymy relationships in both taxonomies. These local constraints are propagated throughout the hierarchies to produce a global solution.  There is a certain amount of noise in the diﬀerent phases of the process. First, the taxonomies were automatically acquired and assigned to semantic ﬁles. Second, the bilingual dictionary translates words, not senses, which introduces irrelevant candi- date connections.  The size and coverage of the bilingual dic- tionaries used to establish the candidate connections is an important issue. A dic- tionary with larger coverage increases the amount of nodes with candidate connec- tions and thus the algorithm coverage 6 Proposals for Further Work Some issues to be addressed to improve the al- gorithm performance are the following:  Further test and evaluate the precision of the algorithm. In this direction we plan apart from performing wider hand check- ing of the results, both to ﬁle and synset level to use the presented technique to link WN1.5 with WN1.6. Since there is already a mapping between both versions, the ex- periment would provide an idea of the ac- curacy of the technique and of its applica- bility to diﬀerent hierarchies of the same language. In addition, it would constitute an easy way to update existing lexical re- sources.  Use other relationships apart from hy- perhyponymy to build constraints to se- lect the best connection (e.g. sibling, cousin, synonymy, meronymy, etc.).  To palliate the low coverage of the bilingual dictionaries, candidate translations could be inferred from connections of surround- ing senses. For instance, if a sense has no candidate connections, but its hypernym precision over precision over total precision TOK, FOK TOK, FNOK over TOK animal 424 (93) 62 (95) 486 (93) food 166 (94) 83 (100) 149 (96) cognition 200 (67) 245 (99) 445 (82) communication 536 (77) 234 (99) 760 (81) Table 5: Precision results over all words for the test taxonomies. does, we could consider as candidate con- nections for that node all the hyponyms of the synset connected to its hypernym.  Use the algorithm to enrich the Spanish part of EuroWordNet taxonomy. It could also be applied to include taxonomies for other languages not currently in the ewn project. In addition, some ideas to further exploit the possibilities of these techniques are:  Use ewn instead of wn as the target tax- onomy. This would largely increase the coverage, since the candidate connections missing in the bilingual dictionaries could be obtained from the Spanish part of ewn, and viceversa. In addition, it would be use- ful to detect gaps in the Spanish part of ewn, since a ewn synset with no Spanish words in ewn, could be assigned one via the connections obtained from the bilingual dictionaries.  Since we are connecting dictionary senses (the entries in the mrd used to build the taxonomies) to ewn synsets: First of all, we could use this to disambiguate the right sense for the genus of an entry. For in- stance, in the Spanish taxonomies, the genus for the entry queso 1 (cheese) is masa (mass) but this word has several dictio- nary entries. Connecting the taxonomy to ewn, we would be able to ﬁnd out which is the appropriate sense for masa, and thus, which is the right genus sense for queso 1. Secondly, once we had each dic- tionary sense connected to a ewn synset, we could enrich ewn with the deﬁnitions in the mrd, using them as Spanish glosses.  Map the Spanish part of ewn to wn1.6. This could be done either directly, or via mapping wn1.5wn1.6. 7 Acknowledgments This research has been partially funded by the Spanish Research Department (ITEM Project TIC96-1243-C03-03), the Catalan Research De- partment (CREL project), and the UE Com- mission (EuroWordNet LE4003). References A. Ageno, I. Castellon, F. Ribas, G. Rigau, H. Rodrıguez, and A. Samiotou. 1994. TGE: Tlink Generation Environment. In Proceed- ings of the 15th International Conference on Computational Linguistics (COLING94), Kyoto, Japan. M. Alvar, editor. 1987. Diccionario General Ilustrado de la Lengua Espanola VOX. Biblo- graf S.A, Barcelona, Spain. J. Atserias, S. Climent, X. Farreres, G. Rigau, and H. Rodrıguez. 1997. Combining Mul- tiple Methods for the Automatic Construc- tion of Multilingual WordNets. In proceed- ings of International Conference on Recent Advances in Natural Language Processing (RANLP97), Tzigov Chark, Bulgaria. J. Daude, L. Padro, and G. Rigau. 1999. Exper- iments on Applying Relaxation Labeling to Map Multilingual Hierarchies. Technical Re- port LSI-99-5-R, Departament de LSI. Uni- versitat Politecnica de Catalunya. J. O. Eklundh and A. Rosenfeld. 1978. Con- vergence Properties of Relaxation Labelling. Technical Report 701, Computer Science Center. University of Maryland. X. Farreres, G. Rigau, and H. Rodrıguez. 1998. Using WordNet for Building WordNets. In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Pro- cessing Systems, Montreal, Canada. K. Knight and S. Luk. 1994. Building a Large- Scale Knowledge Base for Machine Transla- tion. In Proceedings of the American Associ- ation for Artiﬁcial Inteligence (AAAI94). L. Marquez and L. Padro. 1997. A Flex- ible POS Tagger Using an Automatically Acquired Language Model. In Proceedings of the 35th Annual Meeting of the Associ- ation for Computational Linguistics. Joint ACLEACL, pages 238245, Madrid, Spain, July. G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1991. Five Papers on WordNet. International Journal of Lexi- cography. A. Okumura and E. Hovy. 1994. Building japanese-english dictionary based on ontol- ogy for machine translation. In proceedings of ARPA Workshop on Human Language Tech- nology, pages 236241. L. Padro. 1998. A Hybrid Environment for SyntaxSemantic Tagging. Phd. Thesis, Dep. Llenguatges i Sistemes Informatics. Univer- sitat Politecnica de Catalunya, February. http:www.lsi.upc.espadro. P. Procter, editor. 1987. Longman Dictionary of Common English. Longman Group, Har- low, Essex, England. J. Richards, D. Landgrebe, and P. Swain. 1981. On the accuracy of pixel relaxation labelling. IEEE Transactions on Systems, Man and Cy- bernetics, 11(4):303309. G. Rigau, H. Rodrıguez, and J. Turmo. 1995. Automatically extracting Translation Links using a wide coverage semantic taxonomy. In proceedings 15th International Conference AI95, Montpellier, France. G. Rigau, J. Atserias, and E. Agirre. 1997. Combining Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguis- tics. Joint ACLEACL, pages 4855, Madrid, Spain, July. G. Rigau, H. Rodrıguez, and E. Agirre. 1998. Building Accurate Semantic Tax- onomies from MRDs. In Proceedings of COLING-ACL98, Montreal, Canada. C. Torras. 1989. Relaxation and Neural Learn- ing: Points of Convergence and Divergence. Journal of Parallel and Distributed Comput- ing, 6:217244. M. Utiyama and K. Hasida. 1997. Bottom-up Alignment of Ontologies. In Proceedings of IJCAI workshop on Ontologies and Multilin- gual NLP, Nagoya, Japan. A. Voutilainen and L. Padro. 1997. Developing a Hybrid NP Parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, ANLP, pages 8087, Washington DC. ACL.",
  "39.pdf": "arXiv:cs9906026v1 [cs.CL] 25 Jun 1999 Robust Grammatical Analysis for Spoken Dialogue Systems Gertjan van Noord, Gosse Bouma, Rob Koeling, Mark-Jan Nederhof Alfa-Informatica  BCN University of Groningen June 1998 Accepted for Journal of Natural Language Engineering Abstract We argue that grammatical analysis is a viable alternative to concept spotting for processing spoken input in a practical spoken dialogue system. We discuss the structure of the grammar, and a model for robust parsing which combines linguistic sources of information and statistical sources of information. We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input. 1 Introduction The NWO Priority Programme Language and Speech Technology is a research programme aiming at the development of spoken language information systems. Its immediate goal is to develop a demonstrator of a public transport information system, which operates over ordinary telephone lines. This demonstrator is called OVIS, Openbaar Vervoer Informatie Systeem (Public Transport Information System). The language of the system is Dutch. At present, a prototype is in operation, which is a version of a German system developed by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German sys- tem processes spoken input using concept spotting, which means that the smallest information- carrying units in the input are extracted, such as locative phrases (mostly names of train stations) and temporal expressions, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus per- ceived are ignored. The use of concept spotting is common in spoken-language information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing are that it is relatively easy to develop the NLP component, since larger sentence constructs do not have to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction (OVIS2) is based on a grammar which describes grammatical sentences, i.e. complete and well-formed user utterances, and thus differs radically from a concept spotting approach. This article presents a detailed account of a computational grammar for Dutch, and a robust parsing algorithm which incorporates this grammatical knowl- edge as well as other knowledge sources, such as acoustic evidence and Ngram statistics. We argue that robust parsing can be based on sophisticated grammatical analysis. In particular, the grammar describes full sentences, but in doing so, also describes the grammar of temporal ex- pressions and locative phrases which are crucial for concept spotting. Robustness is achieved by taking these phrases into consideration, if a full parse of an utterance is not available. We show 1 that our approach is feasible in terms of both accuracy and computational resources, and thus is a viable alternative to pure concept spotting. Whereas some (e.g. Moore, Pereira, and Murveit (1989)) argue that grammatical analysis may improve recognition accuracy, our current experiments have as yet not been able to reveal a sub- stantial advantage in this respect. However, the grammatical approach may become essential as soon as the application is extended in such a way that more complicated grammatical construc- tions need to be recognized. In that case, simple concept spotting may not be able to correctly process all constructions, whereas the capabilities of the grammatical approach extend much fur- ther. The structure of this paper is as follows. In section 2 we describe the grammar for OVIS2. We present the grammar in some detail, since we believe it constitutes an interesting compromise between linguistic and computational considerations. Readers interested in processing issues rather than the details of linguistic analysis might prefer to skip section 2 (possibly except the ﬁrst paragraph) and jump to section 3 immediately. That section describes the robust parsing algorithm. Section 4 reports test results, showing that grammatical analysis allows fast and accu- rate processing of spoken input. 2 A computational grammar for Dutch In developing the OVIS2 grammar we have tried to combine the short-term goal of developing a grammar which meets the requirements imposed by the application (i.e. robust processing of the output of the speech recogniser, extensive coverage of locative phrases and temporal expressions, and the construction of ﬁne-grained semantic representations) with the long-term goal of devel- oping a general, computational, grammar which covers all the major constructions of Dutch. The design and organisation of the grammar, as well as many aspects of the particular gram- matical analyses we propose, are based on Head-driven Phrase Structure Grammar (Pollard and Sag, 1994). We depart from this formalism mostly for computational reasons. As is explained be- low, the grammar is compiled into a restricted kind of deﬁnite clause grammar for which efﬁcient processing is feasible. The semantic component follows the approach to monotonic semantic in- terpretation using quasi-logical forms presented originally in Alshawi (1992). The grammar currently covers the majority of verbal subcategorisation types (intransitives, transitives, verbs selecting a PP, and modal and auxiliary verbs), NP-syntax (including pre- and post-nominal modiﬁcation, with the exception of relative clauses), PP-syntax, the distribution of VP-modiﬁers, various clausal types (declaratives, yesno and WH-questions, and subordinate clauses), all temporal expressions and locative phrases relevant to the domain, and various typi- cal spoken-language constructs. Due to restrictions imposed by the speech recogniser, the lexicon is relatively small (3200 word forms, many of which are names of stations and cities). In sections 2.1- 2.3 we introduce the grammar formalism from both a computational and lin- guistic perspective. Section 2.4 describes the grammar of noun, prepositional, and verb phrases, subordinate and main clauses, WH-questions and topicalisation, and a number of domain speciﬁc constructions. Sections 2.5 and 2.6, ﬁnally, are concerned with semantics and the translation of quasi-logical forms into (application-speciﬁc) update-expressions. 2.1 Formalism The formalism that we use for the OVIS2 Grammar is a variant of Deﬁnite Clause Grammar (DCG) (Pereira and Warren, 1980). We have chosen for DCG because:  DCG provides for a balance between computational efﬁciency on the one hand and linguistic expressiveness on the other.  DCG is a (simple) member of the class of declarative and constraint-based grammar for- malisms. Such formalisms are widely used in linguistic descriptions for NLP. 2  DCG is straightforwardly related to context-free grammar. Almost all parsing technology is developed for CFG; extending this technology to DCG is usually possible (although there are many non-trivial problems as well).  The compilation of richer constraint-based grammar formalisms into DCG is well investi- gated and forms the basis of several wide-coverage and robust grammar systems (i.e. the Alvey-grammar (Briscoe et al., 1987; Carroll, 1993; Briscoe and Carroll, 1993) and the Core Language Engine (Alshawi, 1992)). The formalism for the grammar of OVIS2 imposes the following additional requirements:  External Prolog calls (in ordinary DCG these are introduced in right-hand sides using curly brackets) are allowed, but must be resolved during grammar compilation time.  Rules can be mapped to their context-free skeleton (by taking the functor symbol of the terms appearing in the right-hand and left-hand sides of the rule). This implies that we do not allow the under-speciﬁcation of categories in rules. This is motivated by our desire to experiment with parsing strategies in which part of the work is achieved on the basis of the context-free skeleton of the grammar. It also facilitates indexing techniques.  An identiﬁer is assigned to each rule. Such rule identiﬁers have a number of possible uses (debugging, grammar ﬁltering, grammar documentation).  The grammar speciﬁes for each rule which daughter is the head. This allows head-driven parsing strategies. An efﬁcient head-corner parsing strategy for this formalism is discussed in van Noord (1997a). The restriction that external Prolog calls must be resolved at compilation time implies that we do not use delayed evaluation. More in particular, lexical rules (deriving a lexical entry from a given basic lexical entry) must be applied at compile time and are not interpreted as (relational) constraints on under-speciﬁed lexical entries, as in van Noord and Bouma (1994). Although we have experimented with combinations of delayed evaluation and memoisation, as described in Johnson and Dorre (1995), the resulting systems were not efﬁcient enough to be applied in the kind of practical system considered here. Grammar rules. A grammar rule is deﬁned by a ternary predicate, rule3. The ﬁrst argument of this predicate is a ground Prolog term indicating the rule identiﬁer. The second argument of the rule is the mother category. Categories are non-variable Prolog terms. The third argument of the rule is a list of categories. Note that we require that the length of the list is given, and that none of the categories appearing in the list is a variable. An example of a grammar rule is provided: rule(vp_vpnp, vp(Subj,Agr,Sem), [v(Subj,Agr,trans,l(Arg,Sem)),np(_,Arg)]). (1) Terminal symbols cannot be introduced in rules directly, but are introduced by means of lexical entries. Lexical entries. The lexicon is deﬁned by the predicate lex2. As an example, the lexical entry sleeps could be encoded as: lex(sleeps,v(np,agr(3,sg),intrans,l(X,sleep(X)))). (2) The ﬁrst argument is the terminal symbol introduced by this lexical category. The second argu- ment is the category (a non-variable term). In cases where a lexical entry introduces a sequence of terminal symbols the ﬁrst argument is also allowed to be a (non-empty) list of atoms. 3 Top category. The top category for the grammar (or start symbol) is deﬁned by the unary predi- cate top category. Its argument is an arbitrary non-variable Prolog term. Feature constraints. Almost all work in computational grammar writing uses feature- structures of some sort. It is fairly standard to compile (descriptions of) such features-structures into ﬁrst-order terms (see Pulman (1996) for a recent overview). We use the HDRUG develop- ment platform (van Noord and Bouma, 1997b), which contains a library for compiling feature constraints into Prolog terms, and various predicates to visualise such Prolog terms as feature structures in matrix notation. The most important operators provided by the HDRUG library are the type assignment op- erator (), the path equality operator (), and the path operator (:). A typical grammar fragment employing those operators is: rule(1,S,[Np,Vp]) :- S  s, np(Np), vp(Vp), Vp:vform  finite, subj_agreement(Vp,Np). np(Np) :- Np  np, Np:lex  -. vp(Vp) :- Vp  v, Vp:lex  -. subj_agreement(Vp,Np) :- Vp:agr  Np:agr. (3) In this rule, the constraint Np:lex  - indicates that the value of the lex attribute of Np is of type -. The constraint Vp:agr  Np:agr indicates that the value of the agr attribute of Vp is identical to the value of the agr attribute of Np. Internally, such a rule could be represented as follows (the actual result of the compilation depends on what attributes are allowed for what types; declarations of this sort are part of the grammar): rule(1,s,[np(Agr,-),v(Agr,-,finite,_,_)]). (4) We often will write such rules in matrix notation, as follows: rule( 1, s,    np agr 1 lex -  ,   v agr 1 lex - vform ﬁnite   ). (5) The feature library also supports boolean combinations of atomic values; these are compiled into Prolog terms using a technique described in Mellish (1988) (who attributes it to Colmerauer) and Pulman (1996). Thus, we may specify agr values such as sg  (sec  thi), denoting an agreement value which is singular and either second or third person. We have also found it useful to provide the predicates unify ifdef3, ifdef4, and unify except3. The predicate unify ifdef(C1,C2,Att) can be used to require that if both C1 and C2 can have the attribute Att (i.e. C1, C2 are of a type for which Att is a possible feature), then the values C1:Att and C2:Att must be identical. The predicate ifdef(Att,Cat,Val,Otherwise) is used to require that Cat:Att is identical to Val if Att is an appropriate feature for Cat. Otherwise Val is identical to Otherwise. The predicate unify except(C1,C2,Path) uniﬁes C1 and C2, with the exception of the value of Path, which must be deﬁned for both C1 and C2, but which may have incompatible values. These predicates simplify the deﬁnition of the grammar code below. 2.2 Signs In uniﬁcation-based grammar formalisms, linguistic information is represented by means of typed feature-structures. Each word or phrase in the grammar is associated with such a feature- structure, in which syntactic and semantic information is bundled. Within Head-driven Phrase 4 Structure Grammar (HPSG), such feature-structures are called signs, a terminology which we will follow here. At present, the grammar makes use of 15 different types of sign, where each type roughly corresponds to a different category in traditional linguistic terminology. For each type of sign, a number of features are deﬁned. For example, for the type NP, the features AGR, NFORM, CASE, and SEM are deﬁned. These features are used to encode the agreement properties of an NP, (mor- phological) form, case and semantics, respectively. A more detailed presentation of these features follows below. There are a number of features which occur in most types of sign, and which play a special role in the grammar. The feature SC (SUBCATEGORISATION) (present on signs of type v, sbar, det, a, n and p), for instance, is a feature whose value is a list of signs. It represents the subcategorisation properties of a given sign. As will be explained below, it is used to implement rules which perform functor-argument application (as in Categorial Grammar). The feature SLASH is present on v, ques and sbar. Its value is a list of signs. It is used to implement a (restricted) version of the account of nonlocal dependencies proposed in Pollard and Sag (1994) and Sag (1997). The value of SLASH is the list of signs which are missing from a given constituent. Such a missing element is typically connected to a preposed element in a topicalisation sentence or WH-question. The same mechanism can also be used for relative clauses. The feature VSLASH is similar to SLASH in that it records the presence of a missing element, a verb in this case. It is used to implement an account of Dutch main clauses, based on the idea that main clauses are structurally similar to subordinate clauses, except for the fact that the ﬁnite verb occurs as ﬁrst or second constituent within the clause and the clause ﬁnal position where ﬁnite verbs occur in subordinate clauses is occupied by an empty verbal sign (i.e. an element which is not visible in the phonological or orthographic representation of the sentence). The feature SEM is present on all signs. It is used to encode the semantics of a word or phrase, encoded as a quasi logical form (Alshawi, 1992). The feature MOD is present on the types a, pp, p, adv, sbar and modiﬁer. It is used to account for the semantics of modiﬁers. Its value is a list of quasi-logical forms. In the sections below on syntax, we only give an informal impression of the semantics. The details of the semantic construction rules and principles are dealt with in section 2.5. An important restriction imposed by the grammar-parser interface is that each rule must spec- ify the category of its mother and daughters. A consequence of this requirement is that general rule-schemata, as used in Categorial Grammar and HPSG cannot be used in the OVIS2 grammar. A rule which speciﬁes that a head daughter may combine with a complement daughter, if this complement uniﬁes with the ﬁrst element on SC of the head (i.e. a version of the categorial rule for functor-argument application) cannot be implemented directly, as it leaves the categories of the daughters and mother unspeciﬁed. Nevertheless, generalisations of this type do play a role in the grammar. We adopt an architecture for grammar rules similar to that of HPSG, in which individual rules are classiﬁed in various structures, which are in turn deﬁned in terms of general principles. Rules normally introduce a structure in which one of the daughters can be identiﬁed as the head. The head daughter either subcategorises for the other (complement) daughters or else is modiﬁed by the other (modiﬁer) daughters. The two most common structures are the head-complement and head-modiﬁer structure.1 In ﬁgure 1 we list the deﬁnitions for these structures and the principles they refer to, except for the ﬁller principle, which is presented in the section on topicalisation. Head-complement and head-modiﬁer structures are instances of headed structures. The deﬁnition of headed structure refers to the HEAD-FEATURE, VALENCE, and FILLER principles, and further- more ﬁxes the semantic head of a phrase. Note that the deﬁnition of hd-struct has a number of parameters. The idea is that a headed structure will generally consist of a head daughter, and 1Other structures are the main-clause and head-ﬁller structure. These are discussed in the sections on main-clause syntax and topicalisation. 5 hd_comp_struct(Head,Complements,Mother) :- hd_struct(Head,Complements,Head,Mother). hd_mod_struct(Head,Modifier,Mother) :- hd_struct(Head,[],Modifier,Mother), Head:sem  HeadSem, Modifier:mod  [HeadSem]. hd_struct(Head,Complements,SemanticHead,Mother) :- head_feature_principle(Head,Mother), valence_principle(Head,Complements,Mother), filler_principle(Head,[],Mother), SemanticHead:sem  Mother:sem. head_feature_principle(Head,Mother) :- unify_ifdef(Head,Mother,vform), unify_ifdef(Head,Mother,agr), unify_ifdef(Head,Mother,case), unify_ifdef(Head,Mother,mod), unify_ifdef(Head,Mother,pform), unify_ifdef(Head,Mother,aform), unify_ifdef(Head,Mother,vslash), unify_ifdef(Head,Mother,subj). valence_principle(Head,Complements,Mother) :- ifdef(sc,Head,HeadSc,[]), ifdef(sc,Mother,MotherSc,[]), append(Complements,MotherSc,HeadSc) Figure 1: Structures and Principles 6 furthermore of zero or more complement daughters and possibly a modiﬁer. Head-complement and head-modiﬁer structures differ from each other only in that the ﬁrst introduces complements, but no modiﬁers, whereas the second introduces no complements, but a modiﬁer. Moreover, the syntactic head is also the semantic head in head-complement structures, but not in a head-modiﬁer structure. In head-modiﬁer structures, the semantic contribution of the head to the meaning of the phrase as a whole is handled by unifying the head semantics with the value of (the ﬁrst element of) MOD on the modiﬁer. The HEAD FEATURE PRINCIPLE states for a number of features (the head-features) that their value on the head daughter and mother must be uniﬁed. As this principle generalises over vari- ous types of sign, its deﬁnition requires the predicate unify ifdef. The VALENCE PRINCIPLE determines the value of the valence feature SC. The value of SC on the head daughter of a rule is the concatenation (append) of the list of complement daughters and the value of SC on the mother. Another way to put this is that the value of SC on the mother is the value of SC on the head daughter minus the elements on SC that correspond to the complement daughters. Note that the formulation of the VALENCE PRINCIPLE is complicated by the fact that SC (or SUBJ) may sometimes not be deﬁned on the mother. In that case, it is assumed that the value of SC on the head daughter must correspond exactly to the list of complement daughters. The constraint ifdef(sc,Mother,MotherSc,[]) states that the value of SC on Mother uniﬁes with MotherSc, if SC is deﬁned for the type of Mother. Otherwise, MotherSc is assigned the value [] (i.e. the empty list). The structures deﬁned in ﬁgure 1 are used in the deﬁnition of grammar rules. The np-det-n rule introduces a head-complement structure in which (following the traditional semantic analysis) the determiner is the head, and the noun the complement: rule(np_det_n, NP, [Det, N]) :- NP  np, Det  det, N  n, NP:nform  norm, hd_comp_struct(Det,[N],NP). (6) The n-adj-n rules introduces a head-modiﬁer structure where the adjective is the modiﬁer: rule(n_adj_n, N1, [AdjP, N0]) :- N1  n, AdjP  a, N0  n, AdjP:agr  N0:agr, hd_mod_struct(N0,AdjP,N1). (7) Note that for a given rule, the types of the mother and daughters must be speciﬁed, and furthermore, the number of complements is always speciﬁed. This implies that the constraints in the principles in ﬁgure 1 can be reduced to a number of basic constraints on the values of particular features deﬁned for the signs in the rule. The previous two rules can be depicted in matrix notation as (where  denotes the empty list): np det n:   np agr 1 nform norm sem 2      det sc  3  agr 1 sem 2   3 n (8) n adj n:   n sc 1 agr 2 sem 3      a sc  agr 2 sem 3 mod  4      n sc 1 agr 2 sem 4   (9) An overview of all grammar rules deﬁned in the fragment at the moment, together with the structures and principles from which they inherit, is given in ﬁgure 2. The classiﬁcation of rules into structures, which are in turn deﬁned in terms of principles, allows us to state complicated rules succinctly and to express a number of generalizations. Nev- ertheless, it is also clear that the rules could have been more general, if rule schemata (in which 7 np_det_n pp_p_np vp_arg_v vp_v_arg v_part_v v_v_v sbar1 sbar2 n_adj_n n_n_pp vp_mod_v vp_v_mod mod_topic topicalization vfirst subject_first mod_adv mod_pp hd_mod hd_comp hd hd_filler main_clause head_feature filler valence mod_np vgap Figure 2: The Rule Hierarchy (with PRINCIPLES shown in boxes, structures in ovals, and rules without frame). Note that the mod np rule (a unary rule which transforms temporal NPs into verbal modiﬁers) and the vgap rule (a rule which introduces verbal gaps) are exceptional in that they do not inherit from general principles. the type of the daughters, or even the number of daughters is not necessarily speciﬁed) had been allowed. Given this restriction, one may even wonder whether the VALENCE PRINCIPLE (and the feature SC that comes with it) cannot be eliminated in favour of more speciﬁc rules. Va- lence features are particularly important for grammars employing rule schemata, but they are much less crucial for more traditional types of grammar. Although eliminating valence features is not impossible in principle, we believe that the present set-up still has advantages, although these are less apparent than in grammars which make use of rule schemata. Expressing valence information lexically, instead of using more detailed syntactic rules, has the advantage that id- iosyncratic subcategorization requirements (such as the restriction that denken (to think) requires a PP-complement headed by aan (about), or the fact that komen (to come) may combine with the particle aan (the combination of which means to arrive)) need not be stated in the rules. Similarly, all constraints having to do with case marking and agreement can be expressed lexically, as well as the semantic relation between a head and its dependents. 2.3 The lexicon The lexicon is a list of clauses lex(Word,Sign), associating a word (or sequence of words) with a speciﬁc sign. Constraint-based grammars in general, and lexicalist constraint-based grammars in particu- lar, tend to store lots of grammatical information in the lexicon. This is also true for the OVIS2 grammar. A lexical entry for a transitive verb, for instance, not only contains information about the morphological form of this verb, but also contains the features SC and SUBJ for which quite detailed constraints may be deﬁned. Furthermore, for all lexical signs it is the case that their se- mantics is represented by means of a feature-structure. This structure can also be quite complex. To avoid massive reduplication of identical information in the lexicon, the use of inheritance is therefore essential. In ﬁgure 3, we illustrate the use of inheritance in the lexicon. All lexical entries for verbs have a number of properties in common, such as the fact that they are of type v, and take a normal (non-locative and non-temporal) NP as subject. This is expressed by the template v(V). Intran- 8 intransitive(Pred,Sign) :- iv(Sign), iv_sem(Sign,Pred). transitive(Pred,Sign) :- tv(Sign), tv_sem(Sign,Pred). v(V) :- V  v, V:lex  basic, V:vslash  [], V:subj  [Subj], Subj  np, Subj:nform  norm. iv(V) :- v(V), V:sc  []. tv(V) :- v(V), V:sc  [Obj], Obj  np, Obj:nform  norm, Obj:case  acc. weather_v(V) :- iv(IV), unify_except(IV,V,subj:h:nform), V:subj:h:nform  it. Figure 3: Fragment of the lexical hierarchy sitive verbs (iv(V)) can now be characterised syntactically as verbs which do not subcategorise for any (non-subject) complements. Transitive verbs (tv(V)) subcategorise for an NP with ac- cusative case. The templates intransitive(Pred,Sign) and transitive(Pred,Sign), ﬁnally, combine the syntactic and semantic properties of intransitive and transitive verbs. The variable Pred is used in the semantics to ﬁx the value of the predicate deﬁned by a particu- lar verb. A limited form of non-monotonic inheritance is supported (see Carpenter (1992) and Bouma (1992) for more general approaches). For instance, weather verbs require the dummy pronoun het (it) as subject, but behave otherwise as intransitive verbs. This can be expressed by letting weather v inherit from iv, with the exception of the value of the NFORM attribute of (head of the list containing) the subject, which is assigned an exceptional value. The attribute- value matrices for the templates iv(V) and tv(V) are: iv(   v lex basic sc  subj np nform norm  vslash    ). tv(   v lex basic sc   np nform norm case acc    subj np nform norm  vslash    ). (10) The lexicon itself (i.e. the predicate lex2) is deﬁned in terms of the predicates entry, inflection and lexical rules: lex(Word,Sign) :- entry(Root,Sign0), inflection(Root,Word,Sign0,Sign1), lexical_rules(Sign1,Sign). (11) The deﬁnition of entry(Root, Sign) deﬁnes for each root form what its associated sign is. For instance, for verbs we must typically distinguish a ﬁrst person singular form, a second and third person singular form, and a plural form (which is also the form of the inﬁnitive). The pred- icate inflection deﬁnes how inﬂected forms are derived. For example, there is an inﬂection rule which adds a t to the base form of a verb, and speciﬁes that its agreement features are third person singular, and its VFORM value is ﬁn. Lexical rules can be used to transform the sign as- sociated with a lexical entry. For instance, the account of nonlocal dependencies sketched below 9 makes use of a lexical rule which removes a sign from SC and places it on SLASH. A more de- tailed account of this lexical rule is given in the section on nonlocal dependencies. As an example, assume the stem arriveer (to arrive) is deﬁned as an intransitive: entry(arriveer,Sign):- intransitive(arriveren,Sign). (12) Such a deﬁnition will give rise to a number of lexical entries. One of these will be the third person singular ﬁnite form: lex(arriveert,   v lex basic vform ﬁn sc  subj   np nform norm agr sg  thi    vslash    ). (13) 2.4 Syntactic Coverage Below, we describe the syntactic coverage of the grammar. The grammar is not intended as a general, wide-coverage, grammar for Dutch. This implies not only that coverage in the lexical domain is limited, but also that several grammatical constructions are not taken into consider- ation (e.g. passives) or accounted for only to a certain extent (e.g. the grammar of Dutch verb clusters). The coverage of the grammar is quite satisfactory for the OVIS application, however. For instance, when evaluating the grammar on a corpus of 1000 transcribed test-sentences, we obtained a semantic concept accuracy of 95 (see section 4.2 for discussion). 2.4.1 Noun phrases The four types which are relevant in the syntax of noun phrases are np (noun phrase), det (deter- miner), a (adjective) and n (noun). Each type has the attributes AGR and SEM. Furthermore, det and n have an attribute SC. The np type has three further attributes: CASE, NFORM and PFORM. Finally, a is also speciﬁed for MOD. The features AGR (agreement), CASE, and NFORM (noun form) are used to encode agreement properties (encoded as a boolean combination of person, number, determiner and deﬁniteness), the case value and the form of an item. Their possible values are listed in (14). Note that AGR contains the information needed for subject-verb agreement, as well as for NP-internal agreement (between determiner, adjective, and noun). The agreement types de and het (the two forms of the deﬁnite article) distinguish between neuter and nonneuter nouns. CASE and NFORM are relevant for full NPs only. Agr (ﬁr  sec  thi)  (sg  plu)  (de  het)  (def  indef) Case nom  acc Nform norm  loc  temp  num (14) Full NPs never take complements, so they do not have a feature SC. Adjectives may modify a noun, therefore the feature MOD is deﬁned for type a. The two rules we presented in (6) and (7) (section 2.4) are used to form NPs consisting of a determiner and a (possibly complex) noun, and Ns consisting of an adjective followed by a (possibly complex) noun. The derivation of the NP de volgende intercity (the next intercity) is shown in ﬁgure 4. 10   np agr 1 sg  thi  de  def nform norm sem de(volgende(intercity))     det agr 1 sc 2 sem de(volgende(intercity))   de 2   n agr 1 sem volgende(intercity)     a agr 1 sem volgende(intercity) mod intercity   volgende   n agr 1 sem intercity   intercity Figure 4: de volgende intercity (the next intercity) 2.4.2 Prepositional phrases Prepositional phrases are of type pp and are headed by prepositions, i.e. elements of type p. Prepositions subcategorize (usually) for an NP, so the value of SC on P will be a list of length one, containing the NP-complement. The feature PFORM takes as value the speciﬁc form of the preposition heading the PP (i.e. van, op, naar, ...). This information can be used to let a verb select a PP headed by a speciﬁc preposition. Full PPs can modify nouns or verb phrases. Therefore, PP has a feature MOD. MOD has to be present on P as well, as the relation between the semantics of the preposition and the element it modiﬁes is encoded as part of the lexical entry of a preposition. Here, we give the rule which forms PPs and the rule which lets a PP combine as a modiﬁer with a noun. pp p np:   pp pform 1 sem 2 mod 3      p pform 1 sc  4  sem 2 mod 3   4 np (15) n n pp:   n sc 1 agr 2 sem 3      n sc 1 agr 2 sem 4     pp sem 3 mod  4    (16) Using these rules, we can derive the phrase intercity uit Goes (intercity from Goes) as illustrated in ﬁgure 5. It should be noted that since adjectives precede the nouns they modify and PPs follow them, an expression such as volgende intercity uit Groningen (next intercity from Groningen) will receive two parses. This appears to be a case of spurious ambiguity. There are intensional adjectives, such as zogenaamde (alleged), which need to be able to take scope over a complex noun, but it seems that modifying PPs never need to take scope over a adjective  noun combination. It is not easy to rule out the latter type of derivation, however, without introducing additional features. 11   n agr 1 sg  thi  de sem intercity(x)  uit(goes,x)     n agr 1 sc 2 sem intercity(x)   intercity 2   pp pform uit sem intercity(x)  uit(goes,x) mod 1 intercity(x)     p pform uit sem intercity(x)  uit(goes,x) mod 1   uit np sem goes  goes Figure 5: intercity uit Goes (intercity from Goes) 2.4.3 Verb phrases Both verbs and verb phrases are of type v:   v lex ylex  nlex null null  nonnull vform ﬁn  inf  te  psp sc listof(Sign) subj listof(Sign) sem Qlf slash listof(Sign) vslash Vslash   (17) The features LEX, NULL, and VFORM are speciﬁc for v. The feature VFORM is used to distin- guish ﬁnite, inﬁnitive, te-inﬁnitive and past participle verbs (and verb phrases headed by such verbs). The feature LEX is used to distinguish lexical verbs (ylex) from verbal phrases that are not lexical (nlex). The feature ylex subsumes two further subtypes basic  complex, to distinguish basic and complex lexical verbs. The latter are combinations of a verb and a separable preﬁx (aankomen, arrive) or combinations of a modal verb and a main verb (wil vertrekken, want to leave). The feature NULL is used to distinguish verbal traces (i.e. verbal signs without phono- logical content) from other verbal signs. The features SUBJ, SLASH, and VSLASH and NULL are discussed in the section below on sentential syntax. There are a number of similar rules for combining a verb or a verbal projection with one of its complements. One rule combines a noun phrase complement with a verbal head (een kaartje 12 kopen, buy a ticket): vp np v:   v lex nlex sc 1 vform 2 slash 3 vslash 4    6 np   v sc  6  1  vform 2 slash 3 vslash 4   (18) Since PPs may either precede or follow the head (vanuit Leiden vertrekken, vertrekken vanuit Leiden, depart from Leiden), there are two rules to combine such a PP and a verbal head. Finally, there is a rule which combines a verbal head with a te-inﬁnitive (weigeren naar Groningen te komen, refuse to come to Groningen). The result of combining a verb (or verbal projection) with its complement is a phrase (i.e. the value of LEX on the mother is nlex). A verbal modiﬁer can be either an adverb, a PP, or a temporal NP. There are unary rules rewriting signs of type modiﬁer into each of these categories. One such rule is the following: mod adv:  modiﬁer mod 1     adv mod 1  (19) At the moment, we allow all modiﬁers to precede or follow the verb (ik moet morgen in Assen zijn in Assen zijn morgen morgen zijn in Assen, I must be in Assen tomorrow, ik moet tien uur in Assen zijn?in Assen zijn tien uur, I must be in Assen at ten oclock). Therefore, there are two similar rules, vp v mod and vp mod v, in which a verb combines with a modiﬁer. The ﬁrst is illustrated here: vp mod v:   v lex nlex sc 1 vform 2 slash 3 vslash 4     modiﬁer mod  5     v sc 1 vform 2 slash 3 vslash 4 sem 5   (20) A special type modiﬁer (with SEM and MOD as only attributes) in combination with three unary rules is used to introduce the various types of verbal modiﬁer. A sample derivation is given in ﬁgure 6 (the value of the features SLASH and VSLASH is not shown, but is   on all verbal signs in this derivation). Finally, there are two VP-rules that give rise to complex lexical expressions, instead of phrases. Firstly, consider the v v v rule: v v v:   v lex complex sc 1 vform 2 slash 3 vslash 4 sem 5      v lex basic sc  6  1  vform 2 slash 3 vslash 4 sem 5   6   v lex ylex vslash    (21) The v v v rule is used to derive phrases in which a modal verb precedes its inﬁnitival comple- ment ((dat ik om tien uur) wil vertrekken, that I want to leave at ten oclock). We adopt an analysis of such constructions in which modals inherit the arguments on SC of the inﬁnitival verb with which they combine. This is illustrated for the root wil (want) in (22). 13   v lex nlex sc  subj 4 vform ﬁn sem 1 missen(e,subj,trein)  in(goes,e)     modiﬁer sem 1 mod  2      pp pform in sem 1 mod  2    in Goes   v lex nlex sc  subj 4 vform ﬁn sem 2 missen(e,subj,trein)   3   np agr sg  ... case acc nform norm sem trein   de trein   v lex basic sc  3  subj 4 vform ﬁn sem 2   mist Figure 6: (dat Rob) in Goes de trein mist (that Rob misses the train in Goes) 14   v lex nlex sc  subj 4 vform ﬁn sem 1 willen(subj,kopen(subj,kaartje))   2 np sem kaartje  een kaartje   v lex complex sc  2  subj 4 vform ﬁn sem 1     v lex basic sc  3 , 2  subj 4 vform ﬁn sem 1   wil 3   v lex basic sc  2  vform inf sem kopen(subj,kaartje)   kopen Figure 7: (dat ik) een kaartje wil kopen (that I want to buy a ticket) (22) wil    v lex basic sc    v lex ylex sc 1 vform inf   1    This allows us to derive phrases such as (dat ik) een kaartje wil kopen (that I want to buy a ticket) where the ﬁnite modal verb combines with the inﬁnitival verb before combining with the object of kopen (ﬁgure 7). Note that it is essential that the modal verb selects a [LEX ylex] argument in this case, as this excludes the derivation of ungrammatical expressions such as (dat ik) wil een kaartje kopen. The result of combining a modal with an inﬁnitival verb is [LEX complex] (i.e. subsumed by [LEX ylex]). This implies that such combinations can be selected by another modal verb (i.e. (dat ik) een kaartje zou willen kopen, that I would like to buy a ticket). 15 Next, consider the v part v rule: v part v:   v lex complex sc 1 vform 2 slash 3 vslash 4 sem 5    6 part   v lex ylex sc  6  1  vform 2 slash 3 vslash 4 sem 5   (23) The rule v part v is used to account for constructions such as (dat ik voor tien uur) aan wil komen (that I want to arrive before ten oclock). The preﬁx (or particle) aan of the verb aankomen (arrive) is separated from the root komen in this case. As the root komen speciﬁes that it selects such a particle on its SC-list, the modal verb inherits this speciﬁcation. The rule v part v allows us to combine a verb or verbal complex with a particle. There are two reasons for not using an analogue of the vp np v-rule in this case. First, modiﬁers may not appear in between a particle and the verbal complex selecting this particle ( (dat ik) aan om tien uur wil komen). This is accounted for by requiring that the head in the rule for particles must be [LEX ylex] (and combinations of a modiﬁer and a verbal head are always [LEX nlex]). Second, particles may appear inside a verb cluster ((dat ik voor tien uur) zou aan willen komen, that I would like to arrive before ten oclock). This implies that the result of combining a particle with a verb cluster must be [LEX ylex], instead of [LEX nlex] as speciﬁed on the vp np v-rule. It should be obvious that these two rules, and the limited form of argument inheritance we allow (i.e. structure sharing of SC-lists only, and no concatenation of SC-lists), is not sufﬁcient to account for the full range of verb clustering data in Dutch. For one thing, the grammar as it stands cannot handle inverted word orders ((dat ik de trein) halen moet, that I must catch the train), where the inﬁnitive precedes the modal verb. It is rather straightforward to include rules for inverted word orders. A potentially more problematic omission is the fact that perception verbs (horen, zien) and causative laten, which also introduce verb clusters ((dat ik) Rob een kaartje laat kopen, that I let Rob buy a ticket), cannot be accounted for. The analysis of this construction in van Noord and Bouma (1997a) is based on the notion argument inheritance. This presupposes the possibility of recursive constraints in syntax (to concatenate SC-lists) as well as rules with an indeﬁnite number of daughters. Both are excluded within the present formalism. 2.4.4 Subordinate clauses Subordinate clauses containing a VP headed by a ﬁnite verb are of type sbar (the name sbar stems from X-bar grammar, where clauses introduced by a complementizer are (barred) projections of S). As ﬁnite subordinate clauses are always introduced by a complementizer, we assume that this complementizer is the head of the clause and that it subcategorises for a subject NP and a (ﬁnite) VP. The lexical entry for the complementizer dat (that), for instance, is: (24) dat    comp sc  1 np case nom  ,   v vform ﬁn sc  subj  1  sem 2 slash 3    sem 2 mod  slash 3   The complementizer uniﬁes the NP on its SC with the subject of the VP. This implies that the NP is interpreted as subject of the VP. Furthermore, the complementizer has no independent 16   sbar sem 1 gaan(e,markjan)  naar(amsterdam,e) mod      comp sc  2 , 3  sem 1 mod    dat 2   np agr sg  ... case nom sem markjan   Mark-Jan 3   s sc  subj  2  vform ﬁn sem 1   naar Amsterdam gaat Figure 8: dat Mark-Jan naar Amsterdam gaat (that Mark-Jan is going to Amsterdam) semantics, but simply passes on the semantics of the VP. Since dat clauses cannot be modiﬁers, its MOD feature is empty. Other complementizers such as omdat (because) will have a non-empty value for this attribute to indicate that subordinate sentences headed by such complementizers can occur as modiﬁer. The rule constructing subordinate clauses is deﬁned as follows:2   sbar slash 1 sem 2 mod 3      comp sc  4 , 5  slash 1 sem 2 mod 3   4 np case nom  5   v sc  subj  4  vform ﬁn slash 1 vslash    (25) A sample derivation is given in ﬁgure 8. 2.4.5 Main clauses Main clauses with a ﬁnite verb in initial position (as in yesno-questions) are of type ques. Main clauses in which the ﬁnite verb appears in second position (as in declarative sentences or WH- questions) are of type root. The attributes associated with these types are:   ques subj listof(Sign) sem Qlf slash listof(Sign)   root sem Qlf  (26) Dutch main clauses differ from subordinate clauses in that the ﬁnite verb in main clauses appears in ﬁrst or second position. There is a tradition, both in transformational and non- transformational grammar, to account for this fact by postulating a dependency between the ﬁnite verb and the position where ﬁnite verbs occur in subordinate clauses. The advantage of postulating such a dependency is that the grammar rules used for subordinate clauses are also 2There is an additional rule, for constructing subordinate clauses with a missing (extracted) subject. This rule (sbar2) could be used in an account of nonlocal dependencies which allows for extraction out of subordinate clauses as well. 17 vgap:   v lex basic null null vform ﬁn sc 1 subj  slash  vslash   vslash vsc 1 vsem 2   sem 2    ǫ Figure 9: Verbal Gap applicable in main clauses. In transformational grammar, a dependency of this type can be es- tablished by means of a head-movement operation which moves the verb from its ﬁnal position to a position at the beginning of the sentence. Within the framework of HPSG (Netter, 1992; Frank, 1994) we can obtain a similar dependency by postulating a verbal trace, i.e. a verbal sign without phonological content, at the end of the clause. Using this verbal trace as the head, we can use the VP rules discussed above to build up a VP as usual. The rule for introducing such a verbal trace is given in ﬁgure 9. Note that the sign for verbal traces differs from that of an ordinary verb in that its subcategorisation list in not instantiated, but made reentrant with VSLASH:VSC. Similarly, the semantics of the verbal gap is reentrant with VSLASH:VSEM. Furthermore, a verbal gap is a basic (i.e. non-complex) lexical verb, with no phonological content (i.e. [NULL null]). We can also safely assume that verbal traces are ﬁnite, as main clauses are always headed by a ﬁnite verb. The value of SUBJ is the empty list, as VPs headed by a verbal trace never combine with a subject directly (as will be shown below). Finally, SLASH also can be assumed to be empty.3 There are two rules which combine a ﬁnite verb with a VP containing a verbal trace, and which also introduce a subject (ﬁgure 10). Both rules are highly similar (they are therefore both instances of a MAIN-CLAUSE-STRUCT). The only difference is the category of the mother, and the order of the daughters. The vﬁrst-rule introduces phrases of the type ques, i.e. instances of verb- ﬁrst clauses, in which the subject follows the main verb. The subject-ﬁrst-rule introduces phrases of type root, in which the subject is ﬁrst, and the main verb follows the subject. The constraints imply, among others, that the VP must contain a verbal trace, that the SC-information of the main verb is reentrant with VSLASH:VSC of the VP (and thus, indirectly, with the SC-value of the verbal trace), and that the semantics of main verb is shared with the value of VSLASH:VSEM on the VP (and thus, indirectly, with the semantics of the verbal trace). Note also that the VP acts as semantic head of the construction. This is necessary in order to ensure that the effect of verbal modiﬁers within the VP is properly taken into account. An example derivation of a subject ﬁrst main clause is given in ﬁgure 11. 2.4.6 Wh-questions and topicalisation In the previous section, we have introduced a rule for verb-initial and subject-initial main clauses. The ﬁrst phrase in a main clause can also be a (non-subject) complement or a modiﬁer. This is typically the case for (non-subject) WH-questions. Sentences with a fronted complement are treated as instances of a non-local dependency construction (where the dependency is mediated 3 The lexical rule which moves complements from SC to SLASH does not apply to verbal traces. Instead, it can be applied to the ﬁnite verb which binds the trace. Also, if a verbal gap combines with a complement having a non-empty SLASH, the relevant passing on of the SLASH value is handled by the ﬁnite verb which binds the trace. This is possible because the SC-list of the verbal trace and the binder will be shared. 18   ques slash 1 sem 2      v sc 3 subj  4  vform ﬁn lex basic slash 1 vslash  sem 5   4 np case nom    v sc  subj  vform ﬁn slash  vslash   vslash vsc 3 vsem 5   sem 2   root sem 2   4 np case nom    v sc 3 subj  4  vform ﬁn lex basic slash  vslash  sem 5     v sc  subj  vform ﬁn slash  vslash   vslash vsc 3 vsem 5   sem 2   Figure 10: Rules for verb-ﬁrst and subject ﬁrst main clauses (yn questions and simple declarative sentences) through SLASH). In sentences with a fronted modiﬁer, it is assumed that the ﬁrst element modiﬁes the remainder of the clause, and thus a local treatment can be given. Examples of sentences with a fronted complement are given in (27). (27) a. Naar welk station wilt u reizen? To which station do you want to travel b. De laatste trein kunt u nog halen. The last train, you can still catch These examples are handled by means of a lexical COMPLEMENT-EXTRACTION rule applicable to verbs, and a syntactic HEAD-FILLER-rule for combining the fronted element with a ques-phrase containing a non-empty SLASH-value. The COMPLEMENT-EXTRACTION rule can apply in two ways: First, it can take a complement from SC and put it on SLASH (28a). This implies that this complement will not be found locally, but that it will be uniﬁed with an element in fronted position. Second, it can make the SLASH value of a verb reentrant with the SLASH value of one of its complements (28b). This implies that true non-local dependencies are possible, as the head of a phrase can pass on information about missing elements from one of its dependents. If the complement-extraction rule does not apply, the SLASH value of the verb, as well as the SLASH value of all its complements, is set to  (the empty list). (28) a.   v sc  1 pp subj 2 vform 3 ...      v sc  slash 1 subj 2 vform 3 ...   19 root sem 1 kopen(e,rob,geen-kaartje)  2   np agr sg  ... case nom sem rob   Rob   v lex basic vform ﬁn sc 3 subj  2  sem 1   koopt   v lex nlex vform ﬁn sc  subj  vslash   vslash vsc 3 vsem 5   sem 1   4   np case acc sem geen-kaartje   geen kaartje   v lex basic vform ﬁn null null sc 3  4  subj  vslash   vslash vsc 3 vsem 1   sem 1   epsilon Figure 11: Rob koopt geen kaartje (Rob does not buy a ticket) 20 root 1 np de laatste trein  ques slash 2  1     v sc 3 slash 2   kunt np u  v vslash  vsc 3     v sc 3  4  vslash  vsc 3    ǫ 4   v sc  slash 2  1    halen Figure 12: De laatste trein kunt u halen b.   v sc v subj 2 vform 3 ...      v sc v slash 1  slash 1 subj 2 vform 3 ...   An example of a derivation involving SLASH is given in ﬁgure 12. The COMPLEMENT EX- TRACTION rule has applied to halen (to catch) to produce a verbal sign with an empty SC-list and an np on SLASH. A verbal trace contains a reentrancy between its SC-list and its VSLASH:VSC-list. When the verbal trace combines with halen, the information that halen has an np on SLASH will therefore also be instantiated on VSLASH:VSC. This information is passed up to the resulting verb phrase. The complement extraction rule also applies to the ﬁnite verb kunt (can), but in this case it establishes a reentrancy between the SLASH value of the verb on the SC-list of kunt and the SLASH-value of kunt itself. The VFIRST rule uniﬁes the SC-list of kunt with the VSLASH:VSC-list of the verb phrase halen ǫ, and thus, SLASH (of the verb on SC of kunt, and thus on kunt itself) is instantiated as np. This information is passed on to the resulting ques phrase, which can then be combined with the initial np using the TOPICALISATION rule in 29. root sem 1   2   ques slash 2 sem 1   (29) It should be noted that our account of non-local dependencies differs from earlier slash-based accounts, such as those in Gazdar et al. (1985) and Pollard and Sag (1994) in that it does not make use of a FOOT FEATURE principle. Instead, we adopt the approach of Sag (1997), who imposes the canonical constraint that the SLASH-value of a head is the set-union of the SLASH-values of 21 its daughters. An EXTRACTION lexical rule can be used to remove an element from SC (COMPS) and to add this element to the set of elements on SLASH. In our implementation, we have made several simplifying assumptions. First, SLASH is not a set, but a list. Second, this list can contain at most one element. This assumption (which has the effect of restricting the number of missing elements from a phrase to at most one) is too restrictive for a highly limited number of cases in English, but appears to be valid for Dutch. Third, instead of imposing a general constraint that SLASH must be the concatenation on the SLASH values of all elements on SC, we allow the COMPLEMENT EXTRACTION rule to unify the value of SLASH with one speciﬁc element on SC. We have to make this assumption, as the more general alternative requires the use of delayed evaluation, something which we wish to avoid in this grammar, or difference lists. While the latter alternative is possible within the present formalism, it also introduces a number of complications which are avoided in the present implementation. The fourth and ﬁnal simpliﬁcation is that COMPLEMENT EXTRACTION and SLASH feature passing is only possible for verbs. This is certainly too restrictive, as extraction out of subordinate clauses of type sbar (welke trein zegt Gertjan dat Rob gemist heeft?, which train does Gertjan say that Rob has missed) and out of pps (Waar gaat deze trein naar toe?, Where does this train go to), and a number of other types of phrase is possible as well. Sentences where the ﬁrst phrase is a modiﬁer are dealt with without appealing to SLASH. Instead, it is assumed that in sentences such as (30), the fronted elements modify the following ques phrase. This requires an additional (mod-topic) rule, given in (31). (30) a. Hoe laat gaat de volgende trein naar Zwolle? When does the next train to Zwolle leave? b. Woensdag moet ik om tien uur in Zwolle zijn. Wednesday, I must be in Zwolle by ten oclock. root sem 1     modiﬁer sem 1 mod  2      ques slash  sem 2   (31) Of course, this account rests on the assumption that modiﬁers of embedded verbs or phrases cannot be fronted, an assumption which is almost certainly false in general (see Hukari and Levine (1995), for instance), but which appears to be rather unproblematic for present purposes. 2.4.7 Special grammar rules The domain which has been selected for OVIS (information dialogues concerning public trans- portation) and the fact that OVIS deals with spoken language, imply that it is crucial that a number of grammatical phenomena are described in a robust manner. In particular, temporal expressions, locative expressions (names of cities and stations), and a number of typical spoken language constructions, such as greetings, occur frequently in such dialogues. The grammar rules and lexical entries for these phenomena make use of the OVIS2 grammar formalism, but are not organised according to the linguistic principles discussed above. This is true not only for the syntax, but also for semantics. The reason for dealing with these phenomena by means of a set of more or less ad hoc rules and lexical entries is that the constructions discussed below are often extremely idiosyncratic. At the same time, describing the regularities that can be observed does not seem to require the overhead of the grammar architecture we assume for the rest of the grammar. The most economical and robust solution seemed therefore to encapsulate the grammar for these constructions in relatively independent grammar modules. 2.5 Semantics The output of the grammatical analysis is a semantic, linguistically motivated and domain- independent, representation of the utterance, in the form of a Quasi Logical Form (QLF). The 22 QLF formalism was developed in the framework of the Core Language Engine (CLE, (Alshawi, 1992; Alshawi and Crouch, 1992)). Since then, the formalism was used and further developed in projects such as the Spoken Language Translator (Agnas et al., 1994), Clare (Alshawi et al., 1992), in the Fracas-project (Cooper et al., 1994) and in Trace  Uniﬁcation Grammar (Block, 1994). In OVIS the QLF is translated into a domain-speciﬁc update expression, which is passed on to the pragmatic interpretation module and dialogue manager for further processing. The dialogue manager maintains an information state to keep track of the information provided by the user. An update expression is an instruction for updating the information state (Veldhuijzen van Zan- ten, 1996). Below, we motivate our choice for QLFs as semantic representation language and we discuss how these QLFs are translated into updates. 2.5.1 The semantic representation language Predicate logic, (sometimes extended with for example generalised quantiﬁers or discourse markers), is often used to represent the meaning of sentences. Due to its long tradition in describing se- mantics of natural languages it is now a well established and well understood technique. The main advantage of artiﬁcial languages like predicate logic is that they are unambiguous. An ambiguous natural language utterance will therefore correspond to more than one expression in predicate logic, one for each reading of the utterance. The disadvantage of this approach is that for very ambiguous inputs, expensive computations must be carried out to compute all read- ings. The alternative adopted in formalisms based on the idea of monotonic semantic interpretation ((Cooper et al., 1994), see also (Nerbonne, 1992) and (Pinkal, 1995)) is to represent ambiguity by means of under-speciﬁcation and to postpone the computation of individual readings as long as possible. Representing ambiguity by under-speciﬁcation, and postponing the computation of individ- ual readings, has at least two computational advantages. First, parsing can beneﬁt signiﬁcantly from the fact that ambiguities which are only semantic (i.e. do not have a syntactic counterpart) are represented by a single derivation. Second, ambiguity resolution can often proceed without enumerating all possible readings of an input separately. A striking example of the latter situa- tion is the translation of QLFs that are ambiguous with respect to quantiﬁer-scope into a domain- speciﬁc meaning representation as it is used by the dialogue manager of the OVIS-system. The utterance in (32a), for instance, gives rise to a single QLF (32b), which could be resolved (ignoring the existential quantiﬁcation over events and the fact that it is a question) to either (32c) or (32d). The domain-speciﬁc reading of (32a) (which corresponds to (32c)) is computed on the basis of (32b) directly, and thus never needs to consider the two different readings of this QLF. (32) a. Gaat er niet een latere (trein)? Is there not a later train? b.   pred not args    pred leave args   index e1  ,   index 3 res λ 4 .later train( 4 ) q exist         c. not(x (later train(x)  leave(e1, x))) d. x (later train(x)  not(leave(e1, x))) 2.5.2 Quasi logical form In ﬁgure 13 we give a QLF as it is produced by the OVIS-grammar. It is a typed feature-structure, whose main components are predicative forms (p form), representing relations (which may also 23 be higher order, such as not and and), and terms. Generalised quantiﬁers are represented by term expressions (t expr). The example in (13) contains two generalised quantiﬁers, corresponding to the (existentially quantiﬁed) event-variables introduced by the two verbal predicates (Davidson, 1967). Note that these quantiﬁers appear as arguments of the predicates, and thus are unscoped with respect to each other.   p form pred want args t expr index e1  , 3 i,   p form pred and args    p form pred leave args t expr index e2  , 3   ,   p form pred at args  5 , hour(4)          Figure 13: QLF for Ik wil om ongeveer vier uur vertrekken (I want to leave at about four oclock) Our implementation of QLF in the OVIS grammar follows roughly the presentation in (Cooper et al., 1994), although some of the apparatus supplied for contextual resolution in that work has been omitted. As the OVIS-grammar uses typed feature-structures, QLFs are represented as feature-structures below. A QLF is either a qlf-term or a qlf-formula. A qlf-term is one of the following:  a term index,4  a constant term,  an term-expression of type t expr and containing the features INDEX, RESTR and QUANT5 (see (13)), where INDEX is a variable, RESTR is an expression of predicate logic (possibly with lambda-abstraction) and QUANT is a generalised quantiﬁer. A QLF formula is one of the following:6  a predicate-argument formula of type p form, and with features PRED and ARGS (see (13)). Predicates may be higher order, arguments may be formulas or terms,  a formula of type v form with features VAR and FORM representing a formula with lambda- abstraction (see(14b)). This is an auxiliary level of representation, introduced to facilitate the interaction between grammar-rules and lexical entries,  a formula of type s form (see(14b)), with features SCOPE and FORM. The value of SCOPE is either a variable or a list of indices indicating the relative scope of term expressions (gener- alised quantiﬁers) (see (14c)). 4In the original formalism indices and variables are distinguished. An index uniquely identiﬁes a term expression. At this moment indices and variables have the same function in our implementation. We may need to distinguish between them later. 5 In chapter 5 of (Cooper et al., 1994) term expressions also contain a slot CAT for specifying information about the lexical form and syntacticsemantic type of an expression (e.g. quantiﬁer, pronoun, etc.) and a slot REF for specifying the (contextual) referent of an expression. We do use CAT, but have omitted it from the presentation below. We currently do not use REF. 6In chapter 5 of (Cooper et al., 1994) two more formula constructs are introduced. These are not used in the current implementation. 24 a. Everybody two languages qlf   t expr index 1 restr λ 2 .person( 2 ) quant every     t expr index 3 restr λ 4 .language( 4 ) quant two   pl x.(person(x)  ....) two y.(language(y)  ....) b. speaks qlf   v form var e1 form   s form scope 5 form   p form pred speak args e1, 6 , 7        pl speak(e1, ..., ...) c. Everybody here speaks two languages qlf   s form scope 5 form   p form pred speak args  e1,   t expr index 1 restr λ 2 .person( 2 ) quant every  ,   t expr index 3 restr λ 4 .language( 4 ) quant two        pl x.(person(x)  (twoy.language(y)  speak(e1, x, y))) twoy.(language(y)  x.(person(x)  speak(e1, x, y))) Figure 14: The relation between an expression in QLF and a fomula of predicate logic 25 a. lex(alle,   sc n sem 1  sem   t expr restr 1 quant all     ). b. lex(trein,   n sem   v form var 1 form   s form form   p form pred train args  1          ). c. lex(eerste,   a sem   v form var 1 form   s form scope 2 form   p form pred and args   p form pred ﬁrst args  1   , 3        mod    v form var 1 form   s form scope 2 form 3        ). Figure 15: Examples showing the semantics of determiners, nouns, and adjectives. The deﬁnitions can best be illustrated with a simple example in which we compare a QLF expression with its corresponding formula in predicate logic. In ﬁgure 14 the sentence Every- body speaks two languages is given both a translation in QLF and in predicate logic. In the QLF- translation of the full sentence the scope order ( 5 ) of the two quantiﬁers is left unspeciﬁed. Re- solving scope order amounts to instantiating 5 to [ 1 , 3 ] (for everybody there are two languages that she speaks) or to [ 3 , 1 ] (there are two languages that everybody speaks). 2.5.3 Construction of QLFs During grammatical analysis QLFs are constructed compositionally (see also (Alshawi and Crouch, 1992)). In head complement structures the head daughter is the syntactic as well as the semantic head of the structure. This means that the semantic content of the complement con- stituents is combined with the semantic content of the head. The value of the SEM feature of the head is passed up to the mother (see ﬁgure 1). In head modiﬁer structures the modiﬁer is the semantic head. The semantics of the syntactic head of the structure is plugged into the MOD feature of the modiﬁer. Below we will show how the semantics of the modiﬁer is combined with the semantics of the constituent it modiﬁes. The value of the SEM feature of the modiﬁer is passed up to the mother. We now discuss the semantics of various linguistic categories. Determiners subcategorise for a noun (see Figure 15(a.)). The semantics of the noun is uniﬁed with restriction of the determiner. 26 a. lex(vertrekken,   v sem   v form var e1 form   s form form   p form pred leave args t expr index e1  , 3        subj np sem 3    ). b. lex(willen,   v sc    v sc 2 vform inf sem 4 subj np sem 7     2  sem   v form var e3 form   s form form   p form pred want args t expr index e3  , 6 , 4        subj   np sem 6 t expr index 7       ). Figure 16: Verbal semantics. Nouns introduce a v form (ﬁg. 15(b.)) Note that it is also assumed that quantiﬁers may scope at this point. Adjectives are modiﬁers (ﬁg.15(c.)). They operate on structures whose semantic content is of type v form. The lambda variables of the two formulas are uniﬁed and the semantic content of the structure is the conjunction of the logical formula of the adjective and the logical formula of the structure it modiﬁes. The semantics of verbs corresponds with a v form (see ﬁg. 16a). The value of VAR is reentrant with the INDEX of the event introduced by the verb. The semantics of the subject is uniﬁed with the second element of the argument list of the verb. Intransitive verbs have two semantic arguments, corresponding to the event and subject, respectively. Transitive verbs have three arguments, where the third argument is uniﬁed with the semantics of the single element on SC. Modal verb are subject-control verbs. This means that the subject of the VP-complement is con- trolled by the subject of the modal verb. Semantically, this means that the INDEX of the subject must be uniﬁed with the semantics of the subject of the VP-complement. Note also that we as- sume that assume that the SC-list of a modal verb may contain complements introduced by the VP-complement (as explained in section 2.4.3). These are not relevant for the semantics of the modal verb. The semantics of adverbial phrases resembles that of adjectives. In ﬁgure (17) the semantics of prepositions heading a PP which acts as a verbal modiﬁer is given. PP-modiﬁers introduce a conjunction, with the verbal semantics as ﬁrst argument, and the prepositional semantics as 27 lex(op,   p sc np sem 1  sem   v form var 2 form   s form scope 3 pred and args  4 ,   p form pred on args  2 , 1         mod    v form var 2 form   s form scope 3 form 4        ). Figure 17: Adverbial semantics for prepositions. second. The INDEX of the VP is the ﬁrst argument of the predicate introduced by the preposition, the semantics of the NP-object of the preposition corresponds to the second argument. In Dutch, temporal NPs can act as verbal modiﬁers: (33) a Ik wil zondag vertrekken I want to leave on Sunday b Ik wil drie januari naar Amsterdam I want to go to Amsterdam on the third of January c Ik wil er uiterlijk drie uur zijn I want to arrive at the latest at three oclock As NPs normally do not have a modiﬁer semantics, there is a unary rule that transforms temporal NPs into modiﬁers (ﬁgure 18). The structure that is modiﬁed is speciﬁed in the MOD feature. The semantic content of the modiﬁer is constructed as if it was a PP with P FORM om (at). The semantic content of the (temporal) NP daughter is plugged into the second position of the argument list of the preposition. 2.6 Constructing updates from QLFs The dialogue manager keeps track of the information provided by the user by maintaining an information state or form (Veldhuijzen van Zanten, 1996). This form is a hierarchical structure, with slots and values for the origin and destination of a connection, for the time at which the user wants to arrive or depart, etc. An example is given in (34a). Each user utterance leads to an update of the information state. An update is an instruction for updating the information in an information state. Updating can mean that new information is added or that given information is conﬁrmed, retracted or corrected. For example, given the information state in (34a), the update in (34b) (which might be the translation of No, I do not want to travel to Leiden but to Abcoude!) leads to the information state in (34c). The  -operator in (34b) indicates that the information within its scope (indicated by square brackets) is to be retracted, and the !-operator indicates a correction. 28   modiﬁer sem   v form var 1 form   s form scope 2 form   p form pred and args  3 ,   p form pred at args  1 , 4           mod    v form var 1 form   s form scope 2 form 3           np nform temp sem 4   Figure 18: Rule mod np to treat temporal noun phrases as modiﬁers. (34) a.  travel   origin   place town groningen moment  at  time  clock hour 3    destination  place town leiden     b. travel.destination. ([ place.town.leiden]; [! place.town.abcoude]) c.  travel   origin   place town groningen moment  at  time clock hour 3   destination  place town abcoude     The result of parsing is a QLF, a linguistically motivated and domain-independent represen- tation of the meaning of a sentence. The translation of a QLF into a domain-speciﬁc update is done by applying translation-rules to the individual parts of a QLF. These translation rules may be context-sensitive. In particular, some parts of the QLF provide the context which determines how other parts are to be translated. For example, the QLF in (35) (corresponding to the phrase leave at four oclock contains two p forms, one for the predicate leave and one for four oclock. The second gives rise to an update expression moment.at.time.clock hour.4. The ﬁrst pro- vides the contextual information that the moment referred to is a departure-time. The translation can therefore be extended to origin.moment.at.time.clock hour.4. There is no linguistic information which indicates that a special update-operator has to be used. In such cases, it is assumed that the information is new, and thus the assert-operator () can be used, giving rise to the translation for the full phrase: origin.moment.at.[ time.clock hour.4]. (35)   p form pred and args    p form pred leave args t expr index e1  , 3   ,   p form pred at args e1, hour(4)      Contextual translation is a powerful technique. For instance, the utterance Groningen Amster- dam gives rise to a conjunctive QLF, containing two term expressions for locations. Translating 29 each of the conjuncts individually would make it impossible to decide whether an origin or des- tination location is being speciﬁed. By translating the conjunction in one step (and assuming that the order of conjuncts corresponds to the order in the utterance), we can resolve the ﬁrst locative to origin and the second to destination. As another example, the adverb graag is ignored in the translation from QLF to update if it occurs as part of a full sentence (ik wil graag naar Amsterdam, I would like to go to Amsterdam), but is translated as yes (i.e. a conﬁrmation of information provided by the system) if it occurs in isolation. Such a translation is motivated by dialogues of the following type: (36) [system:] Dus U wilt van Amsterdam naar Groningen reizen? So you want to travel from Amsterdam to Groningen? [user:] Graag. Please. Similarly, the translation of the negations nee (no) and niet (not) depends on context. If the two occur in isolation, they indicate a denial of information provided by the system. However, if nee is followed by another phrase, say a locative, it signals a correction (37a), whereas if niet is followed by another phrase, it signals a denial (37b). (37) a. Nee, naar Assen (No, to Assen) destination.[!place.assen] b. Niet naar Assen (Not to Assen) destination.[place.assen] It should be noted that the translation of QLFs to updates uses primarily the information pro- vided by NPs, PPs and adverbs. Verbs typically provide the context for translating other parts of the QLF. Also, as quantiﬁcation plays no role in updates, the scope of generalised quantiﬁers can be largely ignored. Thus, we are able to translate QLFs into domain-speciﬁc meanings without resolving quantiﬁer scope. 3 Robust parsing of word-graphs 3.1 Word-graphs The input to the NLP module consists of word-graphs produced by the speech recogniser (Oerder and Ney, 1993). A word-graph is a compact representation for all sequences of words that the speech recogniser hypothesises for a spoken utterance. The states of the graph represent points in time, and a transition between two states represents a word that may have been uttered between the corresponding points in time. Each transition is associated with an acoustic score representing a measure of conﬁdence that the word perceived there was actually uttered. These scores are negative logarithms of probabilities and therefore require addition as opposed to multiplication when two scores are combined. An example of a typical word-graph is given as the ﬁrst graph in ﬁgure 19. At an early stage, the word-graph is normalised to eliminate the pause transitions. Such tran- sitions represent periods of time for which the speech recogniser hypothesises that no words are uttered. After this optimisation, the word-graph contains exactly one start state and one or more ﬁnal states, associated with a score, representing a measure of conﬁdence that the utterance ends at that point. The word-graphs in ﬁgure 19 provide an example. From now on, we will assume word-graphs are normalised in this sense. Below, we refer to transitions in the word-graph using the notation trans(vi, vj, w, a) for a transition from state vi to vj with symbol w and acoustic score a. Let ﬁnal(vi, a) refer to a ﬁnal state vi with acoustic score a. 30 1 2 8222 3 zondag19128 4 1143 5 2374 6 5851 twee5636 7 7008 tussen6752 8 drie14474 9 precies17831 de5594 dertien16768 veertien16869 4402 10 vier13338 vier12208 12 vierde15619 uur4570 1217 11 3535 2330 13 februari27792 februari26622 14 31816 1 3 zondag27350 6 twee5636 7 de6737 tussen6752 8 drie14474 9 dertien17911 precies17831 veertien18012 10 vier18984 12 vierde22395 vier13338 vier12208 vierde15619 uur4570 13 februari31327 februari30122 februari26622 318160 Figure 19: Word-graph and normalized word-graph for the utterance Zondag vier februari (Sunday Februari fourth). The special label  in the ﬁrst graph indicates a pause transition. These transitions are eliminated in the second graph. 31 3.2 Parsing word-graphs The normalized word-graph is parsed by an appropriate parser. Parsing algorithms for strings can be generalized to parse such word-graphs (for some examples cf. van Noord (1995)). In the ideal case, the parser will ﬁnd a path in the word-graph that can be assigned an analysis according to the grammar, such that the path covers the complete time span of the utterance, i.e. the path leads from the start state to a ﬁnal state. The analysis gives rise to an update of the dialogue state, which is then passed on to the dialogue manager. However, often no such paths can be found in the word-graph, due to:  errors made by the speech recognizer,  linguistic constructions not covered in the grammar, and  irregularities in the spoken utterance. Even if no full analysis of the word-graph is possible, it is usually the case that useful infor- mation can be extracted from the word-graph. Consider for example the utterance: (38) Ik wil van van Assen naar Amsterdam I want from from Assen to Amsterdam The grammar will not assign an analysis to this utterance due to the repeated preposition. How- ever, it would be useful if the parser would discover the prepositional phrases van Assen and naar Amsterdam since in that case the important information contained in the utterance can still be recovered. Thus, in cases where no full analysis is possible we would like to fall back on an approach reminiscent of concept spotting. The following proposal implements this idea. Firstly, the grammar is deﬁned in such a way that each maximal projection such as S, NP, PP, etc., can be analysed as a top category. This is well-motivated because utterances very often consist of a single NP or PP (section 3.3). Often, the task of the parser is to discover all instances of the top category from the start state of the word-graph to a ﬁnal state. But in our case, we require that the parser discovers all instances of the top category anywhere in the word-graph, i.e. for all partial paths in the word- graph. This has the desired effect for example (38): both PPs will be found by the parser. Thus we require that the parser ﬁnds all major categories anywhere in the word-graph. If a bottom-up chart parser is used, then we might use the inactive chart items for this purpose. However, since we do not want to be forced to a particular parsing strategy, we have chosen to adopt a different approach. In section 3.4 we show that in a logic programming setting the use of underspeciﬁcation of the state names associated with the top-most goal obtains the desired effect, without loss of efﬁciency. Therefore, after the parser has ﬁnished, we have a word-graph annotated with a number of instances of top categories. For each of these categories we are interested in the word-graph state where this category starts (vi), the word-graph state where this category ends (vj), the sequence of symbols associated with this category (x), the accumulated acoustic score (a), and the qlf (q). Let parsed(vi, vj, x, a, q) refer to such categories. We are interested in paths from the start state to the ﬁnal state consisting of a number of categories and transitions in the word-graph (the latter are called skips). The problem consists in ﬁnding the optimal path, according to a number of criteria. This problem is formalized by deﬁning the annotated word-graph as a directed acyclic graph (section 3.5). The vertices of this graph are the states of the word-graph; the edges are the transitions of the word-graph and the categories found by the parser. The criteria which are used to favor some paths over other paths are expressed as a weight function on the edges of the graph. The criteria we might take into account are discussed in section 3.6. For instance, a typical criterion will favor paths consisting of a small number of categories, and a small number of skips. The case in which the parser found a full analysis from the start state of the word-graph to a ﬁnal state then reduces to a special case: the analysis solely consisting of that category will be favored over sequences of partial analyses. 32 Obviously, it is not a good idea to generate all possible sequences of categories and skips, and then to select the best path from this set: in typical word-graphs there are simply too many dif- ferent paths. If a certain uniformity requirement on weights is met, however, then efﬁcient graph search algorithms are applicable. The particular algorithm implemented in OVIS2, namely a vari- ant of the DAG-SHORTEST-PATH algorithm (Cormen, Leiserson, and Rivest, 1990) is discussed in section 3.7. The criteria used to determine the best path may also include Ngram statistics. It turns out that in those cases some complications arise in the deﬁnition of the annotated word-graph. This is explained in section 3.8. In a previous implementation (Nederhof et al., 1997) we used a version of Dijkstras algorithm. A comparison is presented in section 3.9. Finally, section 3.10 discusses methods in which the parser is applied only to a single path of the word-graph. 3.3 Grammar We require that grammatical analysis ﬁnds all maximal projections anywhere in the input word- graph. This implies that the top category of the grammar should be deﬁned in such a way that it derives each of these maximal projections. For this reason, the grammar contains the declaration: top_category(X) :- X  start. (39) Furthermore, there are unary rules rewriting this start category into each of the relevant max- imal projections. One such rule is: rule(start_np,Start,[Np]) :- Start  start, Np  np, Start:sem  Np:sem. (40) Similar rules are deﬁned for pp, sbar, root, advp, etc. 3.4 Parser Five different parsing algorithms were implemented and compared (a bottom-up Earley parser, an inactive chart parser, an LR parser, a left-corner parser and a head-corner parser). The most efﬁcient parser (both in terms of CPU-time and memory usage) for this application turned out to be a head-corner parser implemented with goal-weakening and selective memoization. The head-corner parser is presented in detail in van Noord (1997a). In order to apply this (or any of the other) parser(s) for robust processing, we use underspec- iﬁcation of the state names for the input parse goal in order to parse the start category anywhere in the word-graph. Normally the parser will be called using a goal such as the following: ?- parse(start(Sem),q0,q16). (41) indicating that we want to ﬁnd a path from state q0 to q16 which can be analysed as a category start(Sem) (a sentence with a semantic representation that is yet to be discovered). If we want to recognize top categories at all positions in the input, then we can simply generalize the parse goal to: ?- parse(start(Sem),_,_). (42) Now it may seem that such an underspeciﬁed goal will dramatically slow down the parser, but this turns out to be a false expectation, at least for the head-corner and left-corner parsers. In fact we have experienced no such decrease in efﬁciency. This can only be understood in the light of the use of memoization: even though we now have a much more general goal, the number of different goals that we need to solve is much smaller. 33 3.5 Annotated word-graph An annotated word-graph is a word-graph annotated with the results of the parser. Such an annotated word-graph is deﬁned with respect to an input word-graph (given by the functions trans and ﬁnal) and with respect to the results of parsing (given by the function parsed). The annotated word-graph is a directed acyclic graph (V, E) where  V is the set of vertices consisting of the states of the word-graph v0 . . . vn, and a new vertex vn1. v0 is the start state. vn1 is the ﬁnal state.  E is the set of edges consisting of: 1. skip edges. For all trans(vi, vj, w, a) there are edges (vi, vj, w, a, ǫ). 2. category edges. For all parsed(vi, vj, x, a, q) there are edges (vi, vj, x, a, q). 3. stopping edges. For all ﬁnal(vi, a) there are edges (vi, vn1, ǫ, a, ǫ). 3.6 Weights The weights that are associated with the edges of the graph can be sensitive to the following factors.  Acoustic score. Obviously, the acoustic score present in the word-graph is an important factor. The acoustic scores are derived from probabilities by taking the negative logarithm. For this reason we aim to minimize this score. If edges are combined, then we have to sum the corresponding acoustic scores.  Number of skips. We want to minimize the number of skips, in order to obtain a prefer- ence for the maximal projections found by the parser. Each time we select a skip edge, the number of skips is increased by 1.  Number of maximal projections. We want to minimize the number of such maximal pro- jections, in order to obtain a preference for more extended linguistic analyses over a series of smaller ones. Each time we select a category edge, this number is increased by 1.  Quality of the QLF in relation to the context. We are experimenting with evaluating the qual- ity of a given QLF in relation to the dialogue context, in particular the question previously asked by the system (Koeling, 1997).  Ngram statistics. We have experimented with bigrams and trigrams. Ngram scores are ex- pressed as negative logarithms of probabilities. This implies that combining Ngram scores requires addition, and that lower scores imply higher probability. The only requirement we make to ensure that efﬁcient graph searching algorithms are ap- plicable is that weights are uniform. This means that a weight for an edge leaving a vertex vi is independent of how state vi was reached. In order to be able to compute with such multidimensional weights, we express weights as tuples c1, . . . , ck. For each cost component ci we specify an initial weight, and we need to specify for each edge the weight of each cost component. To specify how weights are updated if a path is extended, we use the function uw that maps a pair of a multidimensional weight and an edge a to multidimensional weight. 7 Moreover, we need to deﬁne an ordering on such tuples. In order to experiment with different implementations of this idea we refer to such a collection of speciﬁcations as a method. Summarizing, such a weight method is a triple W  ini, uw,  where 7We do not deﬁne a weight function on edges, but we specify how weights are updated if a path is extended, for generality. This approach allows e.g. for the possibility that different cost components employ different operations for combining weights. For example, some cost components may use addition (e.g. for weights which are expressed as negative logarithms derived from probabilities), whereas other cost components may require multiplication (e.g. for probabilities). 34 1. ini is the initial weight; 2. uw is the update weight function; 3.  is an ordering on weights Example: speech method. As a trivial example of such a method, consider the problem of ﬁnding the best path through the word-graph ignoring all aspects but the acoustic scores present in the word-graph. In order to implement a method Wspeech to solve this problem, we deﬁne weights using a unary tuple c. The initial weight is ini  0 and uw is deﬁned as follows: uw(c, (vi, vj, w, a, q))     c  a for skip edges  for category edges c  a for stopping edges (43) Note that we specify an inﬁnite weight for category edges because we want to ignore such edges for this simple method (i.e. we are simply ignoring the results of the parser). We deﬁne an ordering  on such tuples, simply by stating that w  w iff w  w. Example: nlp speech method. A more interesting example is provided by the following method which not only takes into account acoustic scores, but also the number of skip edges and category edges. Weights are expressed as c1, c2, c3, where c1 is the number of skips, c2 is the number of categories, and c3 is the acoustic score. We deﬁne ini  0, 0, 0 and uw is deﬁned as follows. uw(c1, c2, c3, (vi, vj, w, a, q)) :    c1  1, c2, c3  a for skip edges c1, c2  1, c3  a for category edges c1, c2, c3  a for stopping edges (44) Finally, we deﬁne the ordering on such tuples: c1, c2, c3  c 1, c 2, c 3 iff :    c1  c 1 or c1  c 1 and c2  c 2 or c1  c 1 and c2  c 2 and c3  c 3 (45) 3.7 Search algorithm The robustness component can be characterised as a search in the annotated word-graph. The goal of the search is the best path from v0 to vn1. This search reduces to a well-known graph search problem, namely the problem of ﬁnding the shortest path in a directed acyclic graph with uniform weights. We use a variant of the DAG-SHORTEST-PATH algorithm (Cormen, Leiserson, and Rivest, 1990). This algorithm ﬁnds shortest paths for uniformly weighted directed acyclic graphs. The ﬁrst step of the algorithm is a topological sort of the vertices of the graph. It turns out that the state names of the word-graph that we obtain from the speech recogniser are already topologi- cally sorted: state names are integers, and edges always connect to larger integers. The second step of the algorithm maintains an array A which records for each state vk the weight associated with the best path known from v0 to vk. A similar array, P, is used to represent for each state the history of this best path, as a sequence of QLFs (since that is what we want to obtain eventually). The ﬁrst step of the algorithm initialises these arrays such that for each state vi(i  0)A[vi]  , and P[vi]  NIL. For v0 we specify A[v0]  ini and P[v0]  ǫ. After this initialisation phase 35 the algorithm treats each edge of the graph in topologically sorted order of the source vertex, as follows: foreach state vi (in topologically sorted order) do foreach edge (vi, vj, w, a, q) do relax (vi, vj, w, a, q) (46) The function relax is deﬁned on edges and updates the arrays if a better path to a vertex has been found: function relax (vi, vj, w, a, q) if uw(A[vi], (vi, vj, w, a, q))  A[vj] then A[vj] : uw(A[vi], (vi, vj, w, a, q)) P[vj] : P[vi].q (47) When the algorithm ﬁnishes, P[vn1] constitutes the sequence of QLFs associated with the best path found. The weight of this path is given by A[vn1]. This algorithm is efﬁcient. Its running time is O(V  E), where V is the number of vertices and E is the number of edges. Therefore, it can be expected that this part of processing should not decrease parsing efﬁciency too much, since the number of edges is O(V 2).8 For a more detailed account of the correctness and complexity of this algorithm, see Cormen, Leiserson, and Rivest (1990). 9 A simple generalisation of the algorithm has been implemented in order to obtain the N best solutions. In this case we maintain in the algorithm for each vertex the N best paths found so far. Such a generalisation increases the worst-case complexity by only a constant factor, and is very useful for development work. 3.8 Complications for Ngrams In this section we want to extend the nlp speech method to take into account Ngram probabilities. Obviously, we can extend the weight tuple with a further argument which expresses the accu- mulated weight according to the Ngram probabilities. However, a potential problem arises. If we extend a given path by using the transition labeled w, then we want to take into account the probability of reading this word w given the previous N  1 words. However note that in the deﬁnition of the annotated word-graph given above these words are not readily available. Even if we make sure that each path is associated with the last words seen so far, we must be careful that weights remain uniform. The solution to this problem is to alter the deﬁnition of the graph, in such a way that the relevant N  1 words are part of the vertex. If we want to take into account Ngram probabilities (N  2, 3, . . .), then the vertices of the graph will be tuples (v, w1 . . . wN1) where v is a state of the word-graph as before, and w1 . . . wN1 are the previous N 1 words. For example, in the case of bigrams (N  2), vertices are pairs consisting of a word-graph state and a word (the previous word). A number of special symbols yN1 . . . y1 is introduced as beginning-of-sentence markers. The start vertex is now (v0, yN1 . . . y1). The notation x : k is used to refer to the last k elements of x. The annotated word-graph for Ngrams is a weighted graph (V, E) and some ﬁxed N, such that:  V is a set of pairs (v, w1 . . . wN1) where v is a word-graph state and wi are labels in the word-graph. The start vertex is (v0, yN1 . . . y1); the ﬁnal vertex is (vn1, ǫ).  E is the set of edges consisting of: 8This compares well with the O(V 3) complexity which can be obtained for most parsers. 9Note that the algorithm is different from the Viterbi algorithm. The latter algorithm ﬁnds the best path through a possibly cyclic weighted graph, for a given sequence of observed outputs. In the current application we require an algorithm to ﬁnd the best path in an acyclic weighted graph (without an additional observed output sequence). 36 1. skip edges. For all trans(vi, vj, w, a), and all vertices Vi  (vi, x) and Vj  (vj, xw : N  1), there are edges (Vi, Vj, w, a, ǫ). 2. category edges. For all parsed(vi, vj, x2, a, q), and for all vertices Vi  (vi, x1) and Vj  (vj, x1x2 : N  1), there are edges (Vi, Vj, x2, a, q). 3. stopping edges. For all ﬁnal(vi, a) and for all vertices Vi  (vi, x) there are edges (Vi, (vn1, ǫ), ǫ, a, ǫ). Example: nlp speech trigram method. The start state of the graph search now is the vertex (v0, y2y1). Weights are expressed as 4-tuples by extending the triples of the nlp speech method with a fourth component expressing trigram weights. These trigram weights are expressed using negative logarithms of (estimates of) probabilities. Let tri be the function which returns for a given sequence of three words this number. Moreover, the deﬁnition of this function is extended for longer sequences of words in the obvious way by deﬁning tri(w0w1w2x)  tri(w0w1w2)  tri(w1w2x). The initial weight is deﬁned as ini  0, 0, 0, 0. Weights are updated as follows: uw(c1, c2, c3, c4, ((vi, w0w1), (vj, y), x, a, q))     c1  1, c2, c3  a, c4  tri(w0w1x) for skip edges c1, c2  1, c3  a, c4  tri(w0w1x) for category edges c1, c2, c3  a, c4 for stopping edges (48) Finally, we deﬁne an ordering on such tuples. The function total is deﬁned on tuples as fol- lows. Here knlp and kwg are constants. total(c1, c2, c3, c4)  c4  knlp  (c1  c2)  kwg  c3. (49) We then deﬁne the ordering as: c1, c2, c3, c4  c 1, c 2, c 3, c 4 iff total(c1, c2, c3, c4)  total(c 1, c 2, c 3, c 4). (50) 3.9 Comparison with Dijkstras algorithm In a previous version of the implementation we used a generalised version of DIJKSTRAs al- gorithm (Dijkstra, 1959), (Nilsson, 1971), (Cormen, Leiserson, and Rivest, 1990), instead of the DAG-SHORTEST-PATH presented above. Dijkstras algorithm is more general in that it is not re- stricted to acyclic graphs. On the other hand, however, Dijkstras algorithm requires that weights on edges are positive (paths can only get worse if they are extended). A potential advantage of Dijkstras algorithm for our purposes is that the algorithm often does not have to investigate all edges. If edges are relatively expensive to compute, then Dijkstras algorithm might turn out to be faster. For instance, we can obtain a modest increase in efﬁciency by exploiting Dijkstras algorithm if we delay some of the work the parser has to do for some category q, until the robustness component actually has a need for that category q. Since Dijkstras algorithm will not visit every q, the amount of work is reduced. We exploited this in our implementation as follows. The parser works in two phases. In the ﬁrst phase (recognition) no semantic constraints are taken into account (in order to pack all ambiguities). In the second phase semantic constraints are applied. This second phase can then be delayed for some category q until Dijkstras algorithm uses an edge based on q. For a number of categories, therefore, this second phase can be skipped completely. However, we had three reasons for preferring the DAG-SHORTEST-PATH algorithm given above. Firstly, this algorithm is simpler than Dijkstras algorithm. Secondly, negative weights do show up in a number of circumstances. And thirdly, the expected efﬁciency gain was not observed in practice. 37 An example where negative weights may show up is the following. Suppose we deﬁne a method which takes into account Ngram scores but nothing else, i.e. all other sources of infor- mation such as acoustic scores are ignored. It turns out that a straightforward implementation of such a method is non-optimal since it will favour paths in the word-graph consisting of a small number of long words over paths (of the same duration) consisting of a larger number of smaller words, only because more scores have to be added. A simple and effective way to eliminate this effect, is to subtract a constant from each score. However, this subtraction may yield negative numbers. 3.10 Best-ﬁrst methods Rather than integrating parsing and disambiguation of the word-graph as a single procedure, as we proposed above, it is also possible to try to disambiguate the word-graph ﬁrst, and then use the parser to parse the resulting path in the word-graph. We have implemented two versions of this approach. Both versions use the search algorithm discussed above, by applying a method which takes into account the acoustic scores and Ngram scores. One version uses N  2, the other version uses N  3. In section 4 we refer to these two methods as best 1 bigram and best 1 trigram respectively. We have experimented with such methods in order to evaluate the contribution of gram- matical analysis to speech recognition. If, for instance, the integrated method nlp speech trigram performs signiﬁcantly better than best 1 trigram then we can conclude that grammatical analysis improves speech recognition. The results below, however, do not permit this conclusion. 4 Evaluation We present a number of results to indicate how well the NLP component currently performs. In the NWO Priority Programme, two alternative natural language processing modules are devel- oped in parallel: the grammar-based module described here, and a data-oriented (statistical, probabilistic, DOP) module. Both of these modules ﬁt into the system architecture of OVIS. The DOP approach is documented in a number of publications (Scha, 1990; Bonnema, Bod, and Scha, 1997; Bod and Scha, 1997). In order to compare both NLP modules, a formal evaluation has been carried out on 1000 new, unseen, representative word graphs (obtained using the latest version of the speech recognizer). Full details on the evaluation procedure, and all evaluation results, are described elsewhere (van Noord, 1997b; Bonnema, van Noord, and van Zanten, 1998). For these word graphs, annotations were provided by our project partners consisting of the actual sentences (test sentences), and updates (test updates). The Ngram models used by our implementation were constructed on the basis of a corpus of almost 60K user utterances (almost 200K words). Some indication of the difﬁculty of the test-set of 1000 word-graphs is presented in table 1, both for the input word-graphs and for the normalised word-graphs. The table lists the number of transitions, the number of words of the actual utterances, the average number of transitions per word, the average number of words per graph, the average number of transitions per graph, and ﬁnally the maximum number of transitions per graph. The number of transitions per word in the normalized word-graph is an indication of the additional ambiguity that the parser encounters in comparison with parsing of ordinary strings. A further indication of the difﬁculty of this set of word-graphs is obtained if we look at the word and sentence accuracy obtained by a number of simple methods. The string comparison on which sentence accuracy and word accuracy are based is deﬁned by the minimal number of substitutions, deletions and insertions that is required to turn the ﬁrst string into the second (Levenshtein distance d). Word accuracy is deﬁned as 1  d n where n is the length of the actual utterance. 38 Table 1: Characterization of test set (1). graphs transitions words tw wg tg max(tg) input 1000 48215 3229 14.9 3.2 48.2 793 normalized 1000 73502 3229 22.8 3.2 73.5 2943 Table 2: Characterization of test set (2). method WA SA speech 69.8 56.0 possible 90.4 83.7 bigram 69.0 57.4 trigram 73.1 61.8 speech bigram 81.1 73.6 speech trigram 83.9 76.2 The method speech only takes into account the acoustic scores found in the word-graph. The method possible assumes that there is an oracle which chooses a path such that it turns out to be the best possible path. This method can be seen as a natural upper bound on what can be achieved. The methods bigram (trigram) report on a method which only uses a bigram (trigram) language model. The methods speech bigram (speech trigram) use a combination of bigram (tri- gram) statistics and the speech score. 4.1 Efﬁciency Table 3 reports on the efﬁciency of the NLP components for the set of 1000 wordgraphs and test utterances. The ﬁrst two rows present the results for sentences; the remaining rows provide the results for word-graphs. Listed are respectively the average number of milliseconds per input; the maximum number of milliseconds; and the maximum space requirements (per word-graph, in Kbytes). For most word-graphs we used the nlp speech trigram method as described above. For large word-graphs (more than 100 transitions), we ﬁrst selected the best path in the word-graph based on acoustic scores and N-gram scores only. The resulting path was then used as input for the parser. In the case of these large word-graphs, N2 indicates that bigram scores were used, for N3 trigram scores were used. CPU-time includes tokenizing the word-graph, removal of pause transitions, lexical lookup, parsing, the robustnessdisambiguation component, and the production of an update expres- sion. 10 For word-graphs the average CPU-times are actually quite misleading because CPU-times vary enormously for different word-graphs. For this reason, we present in table 4 the proportion of word-graphs (in ) that can be treated by the NLP component within a given amount of CPU- time (in milliseconds). 4.2 Accuracy The results for word accuracy given above provide a measure for the extent to which linguistic processing contributes to speech recognition. However, since the main task of the linguistic com- 10For the grammar-based methods, CPU-time was measured on a HP 9000780 machine running HP-UX 10.20, with SICStus Prolog 3 patch level 3. The statistics for the data-oriented module were obtained on a Silicon Graphics Indigo with a MIPS R10000 processor, running IRIX 6.2. 39 Table 3: Efﬁciency (1). input method mean msec max msec max Kbytes test sentence data-oriented 91 8632 14064 test sentence grammar-based 28 610 524 word graphs data-oriented 7011 648671 619504 word graphs grammar-based N2 298 15880 7143 word graphs grammar-based N3 1614 690800 34341 Table 4: Efﬁciency (2). method 100 500 1000 5000 10000 data-oriented 52.7 70.8 76.6 90.6 94.2 grammar-based N2 58.6 87.0 94.6 99.5 99.8 grammar-based N3 58.5 78.9 87.3 96.7 98.7 ponent is to analyze utterances semantically, an equally important measure is concept accuracy, i.e. the extent to which semantic analysis corresponds with the meaning of the utterance that was actually produced by the user. For determining concept accuracy, we have used a semantically annotated corpus of 10K user responses. Each user response was annotated with an update representing the meaning of the utterance that was actually spoken. The annotations were made by our project partners in Ams- terdam, in accordance with the existing guidelines (Veldhuijzen van Zanten, 1996). Updates take the form described in section 2.5. An update is a logical formula which can be evaluated against an information state and which gives rise to a new, updated information state. The most straightforward method for evaluating concept accuracy in this setting is to compare (the normal form of) the update produced by the grammar with (the normal form of) the annotated update. A major obstacle for this approach, however, is the fact that very ﬁne-grained semantic distinctions can be made in the update-language. While these distinctions are relevant semantically (i.e. in certain cases they may lead to slightly different updates of an information state), they can often be ignored by a dialogue manager. For instance, the two updates below are semantically not equivalent, as the ground-focus distinction is slightly different. user.wants.destination.place.([ town.leiden];[! town.abcoude]) user.wants.destination.([ place.town.leiden];[! place.town.abcoude]) (51) However, the dialogue manager will decide in both cases that this is a correction of the destina- tion town. Since semantic analysis is the input for the dialogue manager, we have measured concept accuracy in terms of a simpliﬁed version of the update language. Inspired by a similar proposal in Boros et al. (1996), we translate each update into a set of semantic units, where a unit in our case is a triple CommunicativeFunction, Slot, Value. For instance, the two examples above both translate as  denial, destination town, leiden   correction, destination town, abcoude  Both the updates in the annotated corpus and the updates produced by the system were trans- lated into semantic units. Semantic accuracy is given in table 5 according to four different deﬁnitions. Firstly, we list the proportion of utterances for which the corresponding semantic units exactly match the semantic units of the annotation (match). Furthermore we calculate precision (the number of correct seman- tic units divided by the number of semantic units which were produced) and recall (the number 40 Table 5: Accuracy. Input Method String accuracy Semantic accuracy WA SA match precision recall CA test sentence data-oriented NA NA 93.0 94.0 92.5 91.6 test sentence grammar-based NA NA 95.7 95.7 96.4 95.0 word-graph data-oriented 76.8 69.3 74.9 80.1 78.8 75.5 word-graph grammar-based N2 82.3 75.8 80.9 83.6 84.8 80.9 word-graph grammar-based N3 84.2 76.6 82.0 85.0 86.0 82.6 of correct semantic units divided by the number of semantic units of the annotation). Finally, following Boros et al. (1996), we also present concept accuracy as CA  100  1  SUS  SUI  SUD SU   where SU is the total number of semantic units in the translated corpus annotation, and SUS, SUI, and SUD are the number of substitutions, insertions, and deletions that are necessary to make the translated grammar update equivalent to the translation of the corpus update. We achieve the results listed in table 5 for the test-set of 1000 word-graphs. String accuracy is presented in terms of word-accuracy (WA) and sentence accuracy (SA). Conclusion The results given above lead to the following conclusions.  Sophisticated grammatical analysis is fast enough for practical spoken dialogue systems.  Moreover, grammatical analysis is effective for the purposes of the present application. Al- most all user utterances can be analysed correctly. This is somewhat surprising. Typically, linguistic ambiguities are a major obstacle for practical NLP systems. The current appli- cation is very simple in the sense that such linguistic ambiguities do not seem to play a signiﬁcant role. The ambiguities introduced by the speech recognizer (as multiple paths in the word-graph) are a far more important problem.  Grammatical analysis does not seem to help much to solve the problem of disambiguating the word-graph. The best method incorporating grammatical analysis performs about as well as a method which solely uses N-gram statistics and acoustic scores for disambiguation of the word-graph. However, in the latter case grammatical analysis of the type proposed here is useful in providing a robust analysis of the best path. We have argued in this paper that sophisticated grammatical analysis in combination with a robust parser can be applied successfully as an ingredient of a spoken dialogue system. Gram- matical analysis is thereby shown to be a viable alternative to techniques such as concept spotting. We showed that for a state-of-the-art application (public transport information system) grammat- ical analysis can be applied efﬁciently and effectively. It is expected that the use of sophisticated grammatical analysis will allow for easier construction of linguistically more complex spoken dialogue systems. 41 Acknowledgements This research was carried out within the framework of the Priority Programme Language and Speech Technology (TST). The TST-Programme is sponsored by NWO (Dutch Organization for Scientiﬁc Research). References [Agnas et al.1994] Agnas, M-S., H. Alshawi, I. Bretan, D. Carter, K. Ceder, M. Collins, R. Crouch, V. Digalakis, B. Ekholm, B. Gamback, J. Kaja, J. Karlgren, B. Lyberg, P. Price, S. Pulman, M. Rayner, C. Samuelsson, and T. Svensson. 1994. Spoken Language Translator: First-year report. Technical report, SICS and SRI Cambridge, Cambridge Computer Research Centre. Available as crc043paper.ps.Z from http:www.cam.sri.comtr. [Allen et al.1996] Allen, J.F., B.W. Miller, E.K. Ringger, and T. Sikorski. 1996. A robust system for natural spoken dialogue. In 34th Annual Meeting of the Association for Computational Linguistics, pages 6270, University of California, Santa Cruz. [Alshawi1992] Alshawi, Hiyan, editor. 1992. The Core Language Engine. ACL-MIT press, Cam- bridge Mass. [Alshawi et al.1992] Alshawi, Hiyan, David Carter, Richard Crouch, Steve Pulman, Manny Rayner, and Arnold Smith. 1992. CLARE: A contextual reasoning and cooperative response framework for the Core Language Engine. Technical report, SRI International, Cambridge Computer Research Centre. Available as cmp-lg9411002 from http:xxx.lanl.govcmp-lg. [Alshawi and Crouch1992] Alshawi, Hiyan and Richard Crouch. 1992. Monotonic semantic in- terpretation. In 30th Annual Meeting of the Association for Computational Linguistics, pages 32 39, Newark, Delaware. [Aust et al.1995] Aust, H., M. Oerder, F. Seide, and V. Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication, 17:249262. [Block1994] Block, Hans Ulrich. 1994. Compiling trace  uniﬁcation grammar. In Tomek Strza- lkowski, editor, Reversible Grammar in Natural Language Processing. Kluwer Academic Publish- ers, Dordrecht, pages 155174. [Bod and Scha1997] Bod, Rens and Remko Scha. 1997. Data-oriented language processing: An overview. Technical Report 38, NWO Priority Programme Language and Speech Technology. [Bonnema, Bod, and Scha1997] Bonnema, Remko, Rens Bod, and Remko Scha. 1997. A DOP model for semantic interpretation. In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguis- tics, Madrid. [Bonnema, van Noord, and van Zanten1998] Bonnema, Remko, Gertjan van Noord, and Gert Veldhuizen van Zanten. 1998. Evaluation results NLP components OVIS2. Tech- nical Report 57, NWO Priority Programme Language and Speech Technology. [Boros et al.1996] Boros, M., W. Eckert, F. Gallwitz, G. Gorz, G. Hanrieder, and H. Niemann. 1996. Towards understanding spontaneous speech: Word accuracy vs. concept accuracy. In Proceed- ings of the Fourth International Conference on Spoken Language Processing (ICSLP 96), Philadel- phia. [Bouma1992] Bouma, Gosse. 1992. Feature structures and nonmonotonicity. Computational Lin- guistics, 18(2). 42 [Briscoe and Carroll1993] Briscoe, Ted and John Carroll. 1993. Generalised probabilistic lr parsing of natural language (corpora) with uniﬁcation-based grammars. Computational Linguistics, 19(1):2560. [Briscoe et al.1987] Briscoe, Ted, Claire Grover, Bran Boguraev, and John Carroll. 1987. A formal- ism and environment for the development of a large grammar of english. In Proceedings of the 10th International Joint Conference on Artiﬁcial Intelligence, pages 703708, Milan. [Carpenter1992] Carpenter, Bob. 1992. Skeptical and creduluous default uniﬁcation with appli- cations to templates and inheritance. In Ted Briscoe, Anne Copestake, and Valerie de Paiva, editors, Default Inheritance within Uniﬁcation-Based Approaches to the Lexicon. Cambridge Uni- versity Press, Cambridge. [Carroll1993] Carroll, John. 1993. Practical uniﬁcation-based parsing of natural language. Ph.D. the- sis, Cambridge University. [Cooper et al.1994] Cooper, Robin, Richard Crouch, Jan van Eijck, Chris Fox, Josef van Genabith, Jan Jaspars, Hans Kamp, Manfred Pinkal, Massimo Poesio, and Stephen Pulman. 1994. Fracas deliverable d8: Describing the approaches. Technical report, Centre For Cognitive Science, Edinburgh. Available by ftp from ftp.cogsci.ed.ac.uk, pubFRACAS. [Cormen, Leiserson, and Rivest1990] Cormen, Leiserson, and Rivest. 1990. Introduction to Algo- rithms. MIT Press, Cambridge Mass. [Davidson1967] Davidson, Donald. 1967. The logical form of action sentences. In Nicholas Rescher, editor, The Logic of Decision and Action. University of Pittsburgh Press. [Dijkstra1959] Dijkstra, E.W. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1:269271. [Frank1994] Frank, Annette. 1994. Verb second by lexical rule or by underspeciﬁcation. Technical report, Institute for Computational Linguistics, Stuttgart. [Gazdar et al.1985] Gazdar, Gerald, Ewan Klein, Geoffrey Pullum, and Ivan Sag. 1985. Generalized Phrase Structure Grammar. Blackwell. [Hukari and Levine1995] Hukari, Thomas E. and Robert D. Levine. 1995. Adjunct extraction. Journal of Linguistics, 31(2):195226. [Jackson et al.1991] Jackson, E., D. Appelt, J. Bear, R. Moore, and A. Podlozny. 1991. A template matcher for robust NL interpretation. In Speech and Natural Language Workshop, pages 190194, Paciﬁc Grove, California, February. [Johnson and Dorre1995] Johnson, Mark and Jochen Dorre. 1995. Memoization of coroutined constraints. In 33th Annual Meeting of the Association for Computational Linguistics, pages 100 107, Boston. [Koeling1997] Koeling, Rob. 1997. Moving on the dialogue game board. In Second Tbilisi Simpo- sium Language, Logic and Computation, Tbilisi State University. [Mellish1988] Mellish, C.S. 1988. Implementing systemic classiﬁcation by uniﬁcation. Computa- tional Linguistics, 14(1):4051. [Moore, Pereira, and Murveit1989] Moore, R., F. Pereira, and H. Murveit. 1989. Integrating speech and natural-language processing. In Speech and Natural Language Workshop, pages 243 247, Philadelphia, Pennsylvania, February. [Nederhof et al.1997] Nederhof, Mark-Jan, Gosse Bouma, Rob Koeling, and Gertjan van No- ord. 1997. Grammatical analysis in the ovis spoken-dialogue system. In Proceedings of the ACLEACL Workshop on Spoken Dialog Systems, pages 6673, Madrid, Spain. 43 [Nerbonne1992] Nerbonne, John. 1992. Constraint-based semantics. In P. Dekker and M. Stokhof, editors, Proceedings of the Eight Amsterdam Colloquium, pages 42544, ITLI Amsterdam. [Netter1992] Netter, Klaus. 1992. On non-head non-movement. In G. Gorz, editor, KONVENS 92. Springer-Verlag. [Nilsson1971] Nilsson, Nils. 1971. Problem Solving Methods in Artiﬁcial Intelligence. McGraw-Hill. [van Noord1995] van Noord, Gertjan. 1995. The intersection of ﬁnite state automata and deﬁnite clause grammars. In 33th Annual Meeting of the Association for Computational Linguistics, pages 159165, MIT Cambridge Mass. Available from http:www.let.rug.nlvannoordpapers. [van Noord1997a] van Noord, Gertjan. 1997a. An efﬁcient implementation of the head corner parser. Computational Linguistics, 23(3):425456. [van Noord1997b] van Noord, Gertjan. 1997b. Evaluation of OVIS2 NLP components. Technical Report 46, NWO Priority Programme Language and Speech Technology. [van Noord and Bouma1994] van Noord, Gertjan and Gosse Bouma. 1994. Adjuncts and the processing of lexical rules. In Proceedings of the 15th International Confer- ence on Computational Linguistics (COLING), pages 250256, Kyoto. Available from http:www.let.rug.nlvannoordpapers. [van Noord and Bouma1997a] van Noord, Gertjan and Gosse Bouma. 1997a. Dutch verb clus- tering without verb clusters. In Patrick Blackburn and Maarten de Rijke, editors, Specifying Syntactic Structures. CSLI Publications  Folli, Stanford, pages 123153. [van Noord and Bouma1997b] van Noord, Gertjan and Gosse Bouma. 1997b. Hdrug, a ﬂexible and extendible development environment for natural language processing. In Proceedings of the EACLACL workshop on Environments for Grammar Development, Madrid. [Oerder and Ney1993] Oerder, Martin and Hermann Ney. 1993. Word graphs: An efﬁcient inter- face between continuous-speech recognition and language understanding. In ICASSP Volume 2, pages 119122. [Pereira and Warren1980] Pereira, Fernando C.N. and David Warren. 1980. Deﬁnite clause gram- mars for language analysis  a survey of the formalism and a comparison with augmented transition networks. Artiﬁcial Intelligence, 13. [Pinkal1995] Pinkal, Manfred. 1995. Logic and Lexicon. Kluwer. [Pollard and Sag1994] Pollard, Carl and Ivan Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago  CSLI. [Pulman1996] Pulman, Steve. 1996. Uniﬁcation encodings of grammatical notations. Computa- tional Linguistics, 22(3):295328. [Sag1997] Sag, Ivan. 1997. English relative clause constructions. Journal of Linguistics. to appear. [Scha1990] Scha, Remko. 1990. Taaltheorie en taaltechnologie; competence en performance. In Computertoepassingen in de Neerlandistiek. Landelijke Vereniging van Neerlandici (LVVN Jaar- boek), Almere. [Veldhuijzen van Zanten1996] Veldhuijzen van Zanten, Gert. 1996. Semantics of update expres- sions. Technical Report 24, NWO Priority Programme Language and Speech Technology. http:odur.let.rug.nl:4321. [Ward1989] Ward, W. 1989. Understanding spontaneous speech. In Speech and Natural Language Workshop, pages 137141, Philadelphia, Pennsylvania, February. 44",
  "40.pdf": "arXiv:cs9906034v1 [cs.CL] 30 Jun 1999 A Uniﬁed Example-Based and Lexicalist Approach to Machine Translation Davide Turcato Paul McFetridge Fred Popowich and Janine Toole Natural Language Laboratory, School of Computing Science, Simon Fraser University 8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada and Gavagai Technology P.O. 374, 3495 Cambie Street, Vancouver, British Columbia, V5Z 4R3, Canada {turk,mcfet,popowich,toole}cs.sfu.ca Abstract We propose an approach to Machine Translation that combines the ideas and methodologies of the Example-Based and Lexicalist theoretical frameworks. The approach has been implemented in a multilingual Machine Translation system. 1 Introduction Human translation is a complex intellectual activity and accordingly Machine Trans- lation (henceforth MT) is a complex scientiﬁc task, involving virtually every aspect of Natural Language Processing. Many approaches have been proposed, each of them in- spired by some insight about translation. Each approach has its own merit, accounting for some aspect of translation better than other approaches, but typically each ap- proachs advantages are countered by weaknesses in other respects. The real challenge is combining diﬀerent approaches and insights into a comprehensive whole. To this end it is important to compare diﬀerent approaches, for two reasons: 1. It is important to see to what extent diﬀerences are substantial or notational. Sometimes diﬀerent approaches look at the same subject from diﬀerent view- points, or use diﬀerent representations, but a formal analysis shows that they are equivalent. This was the case with many formal systems (categorial and phrase structure grammars, ﬁnite state machines and regular grammars, explanation- based generalization and partial evaluation, etc.). In other cases diﬀerences have been demonstrated to be matters of degree (for instance, in the ﬁeld of MT, transfer and interlingua approaches). 2. It is important to see to what extent diﬀerent approaches are mutually exclusive, or whether they can be integrated into one system that encompasses all of them. In this paper we examine the Example-Based and the Lexicalist approaches to MT. Despite the diﬀerences between their paradigms and methods, we argue that: 1. the kind of linguistic resources the two approaches use largely overlap and can be expressed in the same notation; 2. a uniﬁed MT architecture can be proposed that encompasses the methodologies of both approaches. 2 The Two Approaches 2.1 Example-Based MT Arnold et al. (1994:198) outline Example-Based MT (henceforth EBMT) as follows: The basic idea is to collect a bilingual corpus of translation pairs and then use a best match algorithm to ﬁnd the closest example to the source phrase in question. This gives a translation template, which can then be ﬁlled in by word-for-word translation. The paradigm of translation by analogy was ﬁrst introduced by Nagao (1984). In that paper Nagao advocates the use of raw, unanalyzed bilingual data, claiming that linguistic data are more durable than linguistic theories, thus constituting a steadier ground for MT systems. He proposes the use of an unannotated database of examples (possibly collected from a bilingual dictionary) and a set of lexical equivalences simply expressed in terms of word pairs (except for verb equivalences, which are expressed in terms of case frames). The matching process is mainly focused on checking the semantic similarity between the lexical items in the input sentence and the corresponding items in the candidate example. Many variations and extensions to Nagaos ideas followed, under diﬀerent names and acronyms: Example-Based Machine Translation (EBMT, Sumita  Iida 1991), Memory-Based Translation (MBT, Sato  Nagao 1990), Transfer-Driven Machine Trans- lation (TDMT, Furuse  Iida 1992), Case-Based Machine Translation (CBMT, Kitano 1993), etc. There is not always agreement about the usage of such names: for in- stance, some authors use EBMT and MBT interchangeably, while others keep them distinct. However, all these approaches share the basic idea described above. The main directions in which Nagaos original model has been extended are the following: 1. Augment the example database with linguistic annotations and, accordingly, per- form some linguistic analysis on the input before the matching phase. Sato  Nagao (1990) store examples in the form of pairs of word-dependency trees, along with a set of correspondence links. For instance, the English-Japanese pair of sentences in (1) is represented as the Prolog facts in (2): (1) a. He eats vegetables. b. Kare ha yasai wo taberu. (2) ewd_e([e1,[eat,v], [e2,[he,pron]], [e3,[vegetable,n]]]). jwd_e([j1,[taberu,v], [j2,[ha,p], [j3,[kare,pron]]], [j4,[wo,p], [j5,[yasai,n]]]]). clinks([[e1,j1],[e2,j3],[e3,j5]]). Kitano (1993) proposes the annotation of examples with morphological informa- tion for words and then suggests splitting the source and target sentences into segments, i.e. continuous sequences of words. He then proposes to annotate ex- amples with a segment map, i.e. a correspondence between segments in the source and target sentences, in a similar fashion to what Sato  Nagao (1990) do with word-dependency sub-trees. 2. Explicitly store templates in the bilingual database, instead of sentences (Kaji et al. 1992). A template is a sentence where some phrases have been replaced by variables, annotated with linguistic information. For instance: (3) X[PRON] eats Y[NP] - X[PRON] ha Y[NP] wo taberu Templates are learnt from pairs of sentences by parsing them, in order to perform correct replacement of words or phrases with annotated variables, and to obtain cross-linguistic variable-sharing. Furuse  Iida (1992) propose encoding diﬀerent kinds of bilingual correspondences in the database: string-level correspondences (i.e. plain phrase pairs), pattern- level correspondences (pairs of templates containing variables), grammar-level correspondences (pairs of templates containing variable annotated with syntactic categories, like those proposed by Kaji et al.). Moreover, they associate a source expression with several target expressions, each of which is provided with a set of examples that show the contexts in which each target expression can be correctly used. For instance: (4) X wo o-negaishimasu - may I speak to X ((jimukyoku{office}), ...) please give me X ((bangou{number}), ...) where X is the translation of X and each (X{X}) pair in parentheses is an instantiation of such a translation pair. 3. Obtain the translation of a complete sentence by utilizing more than one transla- tion example and combine some fragments of them. For instance, Sato  Nagao (1990) show how to obtain the translation (5), given the examples (6) and (7). (5) He buys a book on international politics  Kare ha kokusaiseiji nitsuite kakareta hon wo kau (6) He buys a notebook  Kare ha nouto wo kau (7) I read a book on international politics  Watashi ha kokusaiseiji nitsuite kakareta hon wo yomu For an input sentence, they construct a matching expression, i.e. a pointer to a translation unit. A translation unit is a word-dependency sub-tree to be found in the example database (the e1, ..., en, j1, ..., jm of example (2)). Such pointers can be optionally followed by a list of commands for dele- tionreplacementadjunctions of nodes dominated by the node pointed to. The replaced or adjoined elements are other matching expressions. For instance, given (2), a matching expression for the sentence (8) might be (9). (8) He eats mashed potatoes (9) [e1,[r,e3,[ex]]] which represents the tree obtained by replacing (r for replace) e3 with ex in e1. In turn, ex is a pointer to a sub-tree for mashed potatoes in some other example. As several matching expressions can be candidates for the same input sentence, Sato  Nagao deﬁne a scoring system for competing translation units, based on their length (the longer, the better) and the semantic similarity between their contexts, i.e. the input sentence and the example from which the translation unit is taken (the more similar, the better). 2.2 Lexicalist MT Lexicalist MT (henceforth LMT) is a variant of the transfer approach to MT. In LMT transfer is a mapping between bags of lexical items, instead of trees (Whitelock 1994). The ﬁrst step of the translation process is the analysis of an input sentence. Such analysis is performed on a purely monolingual basis, independently from considerations of translation direction and language pair. The same kind of declarative grammars are used for parsing and generation. Moreover, grammars tend to follow a lexicalist, sign- based1 approach (Pollard  Sag 1994). Grammar rules are reduced to a small number of general rule schemata. Lexical items are multidimensional signs containing all the information about their modes of combination (subcategorization, head-modiﬁer rela- tions, etc.). As a result of parsing, lexical items are instantiated with indices expressing their interdependencies with other lexical items in the sentence. Transfer is a mapping from a bag of instantiated source lexical items resulting from parsing to a corresponding bag of target lexical items. Bilingual knowledge is reduced to a bilingual lexicon, augmented with cross-linguistic correspondences in the form of equated variables. Transfer is performed by ﬁnding a set of bilingual entries that covers the source bag. The target bag is comprised of the target sides of the selected bilingual entries. Phrasal and idiomatic expressions are accounted for by multi-word bilingual entries, where lexical items on either side have no inherent order and can be discontinuous in the input or output sentence. A schematic bilingual entry is shown in (10), where subscripts represent indices encoding word dependencies. (10) eat :va,b,c  taberu :va,d,e  ha :pd,b  wo :pe,c Generation orders the target lexical items into a grammatical sentence, according to a target grammar and to the constraints expressed by the indices instantiated on target lexical items as a result of transfer. 1Following the Saussurean approach taken in HPSG, we deﬁne signs as structured complexes of phonological, syntactic, semantic, discourse and phrase-structural information (Pollard  Sag 1994:15). 2.3 Comparison It is interesting to note that the introduction of both EBMT and LMT was motivated by the rejection of structural transfer, due to its inadequacy to cope with structurally divergent languages like English and Japanese (Nagao 1984, 179; Whitelock 1994, 343 345). However, the two approaches diﬀer in the way they avoid the recursive traversal of an analysis tree structure in transfer. In EBMT structural transfer is avoided by adopting the following guidelines: 1. Sentence-level correspondences are covered via an explicit stipulation of all such possible correspondences in the bilingual knowledge base. Therefore EBMT ad- vocates a bilingual knowledge base stating equivalences between the maximal translation units, i.e. sentences (or sentence templates). 2. Given such ﬂat structure of the bilingual database, no deep linguistic analysis is required. Transfer is performed by looking up the bilingual database for a suitable match to the input sentence. Linguistic analysis is only performed to the extent it is necessary to eﬀectively perform the template matching. In LMT the following guidelines are adopted: 1. A full linguistic analysis is performed on input sentences. As a result of the lexi- calist, sign-based approach to parsing and the use of indices to represent depen- dencies among lexical items, individual lexical items contain all the information about their structural relationships with the other lexical items. 2. Given that all the information about a sentence structure is stored in lexical items, transfer can be reduced to a mapping of a bag of source lexical items onto a bag of target lexical items. Therefore LMT advocates a bilingual knowledge base stating equivalences between the minimal translation units, i.e. lexical items. Information about word order is also dropped from transfer, as it is considered a monolingual issue, accounted for by the linear precedence constraints expressed in grammar rules. The dependencies expressed by indices are the only information that must be necessarily transferred from source to target lexical items. 3 A Uniﬁed Bilingual Knowledge Base Despite the diﬀerent and somehow antithetic architectures, the kind of bilingual re- sources required by the two approaches tend to converge. We show that it is possible to deﬁne a bilingual knowledge base in such a way that it can serve both approaches. At a formal level, it can be shown that the kind of information used by the two approaches largely overlaps. The information needed for EBMT systems can be ade- quately expressed in a LMT notation. We list here some parallelisms: 1. Case frames as used in EBMT correspond to subcategorization frames in LMT. Therefore an EBMT case frame is equivalent to an LMT verb lexical entry. Aside from word order issues, which will be dealt with later, the same holds for templates where some arguments are left unspeciﬁed, as a comparison between (3) and (10) shows. 2. A comparison between (2) and (10) shows that Sato  Nagaos (1990) word- dependency trees contain the same information as LMT bilingual entries: words, grammatical descriptions, monolingual dependencies, cross-linguistic correspon- dences. 3. TDMT templates correspond to sets of bilingual entries. A bilingual lexicon as used in LMT can adequately represent all the information needed in EBMT. Therefore, we advocate the use of the LMT notation as a theory neutral knowledge representation language that can equally support EBMT and LMT bilingual knowledge bases. The adoption of such notation does not commit one to using one or the other approach. Such a bilingual resource is close in spirit to the kind of Bilingual Knowledge Bank advocated by Sadler  Vendelmans (1990). The neutrality of the proposed notation also relies on the fact the notations seman- tics is underspeciﬁed. The notation is such that it can be interpreted in diﬀerent ways, in developing and using a knowledge base. Particularly, a bilingual entrys source or target side can be interpreted as either a bag or a sequence. In the latter case, word order is relevant in matching some input with a bilingual entry. A further constraint on the matching procedure may be the requirement that input words matching bilingual entry items must be contiguous in the sentence. If both order and contiguity constraints are activated, then a bilingual knowledge base is interpreted as a knowledge base of sentences (or phrases), as in Nagaos (1984) original proposal, or segments, as proposed by Kitano (1993). If the order constraint is activated and the contiguity constraint is dropped, then bilingual entries represent templates. If both constraints are dropped, then bilingual entries represent word-dependency trees or LMT lexical bags. Therefore the same notation can be used with diﬀerent ideas in mind and, to some extent, the same knowledge base can be reused under diﬀerent interpretations. Our experience in large scale bilingual lexical development (English-Spanish) showed that the commitment to a speciﬁc semantics may even be changed after a bilingual knowledge base has been developed, if some conventions in writing entries are observed. We remarked that our lexicographers spontaneously used the obvious convention of writing bilingual entries items in the same order in which words appear in sentences. With some exceptions (for instance, Spanish verbs accompanied by clitic pronouns), the order in which bilingual entry items can appear in sentences turned out to be unique in most cases. This gave us the choice of interpreting our bilingual entries as either bags or sequences, which was an option unforeseen at the beginning. It is also possible to choose a mixed semantics, e.g. interpreting source sides as sequences and target sides as bags. Diﬀerent considerations come into play for diﬀerent languages and translation directions. For instance, the order constraint might be appropriate for a language with a relatively ﬁxed word order, but not for one with a relatively free word order (even more so, when the language at hand is used as a source language). The notation can also be extended to contain explicit place-holders for missing ele- ments, thus resembling templates more closely. For an illustration of such an extension see (Turcato et al. 1997). Besides notation, a second issue is the actual information that the two approaches require of a bilingual knowledge base. As noted above, EBMT tends to require equiv- alences between maximal translation units, i.e. sentences, while LMT tends to require equivalences between minimal units, i.e. lexical items. Such a divergence is actually less dramatic if we take a closer look at the issue. To this end we introduce a distinction between two diﬀerent purposes of an example database. 1. Examples provide information about sentence well-formedness. The required amount of such information is inversely proportional to the amount of linguis- tic analysis performed on the input. At one extreme, we have the case that no analysis is performed. In this case, all the possible sentences should be listed in the bilingual database. The next level is when input words are assigned syn- tactic categories. In this case, sentences in the bilingual database can be either replaced by or used as templates. At the opposite extreme is the case where a complete linguistic analysis is performed. In this case, examples are no longer needed to provide information about well-formedness. Lexical equivalences are suﬃcient. For instance, if we assume that a sentence input is analyzed into a word-dependency tree, then a bilingual example like (2) can be replaced by a set of bilingual lexical entries without any loss of information (provided, as is assumed by a lexicalist approach, that each lexical item contains information about the arguments it subcategorizes for). Therefore a lexicalist approach can be seen as the lower bound on a continuous scale of diﬀerent EBMT approaches, depending on the amount of linguistic analysis performed. 2. Examples provide information about non-compositional translations (e.g. idioms) and contrastive information about diﬀerent ways in which a word is translated in diﬀerent contexts (sense-ambiguous words). This is the case of examples like (4), for instance. This kind of information is equally required by EBMT and LMT systems, regardless of the chosen approach to linguistic analysis, and needs to be expressed in either case by multi-word bilingual entries. To sum up, the required information is the same in EBMT and LMT, to some extent. The extent of the residual diﬀerence is a matter of degree of linguistic analysis performed by the system. 4 A Uniﬁed Architecture Arnold et al. (1994:201) suggest that there is no radical incompatibility between example-based and rule-based approaches, so that the challenge lies in ﬁnding the best combination of techniques from each. Here one obvious possibility is to use traditional rule-based transfer as a fall back, to be used only if there is no complete example-based translation. Rather than proposing a multi-engine approach with a duplication of resources, we propose a single architecture that encompasses the two approaches and integrates the basic tenets of both. A common characteristic of all EBMT approaches is that the translation process is driven by the content of the bilingual knowledge base. The core operation of all such approaches is the match of an input sentence against examples. It is the result of such a match that drives further computation, in terms of calculating similarity, replacing items in the chosen example, combining fragments of diﬀerent examples (this prioritization of transfer is made explicit in approaches like TDMT). On the contrary, in LMT diﬀerent translation steps are clearly separated. As pointed out, parsing is performed on a purely monolingual ground, regardless of the speciﬁc translation ﬂow in which it occurs. We propose a translation architecture that combines the advantages of the two approaches, by using bilingual information to drive the translation process, while preserving the modularity of the system. A bilingual knowledge base as described above is not only a source of bilingual information, but it also encodes a considerable amount of monolingual linguistic in- formation, on either side. A multi-word bilingual entry gives syntactic and semantic information about the analysis of phrasal expressions, collocations and idioms. Even single-word entries give clues about the analysis of lexically ambiguous items. We propose to take advantage of this source of information to drive the parsing process of an input sentence. Given the search space deﬁned by the monolingual lexicon and grammar, the information contained in the bilingual lexicon is used to prioritize certain analyses over others2. A bilingual lexicon lookup before parsing oﬀers a partial analysis (in terms of lexical disambiguation, dependencies and, optionally, word order), which is tried before any other hypothesis supported by the monolingual lexicon and grammar. If, for instance, chart parsing is in use, the example-based approach amounts to prioritizing edges in the chart agenda. Moreover, edges licensed by the same multi- word bilingual entry are assigned a common identiﬁer, so as to ensure that they all fail or succeed together. When several bilingual entries apply, they are prioritized by the cardinality of their side being used. This sorting mechanism implements a kind of elsewhere condition: more speciﬁc entries override more general ones. This device can be regarded as a lexicalist implementation of the scoring mechanism described by Sato  Nagao (1990), according to which longer translation units are preferred over shorter ones. We show how the translation process works with an English-Spanish example: (11) They cut back on investments Lets assume that the English lexicon and the bilingual lexicon contain, respectively, the following (simpliﬁed) entries: back :adva back :na cut :iva,b cut :iva,b,c cut :tva,b,c investment:na on :pa,b 2A similar idea has been proposed by Kinoshita (1998:80) in a diﬀerent theoretical framework. a. back :adva atras :adva b. back :na espalda :na c. cut :iva,b cortar :iva,b d. cut :iva,b,c  back :advc hacer :tva,b,d  economıa :nd e. cut :iva,b,c  back :advc  on :pa,d reducir :tva,b,d f. cut :tva,b,c cortar :tva,b,c g. investment:na inversion :na h. on :pa,b en :pa,b Given such monolingual and bilingual lexical entries, we show below all the pos- sible ways in which the input sentence (11) can be grammatically covered in parsing and correspondingly translated (we omit details about they, which is syntactically un- ambiguous and is dropped in Spanish). The solutions are listed in no speciﬁc order. Note that more than one translation can be given for the same parse, depending on what bilingual entries are used. Conversely, diﬀerent parses can result in the same translation. At the end of each line we also indicate what bilingual entries have been used. cut back on investments iva,b adva pa,c nc  cortan atras en las inversiones {cahg} tva,b,c nc pa,d nd  cortan espalda en las inversiones {fbhg} tva,b,c nc pc,d nd  cortan espalda en las inversiones {fbhg} iva,b,c advc pa,d nd  hacen economıas en las inversiones {dhg} iva,b,c advc pa,d nd  reducen las inversiones {eg} Note that in our speciﬁc example it is irrelevant whether the order and contiguity constraints are enforced on the bilingual lexicon. The parsing strategy we propose ﬁrst tries to ﬁnd a parse consistent with the source side of bilingual entry (e), the longest available. Therefore, assuming that no failure occurs, the ﬁrst translation returned is reducen las inversiones, which is the most correct. If a failure occurs anywhere down the path for all the parses covered by the source side of bilingual entry (e), the source side of bilingual entry (d) is tried next. Therefore, hacen economıas en las inversiones would be the second translation returned. When the bilingual lexicon oﬀers no way of prioritizing among parsing hypotheses, any other available prioritization mechanism can still be used. Also, in using a bilingual lexicon, diﬀerent strategies could be used. For instance, an alternative to using the longest match ﬁrst, would be to look for the cover that uses the fewest number of bilingual entries. As discussed above, Sato  Nagao (1990) also use a second scoring mechanism, based on similarity between the input sentence and the candidate translation units. A lexicalist counterpart of such a mechanism would amount to a word sense disambigua- tion module, provided that senses are associated with words in the bilingual lexicon. In fact, the problem of choosing the right translation for a word or phrase that can be translated in diﬀerent ways amounts to choosing the correct word sense for that word or phrase. This, in turn, is customarily done in the word sense disambiguation litera- ture by looking at the context in which the word or phrase occurs (e.g. Resnik 1995; Yarowsky 1995), thus paralleling Sato  Nagaos (1990) idea. Although we have not implemented any such word sense disambiguation module, it would be straightforward to incorporate such a module in the system architecture without aﬀecting the other modules. As for associating senses with lexical items in the bilingual lexicon, a method for automatically selecting and ranking bilingual entries (unannotated for sense), based on an input words sense and context, has been proposed by Sanﬁlippo  Steinberger (1997). Besides the monolingual considerations discussed above, this approach also has the advantage of biasing parsing towards analyses that are supported by the bilingual lexicon. An analysis, even a correct one, is useless if the transfer component does not have the means to map it onto a target representation. Therefore it is a practical choice to prioritize those analyses that are amenable to a successful transfer. The proposed approach, which has been incorporated into a large scale MT system (Popowich et al. 1997), does not aﬀect the results provided by the system, it only aﬀects the order in which they are provided. Of course, if a system returns the ﬁrst solution found, the system behavior indeed changes. 5 Conclusion A knowledge representation format and a system architecture have been proposed that allow an eﬀective integration of Example-Based and Lexicalist approaches to MT into a uniﬁed approach, which we call Example-Based Lexicalist Machine Translation (EBLMT). This approach combines the advantages of each approach. From the point of view of LMT, it uses bilingual knowledge to drive parsing, providing additional in- formation to solve syntactic ambiguities and prioritizing the parsing agenda in a more eﬃcient way. From the point of view of an EBMT system like (Sato  Nagao 1990), for instance, it allows the removal of the bilingual databases redundancy coming from the overlap of examples. Moreover, the ﬂexibility of the knowledge representation format and the modularity of the architecture allow a system to work in diﬀerent modalities, by simply setting some system parameters. Acknowledgements This research was supported by a Collaborative Research and Development Grant from the Natural Sciences and Engineering Research Council of Canada (NSERC), and by the BC Advanced Systems Institute (ASI). References Arnold, D., L. Balkan, R. Lee Humphreys, S. Meijer  L. Sadler: 1994, Machine Translation. An Introductory Guide, Manchester-Oxford: NCC Blackwell. Furuse, Osamu  Hitoshi Iida: 1992, Cooperation between transfer and analysis in example- based framework, in Proceedings of the Fifteenth [sic] International Conference on Com- putational Linguistics (COLING-92), Nantes, France, pp. 645651. Kaji, Hiroyuki, Yuuko Kida  Yasutsugu Morimoto: 1992, Learning translation templates from bilingual text, in Proceedings of the Fifteenth [sic] International Conference on Compu- tational Linguistics (COLING-92), Nantes, France, pp. 672678. Kinoshita, Jorge: 1998, A transfer dictionary for words and bigrams, in Flavio Moreira de Oliveira, ed., Advances in Artiﬁcial Intelligence  14th Brazilian Symposium on Ar- tiﬁcial Intelligence, SBIA98, Porto Alegre, Brazil, November 46, 1998  Proceedings, Berlin: Springer, pp. 7382. Kitano, Hiroaki: 1993, A comprehensive and practical model of memory-based Machine Trans- lation, in Proceedings of the 13th International Joint Conference on Artiﬁcial Intelligence, Chambery, France, pp. 12761282. Nagao, Makoto: 1984, A framework of a mechanical translation between Japanese and English by analogy principle, in Alick Elithorn  Ranan Banerji, eds., Artiﬁcial and Human Intelligence: edited review papers at the International NATO Symposium on Artiﬁcial and Human Intelligence sponsored by the Special Programme Panel held in Lyon, France October, 1981, Amsterdam, North-Holland: Elsevier Science Publishers, chap. 11, pp. 173180. Pollard, Carl  Ivan Sag: 1994, Head-driven Phrase Structure Grammar, Stanford University, CA: Centre for the Study of Language and Information. Popowich, Fred, Davide Turcato, Olivier Laurens, Paul McFetridge, J. Devlan Nicholson, Patrick McGivern, Maricela Corzo-Pena, Lisa Pidruchney  Scott MacDonald: 1997, A lexicalist approach to the translation of colloquial text, in Proceedings of the 7th Interna- tional Conference on Theoretical and Methodological Issues in Machine Translation, Santa Fe, New Mexico, USA, pp. 7686. Resnik, Philip: 1995, Disambiguating noun groupings with respect to Wordnet senses, in Proceedings of the Third Workshop on Very Large Corpora, Cambridge, Massachusetts, USA, pp. 5468. Sadler, Victor  Ronald Vendelmans: 1990, Pilot implementation of a bilingual knowledge bank, in Proceedings of the 13th International Conference on Computational Linguistics (COLING-90), Helsinki, Finland, pp. 449451. Sanﬁlippo, Antonio  Ralf Steinberger: 1997, Automatic selection and ranking of transla- tion candidates, in Proceedings of the 7th International Conference on Theoretical and Methodological Issues in Machine Translation, Santa Fe, New Mexico, USA, pp. 200207. Sato, Satoshi  Makoto Nagao: 1990, Toward memory-based translation, in Proceedings of the 13th International Conference on Computational Linguistics (COLING-90), Helsinki, Finland, vol. 3, pp. 247252. Sumita, Eiichiro  Hitoshi Iida: 1991, Experiments and prospects of example-based Machine Translation, in Proceedings of the 29th Annual Meeting of the Association for Computa- tional Linguistics (ACL-91), Berkeley, CA, USA, pp. 185192. Turcato, Davide, Olivier Laurens, Paul McFetridge  Fred Popowich: 1997, Inﬂectional in- formation in transfer for lexicalist MT, in Proceedings of the International Conference Recent Advances in Natural Language Processing (RANLP-97), Tzigov Chark, Bulgaria, pp. 98103. Whitelock, Pete: 1994, Shake and bake translation, in C.J. Rupp, M.A. Rosner  R.L. John- son, eds., Constraints, Language and Computation, London: Academic Press, pp. 339359. Yarowsky, David: 1995, Unsupervised word sense disambiguation rivaling supervised methods, in Proceedings of the 33th Annual Meeting of the Association for Computational Linguistics (ACL-95), Cambridge, Massachusetts, USA, pp. 189196.",
  "41.pdf": "arXiv:cs9907003v1 [cs.CL] 5 Jul 1999 Annotation Graphs as a Framework for Multidimensional Linguistic Data Analysis Steven Bird and Mark Liberman Linguistic Data Consortium, University of Pennsylvania 3615 Market St, Philadelphia, PA 19104-2608, USA {sb,myl}ldc.upenn.edu Abstract In recent work we have presented a formal framework for linguistic annotation based on labeled acyclic digraphs. These annotation graphs oﬀer a simple yet powerful method for representing complex annotation structures incorporating hierarchy and overlap. Here, we motivate and illustrate our approach using discourse-level annotations of text and speech data drawn from the CALLHOME, COCONUT, MUC-7, DAMSL and TRAINS annotation schemes. With the help of domain specialists, we have constructed a hybrid multi-level annotation for a fragment of the Boston University Radio Speech Corpus which includes the following levels: segment, word, breath, ToBI, Tilt, Treebank, coreference and named entity. We show how annotation graphs can represent hybrid multi-level structures which derive from a diverse set of ﬁle formats. We also show how the approach facilitates substantive comparison of multiple annotations of a single signal based on diﬀerent theoretical models. The discussion shows how annotation graphs open the door to wide-ranging integration of tools, formats and corpora. 1 Annotation Graphs When we examine the kinds of speech transcription and annotation found in many existing communi- ties of practice, we see commonality of abstract form along with diversity of concrete format. Our survey of annotation practice (Bird and Liberman, 1999) attests to this commonality amidst diversity. (See [www.ldc.upenn.eduannotation] for pointers to online material.) We observed that all annotations of recorded linguistic signals require one unavoidable basic action: to associate a label, or an ordered sequence of labels, with a stretch of time in the recording(s). Such annotations also typically distin- guish labels of diﬀerent types, such as spoken words vs. non-speech noises. Diﬀerent types of annota- tion often span diﬀerent-sized stretches of recorded time, without necessarily forming a strict hierarchy: thus a conversation contains (perhaps overlapping) conversational turns, turns contain (perhaps inter- rupted) words, and words contain (perhaps shared) phonetic segments. Some types of annotation are systematically incommensurable with others: thus disﬂuency structures (Taylor, 1995) and focus struc- tures (Jackendoﬀ, 1972) often cut across conversa- tional turns and syntactic constituents. A minimal formalization of this basic set of prac- tices is a directed graph with ﬁelded records on the arcs and optional time references on the nodes. We have argued that this minimal formalization in fact has suﬃcient expressive capacity to encode, in a reasonably intuitive way, all of the kinds of linguis- tic annotations in use today. We have also argued that this minimal formalization has good properties with respect to creation, maintenance and searching of annotations. We believe that these advantages are especially strong in the case of discourse anno- tations, because of the prevalence of cross-cutting structures and the need to compare multiple anno- tations representing diﬀerent purposes and perspec- tives. Translation into annotation graphs does not mag- ically create compatibility among systems whose semantics are diﬀerent. For instance, there are many diﬀerent approaches to transcribing ﬁlled pauses in English  each will translate easily into an annota- tion graph framework, but their semantic incompati- bility is not thereby erased. However, it does enable us to focus on the substantive diﬀerences without having to be concerned with diverse formats, and without being forced to recode annotations in an agreed, common format. Therefore, we focus on the structure of annotations, independently of domain- speciﬁc concerns about permissible tags, attributes, and values. As reference corpora are published for a wider range of spoken language genres, annotation work is increasingly reusing the same primary data. For instance, the Switchboard corpus [www.ldc.upenn.eduCatalogLDC93S7.html] has been marked up for disﬂuency (Taylor, 1995). See [www.cis.upenn.edutreebankswitchboard- sample.html] for an example, which also includes a separate part-of-speech annotation and a Treebank- style annotation. Hirschman and Chinchor (1997) give an example of MUC-7 coreference annotation applied to an existing TRAINS dialog annotation marking speaker turns and overlap. We shall encounter a number of such cases here. The Formalism As we said above, we take an annotation label to be a ﬁelded record. A minimal but suﬃcient set of ﬁelds would be: type this represents a level of an annotation, such as the segment, word and discourse levels; label this is a contentful property, such as a par- ticular word, a speakers name, or a discourse function; class this is an optional ﬁeld which permits the arcs of an annotation graph to be co-indexed as members of an equivalence class.1 One might add further ﬁelds for holding comments, annotator id, update history, and so on. Let T be a set of types, L be a set of labels, and C be a set of classes. Let R  {t, l, c  t  T, l  L, c  C}, the set of records over T, L, C. Let N be a set of nodes. Annotation graphs (AGs) are now deﬁned as follows: Deﬁnition 1 An annotation graph G over R, N is a set of triples having the form n1, r, n2, r  R, n1, n2  N, which satisﬁes the following conditions: 1. N, {n1, n2  n1, r, n2  A} is a labelled acyclic digraph. 2. τ : N  ℜ is an order-preserving map assigning times to (some of) the nodes. For detailed discussion of these structures, see (Bird and Liberman, 1999). Here we present a frag- ment (taken from Figure 8 below) to illustrate the deﬁnition. For convenience the components of the ﬁelded records which decorate the arcs are separated using the slash symbol. The example contains two word arcs, and a discourse tag encoding inﬂuence on speaker. No class ﬁelds are used. Not all nodes have a time reference. 1 52.46 2 Woh 3 53.14 DIOS:Commit Wokay 1 We have avoided using explicit pointers since we prefer not to associate formal identiﬁers to the arcs. Equivalence classes will be exempliﬁed later. The minimal annotation graph for this structure is as follows: T  {W, D} L  {oh, okay, IOS:Commit} C   N  {1, 2, 3} τ  {1, 52.46, 3, 53.14} A     1, Woh, 2 , 2, Wokay, 3 , 1, DIOS:Commit, 3    XML is a natural surface representation for annotation graphs and could provide the primary exchange format. A particularly simple XML encoding of the above structure is shown below; one might choose to use a richer XML encoding in practice. annotation arc begin id1 time52.46 label type\"W\" name\"oh\" end id2 arc arc begin id2 label type\"W\" name\"okay\" end id3 time53.14 arc arc begin id1 time52.46 label type\"D\" name\"IOS:Commit\" end id3 time53.14 arc annotation 2 AGs and Discourse Markup 2.1 LDC Telephone Speech Transcripts The LDC-published CALLHOME corpora include digital audio, transcripts and lexicons for telephone conversations in several languages, and are designed to support research on speech recognition [www.ldc.upenn.eduCatalogLDC96S46.html]. The transcripts exhibit abundant overlap between speaker turns. What follows is a typical fragment of an annotation. Each stretch of speech consists of a begin time, an end time, a speaker designation, and the transcription for the cited stretch of time. We have augmented the annotation with  and  to indicate partial and total overlap (respectively) with the previous speaker turn. 15 16 Wand 31 994.19 32 994.46 speakerB Wyeah 17 994.65 Wum 33 996.51 19 20 996.59 Wum 35 997.61 speakerB 34 Wwhatevers 22 23 W. 11 991.75 12 speakerA 13 Whe 14 Wsaid W, 18 995.21 Whe speakerA 21 997.40 Wright 25 1002.55 speakerA 24 Wso Whelpful Figure 1: Graph Structure for LDC Telephone Speech Example speaker speaker W W .61 .19 .46 .65 .21 .51 .40 .59 , A he and um said B yeah A so . right B helpful whatevers A 995 994 996 997 um right he so . Figure 2: Visualization for LDC Telephone Speech Example 962.68 970.21 A: He was changing projects every couple of weeks and he said he couldnt keep on top of it. He couldnt learn the whole new area  968.71 969.00 B: mm. 970.35 971.94 A: that fast each time.  971.23 971.42 B: mm. 972.46 979.47 A: um, and he says he went in and had some tests, and he was diagnosed as having attention deficit disorder. Which 980.18 989.56 A: you know, given how hes how far hes gotten, you know, he got his degree at Tufts and all, I found that surprising that for the first time as an adult theyre diagnosing this. um  989.42 991.86 B: mm. I wonder about it. But anyway.  991.75 994.65 A: yeah, but thats what he said. And um  994.19 994.46 B: yeah. 995.21 996.59 A: He um  996.51 997.61 B: Whatevers helpful.  997.40 1002.55 A: Right. So he found this new job as a financial consultant and seems to be happy with that. 1003.14 1003.45 B: Good. Long turns (e.g. the period from 972.46 to 989.56 seconds) were broken up into shorter stretches for the convenience of the annotators and to provide additional time references. A section of this anno- tation which includes an example of total overlap is represented in annotation graph form in Figure 1, with the accompanying visualization shown in Fig- ure 2. (We have no commitment to this particular visualization; the graph structures can be visualized in many ways and the perspicuity of a visualization format will be somewhat domain-speciﬁc.) The turns are attributed to speakers using the speaker type. All of the words, punctuation and disﬂuencies are given the W type, though we could easily opt for a more reﬁned version in which these are assigned diﬀerent types. The class ﬁeld is not used here. Observe that each speaker turn is a dis- joint piece of graph structure, and that hierarchical organisation uses the chart construction (Gazdar and Mellish, 1989, 179ﬀ). Thus, we make a logi- cal distinction between the situation where the end- points of two pieces of annotation necessarily coin- cide (by sharing the same node) from the situation where endpoints happen to coincide (by having dis- tinct nodes which contain the same time reference). The former possibility is required for hierarchical structure, and the latter possibility is required for overlapping speaker turns where words spoken by diﬀerent speakers may happen to sharing the same boundary. 2.2 Dialogue Annotation in COCONUT The COCONUT corpus is a set of dialogues in which the two conversants collaborate on a task of deciding what furniture to buy for a house (Di Eugenio et al., 1998). The coding scheme augments the DAMSL scheme (Allen and Core, 1997) by having some new top-level tags and by further specifying some exist- ing tags. An example is given in Figure 3. The example shows ﬁve utterance pieces, identi- ﬁed (a-e), four produced by speaker S1 and one pro- duced by speaker S2. The discourse annotations can be glossed as follows: Accept - the speaker is agreeing to a possible action or a claim; Commit - the speaker potentially commits to intend to perform a future speciﬁc action, and the commitment is not contin- gent upon the assent of the addressee; Offer - the speaker potentially commits to intend to perform a future speciﬁc action, and the commitment is contin- gent upon the assent of the addressee; Open-Option - the speaker provides an option for the addressees future action; Action-Directive - the utterance is designed to cause the addressee to undertake a spe- ciﬁc action. In utterance (e) of Figure 3, speaker S1 simul- taneously accepts to the meta-action in (d) of not Accept, Commit S1: (a) Lets take the blue rug for 250, (b) my rug wouldnt match Open-Option (c) which is yellow for 150. Action-Directive S2: (d) we dont have to match... Accept(d), Offer, Commit S1: (e) well then lets use mine for 150 Figure 3: Dialogue with COCONUT Coding Scheme D well then lets use mine for 150 e we dont have to match ... d Lets take the blue rug for 250 , a which is yellow for 150 . c Accept d my rug wouldnt match b Commit Action-Directive Open-Option Offer Commit Accept Sp S1 S2 S1 Utt Figure 4: Visualization of Annotation Graph for COCONUT Example having matching colors, and to the regular action of using S1s yellow rug. The latter acceptance is not explicitly represented in the original notation, so we shall only consider the former. In representing this dialogue structure using anno- tation graphs, we will be concerned to achieve the following: (i) to treat multiple annotations of the same utterance fragment as an unordered set, rather than a list, to simplify indexing and query; (ii) to explicitly link speaker S1 to utterances (a-c); (iii) to formalize the relationship between Accept(d) and utterance (d); and (iv) formalize the rest of the annotation structure which is implicit in the textual representation. We adopt the types Sp (speaker), Utt (utterance) and D (discourse). A more reﬁned type system could include other levels of representation, it could distinguish forward versus backward communicative function, and so on. For the names we employ: speaker identiﬁers S1, S2; discourse tags Offer, Commit, Accept, Open-Option, Action-Directive; and orthographic strings representing the utterances. For the classes (the third, optional ﬁeld) we employ the utterance identiﬁers a, b, c, d, e. An annotation graph representation of the COCONUT example can now be represented as in Figure 4. The arcs are structured into three layers, one for each type, where the types are written on the left. If the optional class ﬁeld is speciﬁed, this information follows the name ﬁeld, separated by a slash. The Acceptd arc refers to the S2 utterance simply by virtue of the fact that both share the same class ﬁeld. Observe that the Commit and Accept tags for (a) are unordered, unlike the original annotation. and that speaker S1 is associated with all utterances (a- c), rather than being explicitly linked to (a) and implicitly linked to (b) and (c) as in Figure 3. To make the referent of the Accept tag clear, we make use of the class ﬁeld. Recall that the third component of the ﬁelded records, the class ﬁeld, per- mits arcs to refer to each other. Both the referring and the referenced arcs are assigned to equivalence class d. 2.3 Coreference Annotation in MUC-7 The MUC-7 Message Understanding Conference speciﬁed tasks for information extraction, named entity and coreference. Coreferring expressions are to be linked using SGML markup with ID and REF tags (Hirschman and Chinchor, 1997). Figure 5 is a sample of text from the Boston University Radio Speech Corpus [www.ldc.upenn.eduCatalogLDC96S36.html], marked up with coreference tags. (We are grateful to Lynette Hirschman for providing us with this annotation.) Noun phrases participating in coreference are wrapped with coref...coref tags, which can bear the attributes ID, REF, TYPE and MIN. Each such phrase is given a unique identiﬁer, which may be referenced by a REF attribute somewhere else. Our example contains the following references: 3  2, 4  2, 6  5, 7  5, 8  5, 12  11, 15  13. The TYPE attribute encodes the relationship between the anaphor and the antecedent. Currently, only the identity relation is marked, and so coreferences form an equivalence class. Accordingly, our example contains the following equivalence classes: {2, 3, 4}, {5, 6, 7, 8}, {11, 12}, {13, 15}. In our AG representation we choose the ﬁrst num- ber from each of these sets as the identiﬁer for the equivalence class. MUC-7 also contains a speciﬁca- tion for named entity annotation. Figure 7 gives an example, to be discussed in 3.2. This uses empty COREF ID\"2\" MIN\"woman\" This woman COREF receives three hundred dollars a month under COREF ID\"5\" General Relief COREF , plus COREF ID\"16\" MIN\"four hundred dollars\" four hundred dollars a month in COREF ID\"17\" MIN\"benefits\" REF\"16\" A.F.D.C. benefits COREF COREF for COREF ID\"9\" MIN\"son\" COREF ID\"3\" REF\"2\" her COREF son COREF , who is COREF ID\"10\" MIN\"citizen\" REF\"9\" a U.S. citizen COREF. COREF ID\"4\" REF\"2\" She COREF s among COREF ID\"18\" MIN\"aliens\" an estimated five hundred illegal aliens on COREF ID\"6\" REF\"5\" General Relief COREF out of COREF ID\"11\" MIN\"population\" COREF ID\"13\" MIN\"state\" the state COREF s total illegal immigrant population of COREF ID\"12\" REF\"11\" one hundred thousand COREF COREF COREF . COREF ID\"7\" REF\"5\" General Relief COREF is for needy families and unemployable adults who dont qualify for other public assistance. Welfare Department spokeswoman Michael Reganburg says COREF ID\"15\" MIN\"state\" REF\"13\" the state COREF will save about one million dollars a year if COREF ID\"20\" MIN\"aliens\" REF\"18\" illegal aliens COREF are denied COREF ID\"8\" REF\"5\" General Relief COREF . Figure 5: Coreference Annotation for BU Example 2 0.32 3 0.62 woman 13 7.06 14 7.19 her CR2 15 7.62 CRson9 4 2.74 6 3.80 CR5 5 3.28 General son 7 4.31 plus 16 7.83 who 8 4.52 9 4.80 hundred 17 7.97 is 12 6.87 for 19 8.40 20 8.96 citizen 1 0.0 This CRwoman2 receives...in Relief four CRfour hundred dollars16 10 5.61 dollars...in CRbenefits16 11 6.34 A.F.D.C. benefits 18 8.02 a CRcitizen9 U.S. Figure 6: Annotation Graph for Coreference Example tags to get around the problem of cross-cutting hier- archies. This problem does not arise in the annota- tion graph formalism; see (Bird and Liberman, 1999, 2.7). 3 Hybrid Annotations There are many cases where a given corpus is anno- tated at several levels, from discourse to phonetics. While a uniform structure is sometimes imposed, as with Partitur (Schiel et al., 1998), established practice and existing tools may give rise to corpora transcribed using diﬀerent formats for diﬀerent lev- els. Two examples of hybrid annotation will be dis- cussed here: a TRAINSDAMSL annotation, and an eight-level annotation of the Boston University Radio Speech Corpus. 3.1 DAMSL annotation of TRAINS The TRAINS corpus (Heeman and Allen, 1993) is a collection of about 100 dialogues containing a total of 5,900 speaker turns [www.ldc.upenn.eduCatalog LDC95S25.html]. Part of a transcript is shown below, where s and u designate the two speakers, sil denotes silent periods, and  denotes boundaries of speaker overlaps. utt1 : s: hello sil can I help you utt2 : u: yes sil um sil I have a problem here utt3 : I need to transport one tanker of orange juice to Avon sil and a boxcar of bananas to Corning sil by three p.m. utt4 : and I think its midnight now utt5 : s: uh right its midnight utt6 : u: okay so we need to sil um get a tanker of OJ to Avon is the first thing we need to do utt7 :  so  utt8 : s:  okay  utt9 : click so we have to make orange juice first utt10 : u: mm-hm sil okay so were gonna pick up sil an engine two sil from Elmira utt11 : go to Corning sil pick up the tanker utt12 : s: mm-hm utt13 : u: go back to Elmira sil to get sil pick up the orange juice utt14 : s: alright sil um well sil we also need to make the orange juice sil so we need to get  oranges sil to Elmira  utt15 : u:  oh we need to pick up  oranges oh  okay  utt16 : s:  yeah  utt17 : u: alright so sil engine number two is going to pick up a boxcar Accompanying this transcription are a number of xwaves label ﬁles containing time-aligned word-level and segment-level transcriptions. Below, the start of ﬁle speaker0.words is shown on the left, and the start of ﬁle speaker0.phones is shown on the right. The ﬁrst number gives the ﬁle oﬀset (in seconds), and the middle number gives the label color. The ﬁnal part This woman receives b_numex TYPE\"MONEY\" three hundred dollars e_numex a month under General Relief, plus b_numex TYPE\"MONEY\" four hundred dollars e_numex a month in A.F.D.C. benefits for her son, who is a b_enamex TYPE\"LOCATION\" U.S. e_enamex citizen. Shes among an estimated five hundred illegal aliens on General Relief out of the states total illegal immigrant population of one hundred thousand. General Relief is for needy families and unemployable adults who dont qualify for other public assistance. b_enamex TYPE\"ORGANIZATION\" Welfare Department e_enamex spokeswoman b_enamex TYPE\"PERSON\" Michael Reganburg e_enamex says the state will save about b_numex TYPE\"MONEY\" one million dollars e_numex a year if illegal aliens are denied General Relief. Figure 7: Named Entity Annotation for BU Example is a label for the interval which ends at the speci- ﬁed time. Silence is marked explicitly (again using sil) so we can infer that the ﬁrst word hello occu- pies the interval [0.110000, 0.488555]. Evidently the segment-level annotation was done independently of the word-level annotation, and so the times do not line up exactly. 0.110000 122 sil 0.100000 122 sil 0.488555 122 hello 0.220000 122 hh 0.534001 122 sil 0.250000 122 eh ; 0.640000 122 can 0.330000 122 l 0.690000 122 I 0.460000 122 ow1 0.930000 122 help 0.530000 122 k 1.068003 122 you 0.570000 122 ih 14.670000 122 sil 0.640000 122 n 14.920000 122 uh 0.690000 122 ay 15.188292 122 right 0.760000 122 hh The TRAINS annotations show the presence of backchannel cues and overlap. An example of over- lap is shown below: 50.130000 122 sil 50.260000 122 so 50.330000 122 we 50.480000 122 need 50.540000 122 to 50.651716 122 get 51.094197 122 sil 51.306658 122 oh 51.360000 122 oranges 51.410000 122 we 51.470000 122 sil 51.540000 122 to 51.560000 122 need 51.620000 122 to 51.850000 122 pick 51.975728 122 Elmira 52.020000 122 up 52.470000 122 oranges 52.666781 122 oh 52.807837 76 sil 52.940000 122 okay 53.047996 76 yeah 53.535600 122 sil 53.785600 122 alright 54.303529 122 so As seen in Figure 2 and explained more fully in (Bird and Liberman, 1999), overlap carries no impli- cations for the internal structure of speaker turns or for the position of turn-boundaries. Now, independently of this annotation there is also a dialogue annotation in DAMSL, as shown in Figure 8. Here, a dialog is broken down into turns and thence into utterances, where the tags contain discourse-level annotation. In representing this hybrid annotation as an AG we are motivated by the following concerns. First, we want to preserve the distinction between the TRAINS and DAMSL components, so that they can remain in their native formats (and be manipulated by their native tools) and be converted indepen- dently to AGs then combined using AG union, and so that they can be projected back out if necessary. Second, we want to identify those boundaries that necessarily have the same time reference (such as the end of utterance 17 and the end of the word Elmira), and represent them using a single graph node. Contributions from diﬀerent speakers will remain disconnected in the graph structure. Finally, we want to use the equivalence class names to allow cross-references between utterances. A fragment of the proposed annotation graph is depicted using our visualization format in Figure 9. Observe that, for brevity, some discourse tags are not represented, and the phonetic segment level is omitted. Note that the tags in Figure 8 have the form of ﬁelded records and so, according to the AG deﬁni- tion, all the attributes of a tag could be put into a single label. We have chosen to maximally split such records into multiple arc labels, so that search predicates do not need to take account of inter- nal structure, and to limit the consequences of an erroneous code. A relevant analogy here is that of pre-composed versus compound characters in Uni- code. The presence of both forms of a character in a text raises problems for searching and collating. This problem is avoided through normalization, and this is typically done by maximally decomposing the characters. 3.2 Multiple annotations of the BU corpus Linguistic analysis is always multivocal, in two senses. First, there are many types of entities and Dialog Idd92a-2.2 Annotation-date\"08-14-97\" Annotator\"Reconciled Version\" Speech\"d92a-2.2dialog.fea\" StatusVerified ... Turn IdT9 Speaker\"s\" Speech\"-s 44.853889 -e 52.175728\" ... Utt Idutt17 AgreementNone Influence-on-listenerAction-directive Influence-on-speakerCommit Info-levelTask Response-to\"\" Speech\"-s 45.87 -e 52.175728\" StatementAssert [sil] um well [sil] we also need to make the orange juice [sil] so we need to get  oranges [sil] to Elmira  Turn IdT10 Speaker\"u\" Speech\"-s 51.106658 -e 53.14\" Utt Idutt18 AgreementAccept Influence-on-listenerAction-directive Influence-on-speakerCommit Info-levelTask Response-to\"utt17\" Speech\"-s 51.106658 -e 52.67\" StatementAssert UnderstandingSU-Acknowledge  oh we need to pick up  oranges Utt Idutt19 AgreementAccept Influence-on-speakerCommit Info-levelTask Response-to\"utt17\" Speech\"-s 52.466781 -e 53.14\" UnderstandingNone oh  okay  Turn IdT11 Speaker\"s\" Speech\"-s 52.047996 -e 53.247996\" Utt Idutt20 AgreementAccept Info-levelTask Response-to\"utt18\" Speech\"-s 52.047996 -e 53.247996\" UnderstandingSU-Acknowledge  yeah  ... Dialog Figure 8: DAMSL Annotation of a TRAINS Dialogue so we need to get oranges ... .36 .65 .09 to Elmira .13 .26 .33 .48 .54 .47 .54 .30 .41 .56 .62 .85 .02 .47 .66 .94 .97 yeah .80 W Utt oh we need to pick up oranges oh okay oh we need to pick up oranges oh okay W Utt D TurnT10 ... so we need to get oranges to Elmira U17 oh we need to pick up oranges U18 Resp U17 .04 TurnT9 D IOS:Commit IOL:Action-directive IOS:Commit IOL:Action-directive oh okay U19 yeah U20 Resp U18 Turn T11 IOS:Commit Figure 9: Graph Structure for TRAINS Example relations, on many scales, from acoustic features spanning a hundredth of a second to narrative structures spanning tens of minutes. Second, there are many alternative representations or construals of a given kind of linguistic information. Sometimes these alternatives are simply more or less convenient for a certain purpose. Thus a researcher who thinks theoretically of phonological features organized into moras, syllables and feet, will often ﬁnd it convenient to use a phonemic string as a representational approximation. In other cases, however, diﬀerent sorts of transcription or annotation reﬂect diﬀerent theories about the ontology of linguistic structure or the functional categories of communication. The AG representation oﬀers a way to deal pro- ductively with both kinds of multivocality. It pro- vides a framework for relating diﬀerent categories of linguistic analysis, and at the same time to compare diﬀerent approaches to a given type of analysis. As an example, Figure 10 shows an AG- based visualization of eight diﬀerent sorts of annotation of a phrase from the BU Radio Corpus, produced by Mari Ostendorf and others at Boston University, and published by the LDC [www.ldc.upenn.eduCatalogLDC96S36.html]. The basic material is from a recording of a local public radio news broadcast. The BU annotations include four types of information: orthographic transcripts, broad phonetic transcripts (including main word stress), and two kinds of prosodic annotation, all time-aligned to the digital audio ﬁles. The two kinds of prosodic annotation implement the system known as ToBI [www.ling.ohio-state.eduphoneticsE ToBI]. ToBI is an acronym for Tones and Break Indices, and correspondingly provides two types of information: Tones, which are taken from a ﬁxed vocabulary of categories of (stress-linked) pitch accents and (juncture-linked) boundary tones; and Break Indices, which are integers characterizing the strength and nature of interword disjunctures. We have added four additional annota- tions: coreference annotation and named entity annotation in the style of MUC-7 [www.muc.saic.comproceedingsmuc 7 toc.html] provided by Lynette Hirschman; syntactic structures in the style of the Penn TreeBank (Marcus et al., 1993) provided by Ann Taylor; and an alternative annotation for the F0 aspects of prosody, known as Tilt (Taylor, 1998) and provided by its inventor, Paul Taylor. Taylor has done Tilt annotations for much of the BU corpus, and will soon be publishing them as a point of comparison with the ToBI tonal annotation. Tilt diﬀers from ToBI in providing a quantitative rather than qualitative characterization of F0 obtrusions: where ToBI might say this is a LH pitch accent, Tilt would say This is an F0 obtrusion that starts at time t0, lasts for duration d seconds, involves a Hz total F0 change, and ends l Hz diﬀerent in F0 from where it started. As usual, the various annotations come in a bewil- dering variety of ﬁle formats. These are not entirely trivial to put into registration, because (for instance) the TreeBank terminal string contains both more (e.g. traces) and fewer (e.g. breaths) tokens than the orthographic transcription does. One other slightly tricky point: the connection between the word string and the break indices (which are ToBIs character- izations of the nature of interword disjuncture) are mediated only by identity in the ﬂoating-point time values assigned to word boundaries and to break indices in separate ﬁles. Since these time values are expressed as ASCII strings, it is easy to lose the identity relationship without meaning to, simply by reading in and writing out the values to programs that may make diﬀerent choices of internal variable type (e.g. ﬂoat vs. double), or number of decimal digits to print out, etc. Problems of this type are normal whenever multi- ple annotations need to be compared. Solving them is not rocket science, but does take careful work. When annotations with separate histories involve mutually inconsistent corrections, silent omissions of problematic material, or other typical developments, the problems are multiplied. In noting such diﬃcul- ties, we are not criticizing the authors of the annota- tions, but rather observing the value of being able to put multiple annotations into a common framework. Once this common framework is established, via translation of all eight strands into AG graph terms, we have the basis for posing queries that cut across the diﬀerent types of annotation. For instance, we might look at the distribution of Tilt parameters as a function of ToBI accent type; or the distribution of Tilt and ToBI values for initial vs. non-initial members of coreference sets; or the relative size of Tilt F0-change measures for nouns vs. verbs. We do not have the space in this paper to dis- cuss the design of an AG-based query formalism at length  and indeed, many details of practical AG query systems remain to be decided  but a short discussion will indicate the direction we propose to take. Of course the crux is simply to be able to put all the diﬀerent annotations into the same frame of reference, but beyond this, there are some aspects of the annotation graph formalism that have nice properties for deﬁning a query system. For example, if an annotation graph is deﬁned as a set of arcs like those given in the XML encoding in 1, then every member of the power set of this arc set is also a well-formed annotation graph. The power set construction provides the basis for a useful query algebra, since it deﬁnes the complete set of possible values for queries over the AG in question, and is obviously closed under intersection, union and rela- tive complement. As another example, various time- based indexes are deﬁnable on an adequately time- anchored annotation graph, with the result that many sorts of precedence, inclusion and overlap rela- tions are easy to calculate for arbitrary subgraphs. See (Bird and Liberman, 1999, 5) for discussion. In this section, we have indicated some of the ways in which the AG framework can facilitate the anal- ysis of complex combinations linguistic annotations. These annotation sets are typically multivocal, both in the sense of covering multiple types of linguistic information, and also in the sense of providing multi- ple versions of particular types of analysis. Discourse studies are especially multivocal in both senses, and so we feel that this approach will be especially help- ful to discourse researchers. 4 Conclusion This proliferation of formats and approaches can be viewed as a sign of intellectual ferment. The fact that so many people have devoted so much energy to ﬁelding new entries into this bazaar of data formats indicates how important the computational study of communicative interaction has become. However, for many researchers, this multiplicity of approaches 2 4 5 6 ToBI Breath W ToBI 7 8 9 COREF Treebank NE PP QP NP PP NP WH NP SBAR NP-T VP NP-PRD LOC 2 Tilt NP NP a22.7 fr hr wmn 1 2 3 4 5 6 7 8 9 This receives three hdrd dlrs mnth under General Relief plus four hdrd dlrs mnth in A.F.D.C. benefits son who is U.S. citizen a a a brth H !H !H- !H L-L H H H H L-H H H H L-L LH H-L L L-H H L-L L a125.9 t-.536 f271.8 1 t-.442 3 1 2 QP NP NP-SBJ NP PP NP-ADV a70.0 a24.9 t-.57 t.349 a38.9 t.317 f133.8 f177.4 a85.3 t.563 a20.9 t-.577 a27.2 t.515 a15.3 t-.069 a67.5 t.194 a56.0 t.131 a35.5 t.587 a59.4 t-.203 f166.6 1 1 3 1 1 1 1 4 2 1 4 2 1 1 1 1 4 1 1 4- 4 MONEY MONEY 1 1 2 4 1 woman2 benefits16 four hundred dollars16 son9 5 NP S VP NP Figure 10: Visualization for BU Example has produced headaches and confusion, rather than productive scientiﬁc advances. We need a way to integrate these approaches without imposing some form of premature closure that would crush experi- mentation and innovation. Both here, and in associated work (Bird and Liberman, 1999), we have endeavored to show how all current annotation formats involve the basic actions of associating labels with stretches of recorded signal data, and attributing logical sequence, hierarchy and coindexing to such labels. We have grounded this assertion by deﬁning annotation graphs and by showing how a disparate range of annotation formats can be mapped into AGs. This work provides a central piece of the algebraic foundation for inter-translatable formats and inter-operating tools. The intention is not to replace the formats and tools that have been accepted by any existing community of practice, but rather to make the descriptive and analytical practices, the formats, data and tools universally accessible. This means that annotation content for diverse domains and theoretical models can be created and maintained using tools that are the most suitable or familiar to the community in question. It also means that we can get started on integrating annotations, corpora and research ﬁndings right away, without having to wait until ﬁnal agreement on all possible tags and attributes has been achieved. There are many existing approaches to discourse annotation, and many options for future approaches. Our explorations presuppose a particular set of goals: (i) generality, speciﬁcity, simplicity; (ii) searchability and browsability; and (iii) maintainability and durability. These are discussed in full in (Bird and Liberman, 1999, 6). By identifying a common conceptual core to all annotation structures, we hope to provide a foundation for a wide-ranging integration of tools, formats and corpora. One might, by analogy to translation systems, describe AGs as an interlingua which permits free exchange of annotation data between n systems once n interfaces have been written, rather than n2 interfaces. Although we have been primarily concerned with the structure rather than the content of annota- tions, the approach opens the way to meaningful evaluation of content and comparison of contentful diﬀerences between annotations, since it is possible to do all manner of quasi-correlational analyses of parallel annotations. A tool for converting a given format into the AG framework only needs to be written once. Once this has been done, it becomes a straightforward task to pose complex queries over multiple corpora. Whereas if one were to start with annotations in several distinct ﬁle formats, it would be a major programming chore to ask even a simple question. Acknowledgements We are grateful to the following people for discus- sions and input concerning the material presented here: Chris Cieri, Dave Graﬀ, Julia Hirschberg, Lynette Hirschman, Brian MacWhinney, Ann Tay- lor, Paul Taylor, Marilyn Walker, and three anony- mous reviewers. References James Allen and Mark Core. 1997. Draft of DAMSL: Dialog act markup in several layers. [www.cs.rochester.eduresearchtrainsannotation RevisedManualRevisedManual.html]. Steven Bird and Mark Liberman. 1999. A formal framework for linguistic annotation. Technical Report MS-CIS-99-01, Department of Computer and Information Science, University of Pennsylvania. [xxx.lanl.govabscs.CL9903003], expanded from version presented at ICSLP-98, Sydney. Barbara Di Eugenio, Pamela W. Jordan, and Liina Pylkkanen. 1998. The COCONUT project: Dialogue annotation manual. Technical Report 98-1, University of Pittsburgh, Intelligent Systems Program. [www.isp.pitt.eduintgencoconut.html]. Gerald Gazdar and Chris Mellish. 1989. Natural Language Processing in Prolog: An Introduction to Computational Linguistics. Addison-Wesley. Peter A. Heeman and James Allen. 1993. The TRAINS 93 dialogues. Technical Report TRAINS Technical Note 94-2, Computer Science Department, University of Rochester. [ftp.cs.rochester.edupubpapersai 94.tn2.Trains 93 dialogues.ps.gz]. Lynette Hirschman and Nancy Chinchor. 1997. Muc-7 coreference task deﬁnition. In Message Understanding Conference Proceedings. Published online. [www.muc.saic.comproceedingsmuc 7 toc.html]. Ray Jackendoﬀ. 1972. Semantic Interpretation in Generative Grammar. Cambridge Mass.: MIT Press. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):31330. www.cis.upenn.edutreebankhome.html. Florian Schiel, Susanne Burger, Anja Geumann, and Karl Weilhammer. 1998. The Partitur format at BAS. In Proceedings of the First International Conference on Language Resources and Evaluation. [www.phonetik.uni-muenchen.de BasBasFormatseng.html]. Ann Taylor, 1995. Dysﬂuency Annotation Stylebook for the Switchboard Corpus. University of Pennsylvania, Department of Computer and Information Science. [ftp.cis.upenn.edupubtreebankswbddocDFL- book.ps], original version by Marie Meteer et al. Paul A. Taylor. 1998. The tilt intonation model. In Proceedings of the 5th International Conference on Spoken Language Processing.",
  "42.pdf": "arXiv:cs9907006v1 [cs.CL] 6 Jul 1999 Representing Text Chunks Erik F. Tjong Kim Sang Jorn Veenstra Center for Dutch Language and Speech Computational Linguistics University of Antwerp Tilburg University Universiteitsplein 1 P.O. Box 90153 B-2610 Wilrijk, Belgium 5000 LE Tilburg, The Netherlands eriktuia.ac.be veenstrakub.nl Abstract Dividing sentences in chunks of words is a useful preprocessing step for parsing, in- formation extraction and information re- trieval. (Ramshaw and Marcus, 1995) have introduced a convenient data rep- resentation for chunking by converting it to a tagging task. In this paper we will examine seven diﬀerent data representa- tions for the problem of recognizing noun phrase chunks. We will show that the the data representation choice has a minor in- ﬂuence on chunking performance. How- ever, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best pub- lished chunking results for a standard data set. 1 Introduction The text corpus tasks parsing, information ex- traction and information retrieval can beneﬁt from dividing sentences in chunks of words. (Ramshaw and Marcus, 1995) describe an error- driven transformation-based learning (TBL) method for ﬁnding NP chunks in texts. NP chunks (or baseNPs) are non-overlapping, non-recursive noun phrases. In their experiments they have modeled chunk recognition as a tagging task: words that are inside a baseNP were marked I, words outside a baseNP received an O tag and a special tag B was used for the ﬁrst word inside a baseNP immediately following another baseNP. A text example: original: In [N early trading N] in [N Hong Kong N] [N Monday N] , [N gold N] was quoted at [N  366.50 N] [N an ounce N] . tagged: InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO I 366.50I anB ounceI .O Other representations for NP chunking can be used as well. An example is the representation used in (Ratnaparkhi, 1998) where all the chunk-initial words receive the same start tag (analogous to the B tag) while the remainder of the words in the chunk are paired with a diﬀerent tag. This removes tag- ging ambiguities. In the Ratnaparkhi representation equal noun phrases receive the same tag sequence re- gardless of the context in which they appear. The data representation choice might inﬂuence the performance of chunking systems. In this pa- per we discuss how large this inﬂuence is. Therefore we will compare seven diﬀerent data representation formats for the baseNP recognition task. We are particularly interested in ﬁnding out whether with one of the representation formats the best reported results for this task can be improved. The second section of this paper presents the general setup of the experiments. The results can be found in the third section. In the fourth section we will describe some related work. 2 Methods and experiments In this section we present and explain the data rep- resentation formats and the machine learning algo- rithm that we have used. In the ﬁnal part we de- scribe the feature representation used in our exper- iments. 2.1 Data representation We have compared four complete and three partial data representation formats for the baseNP recogni- tion task presented in (Ramshaw and Marcus, 1995). The four complete formats all use an I tag for words that are inside a baseNP and an O tag for words that IOB1 O I I O I I B O I O O O I I B I O IOB2 O B I O B I B O B O O O B I B I O IOE1 O I I O I E I O I O O O I E I I O IOE2 O I E O I E E O E O O O I E I E O IO O I I O I I I O I O O O I I I I O [ . [ . . [ . [ . [ . . . [ . [ . . ] . . ] . . ] ] . ] . . . . ] . ] . Table 1: The chunk tag sequences for the example sentence In early trading in Hong Kong Monday , gold was quoted at  366.50 an ounce . for seven diﬀerent tagging formats. The I tag has been used for words inside a baseNP, O for words outside a baseNP, B and [ for baseNP-initial words and E and ] for baseNP-ﬁnal words. are outside a baseNP. They diﬀer in their treatment of chunk-initial and chunk-ﬁnal words: IOB1 The ﬁrst word inside a baseNP immediately following an- other baseNP receives a B tag (Ramshaw and Marcus, 1995). IOB2 All baseNP-initial words receive a B tag (Ratnaparkhi, 1998). IOE1 The ﬁnal word inside a baseNP immediately preceding another baseNP receives an E tag. IOE2 All baseNP-ﬁnal words receive an E tag. We wanted to compare these data representa- tion formats with a standard bracket representation. We have chosen to divide bracketing experiments in two parts: one for recognizing opening brackets and one for recognizing closing brackets. Addition- ally we have worked with another partial representa- tion which seemed promising: a tagging representa- tion which disregards boundaries between adjacent chunks. These boundaries can be recovered by com- bining this format with one of the bracketing for- mats. Our three partial representations are: [ All baseNP-initial words receive an [ tag, other words receive a . tag. ] All baseNP-ﬁnal words receive a ] tag, other words receive a . tag. IO Words inside a baseNP receive an I tag, others receive an O tag. These partial representations can be combined in three pairs which encode the complete baseNP struc- ture of the data: [  ] A word sequence is regarded as a baseNP if the ﬁrst word has re- ceived an [ tag, the ﬁnal word has received a ] tag and these are the only brackets that have been as- signed to words in the sequence. [  IO In the IO format, tags of words that have received an I tag and an [ tag are changed into B tags. The result is interpreted as the IOB2 format. IO  ] In the IO format, tags of words that have received an I tag and a ] tag are changed into E tags. The result is interpreted as the IOE2 format. Examples of the four complete formats and the three partial formats can be found in table 1. 2.2 Memory-Based Learning We have build a baseNP recognizer by training a machine learning algorithm with correct tagged data and testing it with unseen data. The machine learn- ing algorithm we used was a Memory-Based Learn- ing algorithm (MBL). During training it stores a symbolic feature representation of a word in the training data together with its classiﬁcation (chunk tag). In the testing phase the algorithm compares a feature representation of a test word with every training data item and chooses the classiﬁcation of the training item which is closest to the test item. In the version of the algorithm that we have used, ib1-ig, the distances between feature representa- tions are computed as the weighted sum of dis- tances between individual features (Daelemans et al., 1998). Equal features are deﬁned to have dis- tance 0, while the distance between other pairs is some feature-dependent value. This value is equal to the information gain of the feature, an information theoretic measure which contains the normalized en- wordPOS context Fβ1 IOB1 L2R1 89.17 IOB2 L2R1 88.76 IOE1 L1R2 88.67 IOE2 L2R2 89.01 [  ] L2R1  L0R2 89.32 [  IO L2R0  L1R1 89.43 IO  ] L1R1  L0R2 89.42 Table 2: Results ﬁrst experiment series: the best Fβ1 scores for diﬀerent left (L) and right (R) wordPOS tag pair context sizes for the seven representation formats using 5-fold cross-validation on section 15 of the WSJ corpus. tropy decrease of the classiﬁcation set caused by the presence of the feature. Details of the algorithm can be found in (Daelemans et al., 1998)1. 2.3 Representing words with features An important decision in an MBL experiment is the choice of the features that will be used for represent- ing the data. ib1-ig is thought to be less sensitive to redundant features because of the data-dependent feature weighting that is included in the algorithm. We have found that the presence of redundant fea- tures has a negative inﬂuence on the performance of the baseNP recognizer. In (Ramshaw and Marcus, 1995) a set of trans- formational rules is used for modifying the classiﬁ- cation of words. The rules use context information of the words, the part-of-speech tags that have been assigned to them and the chunk tags that are asso- ciated with them. We will use the same information as in our feature representation for words. In TBL, rules with diﬀerent context information are used successively for solving diﬀerent problems. We will use the same context information for all data. The optimal context size will be determined by comparing the results of diﬀerent context sizes on the training data. Here we will perform four steps. We will start with testing diﬀerent context sizes of words with their part-of-speech tag. After this, we will use the classiﬁcation results of the best context size for determining the optimal context size for the classiﬁcation tags. As a third step, we will evaluate combinations of classiﬁcation results and ﬁnd the best combination. Finally we will examine the inﬂu- ence of an MBL algorithm parameter: the number of examined nearest neighbors. 1ib1-ig is a part of the TiMBL software package which is available from http:ilk.kub.nl 3 Results We have used the baseNP data presented in (Ramshaw and Marcus, 1995)2. This data was di- vided in two parts. The ﬁrst part was training data and consisted of 211727 words taken from sections 15, 16, 17 and 18 from the Wall Street Journal cor- pus (WSJ). The second part was test data and con- sisted of 47377 words taken from section 20 of the same corpus. The words were part-of-speech (POS) tagged with the Brill tagger and each word was clas- siﬁed as being inside or outside a baseNP with the IOB1 representation scheme. The chunking classiﬁ- cation was made by (Ramshaw and Marcus, 1995) based on the parsing information in the WSJ corpus. The performance of the baseNP recognizer can be measured in diﬀerent ways: by computing the percentage of correct classiﬁcation tags (accuracy), the percentage of recognized baseNPs that are cor- rect (precision) and the percentage of baseNPs in the corpus that are found (recall). We will fol- low (Argamon et al., 1998) and use a combination of the precision and recall rates: Fβ1  (2preci- sionrecall)(precisionrecall). In our ﬁrst experiment series we have tried to dis- cover the best wordpart-of-speech tag context for each representation format. For computational rea- sons we have limited ourselves to working with sec- tion 15 of the WSJ corpus. This section contains 50442 words. We have run 5-fold cross-validation experiments with all combinations of left and right contexts of wordPOS tag pairs in the size range 0 to 4. A summary of the results can be found in table 2. The baseNP recognizer performed best with rel- atively small wordPOS tag pair contexts. Diﬀer- ent representation formats required diﬀerent con- text sizes for optimal performance. All formats with 2The data described in (Ramshaw and Marcus, 1995) is available from ftp:ftp.cis.upenn.edupubchunker wordPOS context chunk tag context Fβ1 IOB1 L2R1 12 90.12 IOB2 L2R1 10 89.30 IOE1 L1R2 12 89.55 IOE2 L1R2 01 89.73 [  ] L2R1  L0R2 00  00 89.32 [  IO L2R0  L1R1 00  11 89.78 IO  ] L1R1  L0R2 11  00 89.86 Table 3: Results second experiment series: the best Fβ1 scores for diﬀerent left (L) and right (R) chunk tag context sizes for the seven representation formats using 5-fold cross-validation on section 15 of the WSJ corpus. wordPOS chunk tag combinations Fβ1 IOB1 21 11 00 11 22 33 90.53 IOB2 21 10 21 89.30 IOE1 12 12 00 11 22 33 90.03 IOE2 12 01 12 89.73 [  ] 21  02 00  00 -  - 89.32 [  IO 20  11 00  11 -  01 12 23 34 89.91 IO  ] 11  02 11  00 01 12 23 34  - 90.03 Table 4: Results third experiment series: the best Fβ1 scores for diﬀerent combinations of chunk tag context sizes for the seven representation formats using 5-fold cross-validation on section 15 of the WSJ corpus. explicit open bracket information preferred larger left context and most formats with explicit closing bracket information preferred larger right context size. The three combinations of partial represen- tations systematically outperformed the four com- plete representations. This is probably caused by the fact that they are able to use two diﬀerent context sizes for solving two diﬀerent parts of the recognition problem. In a second series of experiments we used a cas- caded classiﬁer. This classiﬁer has two stages (cas- cades). The ﬁrst cascade is similar to the classiﬁer described in the ﬁrst experiment. For the second cas- cade we added the classiﬁcations of the ﬁrst cascade as extra features. The extra features consisted of the left and the right context of the classiﬁcation tags. The focus chunk tag (the classiﬁcation of the cur- rent word) accounts for the correct classiﬁcation in about 95 of the cases. The MBL algorithm assigns a large weight to this input feature and this makes it harder for the other features to contribute to a good result. To avoid this we have refrained from using this tag. Our goal was to ﬁnd out the optimal number of extra classiﬁcation tags in the input. We performed 5-fold cross-validation experiments with all combinations of left and right classiﬁcation tag contexts in the range 0 tags to 3 tags. A summary of the results can be found in table 33. We achieved higher Fβ1 for all representations except for the bracket pair representation. The third experiment series was similar to the sec- ond but instead of adding output of one experiment we added classiﬁcation results of three, four or ﬁve experiments of the ﬁrst series. By doing this we sup- plied the learning algorithm with information about diﬀerent context sizes. This information is available to TBL in the rules which use diﬀerent contexts. We have limited ourselves to examining all successive combinations of three, four and ﬁve experiments of the lists (L0R0, 11, 22, 33, 44), (01, 12, 23, 34) and (10, 21, 32, 43). A summary of the results can be found in table 4. The results for four representation formats improved. In the fourth experiment series we have exper- imented with a diﬀerent value for the number of nearest neighbors examined by the ib1-ig algorithm (parameter k). This algorithm standardly uses the single training item closest to the test item. How- 3In a number of cases a diﬀerent base conﬁguration in one experiment series outperformed the best base conﬁg- uration found in the previous series. In the second series LR12 outperformed 22 for IOE2 when chunk tags were added and in the third series chunk tag context 11 outperformed 12 for IOB1 when diﬀerent combinations were tested. wordPOS chunk tag combinations Fβ1 IOB1 33(k3) 11 00(1) 11(1) 22(3) 33(3) 90.89  0.63 IOB2 33(k3) 10 33(3) 89.72  0.79 IOE1 23(k3) 12 00(1) 11(1) 22(3) 33(3) 90.12  0.27 IOE2 23(k3) 01 23(3) 90.02  0.48 [  ] 43(3)  44(3) 00  00 -  - 90.08  0.57 [  IO 43(3)  33(3) 00  11 -  01(1) 12(3) 23(3) 34(3) 90.35  0.75 IO  ] 33(3)  23(3) 11  00 01(1) 12(3) 23(3) 34(3)  - 90.23  0.73 Table 5: Results fourth experiment series: the best Fβ1 scores for diﬀerent combinations of left and right classiﬁcation tag context sizes for the seven representation formats using 5-fold cross-validation on section 15 of the WSJ corpus obtained with ib1-ig parameter k3. IOB1 is the best representation format but the diﬀerences with the results of the other formats are not signiﬁcant. ever (Daelemans et al., 1999) report that for baseNP recognition better results can be obtained by mak- ing the algorithm consider the classiﬁcation values of the three closest training items. We have tested this by repeating the ﬁrst experiment series and part of the third experiment series for k3. In this re- vised version we have repeated the best experiment of the third series with the results for k1 replaced by the k3 results whenever the latter outperformed the ﬁrst in the revised ﬁrst experiment series. The results can be found in table 5. All formats bene- ﬁted from this step. In this ﬁnal experiment series the best results were obtained with IOB1 but the diﬀerences with the results of the other formats are not signiﬁcant. We have used the optimal experiment conﬁgura- tions that we had obtained from the fourth exper- iment series for processing the complete (Ramshaw and Marcus, 1995) data set. The results can be found in table 6. They are better than the results for section 15 because more training data was used in these experiments. Again the best result was obtained with IOB1 (Fβ192.37) which is an im- provement of the best reported Fβ1 rate for this data set ((Ramshaw and Marcus, 1995): 92.03). We would like to apply our learning approach to the large data set mentioned in (Ramshaw and Mar- cus, 1995): Wall Street Journal corpus sections 2-21 as training material and section 0 as test material. With our present hardware applying our optimal ex- periment conﬁguration to this data would require several months of computer time. Therefore we have only used the best stage 1 approach with IOB1 tags: a left and right context of three words and three POS tags combined with k3. This time the chun- ker achieved a Fβ1 score of 93.81 which is half a point better than the results obtained by (Ramshaw and Marcus, 1995): 93.3 (other chunker rates for this data: accuracy: 98.04; precision: 93.71; re- call: 93.90). 4 Related work The concept of chunking was introduced by Abney in (Abney, 1991). He suggested to develop a chunk- ing parser which uses a two-part syntactic analysis: creating word chunks (partial trees) and attaching the chunks to create complete syntactic trees. Ab- ney obtained support for such a chunking stage from psycholinguistic literature. Ramshaw and Marcus used transformation- based learning (TBL) for developing two chunkers (Ramshaw and Marcus, 1995). One was trained to recognize baseNPs and the other was trained to rec- ognize both NP chunks and VP chunks. Ramshaw and Marcus approached the chunking task as a tag- ging problem. Their baseNP training and test data from the Wall Street Journal corpus are still being used as benchmark data for current chunking exper- iments. (Ramshaw and Marcus, 1995) shows that baseNP recognition (Fβ192.0) is easier than ﬁnd- ing both NP and VP chunks (Fβ188.1) and that increasing the size of the training data increases the performance on the test set. The work by Ramshaw and Marcus has inspired three other groups to build chunking algorithms. (Argamon et al., 1998) introduce Memory-Based Se- quence Learning and use it for diﬀerent chunking experiments. Their algorithm stores sequences of POS tags with chunk brackets and uses this in- formation for recognizing chunks in unseen data. It performed slightly worse on baseNP recognition than the (Ramshaw and Marcus, 1995) experiments (Fβ191.6). (Cardie and Pierce, 1998) uses a re- lated method but they only store POS tag sequences forming complete baseNPs. These sequences were applied to unseen tagged data after which post- processing repair rules were used for ﬁxing some fre- quent errors. This approach performs worse than accuracy precision recall Fβ1 IOB1 97.58 92.50 92.25 92.37 IOB2 96.50 91.24 92.32 91.78 IOE1 97.58 92.41 92.04 92.23 IOE2 96.77 91.93 92.46 92.20 [  ] - 93.66 90.81 92.22 [  IO - 91.47 92.61 92.04 IO  ] - 91.25 92.54 91.89 (Ramshaw and Marcus, 1995) 97.37 91.80 92.27 92.03 (Veenstra, 1998) 97.2 89.0 94.3 91.6 (Argamon et al., 1998) - 91.6  91.6 91.6 (Cardie and Pierce, 1998) - 90.7 91.1 90.9 Table 6: The Fβ1 scores for the (Ramshaw and Marcus, 1995) test set after training with their training data set. The data was processed with the optimal input feature combinations found in the fourth experiment series. The accuracy rate contains the fraction of chunk tags that was correct. The other three rates regard baseNP recognition. The bottom part of the table shows some other reported results with this data set. With all but two formats ib1-ig achieves better Fβ1 rates than the best published result in (Ramshaw and Marcus, 1995). other reported approaches (Fβ190.9). (Veenstra, 1998) uses cascaded decision tree learn- ing (IGTree) for baseNP recognition. This algorithm stores context information of words, POS tags and chunking tags in a decision tree and classiﬁes new items by comparing them to the training items. The algorithm is very fast and it reaches the same per- formance as (Argamon et al., 1998) (Fβ191.6). (Daelemans et al., 1999) uses cascaded MBL (ib1- ig) in a similar way for several tasks among which baseNP recognition. They do not report Fβ1 rates but their tag accuracy rates are a lot better than accuracy rates reported by others. However, they use the (Ramshaw and Marcus, 1995) data set in a diﬀerent training-test division (10-fold cross val- idation) which makes it diﬃcult to compare their results with others. 5 Concluding remarks We have compared seven diﬀerent data formats for the recognition of baseNPs with memory-based learning (ib1-ig). The IOB1 format, introduced in (Ramshaw and Marcus, 1995), consistently came out as the best format. However, the diﬀerences with other formats were not signiﬁcant. Some represen- tation formats achieved better precision rates, oth- ers better recall rates. This information is useful for tasks that require chunking structures because some tasks might be more interested in high preci- sion rates while others might be more interested in high recall rates. The ib1-ig algorithm has been able to improve the best reported Fβ1 rates for a standard data set (92.37 versus (Ramshaw and Marcus, 1995)s 92.03). This result was aided by using non-standard param- eter values (k3) and the algorithm was sensitive for redundant input features. This means that ﬁnding an optimal performance or this task requires search- ing a large parameterfeature conﬁguration space. An interesting topic for future research would be to embed ib1-ig in a standard search algorithm, like hill-climbing, and explore this parameter space. Some more room for improved performance lies in computing the POS tags in the data with a better tagger than presently used. References Steven Abney. 1991. Parsing by chunks. In Principle-Based Parsing. Kluwer Academic Pub- lishers,. Shlomo Argamon, Ido Dagan, and Yuval Kry- molowski. 1998. A memory-based approach to learning shallow natural language patterns. In Proceedings of the 17th International Con- ference on Computational Linguistics (COLING- ACL 98). Claire Cardie and David Pierce. 1998. Error- driven pruning of treebank grammars for base noun phrase identiﬁcation. In Proceedings of the 17th International Conference on Computational Linguistics (COLING-ACL 98). Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 1998. TiMBL: Tilburg Memory Based Learner - version 1.0 - Reference Guide. ILK, Tilburg University, The Netherlands. http:ilk.kub.nlilkpapersilk9803.ps.gz. Walter Daelemans, Antal van den Bosch, and Jakub Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 11. Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learn- ing. In Proceedings of the Third ACL Workshop on Very Large Corpora. Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolu- tion. PhD thesis Computer and Information Sci- ence, University of Pennsylvania. Jorn Veenstra. 1998. Fast np chunking us- ing memory-based learning techniques. In BENELEARN-98: Proceedings of the Eigth Belgian-Dutch Conference on Machine Learning. ATO-DLO, Wageningen, report 352.",
  "43.pdf": "arXiv:cs9907007v2 [cs.CL] 7 Jul 1999 Cross-Language Information Retrieval for Technical Documents Atsushi Fujii and Tetsuya Ishikawa University of Library and Information Science 1-2 Kasuga Tsukuba 305-8550, JAPAN {fujii,ishikawa}ulis.ac.jp Abstract This paper proposes a JapaneseEnglish cross- language information retrieval (CLIR) system targeting technical documents. Our system ﬁrst translates a given query containing tech- nical terms into the target language, and then retrieves documents relevant to the translated query. The translation of technical terms is still problematic in that technical terms are often compound words, and thus new terms can be progressively created simply by combining ex- isting base words. In addition, Japanese of- ten represents loanwords based on its phono- gram. Consequently, existing dictionaries ﬁnd it diﬃcult to achieve suﬃcient coverage. To counter the ﬁrst problem, we use a compound word translation method, which uses a bilin- gual dictionary for base words and collocational statistics to resolve translation ambiguity. For the second problem, we propose a translitera- tion method, which identiﬁes phonetic equiva- lents in the target language. We also show the eﬀectiveness of our system using a test collec- tion for CLIR. 1 Introduction Cross-language information retrieval (CLIR), where the user presents queries in one language to retrieve documents in another language, has recently been one of the major topics within the information retrieval community. One strong motivation for CLIR is the growing number of documents in various languages accessible via the Internet. Since queries and documents are in diﬀerent languages, CLIR requires a trans- lation phase along with the usual monolingual retrieval phase. For this purpose, existing CLIR systems adopt various techniques explored in natural language processing (NLP) research. In brief, bilingual dictionaries, corpora, thesauri and machine translation (MT) systems are used to translate queries orand documents. In this paper, we propose a JapaneseEnglish CLIR system for technical documents, focus- ing on translation of technical terms. Our purpose also includes integration of diﬀerent components within one framework. Our re- search is partly motivated by the NACSIS test collection for IR systems (Kando et al., 1998)1, which consists of Japanese queries and JapaneseEnglish abstracts extracted from technical papers (we will elaborate on the NAC- SIS collection in Section 4). Using this col- lection, we investigate the eﬀectiveness of each component as well as the overall performance of the system. As with MT systems, existing CLIR systems still ﬁnd it diﬃcult to translate technical terms and proper nouns, which are often unlisted in general dictionaries. Since most CLIR systems target newspaper articles, which are comprised mainly of general words, the problem related to unlisted words has been less explored than other CLIR subtopics (such as resolution of transla- tion ambiguity). However, Pirkola (1998), for example, used a subset of the TREC collection related to health topics, and showed that com- bination of general and domain speciﬁc (i.e., medical) dictionaries improves the CLIR perfor- mance obtained with only a general dictionary. This result shows the potential contribution of technical term translation to CLIR. At the same time, note that even domain speciﬁc dictionaries 1http:www.rd.nacsis.ac.jpntcadmindex-en.html do not exhaustively list possible technical terms. We classify problems associated with technical term translation as given below: (1) technical terms are often compound word, which can be progressively created simply by combining multiple existing morphemes (base words), and therefore it is not en- tirely satisfactory to exhaustively enumer- ate newly emerging terms in dictionaries, (2) Asian languages often represent loanwords based on their special phonograms (primar- ily for technical terms and proper nouns), which creates new base words progressively (in the case of Japanese, the phonogram is called katakana). To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999), which selects appropri- ate translations based on the probability of oc- currence of each combination of base words in the target language. For problem (2), we use transliteration (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998). Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration meth- ods relying on the property of the Chinese phonetic system, which cannot be directly ap- plied to transliteration between English and Japanese. Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. However, since their method needs large-scale phoneme inven- tories, we propose a simpler approach using surface mapping between English and katakana characters, rather than sounds. Section 2 overviews our CLIR system, and Section 3 elaborates on the translation mod- ule focusing on compound word translation and transliteration. Section 4 then evaluates the eﬀectiveness of our CLIR system by way of the standardized IR evaluation method used in TREC programs. 2 System Overview Before explaining our CLIR system, we clas- sify existing CLIR into three approaches in terms of the implementation of the translation phase. The ﬁrst approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998), while the second ap- proach translates documents into the query lan- guage (Gachot et al., 1996; Oard and Hack- ett, 1997). The third approach transfers both queries and documents into an interlingual rep- resentation: bilingual thesaurus classes (Mon- gar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996). We prefer the ﬁrst approach, the query translation, to other approaches because (a) translating all the documents in a given col- lection is expensive, (b) the use of thesauri re- quires manual construction or bilingual compa- rable corpora, (c) interlingual vector space mod- els also need comparable corpora, and (d) query translation can easily be combined with existing IR engines and thus the implementation cost is low. At the same time, we concede that other CLIR approaches are worth further exploration. Figure 1 depicts the overall design of our CLIR system, where most components are the same as those for monolingual IR, excluding translator. First, tokenizer processes documents in a given collection to produce an inverted ﬁle (surrogates). Since our system is bidirec- tional, tokenization diﬀers depending on the target language. In the case where documents are in English, tokenization involves eliminat- ing stopwords and identifying root forms for inﬂected words, for which we used Word- Net (Miller et al., 1993). On the other hand, we segment Japanese documents into lexical units using the ChaSen morphological ana- lyzer (Matsumoto et al., 1997) and discard stop- words. In the current implementation, we use word-based uni-gram indexing for both English and Japanese documents. In other words, com- pound words are decomposed into base words in the surrogates. Note that indexing and re- trieval methods are theoretically independent of the translation method. Thereafter, the translator processes a query in the source language (S-query) to output the translation (T-query). T-query can con- sist of more than one translation, because mul- tiple translations are often appropriate for a sin- gle technical term. Finally, the IR engine computes the sim- ilarity between T-query and each document in the surrogates based on the vector space model (Salton and McGill, 1983), and sorts doc- ument according to the similarity, in descending order. We compute term weight based on the notion of TFIDF. Note that T-query is decom- posed into base words, as performed in the doc- ument preprocessing. In Section 3, we will explain the translator in Figure 1, which involves compound word translation and transliteration modules. IR engine result T-query translator S-query tokenizer documents surrogates Figure 1: The overall design of our CLIR system 3 Translation Module 3.1 Overview Given a query in the source language, tokeniza- tion is ﬁrst performed as for target documents (see Figure 1). To put it more precisely, we use WordNet and ChaSen for English and Japanese queries, respectively. We then discard stop- words and extract only content words. Here, content words refer to both single and com- pound words. Let us take the following query as an example: improvement of data mining methods. For this query, we discard of, to extract im- provement and data mining methods. Thereafter, we translate each extracted con- tent word individually. Note that we currently do not consider relation (e.g. syntactic relation and collocational information) between content words. If a single word, such as improvement in the example above, is listed in our bilingual dictionary (we will explain the way to produce the dictionary in Section 3.2), we use all pos- sible translation candidates as query terms for the subsequent retrieval phase. Otherwise, compound word translation is performed. In the case of Japanese-English translation, we consider all possible segmenta- tions of the input word, by consulting the dic- tionary. Then, we select such segmentations that consist of the minimal number of base words. During the segmentation process, the dictionary derives all possible translations for base words. At the same time, transliteration is performed whenever katakana sequences un- listed in the dictionary are found. On the other hand, in the case of English-Japanese transla- tion, transliteration is applied to any unlisted base word (including the case where the input English word consists of a single base word). Fi- nally, we compute the probability of occurrence of each combination of base words in the target language, and select those with greater proba- bilities, for both Japanese-English and English- Japanese translations. 3.2 Compound Word Translation This section brieﬂy explains the compound word translation method we previously pro- posed (Fujii and Ishikawa, 1999). This method translates input compound words on a word-by- word basis, maintaining the word order in the source language2. The formula for the source compound word and one translation candidate are represented as below. S  s1, s2, . . . , sn T  t1, t2, . . . , tn 2A preliminary study showed that approximately 95 of compound technical terms deﬁned in a bilingual dic- tionary maintain the same word order in both source and target languages. Here, si and ti denote i-th base words in source and target languages, respectively. Our task, i.e., to select T which maximizes P(TS), is transformed into Equation (1) through use of the Bayesian theorem. arg max T P(TS)  arg max T P(ST)  P(T) (1) P(ST) and P(T) are approximated as in Equa- tion (2), which has commonly been used in the recent statistical NLP research (Church and Mercer, 1993). P(ST)  n  i1 P(siti) P(T)  n1  i1 P(ti1ti) (2) We produced our own dictionary, because conventional dictionaries are comprised primar- ily of general words and verbose deﬁnitions aimed at human readers. We extracted 59,533 EnglishJapanese translations consisting of two base words from the EDR technical terminol- ogy dictionary, which contains about 120,000 translations related to the information process- ing ﬁeld (Japan Electronic Dictionary Research Institute, 1995), and segment Japanese entries into two parts3. For this purpose, simple heuris- tic rules based mainly on Japanese character types (i.e., kanji, katakana, hiragana, alpha- bets and other characters like numerals) were used. Given the set of compound words where Japanese entries are segmented, we correspond English-Japanese base words on a word-by-word basis, maintaining the word order between En- glish and Japanese, to produce a Japanese- EnglishEnglish-Japanese base word dictionary. As a result, we extracted 24,439 Japanese base words and 7,910 English base words from the EDR dictionary. During the dictionary produc- tion, we also count the collocational frequency for each combination of si and ti, in order to estimate P(siti). Note that in the case where 3The number of base words can easily be identiﬁed based on English words, while Japanese compound words lack lexical segmentation. si is transliterated into ti, we use an arbitrar- ily predeﬁned value for P(siti). For the esti- mation of P(ti1ti), we use the word-based bi- gram statistics obtained from target language corpora, i.e., documents in the collection (see Figure 1). 3.3 Transliteration Figure 2 shows example correspondences be- tween English and (romanized) katakana words, where we insert hyphens between each katakana character for enhanced readability. The basis of our transliteration method is analogous to that for compound word translation described in Section 3.2. The formula for the source word and one transliteration candidate are rep- resented as below. S  s1, s2, . . . , sn T  t1, t2, . . . , tn However, unlike the case of compound word translation, si and ti denote i-th symbols (which consist of one or more letters), respec- tively. Note that we consider only such Ts that are indexed in the inverted ﬁle, because our transliteration method often outputs a num- ber of incorrect words with great probabilities. Then, we compute P(TS) for each T using Equations (1) and (2) (see Section 3.2), and select k-best candidates with greater probabili- ties. The crucial content here is the way to pro- duce a bilingual dictionary for symbols. For this purpose, we used approximately 3,000 katakana entries and their English translations listed in our base word dictionary. To illustrate our dic- tionary production method, we consider Fig- ure 2 again. Looking at this ﬁgure, one may notice that the ﬁrst letter in each katakana character tends to be contained in its corre- sponding English word. However, there are a few exceptions. A typical case is that since Japanese has no distinction between L and R sounds, the two English sounds collapse into the same Japanese sound. In addition, a single English letter corresponds to multiple katakana characters, such as x to ki-su in text, te-ki-su-to. To sum up, English and romanized katakana words are not exactly iden- tical, but similar to each other. English katakana system shi-su-te-mu mining ma-i-ni-n-gu data dee-ta network ne-tto-waa-ku text te-ki-su-to collocation ko-ro-ke-i-sho-n Figure 2: Examples of English-katakana corre- spondence We ﬁrst manually deﬁne the similarity be- tween the English letter e and the ﬁrst roman- ized letter for each katakana character j, as shown in Table 1. In this table, phonetically similar letters refer to a certain pair of letters, such as L and R4. We then consider the similarity for any possible combination of let- ters in English and romanized katakana words, which can be represented as a matrix, as shown in Figure 3. This ﬁgure shows the similarity between letters in text, te-ki-su-to. We put a dummy letter , which has a positive similarity only to itself, at the end of both En- glish and katakana words. One may notice that matching plausible symbols can be seen as ﬁnd- ing the path which maximizes the total similar- ity from the ﬁrst to last letters. The best path can easily be found by, for example, Dijkstras algorithm (Dijkstra, 1959). From Figure 3, we can derive the following correspondences: te, te, x, ki-su and t, to. The resultant correspondences contain 944 Japanese and 790 English symbol types, from which we also estimated P(siti) and P(ti1ti). As can be predicted, a preliminary experi- ment showed that our transliteration method is not accurate when compared with a word- based translation. For example, the Japanese word re-ji-su-ta (register) is transliterated to resister, resistor and register, with the probability score in descending order. How- 4We identiﬁed approximately twenty pairs of phonet- ically similar letters. ever, combined with the compound word trans- lation, irrelevant transliteration outputs are ex- pected to be discarded. For example, a com- pound word like re-ji-su-ta tensou gengo (reg- ister transfer language) is successfully trans- lated, given a set of base words tensou (trans- fer) and gengo (language) as a context. Table 1: The similarity between English and Japanese letters condition similarity e and j are identical 3 e and j are phonetically similar 2 both e and j are vowels or consonants 1 otherwise 0 te ki su to 3 1 2 3 0 0 0 0 1 2 1 3 1 2 3 t e x t   0 0 0 0 0 0 0 0 3 1 E J Figure 3: An example matrix for English- Japanese symbol matching (arrows denote the best path) 4 Evaluation This section investigates the performance of our CLIR system based on the TREC-type evalu- ation methodology: the system outputs 1,000 top documents, and TREC evaluation software is used to calculate the recall-precision trade-oﬀ and 11-point average precision. For the purpose of our evaluation, we used the NACSIS test collection (Kando et al., 1998). This collection consists of 21 Japanese queries and approximately 330,000 documents (in ei- ther a combination of English and Japanese or either of the languages individually), collected from technical papers published by 65 Japanese associations for various ﬁelds. Each document consists of the document ID, title, name(s) of author(s), namedate of conference, hosting or- ganization, abstract and keywords, from which titles, abstracts and keywords were used for our evaluation. We used as target documents ap- proximately 187,000 entries where abstracts are in both English and Japanese. Each query con- sists of the title of the topic, description, narra- tive and list of synonyms, from which we used only the description. Roughly speaking, most topics are related to electronic, information and control engineering. Figure 4 shows example de- scriptions (translated into English by one of the authors). Relevance assessment was performed based on one of the three ranks of relevance, i.e., relevant, partially relevant and irrel- evant. In our evaluation, relevant documents refer to both relevant and partially relevant documents5. ID description 0005 dimension reduction for clustering 0006 intelligent information retrieval 0019 syntactic analysis methods for Japanese 0024 machine translation systems Figure 4: Example descriptions in the NACSIS query 4.1 Evaluation of compound word translation We compared the following query translation methods: (1) a control, in which all possible translations derived from the (original) EDR technical terminology dictionary are used as query terms (EDR), (2) all possible base word translations derived from our dictionary are used (all), 5The result did not signiﬁcantly change depending on whether we regarded partially relevant as relevant or not. (3) randomly selected k translations derived from our bilingual dictionary are used (random), (4) k-best translations through compound word translation are used (CWT). For system EDR, compound words unlisted in the EDR dictionary were manually segmented so that substrings (shorter compound words or base words) can be translated. For both sys- tems random and CWT, we arbitrarily set k  3. Figure 5 and Table 2 show the recall- precision curve and 11-point average precision for each method, respectively. In these, J-J refers to the result obtained by the Japanese- Japanese IR system, which uses as documents Japanese titlesabstractskeywords comparable to English ﬁelds in the NACSIS collection. This can be seen as the upper bound for CLIR perfor- mance6. Looking at these results, we can con- clude that the dictionary production and prob- abilistic translation methods we proposed are eﬀective for CLIR. 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 precision recall J-J CWT all EDR random Figure 5: Recall-Precision curves for evaluation of compound word translation 6Regrettably, since the NACSIS collection does not contain English queries, we cannot estimate the upper bound performance by English-English IR. Table 2: Comparison of average precision for evaluation of compound word translation avg. precision ratio to J-J J-J 0.204  CWT 0.193 0.946 all 0.171 0.838 EDR 0.130 0.637 random 0.116 0.569 4.2 Evaluation of transliteration In the NACSIS collection, three queries con- tain katakana (base) words unlisted in our bilin- gual dictionary. Those words are ma-i-ni- n-gu (mining) and ko-ro-ke-i-sho-n (colloca- tion). However, to emphasize the eﬀectiveness of transliteration, we compared the following ex- treme cases: (1) a control, in which every katakana word is discarded from queries (control), (2) a case where transliteration is applied to every katakana word and top 10 candidates are used (translit). Both cases use system CWT in Section 4.1. In the case of translit, we do not use katakana entries listed in the base word dictionary. Fig- ure 6 and Table 3 show the recall-precision curve and 11-point average precision for each case, re- spectively. In these, results for CWT corre- spond to those in Figure 5 and Table 2, respec- tively. We can conclude that our transliteration method signiﬁcantly improves the baseline per- formance (i.e., control), and comparable to word-based translation in terms of CLIR per- formance. An interesting observation is that the use of transliteration is robust against typos in docu- ments, because a number of similar strings are used as query terms. For example, our translit- eration method produced the following strings for ri-da-ku-sho-n (reduction): riduction, redction, redaction, reduc- tion. All of these words are eﬀective for retrieval, be- cause they are contained in the target docu- ments. 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 precision recall J-J CWT translit control Figure 6: Recall-Precision curves for evaluation of transliteration Table 3: Comparison of average precision for evaluation of transliteration avg. precision ratio to J-J J-J 0.204  CWT 0.193 0.946 translit 0.193 0.946 control 0.115 0.564 4.3 Evaluation of the overall performance We compared our system (CWTtranslit) with the Japanese-Japanese IR system, where (unlike the evaluation in Section 4.2) transliter- ation was applied only to ma-i-ni-n-gu (min- ing) and ko-ro-ke-i-sho-n (collocation). Fig- ure 7 and Table 4 show the recall-precision curve and 11-point average precision for each sys- tem, respectively, from which one can see that our CLIR system is quite comparable with the monolingual IR system in performance. In ad- dition, from Figure 5 to 7, one can see that the monolingual system generally performs better at lower recall while the CLIR system performs better at higher recall. For further investigation, let us discuss sim- ilar experimental results reported by Kando and Aizawa (1998), where a bilingual dictionary produced from JapaneseEnglish keyword pairs in the NACSIS documents is used for query translation. Their evaluation method is al- most the same as performed in our experiments. One diﬀerence is that they use the OpenText search engine7, and thus the performance for Japanese-Japanese IR is higher than obtained in our evaluation. However, the performance of their Japanese-English CLIR systems, which is roughly 50-60 of that for their Japanese- Japanese IR system, is comparable with our CLIR system performance. It is expected that using a more sophisticated search engine, our CLIR system will achieve a higher performance than that obtained by Kando and Aizawa. 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 precision recall J-J CWT  translit Figure 7: Recall-Precision curves for evaluation of overall performance 5 Conclusion In this paper, we proposed a JapaneseEnglish cross-language information retrieval system, targeting technical documents. We combined a query translation module, which performs 7Developed by OpenText Corp. Table 4: Comparison of average precision for evaluation of overall performance avg. precision ratio to J-J J-J 0.204  CWT  translit 0.212 1.04 compound word translation and translitera- tion, with an existing monolingual retrieval method. Our experimental results showed that compound word translation and transliteration methods individually improve on the baseline performance, and when used together the im- provement is even greater. Future work will in- clude the application of automatic word align- ment methods (Fung, 1995; Smadja et al., 1996) to enhance the dictionary. Acknowledgments The authors would like to thank Noriko Kando (National Center for Science Information Sys- tems, Japan) for her support with the NACSIS collection. References Lisa Ballesteros and W. Bruce Croft. 1998. Resolv- ing ambiguity for cross-language retrieval. In Pro- ceedings of the 21th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 6471. Jaime G. Carbonell, Yiming Yang, Robert E. Fred- erking, Ralf D. Brown, Yibing Geng, and Danny Lee. 1997. Translingual information retrieval: A comparative evaluation. In Proceedings of the 15th International Joint Conference on Artﬁcial Intelligence, pages 708714. Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, and Shih-Chung Tsai. 1998. Proper name trans- lation in cross-language information retrieval. In Proceedings of the 36th Annual Meeting of the As- sociation for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 232236. Kenneth W. Church and Robert L. Mercer. 1993. Introduction to the special issue on computa- tional linguistics using large corpora. Computa- tional Linguistics, 19(1):124. Mark W. Davis and William C. Ogden. 1997. QUILT: Implementing a large-scale cross-language text retrieval system. In Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Re- trieval, pages 9298. Edsgar W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numerische Mathe- matik, 1:269271. Susan T. Dumais, Thomas K. Landauer, and Michael L. Littman. 1996. Automatic cross- linguistic information retrieval using latent se- mantic indexing. In ACM SIGIR Workshop on Cross-Linguistic Information Retrieval. Atsushi Fujii and Tetsuya Ishikawa. 1999. Cross- language information retrieval using compound word translation. In Proceedings of the 18th In- ternational Conference on Computer Processing of Oriental Languages, pages 105110. Pascale Fung. 1995. A pattern matching method for ﬁnding noun and proper noun translations from noisy parallel corpora. In Proceedings of the 33rd Annual Meeting of the Association for Computa- tional Linguistics, pages 236243. Denis A. Gachot, Elke Lange, and Jin Yang. 1996. The SYSTRAN NLP browser: An application of machine translation technology in multilingual in- formation retrieval. In ACM SIGIR Workshop on Cross-Linguistic Information Retrieval. David A. Hull and Gregory Grefenstette. 1996. Querying across languages: A dictionary-based approach to multilingual information retrieval. In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Devel- opment in Information Retrieval, pages 4957. Japan Electronic Dictionary Research Institute. 1995. Technical terminology dictionary (informa- tion processing). (In Japanese). Noriko Kando and Akiko Aizawa. 1998. Cross- lingual information retrieval using automatically generated multilingual keyword clusrters. In Pro- ceedings of the 3rd International Workshop on In- formation Retrieval with Asian Languages, pages 8694. Noriko Kando, Teruo Koyama, Keizo Oyama, Kyo Kageura, Masaharu Yoshioka, Toshihiko Nozue, Atsushi Matsumura, and Kazuko Kuriyama. 1998. NTCIR: NACSIS test collection project. In The 20th Annual BCS-IRSG Colloquium on In- formation Retrieval Research. Kevin Knight and Jonathan Graehl. 1998. Ma- chine transliteration. Computational Linguistics, 24(4):599612. Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Osamu Imaichi, and Tomoaki Imamura. 1997. Japanese morphological analysis system ChaSen manual. Technical Report NAIST-IS-TR97007, NAIST. (In Japanese). George A. Miller, Richard Beckwith, Christiane Fell- baum, Derek Gross, Katherine Miller, and Randee Tengi. 1993. Five papers on WordNet. Techni- cal Report CLS-Rep-43, Cognitive Science Labo- ratory, Princeton University. P. E. Mongar. 1969. International co-operation in abstracting services for road engineering. The In- formation Scientist, 3:5162. Douglas W. Oard and Paul Hackett. 1997. Docu- ment translation for cross-language text retrieval at the University of Maryland. In The 6th Text Retrieval Conference. Akitoshi Okumura, Kai Ishikawa, and Kenji Satoh. 1998. Translingual information retrieval by a bilingual dictionary and comparable corpus. In The 1st International Conference on Language Resources and Evaluation, workshop on translin- gual information management: current levels and future abilities. Ari Pirkola. 1998. The eﬀects of query structure and dictionary setups in dictionary-based cross- language information retrieval. In Proceedings of the 21th Annual International ACM SIGIR Con- ference on Research and Development in Informa- tion Retrieval, pages 5563. Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill. Gerard Salton. 1970. Automatic processing of for- eign language documents. Journal of the Amer- ican Society for Information Science, 21(3):187 194. Paraic Sheridan and Jean Paul Ballerini. 1996. Ex- periments in multilingual information retrieval us- ing the SPIDER system. In Proceedings of the 19th Annual International ACM SIGIR Confer- ence on Research and Development in Informa- tion Retrieval, pages 5865. Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):138. Stephen Wan and Cornelia Maria Verspoor. 1998. Automatic English-Chinese name transliteration for development of multilingual resources. In Pro- ceedings of the 36th Annual Meeting of the Associ- ation for Computational Linguistics and the 17th International Conference on Computational Lin- guistics, pages 13521356.",
  "44.pdf": "arXiv:cs9907008v1 [cs.CL] 6 Jul 1999 Explanation-based Learning for Machine Translation Janine Toole Fred Popowich Devlan Nicholson Davide Turcato Paul McFetridge Natural Language Laboratory, School of Computing Science, Simon Fraser University 8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada and Gavagai Technology P.O. 374, 3495 Cambie Street, Vancouver, British Columbia, V5Z 4R3, Canada {toole, popowich, devlan, turk, mcfet}cs.sfu.ca Abstract In this paper we present an application of explanation-based learning (EBL) in the parsing module of a real-time English-Spanish machine translation system designed to translate closed captions. We discuss the eﬃciencycoverage trade-oﬀs available in EBL and introduce the techniques we use to increase coverage while maintaining a high level of space and time eﬃciency. Our performance results indicate that this approach is eﬀective. 1 Introduction. In this paper we present an application of explanation-based learning (EBL) in the parsing module of a real-time English-Spanish machine translation (MT) system de- signed to translate closed captions. The core idea of EBL is to convert previous analyses into some generalized form that can be called on when similar new examples are en- countered (Mitchell et al. 1986; van Harmelen  Bundy 1988). The main motivation for using an explanation-based learning approach to parsing is to increase eﬃciency. Dramatic increases in speed can be obtained, at the expense of some coverage and in- creased memory requirements. In our MT system, eﬃciency is a major priority because the closed captions included with television and video broadcasts will be translated in real-time. In this paper we discuss the various eﬃciencycoverage trade-oﬀs that are available within the EBL approach and describe the techniques we use to maximize coverage while maintaining eﬃciency as the primary focus. This paper is organized as follows. In section 2 we introduce the EBL approach as it has been used in natural language processing. Section 3 introduces the translation system in which the EBL parser is embedded and discusses the consequences of eﬃ- ciency versus coverage in this domain. In section 4 we introduce our EBL parser and describe the means we use to maximize eﬃciency and coverage. The performance of the system is discussed in section 5. Concluding comments can be found in section 6. 2 Explanation-based Learning in Natural Language Processing. The EBL approach has been extended to natural language processing (NLP) by Rayner (1988), Srivinas  Joshi (1995), among others. They use existing wide-coverage gram- mars (or a tree bank (Simaan 1997)) to analyze a set of training examples. The Corpus Training Corpus Size(sentences) Coverage ATIS 356 80 IBM 1100 40 Alvey 80 50 Table 1: Coverage on 3 diﬀerent corpora, Srivinas  Joshi 1995 analyses are then generalized in various ways so that they are applicable to a wide range of examples beyond the original input string. In some cases these analyses are manually-checked before generalization to ensure they are the correct interpretation for the given input string (Samuelsson 1994). Typically, the generalized parse is stored with a key that is used to subsequently identify when this parse is applicable to a new example. Srivinas  Joshi (1995), for example, use the part of speech sequence of the input string as the key. The generalized structures that are saved during EBL train- ing are referred to as generalized parses, macro-rules, generalized macro-rules, or rule-chunks. For our purposes these terms are inter-changeable. It is a characteristic of general wide-coverage grammars that they are extremely ambiguous. Hence, parsing involves many options and can be quite slow. The advantage of EBL is that the run-time complexity of parsing is reduced since the majority of the parsing occurs oﬀ-line. At run-time, the system need only identify the most similar parse from those available. In theory the accuracy of the system can be increased since an EBL grammar is tuned to the rules actually used in the training domain. The one disadvantage of the EBL approach is that there is a loss of coverage since the EBL grammar is a subset of the original grammar used to derive it. If an example is not covered by the generalized examples in the EBL grammar then an analysis cannot be provided. Rayner  Carter (1996) note that there are two main parameters that can be adjusted in the EBL learning phase: training corpus size and the number and type of macro-rules. They note that the larger the corpus the smaller the loss in coverage. However, it should be noted that the sizecoverage correlation only holds within a corpus. Comparisons cannot be made across diﬀerent corpora. For example, Srivinas  Joshi (1995) conduct tests on three diﬀerent corpora. The coverage details of the EBL grammars they developed are given in Table 1. Both the largest and the smallest corpora have extremely poor coverage. The median-sized corpus exhibits the best coverage. Hence, the coverage of a given corpus depends both on the variability of the structures contained in the corpus as well as on its size. The second parameter that Rayner  Carter identify is the number and type of rule-chunks, or macro-rules, that are generalized. At one end of the spectrum are approaches like Srivinas  Joshi (1995) and Neumann (1994) where the whole parse tree for each training example is turned into one generalized macro-rule. This type of grammar is extremely fast, but the coverage loss is typically high. At the other end of the spectrum, each rule-chunk is derived from a single rule application. This results in a grammar that is identical to the original one; there is no loss in coverage but equally no gain in eﬃciency. Rayner  Samuelsson (1994) have taken the approach of trying to ﬁnd an intermediate solution by creating macro-rules corresponding to four possible units: full utterance, recursive NPs, non-recursive NPs and prepositional phrases. Samuelsson (1994), on the other hand, attempts to identify chunks automatically via an entropy minimization method. Rayner  Carter (1996) generalize to seven possible units. The variation in coverage and speed that can be obtained by varying the number and type of rule chunks is illustrated by comparing the results of Srivinas  Joshi (1995) and Rayner  Carter (1996). Srivinas  Joshi obtain coverage of 40-80 (over 3 corpora) with a 60 fold decrease in time when compared to parsing their test examples with the original parser and grammar. Rayner  Carter claim 95 coverage with a 10 fold decrease in parsing time. There is a third parameter, not noted by Rayner  Carter (1996), that can be ad- justed in EBL. This is the amount of information that is retained in the generalized rule. Minimally it is necessary to remove the information contributed by the lexical items so that the rule can apply to new word strings. Beyond this, there is considerable scope for variation. However, this possibility does not seem to have been taken advantage of. Rayner  Samuelsson (1994), for example, are typical in keeping all of the feature sharing speciﬁed in the original rules which make up the generalized macro-rule. In sum, there are three parameters that can be adjusted in an EBL approach; the size of the training corpus, the size of the rule chunks, and the amount of information that is saved in the generalized parse. Before describing the approach we take, we brieﬂy introduce the MT system in which our EBL approach is embedded. This provides the motivation for our eﬃciency-based approach. 3 The MT System. We have developed a constraint-based lexicalist transfer system, which is designed to translate closed captions from English to Spanish (Popowich et al. 1997). The intended goal of this system is a consumer product that Spanish native speakers will purchase so that they can access North American broadcasting in their own language. This requires real-time translation since the translation occurs once the broadcast signal is received by the viewers television. The translation must be produced extremely quickly since the rate of caption ﬂow is quite high. This translation environment dictates that, in the trade-oﬀ between eﬃciency and coverage, eﬃciency should be given the ﬁrst priority. There are other factors of the domain which support this preference for eﬃciency over coverage. In the context in which the translations will be used, the captions are just one of several information sources available to the viewer. When reading captions, the viewer also has the sound eﬀects and vocal tones of the source language speech, the visual context, and the storyline, each of which contribute to convey meaning. Viewers use these information sources to complement the information from the captions. Shortcomings in the translated captions may be made up by the other information sources. Hence, in this domain we can aﬀord to sacriﬁce some coverage in order to achieve the required eﬃciency. An example of closed captions can be found in Table 2. The translation system consists of analysis, transfer, and generation components as illustrated in Figure 1. Note, this diagram has been simpliﬁed somewhat from the CYNTHIA, PUSH! COME ON, CYNTHIA! GOOD GIRL. COME ON, CYNTHIA. GOOD GIRL. HES GOT THE HEAD. ITS A BOY. OH! OH! OH, YOUVE GOT A BOY! HERE, HONEY. HERE YOU GO. OKAY. OKAY. YEAH. SHOULD NOTIFY THE COUNTY, HUH? THANKS. Table 2: Script fragment details of the actual system. Translation is a non-deterministic multi-phase process: failure in any one process causes back-tracking into a previous process. The rules and lexical entries in the system are deﬁned in terms of feature structures. The analysis and generation phases need not utilise the same theoretical framework. It is only necessary that the appropriate features are used. The analysis component consists of a part of speech (POS) tagger, a segmenter, and an EBL parser. The POS tagger uses a modiﬁed subset of the POS tags used in the Oxford Advanced Learners Dictionary (henceforth OALD). The tagger assigns one tag per word or phrase. The rule-based segmenter splits an input string into one or more substrings which are translated separately. We refer to these substrings as segments. EBL Parser Segmenter Transfer Generation POS Tagger Figure 1: System architecture 4 An Eﬃciency-focused EBL Parser. In our EBL parser, we use a single-rule per training example approach where the whole parse tree for each training segment is converted into one generalized macro- rule. Although this maximizes eﬃciency, it is most challenging in terms of coverage since a given generalized example will only apply to examples of the same length (Sriv- inas  Joshi (1995) and Neumann (1994) discuss techniques to overcome this limita- tion). The generalized parses are keyed oﬀ the POS tag sequence of the input string. At run-time, the input string is tagged and segmented. Each segment is looked up in the EBL index. If the POS sequence is found, then the lexical items from the input string are uniﬁed with the generalized parse(s) keyed for this POS sequence. If uniﬁcation is successful, a parse has been found and the analysis phase is complete. While this standard approach meets our eﬃciency requirements it does pose some challenges for coverage. In the following we describe the techniques we have imple- mented to minimize coverage loss. The generalization techniques we describe, whether concerning training examples, lexical entries or generalized parses, share a common theoretical motivation, based on the relation between our source lexicon, our original grammar and our tagger. Our original grammar is an HPSG-style lexicalist grammar, with most of the syntactic information encoded in lexical entries. In turn, our lexicon was derived from the OALD. The core of our lexical entries are macros corresponding to modiﬁed versions of the lexical tags used in the source lexicon. Each macro deﬁnes one relevant lexical class (e.g. one macro for each verb subcategorization frame) and expands the information concisely encoded in an OALD tag into a full feature structure. On the other hand, our tagger uses the same set of tags. Therefore there is a direct mapping between the lexical macros used in the lexicon and the tags used by the tagger. This correspondence has two relevant consequences. The negative consequence is that our tagset is larger than most standard tagsets (such as that used in the Penn treebank), and encodes information (like subcategorization) usually unavailable in other tagsets. This makes tagging a more challenging task than it is with other tagsets. The positive consequence of having a lexicalist grammar and a direct mapping between lexical categories and tags is that our tags encode, in a nutshell, most of the information relevant to parsing. This enables us to rely mainly on those tags for parsing and be able to remove other features from our lexical entries in the generalization phase without considerably aﬀecting the accuracy of parsing. In other words, we transfer part of the burden of resolving syntactic ambiguity on tagging, thus making the parsing task easier. The ﬁrst way in which we increase coverage is by performing some of our gener- alization before parsing the training corpus. Before parsing a training example, we replace each word in the example with the part of speech tag assigned by our tagger. In addition, we replace the lexicon with one containing a lexical entry for each POS tag. The lexical entry for each of these tag-words is the most general instance of all words that can take this tag. For example, the lexical entry for the tag-word deter- miner is a feature structure corresponding to the generalization of all the individual determiner lexical entries. Having replaced the words with POS tags, we then parse the tag sequence as if it were a regular sentence (Popowich et al. 1997). This approach allows us to maximize the coverage of our tree. Because the input lexical items are extremely general, the parse obtained is quite general and is not a speciﬁc idiosyncratic parse that may be produced from an input string containing words that are in some way unique or unusual. Hence, we end up with parses that are likely to be general and therefore widely applicable to new examples. The second way in which we deal with the coverage issue is by prioritizing the selection of the training examples. Most training sets discussed in the literature are quite small (below 5000), the only exception being the 15,000 utterance corpus of Rayner  Carter (1996). In addition, in many cases it is unclear whether the size of the training and test corpora are restricted by the amount available or are dictated by some other reason. In any case, if the training corpus is part of some larger corpus, there appears to be no principled motivation behind its selection from the larger corpus. In contrast, we prioritize the selection of training examples. In order to maximize the coverage of the grammar, we ensure that our training corpus contains examples that reﬂect the most common constructions found in the corpus. Our method for extracting the training corpus is as follows, and is illustrated in Figure 2. Our base corpus consists of 11 million words of closed captions. As a ﬁrst step we segment and tag this corpus to obtain a second corpus which consists of the part of speech sequences for each segment found in the original corpus. We select the 18,000 most frequent POS sequences to comprise our training corpus. Using this approach we guarantee that we are getting the most mileage out of our EBL grammar since each training example is attested to be a frequently occurring example in this domain. In addition, we increase coverage by using as large a training corpus as possible. 11 million word corpus Tag and Segment the corpus Extract 18,000 most frequent POS sequences segments tagged training data. Parse tag sequences in Generalize parse and save. training input EBL Parses Figure 2: Training Architecture Once an input sequence has been parsed, we essentially have a generalized parse already, since the input string consists of POS tags instead of speciﬁc lexical items. In order to maximize coverage, we take our generalization one step further and speciﬁcally target the parse to reﬂect the needs of the transfer component of the translation system. We ignore the majority of the instantiated features and focus only on the 29 features which are referred to in the MT systems bilingual lexicon. We save any co-indexing found between these features and ignore the speciﬁc values of the features for all but eight of the features. The goal of the bilingual lexicon is to specify mappings between source language lexical items and target language lexical items. Hence, the 29 features which are refer- enced in the bilingual lexicon, and for which we save co-indexing information, concern the status and indexes of gaps, ﬁllers, modiﬁers, and complements, features specifying whether the lexical item is interrogative or relative, and features for number, POS, etc. The eight features for which we save values mainly provide details of the status of com- plements, gaps, ﬁllers, and modiﬁers in the lexical item, as well as its word type and complement form. Although by saving these features we make our parse less general (the most general form would just save the co-indexing), the realities of our system make this approach the most eﬃcient. These eight features diﬀerentiate between many homographic lexical items that have the same part of speech tag, yet have diﬀerent feature structures. If we do not save the values of these features, then an inappropriate lexical item can unify with the parse. This inappropriate item is likely to cause prob- lems in either transfer or generation, requiring the system to eventually backtrack into the parser to select the appropriate lexical item. However, to rely on those components to make up for a bad choice in parsing is time consuming and not the best use of the limited time available to our real-time system. Hence, we save these features in the generalized parse. This means that an inappropriate lexical entry will fail in parsing instead of later in the system. The system can then immediately try other lexical en- tries which may be more appropriate. A simpliﬁed example of a generalized parse can be found in Figure 3. 1 2 1 1 2 Simplified example of generalized parse Training String: case_prep determiner noun Original String: on the beach number: index: index1: Only co-indexing saved. Value saved. index1: number: index: index1: number: index: comp1:status: yes comp1:status: boolean comp1:status:boolean Figure 3: Simpliﬁed Example of a Generalized Parse There are several advantages to this approach. Firstly, our parses are more general since we do not save the speciﬁc values of most of the features. Instead, we save the co-indexing which has been introduced by the rules that were used in creating the parse. In addition, we are only interested in the co-indexing that holds between the 29 features that are relevant to transfer. For the most part, the values of the features will be instantiated during parsing when the parse is uniﬁed with lexical entries. Secondly, this approach is more eﬃcient since we are saving less information for each parse. Hence, for an EBL grammar of a speciﬁc size, this means an increase in the number of generalized parses that can be saved. Thirdly, fewer features means that run-time uniﬁcation is faster. The ﬁnal technique that we use to increase coverage is to generalize the key un- der which parses are saved. For instance, our tagset categorises prepositions into ﬁve classes: noun-modifying, verb-modifying, verb complement, partitive, and pas- sive prepositions. Although the features structures for these lexical items are distinct, in some cases they are combined using the same rules. Hence, the parses diﬀer only in the values of the features and not in the co-indexing that parsing introduces. In order to increase coverage, for selected tags we save parses under a more general tag, such as preposition. On this approach, a generalized parse for an input sequence such as case prep determiner noun, is saved under the key preposition determiner noun. It can then be used to parse any input sequence containing one of the six prepositions followed by a determiner and a noun. Hence, the one generalized parse can apply to a wider range of input. This results in increased coverage. Furthermore, the resulting parse still contains the more detailed information from the more speciﬁcally tagged lexical entries. This information is vital for the accuracy of the transfer module. In addition to the above approaches, we also include a few run-time heuristics to increase coverage. If a generalized parse cannot be found for a POS sequence, we allow the deletion of various lexical items. These include adverbs, adjectives, and other parts of speech whose primary goal is to modify information existing in the utterance. After deleting a lexical item, we again search the index for the EBL grammar to see if this reduced POS sequence can be found. It is the characteristics of our domain that allow us to take some of these approaches. The nature of colloquial text is such that the input to the machine translation system may cover a very wide semantic domain and may not be strictly grammatical. Hence, our source grammar is a grammar of colloquial English rather than a grammar of formal English. In addition, the grammar is just restrictive enough to augment the input lexical entries with suﬃcient information for the subsequent transfer phase. Any additional information which can be recovered in some other way during transfer is superﬂuous and thus avoided. For instance, the English grammar does not enforce agreement between subject and verb, because the Spanish grammar still has the means (by reference to argument indices) to recognize the subject-verb relation and to independently enforce proper agreement on the Spanish output. The unrestrictive nature of our original grammar motivates our unrestrictive approach to our EBL grammar. We maintain only the information that is required for eﬀective transfer. Our heuristic to delete words from the input string is possible because our overall translation goal is not meaning equivalence between the input string and its trans- lation, but meaning subsumption (Popowich et al. 1997). That is, our translation objectives have been met if the meaning of the target string subsumes the meaning of Parser Average time per segment Chart parser 900.5 milliseconds EBL Parser 4.0 milliseconds Table 3: Parser Comparison the source string. This approach is possible because the captions are only one of the information sources available to the user. In sum, our prime objective of eﬃciency dictates that we use the single-rule per training example approach. We mitigate the loss of coverage engendered by this approach by (i) prioritizing the training corpus, (ii) generalizing the input to training by parsing POS sequences instead of word sequences, and (iii) by reducing the amount of information that is stored for each generalized parse. 5 Performance. This section describes a number of experiments carried out to test the eﬀectiveness of our approach. We ﬁrst evaluate the eﬃciency of the system and then review its coverage. The EBL grammar was built from a corpus consisting of the 18,000 most frequent POS sequences found in the segmented 11 million word corpus. Around 11,500 of these sequences could be parsed by our regular chart parser and hence the EBL parser covers these 11,500 sequences. Of the sequences which found a parse, the maximum length tag sequence contains 21 tags and the average length is 7.3 tags. The 11,500 sequences which could be parsed were generalized to 8,757 tag sequence keys. Hence, by generalizing the preposition, verb, and auxiliary tags, we are able to reduce our index by about 25. This means that in our next phase we can increase the number of tag sequences we use in training. We limit the number of parses that are stored for each POS sequence in the index. The maximum is a function of the number of generalizable tags in the sequence. Parsing the training corpus resulted in 20,235 generalized parses stored for an average of 2.3 parses per POS index key. The tests were carried out on a ﬁle of 1000 sentences randomly selected from the 11 million word corpus. The details in Table 3 indicate that the system meets our speed requirements. The average parse time for a segment using the EBL approach is signiﬁcantly faster than our original chart parser. The times given in Table 3 reﬂect the cumulative time spent in the parse module. That is, if the system back-tracks into the parsing module, the additional time spent in this module is added to the total time. Overall, the coverage results are also encouraging. Table 4 provides the details. The EBL database has an overall coverage of 78.6 for multi-word segments produced by our segmenter. That is, 78.6 of the multi-word segments in the test ﬁle are assigned a POS tag sequence that it is in the EBL database. Considering that we take the one rule per training example approach, this is quite impressive. 87.9 of the multi-word segments that were covered by the EBL database found a parse in the EBL database Coverage Performance POS sequence found (overall) 78.6 Parse found (when sequence already found) 87.9 Translation found (when parse already found) 89.9 Table 4: Coverage English input EBL translation Full Grammar translation (a) whyd you let por que tu alquilando por que lo dejaste ganar-Y him win el victoria-N (b) this isnt right esto es no correcto-OK esto no esta bien-Y (c) thats a smart idea eso es una idea lista-Y eso es una lista idea-OK Table 5: Comparison of Output (i.e. one of the stored parses uniﬁed successfully with the input lexical items). Of those multi-word segments which found a parse via the EBL method, 89.9 subsequently found a translation. In all, 62.1 of segments found a translation via the EBL method. This is an increase of 15 over the results found in our original EBL system. That system provided EBL translations for 54.2 of the input segments. In an analysis of 100 of the input sentences, the translations for 59.4 of the sen- tences (which consist of one or more segments) were rated as acceptable (on an ac- ceptable not acceptable scale). In comparison, when translated via the original chart parser, an acceptability rate of 68 was achieved. Our criteria for acceptability are discussed in (Popowich et al. 1997). Some example translations are given in Table 5. AcceptableUnacceptable trans- lations are identiﬁed by YN respectively. In this table, the notation OK is used for an example which is deemed acceptable but which is not as good as the translation provided by the alternative parser. As expected, there are cases where the coverage of the EBL grammar is less than that of the original grammar. This is illustrated by the examples (a) and (b) which failed to ﬁnd a parse via the EBL grammar. In the case of (a), the tag sequence corresponding to this input was not found in the EBL index. Hence, no parse could be found. In the case of (b) a parse was found via the EBL approach, but this parse could not be translated by the transfer and generation com- ponents of the system. Both examples were translated by the fall-back word-for-word translation method. However, there are also cases where the EBL grammar performs better than the original full grammar. The full grammar failed to ﬁnd a correct parse for (c) and the input was translated word-for-word. In contrast, a correct parse was found via the EBL approach, resulting in a more acceptable translation. These results indicate that the EBL approach will successfully meet the needs of our real-time English-Spanish translation system. In addition, they provide clear indication of the areas in which we should focus to improve performance. For example, the coverage rate of 78.6 needs to be increased. To do this we plan to integrate our EBL approach with a partial parser. Secondly, for the cases where a parse is found, we need to increase the accuracy of the found parse. We will be analyzing our results to determine whether the correct parse is not available or if it is available but not selected. Thirdly, we plan to extend the EBL approach to the generation phase of the translation system. 6 Conclusion The explanation-based learning approach to parsing provides an eﬃcient means of providing analyses at the expense of some coverage. In this paper we described how we took advantage of these eﬃciency gains in order to minimize the analysis time in a real-time English to Spanish MT system. We found that even when eﬃciency is the prime objective, there are techniques that can be used to minimize the coverage loss. On several levels we were able to maximize coverage while focussing on eﬃciency. This was achieved by making sure that the examples selected for training were instances of the most frequent constructions, by generalizing the input so that the parses produced would be of the most general type, and by generalizing the saved macro-rules and their key to the minimum needed for subsequent components of the system. References van Harmelen, F. and A. Bundy. 1988. Explanation-Based Generalization  Partial Evaluation (Research Note). Artiﬁcial Intelligence 36, pages 401412. Mitchell, T., Keller, R., and S. Kedar-Cabelli. 1986. Explanation-Based Generalization: A Unifying View. Machine Learning 1:1. Neumann, G. 1994. Application of Explanation-Based Learning for Eﬃcient Processing of Constraint-based Grammars, in Proceedings of 10th IEEE on Artiﬁcial Intelligence for Applications, San Antonio, Texas. Fred Popowich, Davide Turcato, Olivier Laurens, Paul McFetridge, J. Devlan Nicholson, Patrick McGivern, Maricela Corzo-Pena, Lisa Pidruchney, and Scott MacDonald. 1997. A lexical- ist approach to the translation of colloquial text. , in Proceedings of the 7th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 7686, New Mexico, USA. Rayner, M. 1988. Applying Explanation-Based Generalization to Natural Language Pro- cessing, in Proceedings of the International Conference on Fifth Generation Computer Systems, Kyoto, pages 12671274. Rayner, M. and D. Carter. 1996. Fast Parsing Using Pruning and Grammar Specialization, in Proceedings ACL-96, Santa Cruz, California. Rayner, M. and C. Samuelsson. 1994. Corpus-Based Grammar Specialization for Fast Analysis, Agnas et al. in Spoken Language Translator: First Year Report, SRI Technical Report CRC-043. http:www.cam.sri.com, pages 4154. Samuelsson, C. 1994. Grammar Specialization through Entropy Thresholds, in Proceedings of ACL-94, Las Cruces, New Mexico, pages 188194. Simaan, K. 1997. Explanation-Based learning of Data-Oriented Parsing. cmp-lg archive 9708013. Srivinas, B. and A. Joshi. 1995. Some Novel Applications of Explanation-Based Learning to Parsing Lexicalized Tree-Adjoining Grammars. cmp-lg archive 9505023.",
  "45.pdf": "arXiv:cs9907010v1 [cs.CL] 7 Jul 1999 Language Identiﬁcation With Conﬁdence Limits David Elworthy Canon Research Centre Europe 1 Occam Court Occam Road Surrey Research Park Guildford GU2 5YJ United Kingdom dahecre.canon.co.uk Abstract A statistical classiﬁcation algorithm and its ap- plication to language identiﬁcation from noisy input are described. The main innovation is to compute conﬁdence limits on the classiﬁcation, so that the algorithm terminates when enough evidence to make a clear decision has been made, and so avoiding problems with categories that have similar characteristics. A second ap- plication, to genre identiﬁcation, is brieﬂy ex- amined. The results show that some of the problems of other language identiﬁcation tech- niques can be avoided, and illustrate a more im- portant point: that a statistical language pro- cess can be used to provide feedback about its own success rate. 1 Introduction Language identiﬁcation is an example of a gen- eral class of problems in which we want to as- sign an input data stream to one of several cat- egories as quickly and accurately as possible. It can be solved using many techniques, including knowledge-poor statistical approaches. Typi- cally, the distribution of n-grams of characters or other objects is used to form a model. A comparison of the input against the model de- termines the language which matches best. Ver- sions of this simple technique can be found in Dunning (1994) and Cavnar and Trenkle (1994), while an interesting practical implementation is described by Adams and Resnik (1997). A variant of the problem is considered by Sibun and Spitz (1994), and Sibun and Rey- nar (1996), who look at it from the point of view of Optical Character Recognition (OCR). Here, the language model for the OCR system cannot be selected until the language has been identiﬁed. They therefore work with so-called shape tokens, which give a very approximate en- coding of the characters shapes on the printed page without needing full-scale OCR. For exam- ple, all upper case letters are treated as being one character shape, all characters with a de- scender are another, and so on. Sequences of character shape codes separated by white space are assembled into word shape tokens. Sibun and Spitz then determine the language on the basis of linear discriminant analysis (LDA) over word shape tokens, while Sibun and Reynar ex- plore the use of entropy relative to training data for character shape unigrams, bigrams and tri- grams. Both techniques are capable of over 90 accuracy for most languages. However, the LDA-based technique tends to perform signiﬁ- cantly worse for languages which are similar to one another, such as the Norse languages. Rela- tive entropy performs better, but still has some noticeable error clusters, such as confusion be- tween Croatian, Serbian and Slovenian. What these techniques lack is a measure of when enough information has been accumulated to distinguish one language from another reli- ably: they examine all of the input data and then make the decision. Here we will look at a diﬀerent approach which attempts to overcome this by maintaining a measure of the total ev- idence accumulated for each language and how much conﬁdence there is in the measure. To outline the approach: 1. The input is processed one (word shape) token at a time. For each language, we de- termine the probability that the token is in that language, expressed as a 95 con- ﬁdence range. 2. The values for each word are accumulated into an overall score with a conﬁdence range for the input to date, and compared both to an absolute threshold, and with each other. Thus, to select a language, we require not only that it has a high score (probability, roughly), but also that it is signiﬁcantly better scoring than any other. 3. If the process fails to make a decision on the data that is available, the subset of the lan- guages which have exceeded the absolute threshold can be output, so that even if a ﬁnal decision has not been made, the likely possibilities have been narrowed down. We look at this procedure in more detail below, with particular emphasis on how the underlying statistical model provides conﬁdence intervals. An evaluation of the technique on data similar to that used by Sibun and Reynar follows1. 2 The Identiﬁcation Algorithm The essential idea behind the identiﬁcation al- gorithm is to accumulate the probability of the language given the input tokens for each lan- guage, treating each token as an independent event. To obtain the probability of a language l given a token t, p(lt), we use Bayes rule: p(lt)  p(tl)p(l) p(t) where p(tl) is the probability of the token if the language is known, p(t) is the a priori probabil- ity of the token, and p(l) is the a priori probabil- ity of the language. We will assume that p(l) is constant (all languages are equi-probable) and drop it from the computation; in the tests, we will use the same amount of training data for each language. The other two terms are esti- mated from training data, using the procedure described in section 2.2. 2.1 The language model and the algorithm The input to the algorithm consists of a stream of tokens, such as word shape tokens (as in Si- bun and Spitz, or Sibun and Reynar) or words themselves. The model for each language con- tains the probability of each known token given the language, expressed as three values: the ba- sic probability, and the lower and upper limits 1Some ideas related to the use of conﬁdence limits can also be found in Dagan et al. (1991), applied in a diﬀerent area. of a range containing this probability for a spe- ciﬁc level of conﬁdence. We will denote these by pB(tl), pL(tl), pH(tl), for base, low and high values. The probability that a token which has never been seen before is in a language is also present in the model of the language. In ad- dition, there is a language independent model, containing the p(t) values. No conﬁdence range is used for them, although this would be a sim- ple extension of the technique. The algorithm proceeds by processing tokens, building up evidence about each language in three accumulators. The accumulators rep- resent the overall probability of the language given the entire stream of tokens to date, again as base, low and high values, denoted aB(l), aL(l), aH(l). They are set to zero at the start of processing, and the logarithms of the proba- bilities are added to them as each token is pro- cessed. By taking logarithms of probabilities, we are in eﬀect measuring the amount of evi- dence for each language, expressed as informa- tion content. From a practical point of view, using logarithms also helps keep all the values in a reasonable range and so avoids numerical underﬂow. After processing each token, two tests are ap- plied. Firstly, we examine the base accumulator for the language which has the highest accumu- lated total, and test whether it is greater than a ﬁxed threshold, called the activation threshold. If it is, then we conclude that enough informa- tion has been accumulated to try to make a de- cision. The low value for this language aL(l) is then compared against the high value aH(l) for the next best language l, and if aL(l) exceeds aH(l) language l is output and the algorithm halts. Otherwise, the process continues with the next token, until the best choice language is a clear winner over any other. Finally, if we reach the end of the input data without a decision being made, several options are possible, depending on the needs of the ap- plication. We can simply output the language with the highest base score, even if the second test is not satisﬁed. Alternatively, we can out- put the highest scoring language, and all other languages whose high probability is greater than the low probability of this language. 2.2 Training the model The model is trained using a collection of cor- pora for which the correct language is known. For a given language l and token t, let f(t, l) be the count of the token in that language and f(l) be the total count of all tokens in that language. f(t) is the count of the token t across all the lan- guages, and F the count of all tokens across all languages. The probability of the token occur- ring in the language p(tl) is then calculated by assuming that the probabilities follow a bino- mial distribution. The idea here is that token occurrences are binary events which are either the given token t or are not. For large f(t, l), the underlying probability can be calculated by us- ing the normal approximation to the binomial, giving the base probability pB(tl)  f(t, l) f(l) The standard deviation of this quantity is σ(t, l)   f(l)pB(tl)(1  pB(tl)) The low and high probabilities are found by taking a given number of standard deviations d from the base probability. pL(tl)  f(t, l)  dσ(t, l) f(l) pH(tl)  f(t, l)  dσ(t, l) f(l) In the evaluation below, d was set to 2, giving 95 conﬁdence limits. For lower values of f(t, l), the calculation of the low and high probabilities can be made more exact, by substituting them for the base prob- ability in the calculation of the standard devia- tion, giving pL(tl)  f(t, l)  d  f(l)pL(tl)(1  pL(tl)) f(l) pH(tl)  f(t, l)  d  f(l)pH(tl)(1  pH(tl)) f(l) Approximating 1  pL(tl) and 1  pH(tl) to 1 on the grounds that the probabilities are small, and solving the equations gives pL(tl)  (  d2  4f(t, l)  d)2 4f(l) pH(tl)  (  d2  4f(t, l)  d)2 4f(l) The calculation requires marginally more com- putational eﬀort than the ﬁrst case, and in prac- tice we use it for all but very large values of f(t, l), where the approximation of 1  pL(tl) and 1  pH(tl) to 1 would break down. For very small values of f(t, l), say less than 10, the normal approximation is not good enough, and we calculate the probabilities by reference to the binomial equation for the prob- ability of m (f(t, l)) successes in n ( f(l)) trials: p(m)  pm(1  p)nmn! m!(n  m)! p is the underlying probability of the distribu- tion, and this is what we are after. By choos- ing values for p(m) and solving to ﬁnd p we can obtain a given conﬁdence range. To ob- tain a 95 interval, p(m) is set to 0.025, 0.5 and 0.975, yielding pL(tl), pB(tl), and pH(tl), respectively. In fact, this is not exactly how the probability ranges for low frequency items should be calculated: instead the cumulative probability density function should be calcu- lated and the range estimated from it2. For the present purposes, the low frequency items do not make much of a contribution to the overall success rate, and so the approximation is unim- portant. However, if similar techniques were ap- plied to problems with sparser data, then the procedure here would have to be revised. Finally, we need a probability for tokens which were not seen in the training data, called the zero probability, for which we set m  0 in the above equation giving p(0l)  1  n p(m) It is not clear what it means to have a conﬁdence measure here, and so we use a single value for base, low and high probabilities, obtained by setting p(m) to 0.95. Similar calculations using f(t) in place of f(t, l) and F in place of f(l) give the a priori token probabilities p(t). As already noted, base, low and high value could have been calculated in this case, but as a minor simpliﬁcation, we use only the base probability. 2Thanks to one of the referees for pointing this out. 3 Evaluation To evaluate the technique, a test was run using similar data to Sibun and Reynar. Corpora for eighteen languages from the European Corpus Initiative CDROM 1 were extracted and split into non-overlapping ﬁles, one containing 2000 tokens3, one containing 200 tokens, and 25 ﬁles each of 1, 5, 10 and 20 tokens. The 2000 and 200 token ﬁles were used as training data, and the remainder for test data. Wherever possible the texts were taken from newspaper corpora, and failing that from novels or literature. The identiﬁcation algorithm was run on each test ﬁle and the results placed in one of four categories:  Deﬁnitive, correct decision made.  No decision made by the end of the input, but highest scoring language was correct.  No decision, highest scoring language in- correct.  Deﬁnitive, incorrect decision made. The sum of the ﬁrst two ﬁgures divided by the total number of tests gives a measure of accu- racy; the sum of the ﬁrst and last divided by the total gives a measure of decisiveness, expressed as the proportion of the time a deﬁnitive deci- sion was made. The tests were executed using word shape tokens on the same coding scheme as Sibun and Reynar, and using the words as they appeared in the corpus. No adjustments were made for punctuation, case, etc. Vari- ous activation thresholds were tried: raising the threshold increases accuracy by requiring more information before a decision is made, but re- duces decisiveness. With shapes and 2000 to- kens of training data, at a threshold of 14 or more, all the 20 token ﬁles gave 100 accu- racy. For words themselves, the threshold was set to 22. The results of these tests appear in table 1. The ﬁgures for the activation threshold were determined by experimenting on the data. An interesting area for further work would be to put this aspect of the procedure on a sounder theoretical basis, perhaps by using the a priori probabilities of the individual languages. 3Sibun and Spitz, and Sibun and Reynar, present their results in terms of lines of input, with 1-5 lines corresponding roughly to a sentence, and 10-20 lines to a paragraph. Estimating a line as 10 words, we are there- fore working with signiﬁcantly smaller data sets. The accuracy ﬁgures are generally similar to or better than those of Sibun and Reynar. The corresponding ﬁgures for 200 tokens of training data appear in table 2, for the token identiﬁca- tion task only. One of the strengths of the algorithm is that it makes a decision as soon as one can be made reliably. Table 3 shows the average number of tokens which have to be read before a decision can be made, for the cases where the decision was correct and incorrect, and for both cases together. Again, the results are for word shape tokens, and for words alone. The ﬁgures show that convergence usually happens within about 10 words, with a long tailing oﬀ to the results. The longest time to convergence was 153 shape tokens. A manual inspection of one run (2000 lines of training data, tokens, threshold14) shows that errors are somtimes clustered, although quite weakly. For example, Serbian, Croatian and Slovenian show several confusions between them, as in Sibun and Reynars results. There are two observations to be made here. Firstly, there are about as many other errors between these language and languages which are unre- lated to them, such as Italian, German and Nor- wegian, and so the errors may be due to poor quality data rather than a lack of discrimina- tion in the algorithm. For example, Croatian is incorrectly recognised as Serbian 3 times and as Slovenian once, while the languages which are misrecognised as Croatian are German and Norwegian (once each). Secondly, even where there are errors, the range of possibilities has been substantially reduced, so that a more pow- erful process (such as full-scale OCR followed by identiﬁcation on words rather than shape to- kens, or a raising of the threshold and adding more data) could be brought in to ﬁnish the job oﬀ. That is, the conﬁdence limits have pro- vided a beneﬁt in reducing the search space. The confusion matrix for this case appears in an appendix. 3.1 Broader applicability Although the algorithm was developed with lan- guage identiﬁcation in mind, it is interesting to explore other classiﬁcation problems with it. A simple and rather crude experiment in genre identiﬁcation was carried out, using the Brown corpus. Each section of the corpus (labelled A, Test and Accuracy () Decisiveness () threshold Tokens of test data Tokens of test data 1 5 10 20 All 1 5 10 20 All Tokens (0) 71.6 72.7 69.6 72.0 71.4 88.0 99.3 100 99.8 96.8 Tokens (10) 92.9 98.4 98.4 98.2 97.0 66.0 98.9 99.6 99.8 91.1 Tokens (14) 94.2 99.6 99.1 100 98.2 49.8 98.9 99.6 99.8 87.0 Words (0) 78.4 80.4 77.1 78.7 78.7 97.3 100 100 100 99.3 Words (10) 95.8 97.6 97.1 98.0 97.1 76.9 99.8 100 99.8 94.1 Words (22) 96.9 99.8 99.8 100 99.1 29.3 98.9 99.8 99.8 81.9 Table 1: Performance with 2000 tokens of training data Threshold Accuracy () Decisiveness () Tokens of test data Tokens of test data 1 5 10 20 All 1 5 10 20 All 0 63.3 72.4 48.9 47.1 57.9 72.2 89.6 96.7 96.2 88.7 5 82.2 88.9 75.6 75.8 80.6 58.0 86.4 93.3 93.8 82.9 10 86.0 94.0 88.0 87.6 88.9 45.6 85.1 91.1 92.4 78.6 Table 2: Performance with 200 tokens of training data (word shape tokens only) B, C ... R in the original) was taken as a genre, and ﬁles of similar distribution to the previous experiment were extracted. Because this is a more unconstrained problem, the training set and tests sets were about 10 times the size of the language identiﬁcation task. A 20000 word ﬁle was used as training data, and the remain- ing ﬁles as test data. Accuracy and decisive- ness results appear in table 4. Beyond the ac- tivation threshold of 12, there is no signiﬁcant improvement in accuracy. The technique seems to give good accuracy when there is suﬃcient input (100 words or more), but at the cost of very low decisiveness. Excluding a ﬁxed list of common words such as function words might increase the decisiveness. These results should be taken with a pinch of salt, as the notion of genre is not very well-deﬁned, and it is not clear that sections of the Brown corpus really repre- sent coherent categories, but they may provide a starting point for further investigation. 3.2 On decisiveness Decisiveness represents the degree to which a unique decision has been made with a high de- gree of conﬁdence. In cases where no unique de- cision has been made, the range of possibilities will often have been reduced: a category is only still possible at any stage if its high accumula- tor value is greater than the low accumulator value of the best rated category. To illustrate this, the number of categories which are still possible when all the input was exhausted was examined. The results appear in tables 5 and 6, for the tests of language identiﬁcation from word shape tokens with an activation threshold of 14 and a training set of 2000 tokens, and for genre identiﬁcation with a threshold of 12 and a training set of 20000 tokens. Results are shown for the cases of a correct decision, an incorrect one, and all cases. The average number of possibilities remaining is 1.3 out of 18 for the language identiﬁcation test, and 9.7 out of 15 for the genre test, showing that we are generally near to convergence in the former case, but have only achieved a small reduction in the possibil- ities in the latter, in keeping with the generally low decisiveness. 3.3 A further comparison The classiﬁcation algorithm described above was originally developed in response to Sibun and Spitzs work. There is another approach to language identiﬁcation, which has a certain Threshold Shape tokens Words Correct Incorrect All Correct Incorrect All 0 3.22 1.23 2.65 1.81 1.07 1.66 10 7.33 4.55 7.28 5.31 3.88 5.28 14 9.35 6.50 9.33 22 10.6 8.00 10.6 Table 3: Average number of tokens read before convergence Threshold Accuracy () Decisiveness () Words of test data Words of test data 10 50 100 200 All 10 50 100 200 All 0 47.7 76.0 83.7 80.8 72.1 36.3 38.7 39.5 42.9 39.3 10 50.9 86.9 96.8 99.5 83.5 2.13 15.5 16.5 18.1 13.1 12 50.9 86.9 96.8 99.7 83.6 1.07 14.1 14.9 16.0 11.5 Table 4: Performance on genre identiﬁcation Languages Number of tests remaining Correct Incorrect All 1 1560 6 1566 2 128 7 135 3 37 9 46 4 18 2 20 5 5 1 6 6 5 1 6 7 2 0 2 8 2 0 2 9 1 0 1 10 3 0 3 11 1 0 1 12 2 0 2 13 2 0 2 17 1 0 1 18 7 0 7 Table 5: Categories remaining at end of in- put (language identiﬁcation from word shape tokens) amount in common with ours, described in a patent by Martino and Paulsen (1996). Their approach is to build tables of the most frequent words in each language, and assign them a nor- malised score, based on the frequency of occur- rence of the word in one language compared to Genres Number of tests remaining Correct Incorrect All 1 173 0 173 2 22 0 22 3 31 1 32 4 45 3 48 5 34 4 38 6 65 8 73 7 73 4 77 8 83 3 86 9 84 3 87 10 89 2 91 11 84 0 84 12 128 1 129 13 131 0 131 14 175 1 176 15 253 0 253 Table 6: Categories remaining at end of input (genre identiﬁcation) the total across all the languages. Only the most frequent words for each language are used. The algorithm works by accumulating scores, until a preset number of words has been read or a minimum score has been reached. They also apply the technique to genre identiﬁcation. Since there is a clear similarity, it is perhaps worth highlighting the diﬀerences. In terms of the algorithm, the most important diﬀerence is that no conﬁdence measures are included. The complexities of splitting the data into diﬀerent frequency bands for calculating probabilities are thus avoided, but no test analogous to overlap- ping conﬁdence intervals can be applied. Mar- tino and Paulsen say they obtain a high degree of conﬁdence in the decision after about 100 words, without saying what the actual success rate is; we can compare this with around 10 words (or tokens) for convergence here. 4 Conclusions We have examined a simple technique for clas- sifying a stream of input tokens in which con- ﬁdence measures are used to determine when a correct decision can be made. The results in table 1 show that there is a tradeoﬀ between accuracy and the degree to which the algorithm selects a single language. Not surprisingly, the amount of training data also aﬀects the per- formance, with 2000 tokens being adequate for accuracy close to 100, and convergence typi- cally being reached in the ﬁrst 10 tokens. On a more unconstrained problem, such as genre identiﬁcation from words alone, the algorithm performs less well in both accuracy and deci- siveness even with signiﬁcantly more training data, and is probably not adequate except as a preprocessor to some more knowledge intensive technique. In a sense, language identiﬁcation is not a very interesting problem. As we have noted, there are plenty of techniques which work well, each with its own characteristics and suitability for diﬀerent application areas. What is perhaps more important is the way the statistical infor- mation has been used here. When we take a statistical or data-led approach to NLP, there are two things which can help us trust that the technique is accurate. The ﬁrst is a be- lief that the statistical technique is an adequate model of the underlying process which gener- ates the data, using theoretical considerations or some external source of knowledge to inform this belief. The second is quantitative evalua- tion on test data which has been characterised by an outside source (for example, in the case of part of speech tagging, a corpus which has been manually annotated, or at least automatically tagged and manually corrected). The problem with quantitative evaluation is that we do not know whether it will generalise, so that if we train on one data set, we have only the theo- retical model to reassure that the same model will work on a diﬀerent data set. The idea I have been presenting here is to get the statisti- cal process itself to provide feedback about it- self, through the use of conﬁdence limits which are themselves based in the statistical model. In doing so, we hope to avoid presenting a result for which we lack adequate evidence. Acknowledgement Thanks to Robert Keiller of Canon Research Centre Europe for his advice on computing ac- curate statistics. References Gary Adams and Philip Resnik 1997. A Lan- guage Identiﬁcation Application Built on the Java ClientServer Platform. In Proceedings of the Workshop From Research to Commer- cial Applications: Making NLP Technology Work in Practice, ACLEACL, Madrid. William B. Cavnar and John M. Trenkle 1994. N-Gram Based Text Categorization. In Pro- ceedings of the Third Annual Symposium on Document Analysis and Information Re- trieval, pages 161169, Las Vegas, Nevada. I. Dagan, A. Itai and U. Schwall 1991. Two Languages are Better Than One. In Proceed- ings of the 29th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 130137. Ted Dunning 1994. Statistical Identiﬁcation of Language. Computing Research Laboratory Memo MCCS 940-273, New Mexico State University. Michael J. Martino and Robert C. Paulsen, Jr. 1996. Language Identiﬁcation Process Using Coded Language Words. US Patent Number 5548507 (Issued 1996-08-20). Penelope Sibun and A. Lawrence Spitz. 1994. Language Determination: Natural Language Processing from Scanned Document Images. In Proceedings of the 4th Conference on Ap- plied Natural Language Processing, pages 15 21, Stuttgart, Germany. Association for Com- putational Linguistics. Penelope Sibun and Jeﬀrey C. Reynar. 1996. Language Determination: Examining the Is- sues. In Proceedings of the 5th Annual Sym- posium on Document Analysis and Informa- tion Retrieval, pages 125135, Las Vegas, Nevada. Appendix Confusion matrix for the case of 2000 lines of training data, token, threshold14. An entry in this matrix means that the language on the horizontal axis was classiﬁed as being in the lan- guage on the vertical axis in the indicated num- ber of test samples. (alb  Albanian, cro  Croatian, dan  Dan- ish, dut  Dutch, eng  English, est  Esto- nian, fre  French, ger  German, ita  Italian, lat  Latin, lit  Lithuanian, mal  Malay, nor  Norwegian, por  Portugese, ser  Serbian, slo  Slovenian, spa  Spanish, tur  Turk- ish. Some of the languages are in a Romanised form.) a c d d e e f g i l l m n p s s s t l r a u n s r e t a i a o o e l p u b o n t g t e r a t t l r r r o a r alb 100 cro 96 1 1 dan 100 1 1 dut 100 1 1 eng 99 2 1 est 93 2 1 1 fre 99 1 ger 1 97 1 ita 2 97 lat 1 99 lit 1 98 mal 100 nor 1 98 1 por 98 ser 3 100 1 1 slo 1 1 98 spa 1 1 1 97 tur 99",
  "46.pdf": "arXiv:cs9907012v1 [cs.CL] 8 Jul 1999 Selective Magic HPSG Parsing Guido Minnen Cognitive and Computing Sciences, University of Sussex Falmer, Brighton BN1 9QH United Kingdom Guido.Minnencogs.susx.ac.uk www.cogs.susx.ac.uklabnlpminnenminnen.html Abstract We propose a parser for constraint-logic grammars implementing HPSG that com- bines the advantages of dynamic bottom- up and advanced top-down control. The parser allows the user to apply magic com- pilation to speciﬁc constraints in a gram- mar which as a result can be processed dy- namically in a bottom-up and goal-directed fashion. State of the art top-down process- ing techniques are used to deal with the remaining constraints. We discuss various aspects concerning the implementation of the parser as part of a grammar develop- ment system. In Proceedings of the 9th Conference of the EACL, Bergen, Norway, June 1999. 1 Introduction In case space requirements of dynamic parsing of- ten outweigh the beneﬁt of not duplicating sub- computations. We propose a parser that avoids this drawback through combining the advantages of dy- namic bottom-up and advanced top-down control.1 The underlying idea is to achieve faster parsing by avoiding tabling on sub-computations which are not expensive. The so-called selective magic parser al- lows the user to apply magic compilation to spe- ciﬁc constraints in a grammar which as a result can be processed dynamically in a bottom-up and goal- directed fashion. State of the art top-down process- ing techniques are used to deal with the remaining constraints. Magic is a compilation technique originally de- veloped for goal-directed bottom-up processing of logic programs. See, among others, (Ramakrishnan et al. 1992). As shown in (Minnen, 1996) magic The presented research was carried out at the Uni- versity of Tubingen, Germany, as part of the Sonder- forschungsbereich 340. 1A more detailed discussion of various aspects of the proposed parser can be found in (Minnen, 1998). is an interesting technique with respect to natural language processing as it incorporates ﬁltering into the logic underlying the grammar and enables el- egant control independent ﬁltering improvements. In this paper we investigate the selective applica- tion of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (T FL; Gotz, 1995). Typed feature gram- mars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (Gotz and Meurers, 1997a) and (Meurers and Minnen, 1997). Typed feature grammar constraints that are inex- pensive to resolve are dealt with using the top- down interpreter of the ConTroll grammar develop- ment system (Gotz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation. The proposed parser is related to the so-called Lemma Table deduction system (Johnson and Dorre, 1995) which allows the user to specify whether top- down sub-computations are to be tabled. In contrast to Johnson and Dorres deduction system, though, the selective magic parsing approach combines top- down and bottom-up control strategies. As such it resembles the parser of the grammar development system Attribute Language Engine (ALE) of (Car- penter and Penn, 1994). Unlike the ALE parser, though, the selective magic parser does not presup- pose a phrase structure backbone and is more ﬂexi- ble as to which sub-computations are tabledﬁltered. 2 Bottom-up Interpretation of Magic-compiled Typed Feature Grammars We describe typed feature grammars and discuss their use in implementing HPSG grammars. Subse- quently we present magic compilation of typed fea- ture grammars on the basis of an example and in- troduce a dynamic bottom-up interpreter that can be used for goal-directed interpretation of magic- compiled typed feature grammars. 2.1 Typed Feature Grammars A typed feature grammar consists of a signature and a set of deﬁnite clauses over the constraint language of equations of T FL (Gotz, 1995) terms (Hohfeld and Smolka, 1988) which we will refer to as T FL deﬁnite clauses. Equations over T FL terms can be solved using (graph) uniﬁcation provided they are in normal form. (Gotz, 1994) describes a normal form for T FL terms, where typed feature structures are interpreted as satisﬁable normal form T FL terms.2 The signature consists of a type hierarchy and a set of appropriateness conditions. Example 1 The signature speciﬁed in ﬁgure 1 and 2 and the T FL deﬁnite clauses in ﬁgure 3 con- stitute an example of a typed feature grammar. We relation append ARG1 ARG2 ARG3 list list list constituent ARG1 sign Figure 2: Example of a typed feature grammar sig- nature (part 2) write T FL terms in normal form, i. e., as typed fea- ture structures. In addition, uninformative feature speciﬁcations are ignored and typing is left implicit when immaterial to the example at hand. Equa- tions between typed feature structures are removed by simple substitution or tags indicating structure sharing. Notice that we also use non-numerical tags such as Xs and XsYs . In general all boxed items indicate structure sharing. For expository reasons we represent the ARGn features of the append re- lation as separate arguments. Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Struc- ture Grammar (Pollard and Sag, 1994).3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into T FL deﬁnite clauses which are used to restrict lexical entries. (Gotz and Meurers, 1997b) 2This view of typed feature structures diﬀers from the perspective on typed feature structures as modeling partial information as in (Carpenter, 1992). Typed fea- ture structures as normal form T FL terms are merely syntactic objects. 3See (King, 1994) for a discussion of the appropri- ateness of T FL for HPSG and a comparison with other feature logic approaches designed for HPSG. (1) constituent(  CAT s PHON 1 SEM 5  ):- constituent(   CAT np PHON 2 AGR 4 SEM 6  ), constituent(   CAT v PHON 3 AGR 4 SEM 5  SUBJ 6   ), append( 2 , 3 , 1 ). (2) constituent(   CAT np PHON  mary  AGR third-sing SEM mary lf  ). (3) constituent(   CAT v PHON sleeps AGR third-sing SEM sleep  ). (4) append(, Ys , Ys ). (5) append( X  Xs , Ys ,  X  XsYs ):- append( Xs , Ys , XsYs ). Figure 3: Example of a set of T FL deﬁnite clauses describe a method for compiling implicational con- straints into typed feature grammars and interleav- ing them with relational constraints.4 Because of space limitations we have to refrain from an exam- ple. The ConTroll grammar development system as described in (Gotz and Meurers, 1997b) imple- ments the above mentioned techniques for compiling an HPSG theory into typed feature grammars. 2.2 Magic Compilation Magic is a compilation technique for goal-directed bottom-up processing of logic programs. See, among others, (Ramakrishnan et al. 1992). Because magic compilation does not refer to the speciﬁc constraint language adopted, its application is not limited to logic programsgrammars: It can be applied to rela- tional extensions of other constraint languages such as typed feature grammars without further adap- tions. Due to space limitations we discuss magic com- pilation by example only. The interested reader is referred to (Nilsson and Maluszynski, 1995) for an introduction. Example 2 We illustrate magic compilation of typed feature grammars with respect to deﬁnite clause 1 in ﬁgure 3. Consider the T FL deﬁnite 4(Gotz, 1995) proves that this compilation method is sound in the general case and deﬁnes the large class of type constraints for which it is complete. T string list sign mary sleeps relation elist agr sem PHON AGR SEM list agr sem HD T third-sing mary_lf sleep SUBJ sem nelist TL list cat s np CAT cat v Figure 1: Example of a typed feature grammar signature (part 1) clause in ﬁgure 4. As a result of magic compilation constituent( magic  CAT s PHON 1 SEM 5  ):- magic constituent( magic ), constituent(   CAT np PHON 2 AGR 4 SEM 6  ), constituent(   CAT v PHON 3 AGR 4 SEM 5  SUBJ 6   ), append( 2 , 3 , 1 ). Figure 4: Magic variant of deﬁnite clause 1 in ﬁg- ure 3 a magic literal is added to the right-hand side of the original deﬁnite clause. Intuitively understood, this magic literal guards the application of the deﬁnite clause. The clause is applied only when there exists a fact that uniﬁes with this magic literal.5 The re- sulting deﬁnite clause is also referred to as the magic variant of the original deﬁnite clause. The deﬁnite clause in ﬁgure 5 is the so-called seed which is used to make the bindings as provided by the initial goal available for bottom-up processing. In this case the seed corresponds to the initial goal of parsing the string mary sleeps. Intuitively un- derstood, the seed makes available the bindings of the initial goal to the magic variants of the deﬁ- 5A fact can be a unit clause, i. e., a T FL deﬁnite clause without right-hand side literals, from the gram- mar or derived using the rules in the grammar. In the latter case one also speaks of a passive edge. magic constituent(  CAT s PHON mary, sleeps SEM sem  ). Figure 5: Seed corresponding to the initial goal of parsing the string mary sleeps nite clauses deﬁning a particular initial goal; in this case the magic variant of the deﬁnite clause deﬁn- ing a constituent of category s. Only when their magic literal uniﬁes with the seed are these clauses applied.6 The so-called magic rules in ﬁgure 6 are derived in order to be able to use the bindings provided by the seed to derive new facts that provide the bindings which allow for a goal-directed application of the deﬁnite clauses in the grammar not directly deﬁn- ing the initial goal. Deﬁnite clause 3, for example, can be used to derive a magic append fact which percolates the relevant bindings of the seedinitial goal to restrict the application of the magic variant of deﬁnite clauses 4 and 5 in ﬁgure 3 (which are not displayed). 2.3 Semi-naive Bottom-up Interpretation Magic-compiled logic programsgrammars can be in- terpreted in a bottom-up fashion without losing any of the goal-directedness normally associated with top-down interpretation using a so-called semi-naive bottom-up interpreter: A dynamic interpreter that tables only complete intermediate results, i. e., facts or passive edges, and uses an agenda to avoid re- dundant sub-computations. The Prolog predicates 6The creation of the seed can be postponed until run time, such that the grammar does not need to be com- piled for every possible initial goal. (1) magic constituent(   CAT np PHON list AGR agr SEM sem  ):- magic constituent(  CAT s PHON list SEM sem  ). (2) magic constituent(   CAT v PHON list AGR 4 SEM 5  SUBJ 6   ):- magic constituent(  CAT s PHON list SEM 5  ), constituent(   CAT np PHON list AGR 4 SEM 6  ), (3) magic append( 2 , 3 , 1 ):- magic constituent(  CAT s PHON 1 SEM 5  ), constituent(   CAT np PHON 2 AGR 4 SEM 6  ), constituent(   CAT v PHON 3 AGR 4 SEM 5  SUBJ 6   ). Figure 6: Magic rules resulting from applying magic compilation to deﬁnite clause 1 in ﬁgure 3 in ﬁgure 7 implement a semi-naive bottom-up in- terpreter.7 In this interpreter both the table and the agenda are represented using lists.8 The agenda keeps track of the facts that have not yet been used to update the table. It is important to notice that in order to use the interpreter for typed feature gram- mars it has to be adapted to perform graph uniﬁca- tion.9 We refrain from making the necessary adap- tions to the code for expository reasons. The table is initialized with the facts from the grammar. Facts are combined using a operation called match. The match operation uniﬁes all but one of the right-hand side literals of a deﬁnite clause 7Deﬁnite clauses serving as data are en- coded using the predicate definite clause1: definite clause((Lhs :- Rhs))., where Rhs is a (possibly empty) list of literals. 8There are various othermore eﬃcientways to im- plement a dynamic control strategy in Prolog. See, for example, (Shieber et al., 1995). 9A term encoding of typed feature structures would enable the use of term uniﬁcation instead. See, for ex- ample, (Gerdemann, 1995). in the grammar with facts in the table. The remain- ing right-hand side literal is uniﬁed with a newly derived fact, i. e., a fact from the agenda. By do- ing this, repeated derivation of facts from the same earlier derived facts is avoided. semi naive interpret(Goal):- initialization(Agenda,Table0), update table(Agenda,Table0,Table), member(edge(Goal,[]),Table). update table([],Table,Table). update table([EdgeAgenda0],Table0,Table):- update table w edge(Edge,Edges, Table0,Table1), append(Edges,Agenda0,Agenda), update table(Agenda,Table1,Table). update table w edge(Edge,Edges,Table0,Table):- findall( NewEdge, match(Edge,NewEdge,Table0), Edges), store(Edges,Table0,Table). store([],Table,Table):- store([EdgeEdges],Table0,Table):- member(GenEdge,Table0),  subsumes(GenEdge,Edge), store(Edges,[EdgeTable0],Table). store([ Edges],Table0,Table):- store(Edges,Table0,Table). initialization(Edges,Edges):- findall( edge(Head,[]), definite clause((Head:- [])), Edges). completion(Edge,edge(Goal,[]),Table):- definite clause((Goal :- Body)), Edge  edge(F,[]), select(F,Body,R), edges(R,Table). edges([], ). edges([LitLits],Table):- member(edge(Lit,[]),Table), edges(Lits,Table). Figure 7: Semi-naive bottom-up interpreter 3 Selective Magic HPSG Parsing In case of large grammars the huge space require- ments of dynamic processing often nullify the ben- eﬁt of tabling intermediate results. By combining control strategies and allowing the user to specify how to process particular constraints in the gram- mar the selective magic parser avoids this problem. This solution is based on the observation that there are sub-computations that are relatively cheap and as a result do not need tabling (Johnson and Dorre, 1995; van Noord, 1997). 3.1 Parse Type Speciﬁcation Combining control strategies depends on a way to diﬀerentiate between types of constraints. For ex- ample, the ALE parser (Carpenter and Penn, 1994) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or top-down. In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse type literal (goal) if it has as its single argument a typed feature structure of a type speciﬁed as a parse type.10 All types in the type hierarchy can be used as parse types. This way parse type speciﬁcation sup- ports a ﬂexible ﬁltering component which allows us to experiment with the role of ﬁltering. However, in the remainder we will concentrate on a speciﬁc class of parse types: We assume the speciﬁcation of type sign and its sub-types as parse types.11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guid- ing role in the parsing process and are best inter- preted bottom-up given the lexical orientation of HPSG. The parsing process corresponding to such a parse type speciﬁcation is represented schemati- cally in ﬁgure 8. Starting from the lexical entries, word word word non-parse type goals non-parse type goals Figure 8: Schematic representation of the selective magic parsing process i. e., the T FL deﬁnite clauses that specify the word objects in the grammar, phrases are built bottom- up by matching the parse type literals of the deﬁ- nite clauses in the grammar against the edges in the 10The notion of a parse type literal is closely related to that of a memo literal as in (Johnson and Dorre, 1995). 11When a type is speciﬁed as a parse type, all its sub- types are considered as parse types as well. This is nec- essary as otherwise there may exist magic variants of deﬁnite clauses deﬁning a parse type goal for which no magic facts can be derived which means that the magic literal of these clauses can be interpreted neither top- down nor bottom-up. table. The non-parse type literals are processed ac- cording to the top-down control strategy described in section 3.3. 3.2 Selective Magic Compilation In order to process parse type goals according to a semi-naive magic control strategy, we apply magic compilation selectively. Only the T FL deﬁnite clauses in a typed feature grammar which deﬁne parse type goals are subject to magic compilation. The compilation applied to these clauses is identical to the magic compilation illustrated in section 2.1 except that we derive magic rules only for the right- hand side literals in a clause which are of a parse type. The deﬁnite clauses in the grammar deﬁning non-parse type goals are not compiled as they will be processed using the top-down interpreter described in the next section. 3.3 Advanced Top-down Control Non-parse type goals are interpreted using the stan- dard interpreter of the ConTroll grammar develop- ment system (Gotz and Meurers, 1997b) as devel- oped and implemented by Thilo Gotz. This ad- vanced top-down interpreter uses a search function that allows the user to specify the information on which the deﬁnite clauses in the grammar are in- dexed. An important advantage of deep multiple indexing is that the linguist does not have to take into account of processing criteria with respect to the organization of herhis data as is the case with a standard Prolog search function which indexes on the functor of the ﬁrst argument. Another important feature of the top-down inter- preter is its use of a selection function that interprets deterministic goals, i. e., goals which unify with the left-hand side literal of exactly one deﬁnite clause in the grammar, prior to non-deterministic goals. This is often referred to as incorporating deterministic closure (Dorre, 1993). Deterministic closure accom- plishes a reduction of the number of choice points that need to be set during processing to a minimum. Furthermore, it leads to earlier failure detection. Finally, the used top-down interpreter implements a powerful coroutining mechanism:12 At run time the processing of a goal is postponed in case it is insuﬃciently instantiated. Whether or not a goal is suﬃciently instantiated is determined on the basis of so-called delay patterns.13 These are speciﬁcations 12Coroutining appears under many diﬀerent guises, like for example, suspension, residuation, (goal) freezing, and blocking. See also (Colmerauer, 1982; Naish, 1986). 13In the literature delay patterns are sometimes also referred to as wait declarations or block statements. provided by the user that indicate which restrict- ing information has to be available before a goal is processed. 3.4 Adapted Semi-naive Bottom-up Interpretation The deﬁnite clauses resulting from selective magic transformation are interpreted using a semi-naive bottom-up interpreter that is adapted in two re- spects. It ensures that non-parse type goals are interpreted using the advanced top-down inter- preter, and it allows non-parse type goals that re- main delayed locally to be passed in and out of sub-computations in a similar fashion as proposed by (Johnson and Dorre, 1995). In order to accom- modate these changes the adapted semi-naive inter- preter enables the use of edges which specify delayed goals. Figure 9 illustrates the adapted match operation. The ﬁrst deﬁning clause of match3 passes delayed match(Edge,edge(Goal,Delayed),Table):- definite clause((Goal :- Body)), select(Lit,Body,Lits), parse type(Lit), Edge  edge(Lit,Delayed0), edges(Lit,Table,Delayed0,TopDown), advanced td interpret(TopDown,Delayed). match(Edge,edge(Goal,Delayed),Table):- definite clause((Goal :- TopDown)), advanced td interpret(TopDown,Delayed). Figure 9: Adapted deﬁnition of match3 and non-parse type goals of the deﬁnite clause under consideration to the advanced top-down interpreter via the call to advanced td interpret2 as the list of goals TopDown.14 The second deﬁning clause of match3 is added to ensure all right-hand side lit- erals are directly passed to the advanced top-down interpreter if none of them are of a parse type. Allowing edges which specify delayed goals neces- sitates the adaption of the deﬁnition of edges3. When a parse type literal is matched against an edge in the table, the delayed goals speciﬁed by that edge need to be passed to the top-down interpreter. Consider the deﬁnition of the predicate edges in ﬁgure 11. The third argument of the deﬁnition of edges4 is used to collect delayed goals. When there are no more parse type literals in the right-hand side of the deﬁnite clause under consideration, the second 14The deﬁnition of match3 assumes that there exists a strict ordering of the right-hand side literals in the deﬁnite clauses in the grammar, i. e., parse type literals always preced e non-parse type literals. edges([LitLits],Table,Delayed0,TopDown):- parse type(Lit), member(edge(Lit,Delayed1),Table), append(Delayed0,Delayed1,Delayed). edges(Lit,Table,Delayed,TopDown). edges([], ,Delayed,TopDown):- append(Delayed,Lit,TopDown). Figure 11: Adapted deﬁnition of edges4 deﬁning clause of edges4 appends the collected de- layed goals to the remaining non-parse type literals. Subsequently, the resulting list of literals is passed up again for advanced top-down interpretation. 4 Implementation The described parser was implemented as part of the ConTroll grammar development system (Gotz and Meurers, 1997b). Figure 10 shows the over- all setup of the ConTroll magic component. The Controll magic component presupposes a parse type speciﬁcation and a set of delay patterns to deter- mine when non-parse type constraints are to be in- terpreted. At run-time the goal-directedness of the selective magic parser is further increased by means of using the phonology of the natural language ex- pression to be parsed as speciﬁed by the initial goal to restrict the number of facts that are added to the table during initialization. Only those facts in the grammar corresponding to lexical entries that have a value for their phonology feature that appears as part of the input string are used to initialize the ta- ble. The ConTroll magic component was tested with a larger ( 5000 lines) HPSG grammar of a size- able fragment of German. This grammar provides an analysis for simple and complex verb-second, verb- ﬁrst and verb-last sentences with scrambling in the mittelfeld, extraposition phenomena, wh-movement and topicalization, integrated verb-ﬁrst parentheti- cals, and an interface to an illocution theory, as well as the three kinds of inﬁnitive constructions, nomi- nal phrases, and adverbials (Hinrichs et al., 1997). As the test grammar combines sub-strings in a non-concatenative fashion, a preprocessor is used that chunks the input string into linearization do- mains. This way the standard ConTroll interpreter (as described in section 3.3) achieves parsing times of around 1-5 seconds for 5 word sentences and 10 60 seconds for 12 word sentences.15 The use of magic compilation on all grammar constraints, i.e., 15Parsing with such a grammar is diﬃcult in any sys- tem as it does neither have nor allow the extraction of a phrase structure backbone. input: output: magic compilation on parse type preselection of relevant lexical entries magic-compiled grammar relevant lexical entries parse type specification bottom-up interpretation top-down interpretation typed feature grammar clauses of parse type clauses solutions to initial goal semi-naive extended combined with advanced initial goal Figure 10: Setup of the ConTroll magic component tabling of all sub-computations, leads to an vast in- crease of parsing times. The selective magic HPSG parser, however, exhibits a signiﬁcant speedup in many cases. For example, parsing with the mod- ule of the grammar implementing the analysis of nominal phrases is up to nine times faster. At the same time though selective magic HPSG parsing is sometimes signiﬁcantly slower. For example, parsing of particular sentences exhibiting adverbial subordi- nate clauses and long extraction is sometimes more than nine times slower. We conjecture that these ambiguous results are due to the use of coroutin- ing: As the test grammar was implemented using the standard ConTroll interpreter, the delay pat- terns used presuppose a data-ﬂow corresponding to advanced top-down control and are not ﬁne-tuned with respect to the data-ﬂow corresponding to the selective magic parser. Coroutining is a ﬂexible and powerful facility used in many grammar development systems and it will probably remain indispensable in dealing with many control problems despite its various disadvantages.16 The test results discussed above indicate that the comparison of parsing strategies can be seriously hampered by ﬁne-tuning parsing using delay pat- terns. We believe therefore that further research into the systematics underlying coroutining would be desirable. 5 Concluding Remarks We described a selective magic parser for typed fea- ture grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top-down control. As a result the parser avoids the eﬃciency problems resulting from the huge space re- quirements of storing intermediate results in parsing 16Coroutining has a signiﬁcant run-time overhead caused by the necessity to check the instantiation sta- tus of a literalgoal. In addition, it demands the proce- dural annotation of an otherwise declarative grammar. Finally, coroutining presupposes that a grammar writer possesses substantial processing expertise. with large grammars. The parser allows the user to apply magic compilation to speciﬁc constraints in a grammar which as a result can be processed dy- namically in a bottom-up and goal-directed fashion. State of the art top-down processing techniques are used to deal with the remaining constraints. We discussed various aspects concerning the implemen- tation of the parser which was developed as part of the grammar development system ConTroll. Acknowledgments The author gratefully acknowledges the support of the SFB 340 project B4 From Constraints to Rules: Eﬃcient Compilation of HPSG funded by the Ger- man Science Foundation, and the project PSET: Practical Simpliﬁcation of English Text, a three- year project funded by the UK Engineering and Physical Sciences Research Council (GRL53175), and Apple Computer Inc.. The author wishes to thank Dale Gerdemann and Erhard Hinrichs and the anonymous reviewers for comments and discussion. Of course, the author is responsible for all remaining errors. References Bob Carpenter and Gerald Penn. 1994. ALE  The Attribute Logic Engine, Users guide, version 2.0.2. Technical report, Carnegie Mellon Univer- sity, Pittsburgh, Pennsylvania, USA. Bob Carpenter. 1992. The Logic of Typed Fea- ture Structures - With Applications to Uniﬁcation Grammars, Logic Programs and Constraint Res- olution. Cambridge University Press, New York, USA. Alain Colmerauer. 1982. PrologII: Manuel de reference et modele theorique. Technical report, Groupe dIntelligence Artiﬁcielle, Faculte de Sci- ences de Luminy, Marseille, France. Jochen Dorre. 1993. Generalizing Earley Deduction for Constraint-based Grammars. In Jochen Dorre and Michael Dorna (eds.), 1993. Computational Aspects of Constraint-Based Linguistic Descrip- tion I. DYANA-2, Deliverable R1.2.A. Dale Gerdemann. 1995. Term Encoding of Typed Feature Structures. In Proceedings of the Fourth International Workshop on Parsing Technologies, Prague, Czech Republic. Thilo Gotz and Detmar Meurers. 1997a. Inter- leaving Universal Principles and Relational Con- straints over Typed Feature Logic. In ACLEACL Proceedings, Madrid, Spain. Thilo Gotz and Detmar Meurers. 1997b. The ConTroll System as Large Grammar Development Platform. In Proceedings of the ACL Workshop on Computational Environments for Grammar De- velopment and Linguistic Engineering, Madrid, Spain. Thilo Gotz. 1994. A Normal Form for Typed Fea- ture Structures. Technical report SFB 340 nr. 40, University of Tubingen, Germany. Thilo Gotz. 1995. Compiling HPSG Constraint Grammars into Logic Programs. In Proceedings of the Workshop on Computational Logic for Nat- ural Language Processing, Edinburgh, UK. Erhard Hinrichs, Detmar Meurers, Frank Richter, Manfred Sailer, and Heike Winhart. 1997. Ein HPSG-fragment des Deutschen, Teil 1: Theo- rie. Technical report SFB 340 95, University of Tubingen, Germany. Markus Hohfeld and Gert Smolka. 1988. Deﬁnite Relations over Constraint Languages. Technical Report 53, IBM, Germany. Mark Johnson and Jochen Dorre. 1995. Memoiza- tion of Coroutined Constraints. In ACL Proceed- ings, Cambridge, Massachusetts, USA. Paul King. 1994. Typed Feature Structures as De- scriptions. In Proceedings of of the 15th Confer- ence on Computational Linguistics, Kyoto, Japan. Detmar Meurers and Guido Minnen. 1997. A Com- putational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries. Computational Linguistics, 23(4). Guido Minnen. 1996. Magic for Filter Optimization in Dynamic Bottom-up Processing. In ACL Pro- ceedings, Santa Cruz, California, USA. Guido Minnen. 1998. Oﬀ-line Compilation for Eﬃ- cient Processing with Constraint-logic Grammars. Ph.D. thesis, University of Tubingen, Germany. Technical report SFB 340 nr. 130. Lee Naish. 1986. Negation and Control in Prolog. Springer-Verlag, Berlin, Germany. Ulf Nilsson and Jan Maluszynski. 1995. Logic, Pro- gramming and Prolog. John Wiley  Sons, Chich- ester, UK, 2nd edition. Carl Pollard and Ivan Sag. 1994. Head- Driven Phrase Structure Grammar. University of Chicago Press, Chicago, Illinois, USA. Raghu Ramakrishnan, Divesh Srivastava, and S. Su- darshan. 1992. Eﬃcient Bottom-up Evaluation of Logic Programs. In Joos Vandewalle (ed.), 1992. The State of the Art in Computer Systems and Software Engineering. Kluwer Academic Publish- ers. Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and Implementation of Deductive Parsing. Journal of Logic Programming, 24(1-2). Gertjan van Noord. 1997. An Eﬃcient Implemen- tation of the Head-corner Parser. Computational Linguistics, 23(3).",
  "47.pdf": "arXiv:cs9907013v1 [cs.CL] 8 Jul 1999 In Proceedings of the EACL workshop on Linguistically Interpreted Corpora (LINC), Bergen, Norway, June 1999 Corpus Annotation for Parser Evaluation John Carroll, Guido Minnen Cognitive and Computing Sciences University of Sussex Brighton BN1 9QH, UK {johnca,guidomi}cogs.susx.ac.uk Ted Briscoe Computer Laboratory University of Cambridge Pembroke Street, Cambridge CB2 3QG, UK ejbcl.cam.ac.uk Abstract We describe a recently developed corpus annotation scheme for evaluating parsers that avoids shortcomings of current meth- ods. The scheme encodes grammatical re- lations between heads and dependents, and has been used to mark up a new public- domain corpus of naturally occurring En- glish text. We show how the corpus can be used to evaluate the accuracy of a robust parser, and relate the corpus to extant re- sources. 1 Introduction The evaluation of individual language-processing components forming part of larger-scale natural lan- guage processing (NLP) application systems has re- cently emerged as an important area of research (see e.g. Rubio, 1998; Gaizauskas, 1998). A syntactic parser is often a component of an NLP system; a re- liable technique for comparing and assessing the rel- ative strengths and weaknesses of diﬀerent parsers (or indeed of diﬀerent versions of the same parser during development) is therefore a necessity. Current methods for evaluating the accuracy of syntactic parsers are based on measuring the de- gree to which parser output replicates the analy- ses assigned to sentences in a manually annotated test corpus. Exact match between the parser output and the corpus is typically not required in order to allow diﬀerent parsers utilising diﬀerent grammati- cal frameworks to be compared. These methods are fully objective since the standards to be met and cri- teria for testing whether they have been met are set in advance. The evaluation technique that is currently the most widely-used was proposed by the Grammar Evaluation Interest Group (Harrison et al., 1991; see also Grishman, Macleod  Sterling, 1992), and is often known as parseval. The method com- pares phrase-structure bracketings produced by the parser with bracketings in the annotated corpus, or treebank1 and computes the number of bracketing matches M with respect to the number of brack- etings P returned by the parser (expressed as pre- cision MP) and with respect to the number C in the corpus (expressed as recall MC), and the mean number of crossing brackets per sentence where a bracketed sequence from the parser overlaps with one from the treebank and neither is properly con- tained in the other. Advantages of parseval are that a relatively un- detailed (only bracketed), treebank annotation is re- quired, some level of cross frameworksystem com- parison is achieved, and the measure is moderately ﬁne-grained and robust to annotation errors. How- ever, a number of disadvantages of parseval have been documented recently. In particular, Carpen- ter  Manning (1997) observe that sentences in the Penn Treebank (ptb; Marcus, Santorini  Marc- inkiewicz, 1993) contain relatively few brackets, so analyses are quite ﬂat. (The same goes for the other treebank of English in general use, susanne; Sampson, 1995). Thus crossing bracket scores are likely to be small, however good or bad the parser is. Carpenter  Manning also point out that with the adjunction structure the ptb gives to post noun- head modiﬁers (NP (NP the man) (PP with (NP a telescope))), there are zero crossings in cases where the VP attachment is incorrectly returned, and vice- versa. Conversely, Lin (1995) demonstrates that the crossing brackets measure can in some cases pe- nalise mis-attachments more than once; Lin (1996) argues that a high score for phrase boundary correct- ness does not guarantee that a reasonable semantic reading can be produced. Conversely, many phrase 1Subsequent evaluations using parseval (e.g. Collins, 1996) have adapted it to incorporate con- stituent labelling information as well as just bracketing. boundary disagreements stem from systematic dif- ferences between parsers grammars and corpus an- notation schemes that are well-justified within the context of their own theories. parseval does at- tempt to circumvent this problem by the removal from consideration of bracketing information in con- structions for which agreement between analysis schemes in practice is low: i.e. negation, auxiliaries, punctuation, traces, and the use of unary branching structures. However, in general there are still major prob- lems with compatibility between the annotations in treebanks and analyses returned by parsing systems using manually-developed generative grammars (as opposed to grammars acquired directly from the treebanks themselves). The treebanks have been constructed with reference to sets of informal guide- lines indicating the type of structures to be assigned. In the absence of a formal grammar controlling or verifying the manual annotations, the number of dif- ferent structural conﬁgurations tends to grow with- out check. For example, the ptb implicitly contains more than 10000 distinct context-free productions, the majority occurring only once (Charniak, 1996). This makes it very diﬃcult to accurately map the structures assigned by an independently-developed grammarparser onto the structures that appear (or should appear) in the treebank. A further prob- lem is that the parseval bracket precision measure penalises parsers that return more structure than the treebank annotation, even if it is correct (Srini- vas, Doran  Kulick, 1995). To be able to use the treebank and report meaningful parseval precision scores such parsers must necessarily dumb down their output and attempt to map it onto (exactly) the distinctions made in the treebank2. This map- ping is also very diﬃcult to specify accurately. par- seval evaluation is thus objective, but the results are not reliable. In addition, since parseval is based on measuring similarity between phrase-structure trees, it cannot be applied to grammars which produce dependency- style analyses, or to lexical parsing frameworks such as ﬁnite-state constraint parsers which assign syntactic functional labels to words rather than pro- ducing hierarchical structure. To overcome the parseval grammartreebank mismatch problems outlined above, Lin (1995) pro- poses evaluation based on dependency structure, in which phrase structure analyses from parser and treebank are both automatically converted into sets 2Gaizauskas, Hepple  Huyck (1998) propose an al- ternative to the parseval precision measure to address this speciﬁc shortcoming. of dependency relationships. Each such relation- ship consists of a modiﬁer, a modiﬁee, and option- ally a label which gives the type of the relation- ship. Atwell (1996), though, argues that trans- forming standard constituency-based analyses into a dependency-based representation would lose certain kinds of grammatical information that might be im- portant for subsequent processing, such as logical information (e.g. location of traces, or moved con- stituents). Srinivas, Doran, Hockey  Joshi (1996) describe a related technique which could also be ap- plied to partial (incomplete) parses, in which hierar- chical phrasal constituents are ﬂattened into chunks and the relationships between them are indicated by dependency links. Recall and precision are deﬁned over dependency links. The TSNLP (Lehmann et al., 1996) project test suites (in English, French and German) contain dependency-based annotations for some sentences; this allows for generalizations over potentially con- troversial phrase structure conﬁgurations and also mapping onto a speciﬁc constituent structure. No speciﬁc annotation standards or evaluation measures are proposed, though. 2 Grammatical Relation Annotation In the previous section we argued that constituency- based evaluation for parser evaluation has serious shortcomings3. In this section we outline a recently- proposed annotation scheme based on a dependency- style analysis, and compare it to other related schemes. In the next section we describe a 10K- word test corpus that uses this scheme, and also how it may be used to evaluate a robust parser. Carroll, Briscoe  Sanﬁlippo (1998) describe an annotation scheme in which each sentence in the corpus is marked up with a set of grammatical re- lations (GRs), specifying the syntactic dependency which holds between each head and its dependent(s). The annotation scheme is application-independent, and takes into account language phenomena in English, Italian, French and German. The scheme is based on EAGLES lexiconsyntax work- ing group standards (Sanﬁlippo et al., 1996), but reﬁned within the EU 4th Framework SPARKLE project (see http:www.ilc.pi.cnr.itsparklewp1- preﬁnal) extending the set of relations proposed there. 3Note that the issue we are concerned with here is parser evaluation, and we are not making any more gen- eral claims about the utility of constituency-based tree- banks for other tasks, such as statistical parser training or in quantitative linguistics.   PPPPP                               dependent mod arg mod arg subj or dobj ncmod xmod cmod subj comp ncsubj xsubj csubj obj clausal dobj obj2 iobj xcomp ccomp Figure 2: The GR hierarchy. When the proprietor dies, the establishment should become a corporation until it is either acquired by another proprietor or the government decides to drop it. cmod(when, become, die) ncsubj(die, proprietor, ) ncsubj(become, establishment, ) xcomp(become, corporation, ) mod(until, become, acquire) ncsubj(acquire, it, obj) arg mod(by, acquire, proprietor, subj) cmod(until, become, decide) ncsubj(decide, government, ) xcomp(to, decide, drop) ncsubj(drop, government, ) dobj(drop, it, ) Figure 1: Example sentence and GRs (susanne rel3, lines G22:1460kG22:1480m). For brevity, we give an example of the use of the GR scheme here (ﬁgure 1) rather than duplicat- ing Carroll, Briscoe  Sanﬁlippos description of it. The set of possible relations (i.e. cmod, ncsubj, etc.) is organised hierarchically; see ﬁgure 2. The most generic relation between a head and a dependent is dependent. Where the relationship between the two is known more precisely, relations further down the hierarchy can be used, for example mod(iﬁer) or arg(ument). Relations mod, arg mod, clausal, and their descendants have slots ﬁlled by a type, a head, and its dependent; arg mod has an additional fourth slot initial gr. Descendants of subj, and also dobj have the three slots head, dependent, and ini- tial gr. The x and c preﬁxes to relation names dif- ferentiate clausal control alternatives. The scheme is superﬁcially similar to a syntactic dependency analysis in the style of Lin (1995). How- ever, the scheme contains a speciﬁc, ﬁxed inventory of relations. Other signiﬁcant diﬀerences are:  the GR analysis of control relations could not be expressed as a strict dependency tree since a sin- gle nominal head would be a dependent of two (or more) verbal heads (as with ncsubj(decide, government, ) ncsubj(drop, government, ) in the ﬁgure 1 example ...the government decides to drop it);  any complementiser or preposition linking a head with a clausal or PP dependent is an inte- gral part of the GR (the type slot);  the underlying grammatical relation is speciﬁed for arguments displaced from their canonical positions by movement phenomena (e.g. the ini- tial gr slot of ncsubj and arg mod in the passive ...it is either acquired by another proprietor...);  semantic arguments syntactically realised as modiﬁers (e.g. the passive by-phrase) are indi- cated as suchusing arg mod;  conjuncts in a co-ordination structure are dis- tributed over the higher-level relation (e.g. in ...become ... until ... either acquired ... or ... decides... there are two verbal dependents of become, acquire and decide, each in a separate mod GR;  arguments which are not lexically realised can be expressed (e.g. when there is pro-drop the dependent in a subj GR would be speciﬁed as Pro);  GRs are organised into a hierarchy so that they can be left underspeciﬁed by a shallow parser which has incomplete knowledge of syntax. In addition to constituent structure, both the ptb and susanne contain functional, or predicate- argument annotation, the former particularly em- ploying a rich set of distinctions, often with complex grammatical and contextual conditions on when one function tag should be applied in preference to an- other. For example, the tag TPC (topicalized)  marks elements that appear before the subject in a declarative sentence, but in two cases only: (i) if the fronted element is associated with a T in the position of the gap. (ii) if the fronted element is left- dislocated [...] (Bies et al., 1995: 40). Conditions of this type would be very diﬃcult to encode in an actual parser, so attempting to evaluate on them would be uninfor- mative. Much of the problem is that treebanks of this kind have to specify the behaviour of many in- teracting factors, such as how syntactic constituents should be segmented, labelled and structured hi- erarchically, how displaced elements should be co- indexed, and so on. Within such a framework the further speciﬁcation of how functional tags should be attached to constituents is necessarily highly com- plex. Moreover, functional information is in some cases left implicit4, presenting further problems for precise evaluation. Table 1 gives a rough comparison between the types of information in the GR scheme and in the ptb and susanne. It might be possi- ble semi-automatically to map a treebank predicate- argument encoding to the GR scheme (taking advan- tage of the large amount of work that has gone into the treebanks), but we have not investigated this to date. 3 The Annotated Corpus and Evaluation 3.1 Corpus Annotation Our corpus consists of 500 sentences (10K words) covering a number of written genres. The sentences were taken from the susanne corpus, and each was marked up manually by two annotators5. The manual analysis was performed by the ﬁrst author and was checked and extended by the third 4The predicate is the lowest (right-most branching) VP or (after copula verbs and in small clauses) a con- stituent tagged PRD (Bies et al., 1995: 11). 5The corpus and evaluation software that can be used with it will shortly be made publicly available online. Relation ptb susanne dependent   mod TPCADV etc. p etc. ncmod CLRVOCADV etc. np etc. xmod cmod arg mod LGS a arg   subj   ncsubj SBJ s xsubj csubj subj or dobj   comp   obj   dobj (NP after V) o obj2 (2nd NP after V) iobj CLRDTV i clausal PRD  xcomp e ccomp j Table 1: Rough correspondence between the GR scheme and the functional annotation in the Penn Treebank (ptb) and susanne. author. Inter-annotator agreement was around 95 which is somewhat better than previously reported ﬁgures for syntactic markup (e.g. Leech and Garside, 1991). Marking up was done semi-automatically by ﬁrst generating the set of relations predicted by the evaluation software from the closest system analy- sis to the treebank annotation and then manually correcting and extending these. The mean number of GRs per corpus sentence is 9.72. Table 2 quantiﬁes the distribution of relations occurring in the corpus. The split between modiﬁers and arguments is roughly 6040, with approximately equal numbers of subjects and complements. Of the latter, 40 are clausal; clausal modiﬁers are almost as prevalent. In strong contrast, clausal subjects are highly infrequent (accounting for only 0.2 of the total). Direct objects are 2.75 times more frequent than indirect objects, which are themselves 7.5 times more prevalent than second objects. The corpus contains sentences belonging to three distinct genres. These are classiﬁed in the original Brown corpus as: A, press reportage; G, belles let- tres; and J, learned writing. Genre has been found to aﬀect the distribution of surface-level syntactic conﬁgurations (Sekine, 1997) and also complement types for individual predicates (Roland  Jurafsky, 1998). However, we observe no statistically signif- Relation  occurrences  occurrences dependent 4690 100.0 mod 2710 57.8 ncmod 2377 50.7 xmod 170 3.6 cmod 163 3.5 arg mod 39 0.8 arg 1941 41.4 subj 993 21.2 ncsubj 984 21.0 xsubj 5 0.1 csubj 4 0.1 subj or dobj 1339 28.6 comp 948 20.2 obj 559 11.9 dobj 396 8.4 obj2 19 0.4 iobj 144 3.1 clausal 389 8.3 xcomp 323 6.9 ccomp 66 1.4 Table 2: Frequency of each type of GR (inclusive of subsumed relations) in the 10K-word corpus. icant diﬀerence in the total numbers of the various grammatical relations across the three genres in the corpus. 3.2 Parser Evaluation We replicated an experiment previously reported by Carroll, Minnen  Briscoe (1998), using a robust lexicalised parser, computing three evaluation mea- sures for each type of relation against the 10K-word test corpus (table 3). The evaluation measures are precision, recall, and F-score (van Rijsbergen, 1979)6 of parser GRs against the test corpus annotation. GRs are in general compared using an equality test, except that we allowed the parser to return mod, subj and clausal relations rather than the more speciﬁc ones they subsume, and to leave unspeci- ﬁed the ﬁller for the type slot in the mod, iobj and clausal relations7. The head and dependent slot ﬁllers are in all cases the base forms of single head words, so for example, multi-component heads such as the names of people and companies are reduced to a single word; thus the slot ﬁller corresponding to 6The F-score is a measure combining precision and recall into a single ﬁgure. We use the version in which they are weighted equally, deﬁned as 2  precision  recall(precision  recall). 7The implementation of the extraction of GRs from parse trees is currently being reﬁned, so these minor re- laxations should be removed soon. Relation Precision Recall F-score () () dependent 75.1 75.2 75.1 mod 73.7 69.7 71.7 ncmod 78.1 73.1 75.6 xmod 70.0 51.9 59.6 cmod 67.4 48.1 56.1 arg mod 84.2 41.0 55.2 arg 76.6 83.5 79.9 subj 83.6 87.9 85.7 ncsubj 84.8 88.3 86.5 xsubj 100.0 40.0 57.1 csubj 14.3 100.0 25.0 subj or dobj 84.4 86.9 85.6 comp 69.8 78.9 74.1 obj 67.7 79.3 73.0 dobj 86.3 84.3 85.3 obj2 39.0 84.2 53.3 iobj 41.7 64.6 50.7 clausal 73.0 78.4 75.6 xcomp 84.4 78.9 81.5 ccomp 72.3 74.6 73.4 Table 3: GR accuracy by relation. Bill Clinton would be Clinton. For real-world appli- cations this might not be the desired behaviourone might instead want the token Bill Clintonbut the analyser could easily be modiﬁed to do this. The evaluation results can be used to give a single ﬁgure for parser accuracythe F-score of the depen- dent relationprecision and recall at the most gen- eral level, or more ﬁne-grained information about how accurately groups of, or single relations were produced. The latter would be particularly use- ful during parser grammar development to identify where eﬀort should be expended on making improve- ments. 4 Conclusions We have outlined and justiﬁed a language and application-independent corpus annota- tion scheme for evaluating syntactic parsers, based on grammatical relations between heads and dependents. The scheme has been used in the EU-funded SPARKLE project (see http:www.ilc.pi.cnr.itsparkle.html) to anno- tate English, French, German and Italian corpora, and for evaluating parsers for these languages. In this paper we have described a 10K-word corpus of English marked up to this standard, and shown its use in evaluating a robust parsing system. The corpus and evaluation software that can be used with it will shortly be made publicly available online. Acknowledgments This work was funded by UK EPSRC project GRL53175 PSET: Practical Simpliﬁcation of En- glish Text, CEC Telematics Applications Pro- gramme project LE1-2111 SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering, and by an EPSRC Advanced Fellow- ship to the ﬁrst author. We would like to thank Antonio Sanﬁlippo for his substantial input to the design of the annotation scheme. References Atwell, E. (1996) Comparative evaluation of gram- matical annotation models. In R. Sutcliﬀe, H. Koch  A. McElligott (Eds.), Industrial Pars- ing of Software Manuals, 2546. Amsterdam: Rodopi. Bies, A., Ferguson, M., Katz, K., MacIntyre, R., Tredinnick, V., Kim, G., Marcinkiewicz, M., Schasberger, B. (1995) Bracketing guidelines for Treebank II style Penn Treebank Project. Tech- nical Report, CIS, University of Pennsylvania, Philadelphia, PA. Carpenter, B.  Manning, C. (1997) Probabilistic parsing using left corner language models. In Proceedings of the 5th ACLSIGPARSE Interna- tional Workshop on Parsing Technologies. MIT, Cambridge, MA. Carroll, J., Briscoe E.  Sanﬁlippo, A. (1998) Parser evaluation: a survey and a new proposal. In Proceedings of the International Confer- ence on Language Resources and Evaluation, 447454. Granada, Spain. Available online at ftp:ftp.cogs.susx.ac.ukpubusersjohnca lre98-ﬁnal.ps. Carroll, J., Minnen, G.  Briscoe E. (1998) Can subcategorisation probabilities help a sta- tistical parser?. In Proceedings of the 6th ACLSIGDAT Workshop on Very Large Cor- pora. Montreal, Canada. Available online at http:xxx.lanl.govabscmp-lg9806013. Charniak, E. (1996) Tree-bank grammars. In Pro- ceedings of the 13th National Conference on Ar- tiﬁcial Intelligence, AAAI-96, 10311036. . Collins, M. (1996) A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Meeting of the Association for Compu- tational Linguistics, 184191. Santa Cruz, CA. Gaizauskas, R. (1998) Special issue on evaluation. Computer Speech  Language. Gaizauskas, R., Hepple M.  Huyck, C. (1998) Modifying existing annotated corpora for gen- eral comparative evaluation of parsing. In Pro- ceedings of the LRE Workshop on Evaluation of Parsing Systems. Granada, Spain. Grishman, R., Macleod, C.  Sterling, J. (1992) Evaluating parsing strategies using standardized parse ﬁles. In Proceedings of the 3rd ACL Con- ference on Applied Natural Language Processing, 156161. Trento, Italy. Harrison, P., Abney, S., Black, E., Flickinger, D., Gdaniec, C., Grishman, R., Hindle, D., In- gria, B., Marcus, M., Santorini, B.  Strza- lkowski, T. (1991) Evaluating syntax perfor- mance of parsergrammars of English. In Pro- ceedings of the Workshop on Evaluating Natural Language Processing Systems. ACL. Leech, G.  Garside, R. (1991) Running a grammar factory: the production of syntactically anal- ysed corpora or treebanks. In S. Johansson  A. Stenstrom (Eds.), English Computer Cor- pora: Selected Papers and Bibliography, Mouton de Gruyter, Berlin. Lehmann, S., Oepen, S., Regnier-Prost, S., Net- ter, K., Lux, V., Klein, J., Falkedal, K., Fou- vry, F., Estival, D., Dauphin, E., Compagnion, H., Baur, J., Balkan, L.  Arnold, D. (1996) tsnlp  test suites for natural language pro- cessing. In Proceedings of the International Con- ference on Computational Linguistics, COLING- 96, 711716. Copenhagen, Denmark. Lin, D. (1995) A dependency-based method for evaluating broad-coverage parsers. In Proceed- ings of the 14th International Joint Conference on Artiﬁcial Intelligence, 14201425. Montreal, Canada. Lin, D. (1996) Dependency-based parser evaluation: a study with a software manual corpus. In R. Sutcliﬀe, H-D. Koch  A. McElligott (Eds.), Industrial Parsing of Software Manuals, 1324. Amsterdam, The Netherlands: Rodopi. Marcus, M., Santorini, B.  Marcinkiewicz (1993) Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2), 313330. Roland, D.  Jurafsky, D. (1998) How verb sub- categorization frequencies are aﬀected by cor- pus choice. In Proceedings of the 17th Inter- national Conference on Computational Linguis- tics (COLING-ACL98), 11221128. Montreal, Canada. Rubio, A. (Ed.) (1998) International Conference on Language Resources and Evaluation. Granada, Spain. Sampson, G. (1995) English for the Computer. Ox- ford, UK: Oxford University Press. Sanﬁlippo, A., Barnett, R., Calzolari, N., Flores, S., Hellwig, P., Leech, P., Melero, M., Mon- temagni, S., Odijk, J., Pirrelli, V., Teufel, S., Villegas M.  Zaysser, L. (1996) Subcategoriza- tion Standards. Report of the EAGLES Lexi- conSyntax Interest Group. Available through eaglesilc.pi.cnr.it. Sekine, S. (1997) The domain dependence of pars- ing. In Proceedings of the 5th ACL Conference on Applied Natural Language Processing, 96102. Washington, DC. Srinivas, B., Doran, C., Hockey B.  Joshi A. (1996) An approach to robust partial parsing and eval- uation metrics. In Proceedings of the ESSLLI96 Workshop on Robust Parsing. Prague, Czech Re- public. Srinivas, B., Doran, C.  Kulick, S. (1995) Heuris- tics and parse ranking. In Proceedings of the 4th ACLSIGPARSE International Workshop on Parsing Technologies. Prague, Czech Repub- lic. van Rijsbergen, C. (1979) Information Retrieval. Butterworth, London.",
  "48.pdf": "arXiv:cs9907017v1 [cs.CL] 9 Jul 1999 A Bootstrap Approach to Automatically Generating Lexical Transfer Rules Davide Turcato Paul McFetridge Fred Popowich and Janine Toole Natural Language Laboratory, School of Computing Science, Simon Fraser University 8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada and Gavagai Technology P.O. 374, 3495 Cambie Street, Vancouver, British Columbia, V5Z 4R3, Canada {turk,mcfet,popowich,toole}cs.sfu.ca Abstract We describe a method for automatically generating Lexical Transfer Rules (LTRs) from word equivalences using transfer rule templates. Templates are skeletal LTRs, unspeciﬁed for words. New LTRs are cre- ated by instantiating a template with words, provided that the words belong to the ap- propriate lexical categories required by the template. We deﬁne two methods for cre- ating an inventory of templates and using them to generate new LTRs. A simpler method consists of extracting a ﬁnite set of templates from a sample of hand coded LTRs and directly using them in the gen- eration process. A further method con- sists of abstracting over the initial ﬁnite set of templates to deﬁne higher level tem- plates, where bilingual equivalences are de- ﬁned in terms of correspondences involving phrasal categories. Phrasal templates are then mapped onto sets of lexical templates with the aid of grammars. In this way an inﬁnite set of lexical templates is recur- sively deﬁned. New LTRs are created by parsing input words, matching a template at the phrasal level and using the corre- sponding lexical categories to instantiate the lexical template. The deﬁnition of an inﬁnite set of templates enables the au- tomatic creation of LTRs for multi-word, non-compositional word equivalences of any cardinality. 1 Introduction It is well-known that Machine Translation (henceforth MT) systems need information about the diﬀerent ways in which words can be translated, depending on their syntactic and semantic context. Such lexical transfer rules (henceforth LTRs) are notoriously time-consu- ming for humans to construct. Our task is to auto- matically generate LTRs. An LTR can be seen as a word equivalence plus an associated transfer pattern. By word equivalence we mean a translation pair simply stated in terms of words, in a dictionary-like fashion. A transfer pattern speciﬁes how transfer is to be performed for each of the morphological variants of the words in the equiv- alence and for diﬀerent syntactic contexts. For in- stance, given an English-Spanish word equivalence get lucky  tener suerte the associate transfer pattern would have to account for all the following equivalences: I get lucky Tengo suerte I will get lucky Tendre suerte I would have got lucky Habrıa tenido suerte Getting lucky Teniendo suerte I start getting lucky Empiezo a tener suerte I start getting very lucky Empiezo a tener mucha suerte where the Spanish sentences can be glossed as follows: a. Tengo suerte I have good luck b. Tendre suerte I will have good luck c. Habrıa tenido suerte I would have had good luck d. Teniendo suerte Having good luck e. Empiezo a tener suerte I start to have good luck f. Empiezo a tener mucha suerte I start to have much good luck The last example in the list of translation pairs shows that a transfer pattern also has to account for modiﬁers. In the example, very is translated by the adjective mucha, whereas in a sentence like I start getting very lazy it would be translated by the adverb muy (Empiezo a volverme muy perezoso). Thus, a bilingual lexicon of such transfer rules is a diﬀerent object from a collection of word equivalences, which is the deﬁnition of a bilingual lexicon most of- ten found in the literature about automatic creation of bilingual lexicons ([3], [4], to cite recent examples). As a matter of fact, those techniques and the one de- scribed here are disjoint and complementary, as the output of those tools can be used as input to LTR development. Given a collection of word equivalences, we focus on how transfer patterns can be associated with them to create complete LTRs. This task has rarely been tackled before. Several techniques have been proposed for the automatic ac- quisition of word equivalences (see references above), but very few for the automatic acquisition of full LTRs (e.g. [1]), despite the high cost of their manual de- velopment. Bilingual coding is often a bottleneck in MT system development. Unlike other linguistic re- sources, like grammars, lexicons have an open-ended linear growth and their quality can be directly related to their size. For this reason, there is often a mismatch between the development time frames of bilingual lex- ical resources and other modules. 2 Basic ideas 2.1 Template based generation We use a bootstrap approach to transfer rule creation. An initial hand coded bilingual lexicon is used as a basis for deﬁning a set of transfer rule templates, i.e. skeletal rules unspeciﬁed for words. Subsequently, ap- propriate transfer rule templates are associated to new word equivalences, on the basis of the morphosyn- tactic features of those words, to construct complete LTRs. The approach described here shares this under- lying template-based bootstrap philosophy with the approach described by [6], but diﬀers from it in three key respects: the resources it uses, the way templates are created and the way LTRs are created from word equivalences and templates. LTR templates are also akin to tlinks, as described in [1] and [2].These works describe how to use tlinks for the semi-automatic gen- eration of single-word equivalences. However, they do not deal with the creation of an inventory of tlink types or the generation from multi-word equivalences. For the sake of exposition, we use here a simpliﬁed version of LTRs and templates, showing only words (for LTRs), syntactic categories and indices, the latter represented by tuples of subscript lowercase letters. A schematic LTR and template are shown in (1) and (2), respectively. For a description of the MT system and the full LTR formalism see [5] and [7]. Templates LTRs Coverage 1 5683 33.9  2 8726 52.1  3 10710 63.9  4 12336 73.6  5 13609 81.2  50 15473 92.3  500 16338 97.5  922 16760 100.0  Table 1: Incremental template coverage (1) W s1:Cats1 Indicess1  ... W sm:Catsm Indicessm  W t1:Catt1 Indicest1  ... W tn:Cattn Indicestn (2) Cats1 Indicess1  ... Catsm Indicessm  Catt1 Indicest1  ... Cattn Indicestn Given a word equivalence expressing a translation pair, our goal is to create a transfer rule directly us- able by an MT system. In other words, the goal is to associate a transfer pattern, as informally described above, to a word equivalence. We describe two ap- proaches, the latter of which is an extension of the former. 2.2 The enumerative approach The goal of creating templates and using them in gen- erating LTRs can be accomplished through the follow- ing steps: 1. Create transfer rule templates: (a) Deﬁne a set of LTR templates. Each tem- plate represents a transfer pattern. The template deﬁnition task is carried out by extracting LTR templates from an initial hand-coded bilingual lexicon. This can be easily done by simply removing words from LTRs, normalizing variables by renaming them in some canonical way (so as to avoid two instances of the same templates to only diﬀer by variable names), then ranking tem- plates by frequency, if the application of some cutoﬀ is in order. Table 1 shows the incremental coverage of the set of templates we extracted from our initial hand-coded English-Spanish bilingual lexicon. We refer to the template creation process described here as the enumerative approach to build- ing templates. (b) Associate a set of constraints to each tem- plate. Typically, these are morphosyntac- tic constraints on the words to be matched against the template. Basically, such con- straints ensure that an input word belongs to the same lexical category of the tem- plate item it has to match. The same goal could also be achieved by directly unifying a lexical category associated to a word with the corresponding template item, instead of having separate constraints. 2. Create transfer rules: (a) Given a word equivalence, create an LTR if the lexical descriptions of the words in the translation pair satisfy all the constraints associated with a template (or unify with their corresponding items in the template). In that case, an LTR is created by simply instantiating the successful template with the words in the word equivalence. An enumerative approach to template creation guarantees an adequate coverage for creating most of the LTRs needed in an MT system, as discussed in [6]. A simple LTR automatic generation procedure can be implemented by selecting the most signiﬁcant templates in the database. This can be done, as hinted above, by counting the occurrences of each template in the LTR corpus and choosing those that rank best. The top ranking templates are then used to directly map input word equivalences onto LTRs. This ap- proach was implemented and used, with good results (e.g. in an early test run on a 1544 entry word-list downloaded from the World Wide Web, LTRs were created for 79 of the input word equivalences. Fur- ther results are discussed in section 4. 2.3 The generative approach The idea of adding recursion to the template deﬁnition procedure, thus replacing a ﬁnite set of templates with an inﬁnite one, was brought about by work on phrasal verbs. Phrasal verbs exhibit a larger variability than other collocations. One of the problems is that their translations are often paraphrases, because a target language might lack a direct equivalent to a source phrasal verb. Table 2 illustrates this point (e.g. sit through something  permanecer hasta la ﬁn de algo). A ﬁnite set of templates could still be used for phrasal verbs, but this would require a much larger initial LTR corpus than is necessary for other colloca- tions, in order to preserve a high automatic generation rate. An alternative solution is based on introducing a further level of abstraction, by deﬁning higher level, underspeciﬁed templates which state bilingual equiva- lences in terms of phrasal categories instead of lexical categories. Then, a simple grammar is used to map such phrasal categories onto sets of lexical categories in order to derive completely speciﬁed templates. Despite the template variability in terms of se- quences of syntactic categories, a much higher reg- ularity can be found by deﬁning templates in terms of constituency. For instance, all the lexical equivalences listed for sit in Table 2 can be reduced to two basic patterns in terms of phrasal categories:1 (3) VP  VP (4) VPNP  VPNP where VPNP represents a verbal phrase with a noun phrase gap. A further generalization is that a VP on either side of a template tends to be equivalent to a phrase of the same type, with the same number and type of gaps. We note incidentally that this further generalization does not hold for all categories, in terms of category identity. For example, an English adjective often corresponds to a Spanish prepositional phrase (e.g. fashionable  de moda, stainless  sin tacha). The abstraction process consists of partitioning lex- ical templates into classes such that each class is iden- tiﬁed by a phrasal template, in which a group of lex- ical categories is replaced by a phrasal category. All the lexical templates in a class can be obtained by replacing the phrasal category with one of its lexical projections. Such replacement can be carried out on a purely monolingual basis, by using a simple grammar to deﬁne constituency. The key requirement on the abstraction process is that the resulting abstract tem- plate be invariant with respect to lexical replacement, i.e. the replacement do not involve any other element in the abstract template, beside the replaced phrasal category. This restriction amounts to requiring that a phrasal category be self-contained in terms of variable sharing, i.e. the lexical categories it dominates intro- duce no new variables to be shared with items exter- nal to the phrase itself; or, if such sharing happens, it must be entirely predictable and unambiguous, i.e. the variable sharing lexical category must be marked in such a way that it can be uniquely identiﬁed. Once a phrasal template has been deﬁned, and a grammar is available for mapping phrasal categories onto lexical categories, new, previously unseen lexical templates can be derived that were not present in the initial class which the phrasal template was abstracted over. Depending on the recursivity of the grammars in use, an inﬁnite set of lexical templates can be deﬁned via a ﬁnite set of phrasal template. Therefore, we refer to this process as to a generative approach to building templates. For example, the templates underlying the bilingual entries in (5) allow one to infer the template underlying the bilingual entry for (6). (5) Buddha  buda Wonderland  paıs de las maravillas (6) Halloween  vıspera del Dıa de los Santos Namely, from the lexical templates in (7), the phra- sal template in (8) can be inferred. In turn, the new 1We adopt the convention of using lowercase labels for lexical categories and uppercase labels for phrasal categories. Phrasal verb equivalences: sit back  ponerse comodo iv adv  rv adj sit down  sentarse iv adv  rv sit for sth  posar para algo iv p  iv p sit in for sb  sustituir algn iv adv p  tv sit in on sth  participar como observador en algo iv adv p  iv p n p sit through sth  permanecer hasta la ﬁn de algo iv p  iv p det n p sit tight  esperar iv adj  iv sit up with sb  velar algn iv adv p  tv sit up  incorporarse iv adv  rv sit sb down  sentar algn tv adv  tv sit sb up  incorporar algn tv adv  tv sit sth out  no participar en algo tv adv  neg iv p Glosses for Spanish: a. ponerse comodo put oneself comfortable b. sentarse sit oneself c. posar para algo pose for sth d. sustituir algn replace sb e. participar como observador en algo take part as observer in sth f. permanecer hasta la ﬁn de algo remain until the end of sth g. esperar wait h. velar algn watch over sb i. incorporarse raise oneself j. sentar algn sit sb k. incorporar algn raise sb l. no participar en algo do not take part in sth where: En. sth  Sp. algo  something En. sb  Sp. algn  somebody Table 2: Some phrasal verb equivalences for the verb sit, and associated glosses for Spanish. Word pairs corpus Transfer rule templates Transfer rule Transfer rules grammar Template abstraction Template Figure 1: System architecture. lexical template in (9) can be derived from (8), pro- vided that the relevant grammar licenses the projec- tion of a phrasal category NBAR onto an appropriate sequence of lexical categories. (7) na  na na  na  pa,b  db  nb (8) na  NBARa (9) na  na  pa,b  db  nb  pb,c  dc  nc The overall architecture of the generative approach is shown in Figure 1. The idea of the whole process is to exploit monolingual regularities to account for a non-compositional bilingual equivalence. In a non- compositional equivalence, direct correspondences be- tween lexical items on either side cannot be estab- lished. The two sides are only equivalent as wholes. However, a non-compositional equivalence can be ac- counted for by decomposing it into a phrasal bilingual equivalence and monolingual mappings from phrasal to lexical categories. Implementing a template grammar allows one to obtain an adequate template coverage while requir- ing only a small initial LTR corpus. In our system the template abstraction task was performed manu- ally. Although the implementation of an automatic template abstraction procedure could be foreseen, the high reliability required of templates demands that a strict human control still be placed at some point of the template abstraction and grammar deﬁnition phases. It is also worth pointing that, by our ex- perience, the grammar development task is not very labour intensive. Such grammars have to perform a very limited task, and deal with a restricted and con- trolled input. Basically, they only have to account for the constituent structure of very simple and syntac- tically ordinary phrases. In our case, the grammar development time was usually measured in hours. 3 Implementation In this section the generation of an LTR from a phrasal template is described. We illustrate the procedure with the aid of a worked example. We show how the LTR in (11) is generated from the word equivalence in (10). (10) sit in on sth  participar como observador en algo (11) sit :iva,b,c  in :advc  on :pa,d  participar:iva,b,e  como :pe,f  observador:nf  en :pa,d Pragmatic reasons induced us to take a hybrid ap- proach to the template grammar construction task, combining the enumerative and generative approaches to build templates. It turned out that templates show a larger variability on the Spanish side than on the English side. Table 2 is a good example of this point. This fact might be related to a larger use of concise and productive phrasal verb patterns in English, or perhaps to the fact that our word equivalence cod- ing was driven by English collocations, thus involv- ing the presence of paraphrases on the Spanish side only. In any case it appeared that the enumerative ap- proach was suﬃcient to adequately cover the English VPa,bNPd    VPa,b,e    iva,b,e participar PPe,f    pe,f como NPf nf observador PPa,dNPd   pa,d en NPdNPd ǫ Figure 2: Template right hand side generation tree. side without resulting in a proliferation of templates. Hence, we constructed a set of templates in which only the Spanish side contained a phrasal category, while the English side was fully speciﬁed. For example: (12) iva,b,c  advc  pa,d  VPa,bNPd For each such partially speciﬁed template, the Span- ish side has to be generated. The selection of a phrasal template candidate is performed by doing a lexical lookup for the source words and matching their morphosyntactic represen- tations with the corresponding left hand side tem- plate items. This can be done directly by uniﬁcation or by associating a set of constraints to the phrasal template. In our implementation, constraints are ex- pressed by Prolog goals taking morphosyntactic de- scriptions as arguments. For example, the phrasal template in (12) is selected as a candidate for our in- put translation pair in (10), as the lexical descriptions of sit, in and on match the categories iv, adv and p, respectively. A sequence of words can match several phrasal templates, e.g. in our speciﬁc example the fol- lowing candidate is also selected, since in can also be a noun (e.g. the ins and outs of a problem): (13) tva,b,c  nc  pc,d  VPa,bNPd Given a candidate phrasal template, the core of the generation procedure is a call to a target language grammar, Spanish in this case, which parses the target input words using the given phrasal category (with its associated indices) as its initial symbol. A phrasal template may disjunctively specify sev- eral phrasal categories as initial symbols (or, equiva- lently, diﬀerent phrasal templates may share the same English side, while specifying diﬀerent initial sym- bols). For instance, English adjectives may be equiv- alent to either a Spanish adjectival phrase or preposi- tional phrase, as already mentioned. Figure 2 shows the result of parsing the Spanish input words of our example, using the initial symbol in the phrasal template in (12). Each node shows the assigned syntactic category, along with the indices, either speciﬁed in the initial symbol or instantiated during parsing. The pre-terminal categories in the parse tree, along with the input words, are used to build the following LTR right hand side for the ﬁnal LTR: (14) participar:iva,b,e  como :pe,f  observador:nf  en :pa,d Finally, by instantiating the left hand side of the template with the input English words and replacing the right hand side phrasal category in the phrasal template (12) with the right hand side in (14), the LTR in (11) is obtained. Again, several output LTRs can be generated for the same input words. For example, the following LTRs are also generated for our example: (15) sit :iva,b,c  in :advc  on :pa,d  participar:iva,b  como :pa,e  observador:ne  en :pa,d (16) sit :iva,b,c  in :advc  on :pa,d  participar:iva,b,e  como :pe,f  observador:nf  en :pf,d In (15) the indices show that como is analyzed as a modiﬁer of participar, instead of a complement. In (16) the preposition en modiﬁes observador, instead of participar. In our case, since we are interested in translating only from English to Spanish, we would keep only one of the candidate LTRs shown above, as they all share the same English side. However, given that the LTRs at hand are bidirectional, if one were interested in translating from Spanish to English, one might want to keep all the entries, in order to cover diﬀerent syntactic analyses. The last step is the validation of LTRs by lexicog- raphers, which exclusively consists of removing un- wanted entries. Lexicographers use their linguistic in- tuition and knowledge of the syntactic representations File In Out Val InOut InVal  Enumerative approach: ADJ 542 546 469 468 468 86.3  Phrasal verbs - batch 1 2340 2395 1416 1647 1414 60.4  Generative approach: Phrasal verbs - batch 2 549 1152 486 512 469 85.4  Phrasal verbs - batch 3 478 782 404 418 393 82.2  V  (ADJ or N) 345 617 300 302 292 84.6  ADJ  N 1144 1331 914 914 903 78.9  IV  ADJ 199 346 187 187 187 94.0  Table 3: Overview of system performance. used in LTRs, in order to make a choice among sev- eral competing LTRs for a given translation pair, or to to check the correctness of the analysis underlying an LTR. Information that they are typically expected to check is the way coindexing is performed (e.g. for prepositions, in order to check that they be attached to the correct item) and the syntactic categories as- signed to words, most crucially when unknown words are involved. In our implementation lexicographers are helped in the task by messages that the system associates to candidate LTRs, in order to signal, for instance, the presence of lexically unknown words or lexical ambiguities which are potential sources of er- rors (for instance, verbs which are both transitive and intransitive). The ﬁles output by the system tend to be self-contained in terms of information needed by lexicographers to perform their task, and usually lex- icographers do not need access to any extra linguistic resources. We ﬁnally note that the validation step becomes particularly crucial if the input translation pairs are automatically acquired from resources like bilingual corpora, in order to ﬁlter out LTRs created from noisy bilingual equivalences. 4 Performance Results of this approach are shown in Table 3. Columns are to be interpreted as follows: File : The type of words or collocations in the pro- cessed ﬁle. In : The number of input word equivalences. Out : The number of generated LTRs. Val : The number of LTRs validated by the lexicog- raphers and thus added to the transfer lexicon. InOut : The number of input word equivalences (In) for which some output (Out) was provided. This value is usually lower than the value of Out be- cause more than one LTR can be created for the same word equivalence. Therefore, more than one element of Out can correspond to one ele- ment of InOut. InVal : The number of input word equivalences (In) for which some LTR was validated (Val) by the lexicographers.  : The success rate, obtained by dividing the value of InVal by the value of In. By using the value of InVal instead of the value Val we factor out the extra valid LTRs that can be created for a given input word equivalence, in addition to the ﬁrst one. The listed ﬁles are sorted according to the chrono- logical order in which they were processed, and di- vided according to the methodology in use. One of the most common reasons for failure is the presence of unknown words on the English side. When the in- put contains unknown words, we block generation. In contrast unknown words are accepted on the Span- ish side, matching any possible lexical category. This treatment of Spanish unknown words is one of the reasons that explains the higher value in Out than in InOut. Genuine syntactic ambiguity is the other main reason for such diﬀerence in values. In terms of speed, we ran a test by evaluating the development time of the ﬁle named Phrasal verbs - batch 1 in Table 3. We had a lexicographer timing three activities: coding translation pairs, revising au- tomatically generated LTRs, manually coding LTRs for the translation pairs that failed in automatic gen- eration. The results are shown in Table 4. 5 Conclusion The described methodology makes LTR coding con- siderably faster. According to our test, revising au- tomatically generated LTR ﬁles is about 8 times as fast as manually coding LTRs. If the manual coding of word equivalences is also counted in the automatic generation process, the process is still about 3 times as fast as the manual coding of complete LTRs. Besides speed, this methodology guarantees the syntactic correctness of the output (provided that the templates are syntactically correct, of course). The validation procedure only requires removing unwanted Activity N. of items Items per hour Time for 100 items Coding translation pairs 2340 31.25 3h 12m Revising LTRs 1416 (validated) 50.57 1h 59m Manually coding LTRs 926 6.25 16h 00m Table 4: Speed test results. LTRs, with no further editing intervention. Also, more control over a transfer rule database is provided, as each LTR can be associated with a template. In this way testing, debugging, and maintenance in general are easier and more eﬀective, as these processes can be performed on templates rather than on LTRs. Although the described methodology can hardly aspire to completeness, compared to manual coding, it can be integrated with a manual coding phase, for the translation pairs that fail to generate automati- cally. The gain in terms of labour eﬀort is still pro- portional to the success rate of the generation proce- dure, when compared to an entirely manual coding. Moreover, there is one sense in which automatic gen- eration is more complete than manual coding, namely in the generation of multiple entries for a translation pair. Lexicographers tend to code one LTR per trans- lation pair, whereas the range of candidates proposed by automatic generation can make them aware of valid alternatives analyses unnoticed by them. Lexicographic work is easier, as little technical knowledge is required of lexicographers. Full knowl- edge of the formalism in use for LTRs is only required in the initial phase of LTR corpus constructions. Once a template generation procedure is in place, lexicogra- phers only need a passive knowledge of the formalism, i.e. they must be able to read and understand LTRs, but not to write LTRs. Input word equivalences are expressed in plain nat- ural language. This makes them amenable to acquisi- tion from corpora or MRDs. If manual coding of word equivalences is necessary, only bilingual speaking com- petence is required of lexicographers. No further lin- guistic background or familiarity with any formalism is necessary. The bootstrap approach makes this methodology more suitable for the scaling up of large scale MT sys- tems, than for rapid development of prototypes. The knowledge acquired in prototype development is used in developing a full-ﬂedged system, which is often the most critical phase for MT systems. The adoption of this methodology is also proﬁtable in porting an MT system to a diﬀerent language pair or in developing a multilingual MT system: the methodology is applica- ble to any language pair, and the linguistic knowledge can also be re-used to some extent, depending on the similarity between the languages at hand. References [1] Ann Copestake, Bernie Jones, Antonio Sanﬁlippo, Horacio Rodrıguez, Piek Vossen, Simonetta Mon- temagni, and E. Marinai. Multilingual lexical representation. Technical Report 253, University of Cambridge Computer Laboratory, Cambridge, UK, 1992. [2] Ann Copestake and Antonio Sanﬁlippo. Multilin- gual lexical representation. In Proceedings of the AAAI Spring Symposium on Building Lexicons for Machine Translation, Stanford, California, USA, 1993. [3] Pascale Fung. A statistical view on bilingual lex- icon extraction: From parallel corpora to non- parallel corpora. In Proceedings of the Third Con- ference of the Association for Machine Transla- tion in the Americas (AMTA-98), pages 117, Langhorne, Pennsylvania, USA, 1998. [4] I. Dan Melamed. Empirical methods for MT lex- icon development. In Proceedings of the Third Conference of the Association for Machine Trans- lation in the Americas (AMTA-98), pages 1830, Langhorne, Pennsylvania, USA, 1998. [5] Fred Popowich, Davide Turcato, Olivier Laurens, Paul McFetridge, J. Devlan Nicholson, Patrick McGivern, Maricela Corzo-Pena, Lisa Pidruchney, and Scott MacDonald. A lexicalist approach to the translation of colloquial text. In Proceedings of the 7th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 7686, Santa Fe, New Mexico, USA, 1997. [6] Davide Turcato. Automatically creating bilingual lexicons for Machine Translation from bilingual text. In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Com- putational Linguistics (COLING-ACL98), pages 12991306, Montreal, Quebec, Canada, 1998. [7] Davide Turcato, Olivier Laurens, Paul McFetridge, and Fred Popowich. Inﬂectional information in transfer for lexicalist MT. In Proceedings of the International Conference Recent Advances in Natural Language Process- ing (RANLP-97), pages 98103, Tzigov Chark, Bulgaria, 1997.",
  "49.pdf": "arXiv:cs9907021v1 [cs.CL] 14 Jul 1999 Architectural Considerations for Conversational Systems  The VerbmobilINTARC Experience G. Gorz1, J. Spilker1, V. Strom2, H. Weber1 1 University of Erlangen-Nuremberg, IMMD (Computer Science) VIII  AI Am Weichselgarten 9 D-91058 ERLANGEN Email: goerzinformatik.uni-erlangen.de 2 University of Bonn, Institut of Communication Research and Phonetics 1 Conversational Requirements for Verbmobil Verbmobil1 is a large German joint research project in the area spontaneous speech-to-speech translation sytems which is sponsored by the German Federal Ministery for Research and Ed- ucation. In its ﬁrst phase (19921996) ca. 30 research groups in universities, research institutes and industry were involved, and it entered its second phase in January 1997. The overall goal is develop a system which supports face-to-face negotiation dialogues about the scheduling of meetings as its ﬁrst domain, which will be enlarged to more general scenarios during the sec- ond project phase. For the dialogue situation it is assumed that two speakers with different mother tongues (German and Japanese) have some common knowledge of English. Whenever a speakers knowledge of English is not sufﬁcient, the Verbmobil system will serve him as a speech translation device to which he can talk in his native language. So, Verbmobil is a system providing assistance in conversations as opposed to fully automatic con- versational systems. Of course, it can be used to translate complete dialogue turns. Both types of conversational systems share a lot of common goals, in particular utterance understanding  at least as much as is required to produce a satisfacory translation , processing of spontaneous speech phenomena, speech generation, and robustness in general. A difference can be seen in the fact that an autonomous conversational system needs also a powerful problem solving com- ponent for the domain of discourse, whereas for a translation system the amount of domain knowledge is limited by the purpose of translation, where most of the domain speciﬁc problem solving  except tasks like calendrical computations  has to be done by the dialog partners. A typical dialogue taken from the Verbmobil corpus is the following one: SIL GUTEN TAG HERR KLEIN SIL K-ONNEN WIR UNS AM MONTAG TREFFEN SIL JA DER MONTAG PA-ST MIR NICHT SO GUT SIL JA DANN TREFFEN WIR UNS DOCH AM DIENSTAG SIL AM DIENSTAG HABE ICH LEIDER EINE VORLESUNG SIL BESSER W-ARE ES BEI MIR AM MITTWOCH MITTAGS 1This work was funded by the German Federal Ministry for Research and Technology (BMFT) in the framework of the Verbmobil Project under Grant BMFT 01 IV 101 H  9. The responsibility for the contents of this study lies with the authors. 1 SIL ALSO AM MITTWOCH UM ZEHN BIS VIERZEHN UHR HABE ICH ZEIT SIL DANN LIEBER GLEICH NACH MEINEM DOKTORANDENTREFFEN SIL WOLLEN WIR UNS NICHT LIEBER IN MEINEM B-URO TREFFEN SIL NA JA DAS W-URDE GEHEN SIL JA HERR KLEIN WOLLEN WIR NOCH EINEN TERMIN AUSMACHEN SIL VIELLEICHT GINGE ES AM MITTWOCH IN MEINEM B-URO SIL DAS IST DER VIERZEHNTE MAI SIL AM MITTWOCH DEN VIERZEHNTEN PA-ST ES MIR NICHT SO GUT SIL AM DIENSTAG IN DIESER WOCHE H-ATTE ICH NOCH EINEN TERMIN SIL ALSO DANN AM DIENSTAG DEN DREIZEHNTEN MAI SIL VORMITTAGS ODER AM NACHMITTAG SIL JA MACHEN SIE DOCH EINEN VORSCHLAG SIL JA DANN LASSEN SIE UNS DOCH DEN VORMITTAG NEHMEN SIL JA GUT TSCH-U-S SIL  silence. 2 Prosody and Spontaneous Speech Phenomena To cope with spontaneous speech, prosody plays a decisive role. Integration of prosody into a speech-to-speech translator as an additional speech language interface is a current topic of re- search. Within the Verbmobil project, the experimental system INTARC was designed which performs simultaneous speech-to-speech translation (cf. [6, 4]). In INTARC, particular emphasis has been put on the issues of incrementality and (top-down) component interaction in order to take into account expectations and predictions from higher level linguistic components for lower level components. For this purpose time synchronous versions of traditional processing steps such as word recognition, parsing, semantic analysis and transfer had to be developed. In part com- pletely new algorithms had to be designed in order to achieve sufﬁcient processing performance to compensate for the lack of right context in search. The use of prosodic phrase boundaries became essential to reduce search space in parsing and semantic analysis. A further goal was robustness: If a detailed linguistic analysis fails, the system should be able to produce an approximately correct output. For this purpose, besides the main data ﬂow the system has a second template-based transfer strategy as a supplement, where a rough transfer is performed on the basis of prosodically focused words and a dialogue act detection. Furthermore, various spontaneous speech phenomena like pauses, interjections, and false starts are covered by INTARCs dialogue turn based uniﬁcation grammar (cf. [8, 9]). 3 Incremental, Interactive, and Time Synchronous Processing The general design goals of the INTARC system architecture were time synchronous processing as well as incrementality and interactivity as a means to achieve a higher degree of robustness and scalability. Interactivity means that in addition to the bottom-up (in terms of processing levels) data ﬂow the ability to process top-down restrictions considering the same signal segment for all processing levels. The construction of INTARC 2.0, which has been operational since fall 1996, followed an engineering approach focussing on the integration of symbolic (linguistic) and stochastic (recognition) techniques which led to a generalization of the concept of a one pass beam search. Fig. 1, which is a screen shot of INTARCs user interface, gives an overview of the overall system architecture. To enable component interaction, we designed the communication framework ICE [2, 3] which 2 Figure 1: The architecture of INTARC 2.0 maps an abstract channel model onto interprocess communication. Its software basis is PVM (Parallel Virtual Machine), supporting heterogeneous locally or globally distributed applications. The actual version of ICE runs on four hardware platforms and ﬁve operating systems with interfaces to eight programming languages or dialects. 4 Interactions between Recognizer, SynParser, SemParser, and Prosody To understand the operation of INTARC, we start with an overview of its syntactic parser com- ponent (SynParser). Whereas the dialogue turn based grammar of the system is a full uniﬁcation grammar written in HPSG, SynParser uses only the (probabilistically trained) context-free back- bone of the uniﬁcation grammar  which overgenerates  and a context-sensitive probabilistic model of the original grammars derivations. In particular, the following preprocessing steps had to be executed: 1. Parse a corpus with the original uniﬁcation grammar G to produce an ambiguous tree bank B. 2. Build a stripped-down (type skeleton) grammar G such that for every rule r in G there is a corresponding rule r in G and vice versa. 3. Use an unsupervised reestimation procedure to train G on B (context sensitive statistics). The syntactic parser (SynParser) is basically an incremental probabilistic search engine based on [20] (for earlier versions cf. [18, 19]); it receives word hypotheses and phrase boundary hypothe- ses as input. The input is represented as a chart where frames correspond to chart vertices and word hypotheses are edges which map to pairs of vertices. Word boundary hypotheses (WBHs) are mapped to connected sequences of vertices which lie inside the time interval in which the WBH has been located. The search engine tries to build up trees according to a probabilistic con- text free grammar supplied with higher order Markov probabilities. Partial tree hypotheses are uniformly represented as chart edges. The search for the n best output trees consists of succes- sively combining pairs of edges to new edges guided by an overall beam search strategy. The overall score of a candidate edge pair is a linear combination of three factors which we call de- coder factor, grammar factor and prosody factor. The decoder factor is the well known product of the acoustic and bigram scores of the sequences of word hypotheses covered by the two con- nected edges. The grammar factor is the normalized grammar model probability of creating a certain new analysis edge given the two input edges. The prosody factor (see next section) is calculated from the acoustic WBH scores and a class based tetragram which models sequences of words and phrase boundaries. So, SynParser performs purely probabilistic parsing without uniﬁcations. Only n best trees are transmitted to the semantic parser component (SemParser) to be reconstructed deterministically with uniﬁcation. SemParser uses a chart for representation and reuse of partial analyses. On failure, it issues a top-down request to SynParser. Because we make heavy use of structure shar- ing (to depth n) for all chart edges we were able to achieve polynomial runtime. So, the main processing steps along the path recognizer  SynParser  SemParser are the following:  The recognizer (decoder) performs a beam search producing a huge lattice of word hypothe- ses. 3  SynParser performs a beam search on this lattice to produce a small lattice of tree hypothe- ses.  SemParser executes the uniﬁcation steps in order to pick the best tree that uniﬁes.  Incremental bottom-up and top-down interaction of syntactic and semantic analysis are achieved by chart reconstruction and revision in SemParser.  Furthermore, bottom-up input from recognizer is provided via a morphology module (MORPHY [1]) for compound nouns. First experiments resulted in a runtime of approximately 30 times real time (on a SuperSparc) and a recognition rate for words in valid trees of approximately 50. Current work is focussing on ﬁne tuning for word recognition, morphology, syntactic and semantic parsing. In the following we describe the interactions between the components mentioned.  Interaction RecognizerSynParser (cf. [7])  The (left-hand side connected) word graph is being transmitted by endpoints bottom up.  Possible path extensions are being transmitted by starting points top down.  This leads to the following effects:  A dynamic modiﬁcation of language perplexity for recognition;  Data reduction and search is being moved (partially) from recognizer to parser.  Top-down interactions make only sense if there are strong model restrictions (narrow domain).  Interaction SynParserSemParser (cf. [10])  Probabilistic Viterbi parsing of word graphs with G in polynomial time (without uni- ﬁcations).  Packing and transmission of n best trees(only trees with utterance status!) per frame in O(treenodes) time complexity. Protocol with powerful data compression.  Trees are being reconstructed by SemParser by means of G deterministically. On fail- ure a top-down request for the next best tree is being issued.  On failure, a top-down request for the next best tree is being issued.  Structure sharing (to depth n) for all edges results in polynomial runtime.  This yields a preference for the longest valid utterance. A 100 tree recognition rate results in uniﬁcation grammar parsing in cubic time. So, in our case lattice parsing is tree recognition (decoding):  For each new frame, a vertex and an empty agenda of search steps are created.  All word hypotheses ending in the actual frame are read in as edges and all pairs of edges which can be processed are being scored and pushed on the agenda for that frame.  The score is a weighted linear combination of log probability scores given by the mod- els for acoustics, bigram, grammar and prosody.  As in an acoustic beam recognizer all steps down to a given offset from the maximum score are taken and all others are discarded. 4  The procedure stops when the word recognizer  which supplies word hypotheses with acoustic scores  sends an end of utterance signal. The interaction protocol implies that the ﬁrst tree to be transmitted is the best scored one: Syn- Parser constructs its chart incrementally, always sending the best hypotheses which have utter- ance status. SemParser reconstructs the trees incrementally and reports failures. While Sem- Parser is working  which may lead to a rejection of this tree  SynParser runs in parallel and ﬁnds some new trees. The failure messages are ignored as long as SemParser is still constructing trees. If SemParser becomes inactive, further hypotheses with a lower score are sent. SemParser utilizes its idle time to reconstruct additional trees which may become important during the analysis (speculative evaluation). I.e., if the estimation of an utterance improves over time, its subtrees are in general not accessible to SemParser, since they have never got a high score. With speculative evaluation, however, we often ﬁnd that they have already been constructed, which helps to speed up parsing. Since our grammar is turn-based, this situation is not the exception, but in fact the normal case. Hence, this strategy guarantees that the utterance spanned by the trees increases monotonously in time. A second phase is entered if SynParser has reached the end of the word lattice. In the case that SemParser has accepted one of the previous trees as a valid reading, SynParser is being informed about the success. Otherwise SemParser calls for further tree hypotheses. The selection criteria for the next best hypothesis are exactly the same as in the ﬁrst phase: Long hypotheses are preferred, and in the case of equal length the one with the best internal score is chosen. I.e., in the second phase the length of a potential utterance decreases. If none of the requested trees are accepted, the process stops iff SynParser makes no further trees available. This parameter controls the duration of the second phase. Depending on the choice which trees are sent, SynParser directs the behavior of SemParser. This is the essential reason why SemParser must not perform a search over the whole set of received hypotheses. The stepwise reduction of the length of hypotheses guarantees that the longest pos- sible valid utterance will be found. This is particularly useful to analyze utterance parts when no fully spanning reading can be found. To summarize, the advantages of this protocol are that no search must be performed by Sem- Parser, that the best tree which covers the longest valid utterance is being preferred (graceful degradation) and that dynamic load-balancing is achieved. 5 Issues in Processing Spontaneous Speech: Prosody and Speaker Style 5.1 Prosody The decisive role of prosody for processing spontaneous speech has already been mentioned. Now we describe the integration of prosodic information into the analysis process from an archi- tectural viewpoint. The interaction ParserProsody can be summarized as follows:  Bottom-up hypotheses on the word boundary class are time intervals; they are attached incrementally to word lattice nodes.  A prosodic score is computed from the word path, a trigram for words and phrase bound- aries and an acoustic score for phrase boundaries (maximized).  Prosody detectors are based on statistical classiﬁers, having been trained with prosodically labeled data. 5  No use of word information is made; time assignment is done through syllable kernel de- tection.  Recognition rates are: for accents 78, for phrase boundaries 81, and for sentence mood 85 The prosody module consists of two independently working parts: the phrase boundary detector [15] and the focus detector [13]. The data material investigated consists of spontaneous spoken dialogues on appointment scheduling. A subset of 80 minutes speech has been prosodically labeled: Full prosodic phrases (B3 boundaries) are distinguished from intermediate phrases (B2 boundaries). Irregular phrase boundaries are labeled with B9, and the default label for a word boundary is B0. The B2 and B3 boundaries correspond roughly to the linguistic concept of phrase boundaries, but are not necessarily identical (cf. [16]). In the phrase boundary detector, ﬁrst a parameterization of the fundamental frequency and en- ergy contour is obtained by calculating eleven features per frame: F0 is interpolated in unvoiced segments and decomposed by three band pass ﬁlters. F0, its components, and the time deriva- tives of those four functions yield eight F0 features which describe the F0 contour at that frame globally and locally. Furthermore three bands of a short-time FFT followed by median smoothing are used as energy features. The phrase boundary detector then views a window of (if possible) four syllables. Its output refers to the syllable boundary between the second and the third syllable nucleus (in the case of a 4-syllable window). Syllables are found by a syllabic nucleus detector based on energy features derived from the speech signal. For each window a large feature vector is constructed. A Gaussian distribution classiﬁer was trained to distinguish between all combinations of bound- ary types and tones. The classiﬁer output was then mapped on the the four classes B0, B2, B3, and B9. The a posteriori probabilities are used as conﬁdence measure. When taking the bound- ary with maximal probability the recognition rate for a test set of 30 minutes is 80.76, average recognition rate is 58.85. The focus detection module of INTARC works with a rule-based approach. The algorithm tries to solve focus recognition by global description of the utterance contour, in a ﬁrst approach rep- resented by the fundamental frequency F0. A reference line is computed by detecting signiﬁcant minima and maxima in the F0 contour. The average values between the maximum and minimum lines yield the global reference line. Focus accents occur mainly in the areas of steepest fall in the F0 course. Therefore, in the reference line the points with the highest negative gradient were determined ﬁrst in each utterance. To determine the position of the focus the nearest maximum in this region has been used as approximation. The recognition rate is 78.5 and the average recognition rate is 66.6. The focus detection mod- ule sends focus hypotheses to the semantic module and to the module for transfer and generation. In a recent approach, phrase boundaries from the detector described above where integrated in the algorithm. After optimization of the algorithm even higher rates are expected. As mentioned in the last section, one of the main beneﬁts of prosody in the INTARC system is the use of prosodic phrase boundaries inside the word lattice search. When calculating a prosody factor for an edge pair, we pick the WBH associated with the connect- ing vertex of the edges. This WBH forms a sequence of WBHs and word hypotheses if combined with the portions already spanned by the pair of edges. Tests for the contribution of the prosody factor to the overall search lead to the following results: The same recognition performance in terms of n best trees could be achieved using 20 less edges on the average. A lot of edges are constant in a given search space  namely those used for the representation of the original set of word hypotheses and the empty active rule edges which have a zero span. Counting only those edges which are built up dynamically by the search process a reduction of 65 was measured. 6 In INTARC, the transfer module performs a dialog act based translation. In a traditional deep analysis it gets its input (dialog act and feature structure) from the semantic evaluation mod- ule. In an additional path a ﬂat transfer is performed with the best word chain from the word recognition module and with focus information. During shallow processing the focus accents are aligned to words. If a focus is on a content word a probabilistically selected dialog act is chosen. This dialog act is then expanded to a translation enriched with possible information from the word chain. Flat transfer is only used when deep analysis fails. First results show that the focus-driven transfer produces correct  but sometimes reduced  results for about 50 of the data. For the other half of the utterances information is not sufﬁcient to get a translation; only 5 of the translations are absolutely wrong.. While the deep analysis uses prosody to reduce search space and disambiguate in cases of mul- tiple analyses, the shallow focus based translation can be viewed as directly driven by prosody. 5.2 Speaker Style A new issue in Verbmobils second phase are investigations on speaker style. It is well known that system performance depends on the perplexity of the language models involved. Consequently, one of the main problems is to reduce the perplexity of the models in question. The common way to approach this problem is to specialize the models by additional knowledge about con- texts. The traditional n-gram model uses a collection of conditional distributions instead of one single probability distribution. Normally, a ﬁxed length context of immediately preceding words is used. Since the length of the word contexts is bound by data and computational resources, practicable models could only be achieved by restricting the application domain of a system. Commonly used n-gram models deﬁne P(wC, D) where C is a context of preceding words and D is an application domain. But also ﬁner grained restrictions have been tested in the last decade, e.g. a cache-based n-gram [11]. Intuitively, every speaker has its own individual speaking style. The question is whether it is possible to take advantage of this fact. The ﬁrst step towards specialized speaker models is to prove whether sets of utterances sorted by speakers show signiﬁcant differences in the use of syntactic structure at all. So, ﬁrst of all the whole corpus has been tagged with POS-categories grounded on syntactic properties of words (for tagger and POS-categories see [14]). Using the whole corpus, we determined an empirical distribution Dall over these categories. In order to separate the corpus in typical and non typical speakers we checked the distribution Ds of every speaker s against Dall using the Chi-square test. While we cant say anything about the usage of syntax by non-typical speakers, there is evidence that typical speakers make a similar use of syntax in a rough sense. With a signiﬁcance level of 0.01 the test rejects 23.6 of the speakers. Bi- and trigram models were estimated on the basis of the typical speakers and on the whole corpus in comparison. On a test set of normal speakers only the specialized models showed a slightly higher perplexity than the more general models. In contrast to this the specialization explored with automatic clustering using the K-means method shows a slightly better perplexity on most of the test set speakers. As a distance measure we take difference of two bigrams. The relatively small improvement with specialized models is a result of the small amount of data. Even partitioning of the corpus into few classes leads to a lot of unseen pairs in the specialized bigrams. Hence a general model trained on a larger amount of data could produce better results. Using the results of the experiments above as a guideline we chose a clustering procedure using a different clustering criterion. The procedure is adapted from automatic word clustering [17, 12]. The goal of the procedure is to ﬁnd a partitioning such that the perplexity of the specialized models is minimized. To reduce the parameter problem we used a class-based n-gram instead of the word-based bigram. Class-based n-grams estimates the probability of a word sequence 7 w1 . . . wn by n  i1 P(wiC(wi))  P(C(wi)C(wi1)) bigram class model or n  i1 P(wiC(wi))  P(C(wi)C(wi2)C(wi1)) trigram class model where C(w) denotes the class of word w. P(wiC(wi)) is called the lexical part and P(C(wi)C(wi1)) resp. P(C(wi)C(wi2)C(wi1)) the grammatical part of the model. We per- formed three different experiments to get an expression how a speaking style affects the lexical and grammatical part: 1. 2POS test: P(wiC(wi)) is assumed to be invariant. Only the grammatical part P(C(wi)C(wi1)) is adapted to every cluster. 2. 3POS test: P(wiC(wi)) is assumed to be invariant. Only the grammatical part P(C(wi)C(wi1)C(wi2)) is adapted to every cluster. 3. POSword: Both parts are considered. First clustering tests showed good results: The best result was achieved by adapting both parts Reduction 2POS 6.5 3POS 1.9 POSWord 10 Table 1: Reduction of test set perplexity of the class model. This fact corresponds with the intuitive expectation that speaking style inﬂu- ences the selection of words and grammar rules. 6 Recognition Results for INTARC 2.0 For INTARC 2.0, a series of experiments has been carried out in order to also compare empirically an incremental and interactive system architecture with more traditional ones and to get hints for tuning individual components and their interactions. Basically, we tested three different module conﬁgurations: DM Decoder, Morphy (acoustic word recognition) DMP Decoder, Morphy, Lattice Parser (word recognition in parsed utterances) DMPS Decoder, Morphy, Lattice Parser, Semantic Module (word recognition in understood ut- terances) These conﬁgurations correspond to successively harder tasks, namely to recognize, to analyze and to understand. We used the NIST scoring program for word accuracy to gain comparable results. By doing this we gave preference to a well known and practical measure although we know that it is in some way inadequate. In a system like INTARC 2.0, the analysis tree is of much higher importance than the recovered string. With the general goal of spontaneous speech translation a good se- mantic representation for a string with word errors is more important than a good string with 8 a completely wrong reading. Because there does not yet exist a tree bank with correct readings for our grammar, we had no opportunity to measure something like a tree recognition rate or rule accuracy. The word accuracy results in DMP and DMPS can not be compared to word accuracy as usually applied to an acoustic decoder in isolation, whereas the DM values can be compared in this way. In DMP and DMPS we counted only those words as recognized which could be built into a valid parse from the beginning of the utterance. Words to the right, which could not be integrated into a parse, were counted as deletions  although they might have been correct in standard word accuracy terms. Our evaluation method is much harder than standard word accuracy, but it appears to be a good approximation to rule accuracy. What cannot be parsed is being counted as an error. The difference between DMP and DMPS is that a tree produced by the statistical approximation grammar can be ruled out when being rebuilt by uniﬁcation operations in semantic processing. The loss in recognition performance from DMP and DMPS corresponds to the quality of the statistical approximation. If the approximation grammar had a 100 tree recognition, there would be no gap between DMP and DMPS. The recognition rates of the three conﬁgurations were measured in three different contexts. The ﬁrst row shows the rates of normal bottom-up processing. In the second row, the results of the phrase boundary detector are used to disambiguate for syntax and semantics. The third row shows the results of the system in top-down mode; here no semantic evaluation is done because top-down predictions only affect the interface between SynParser and Recognizer. DM DMP DMPS Word Accuracy 93.9 83.3 47.5 WA with phrase boundary 93.9 84.0 48.6 WA in TD-Mode 94.0 83.4  6.1 Conclusions Splitting composite nouns to reduce the recognizer lexicon shows good results. Search and re- building performed by the morphology module is implemented as a ﬁnite state automaton, so there is no great loss in performance. Incremental recognition is as good as as the standard de- coding algorithms, but the lattices are up to ten times larger. This causes a performance problem for the parser. So we use an approximation of an HPSG-Grammar for search such that syntac- tic analysis becomes more or less a second decoding step. By regarding a wider context, we even reduce the recognition gap between syntax and semantics in comparison with our previous uniﬁcation-based syntax parser (see [18, 19]). For practical usability the tree-recognition rate must be improved. This can be achieved with a bigger training set. The dialogues we used contained only 83 utterances. Further improvement can be achieved by a larger context during training to get a better approximation of the trees built by the uniﬁcation grammar. Prediction of words seems to have no inﬂuence on the recognition rate. This is a consequence of the underlying domain. Since the HSPG grammar is written for spontaneous speech, nearly every utterance should be accepted. The grammar gives no restrictions on possible completions of an utterance. Restrictions can be only obtained by a narrow beam-bound when compiling the prediction table. But this leads to a lower recognition rate because some correct words are pruned. Acknowledgements. We are grateful to all our colleagues within the Verbmobil subproject on Architecture from the universities of Bielefeld, Bonn, Hamburg, and from DFKI Saarbrucken without whose contributions within the last four years this article could not have been written. 9 References [1] Althoff, F., Drexel, G., Lungen, H., Pampel, M., and Schillo, Ch.: The Treatment of Compounds in a Morphological Component for Speech Recognition. In: Gibbon, D. (Ed.): Natural Language Processing and Speech Technology. Results of the 3rd KONVENS Conference, Berlin: Mouton de Gruyter, October 1996 [2] Amtrup, J.: ICE INTARC Communication Environment: Users Guide and Reference Manual. Version 1.4. Verbmobil Technical Document 14, Univ. of Hamburg, December 1995 [3] Amtrup, J., Benra, J.: Communication in large distributed AI systems for natural language process- ing. Proc. of COLING-96, Kopenhagen, August 1996, 3540 [4] Amtrup, J., Drexel, G., Gorz, G., Pampel, M., Spilker, J. and Weber, H.: The parallel time- synchronous speech-to-speech system INTARC 2.0. Submitted to ACL-97 [5] Carter, D.: Improving Language Models by Clustering Training Sentences. Proc. of ANLP 94, Stuttgart, Germany, 1994. Extended version in http:xxx.lanl.govcmp-lg [6] Gorz, G., Kesseler, M., Spilker, J. and Weber, H.: Research on Architectures for Integrated SpeechLanguage Systems in Verbmobil. Proc. of COLING-96, Kopenhagen, August 1996 [7] Hauenstein, A., Weber, H.: An investigation of tightly coupled time synchronous speech language interfaces. Proceedings of KONVENS-94, Vienna, Austria. Berlin: Springer, September 1994 [8] Kasper, W. and Krieger, H.-U.: Integration of prosodic and grammatical information in the analysis of dialogs. In: Gorz, G., Holldobler, S. (Ed.): Proceedings of the 20th German Annual Conference on Artiﬁcial Intelligence, KI-96, Dresden. Berlin: Springer (LNCS) 1996 [9] Kasper, W. and Krieger, H.-U.: Modularizing codescriptive grammars for efﬁcient parsing. Proc. of COLING-96, Kopenhagen, August 1996, 628633. [10] Kasper, W., Krieger, H.-U., Spilker J., and Weber, H.: From word hypotheses to logical form: An efﬁcient interleaved approach. In: Gibbon, D. (Ed.): Natural Language Processing and Speech Technology. Results of the 3rd KONVENS Conference, Berlin: Mouton de Gruyter, October 1996, 7788. [11] Kuhn, R. and DeMori, R: A cache-based natural language model for speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6), June 1990, 570583 [12] Martin, S., Liermann, J., and Ney, H.:Algorithms for bigram and trigram word clustering. Eu- rospeech 95, Madrid, Spain, 1995, 12531256. [13] Petzold, A.: Strategies for focal accent detection in spontaneous speech. Proc. 13th ICPhS Stock- holm, Vol. 3, 1995, 672675 [14] Schmid, H.: Improvements in Part-of-Speech Tagging with an Application to German. http:www.ims.uni-stuttgart.deToolsDecisionTreeTagger.html, 1995. [15] Strom, V.: Detection of accents, phrase boundaries and sentence modality in German with prosodic features. Proc. EUROSPEECH-95, Madrid, 1995, 20392041 [16] Strom, V.: Whats in the pure prosody? Proc. ICSLP 96, Philadelphia, 1996 [17] Ueberla, J.P.: An Extended Clustering Algorithm for Statistical Language Models, 1994, Nr.: 9412003 E-Print Archive: http:xxx.lanl.govcmp-lg. [18] Weber, H.: Time Synchronous Chart Parsing of Speech Integrating Uniﬁcation Grammars with Statistics. Speech and Language Engineering, Proceedings of the Eighth Twente Workshop on Language Technology, (L. Boves, A. Nijholt, Ed.), Twente, 1994, 107119 10 [19] Weber, H.: LR-inkrementelles probabilistisches Chartparsing von Worthypothesenmengen mit Uniﬁkationsgrammatiken: Eine enge Kopplung von Suche und Analyse. Ph.D. Thesis, University of Hamburg, 1995, Verbmobil Report 52. [20] Weber, H., Spilker, J., Gorz, G. (1997): Parsing N Best Trees from a Word Lattice. In: Nebel, B. (Ed). Advances in Artiﬁcial Intelligence. Proceedings of the 21st German Annual Conference on Artiﬁcial Intelligence, KI-97, Freiburg. Berlin: Springer (LNCS) 1997 11"
}