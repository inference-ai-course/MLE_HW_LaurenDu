{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Demo: arXiv cs.CL Papers\n",
    "\n",
    "This notebook demonstrates the complete RAG (Retrieval-Augmented Generation) pipeline for searching through arXiv computer science computational linguistics papers.\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. **PDF Text Extraction** - Extract text from 50 PDF papers\n",
    "2. **Text Chunking** - Split papers into meaningful segments (250-512 tokens)\n",
    "3. **Embedding Generation** - Create dense vector embeddings using sentence-transformers\n",
    "4. **FAISS Indexing** - Build searchable index of embeddings\n",
    "5. **Retrieval** - Query the index to find relevant passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./src')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pdf_processor import PDFProcessor\n",
    "from chunker import TextChunker\n",
    "from embedder import EmbeddingIndexer\n",
    "from retriever import RAGRetriever\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process PDFs if not already done\n",
    "if not os.path.exists('./data/processed_documents.json'):\n",
    "    print(\"Processing PDF files...\")\n",
    "    processor = PDFProcessor(\n",
    "        pdf_directory=\"./PDFs\",\n",
    "        output_directory=\"./data\"\n",
    "    )\n",
    "    documents = processor.process_all_pdfs()\n",
    "else:\n",
    "    print(\"Loading existing processed documents...\")\n",
    "    with open('./data/processed_documents.json', 'r', encoding='utf-8') as f:\n",
    "        documents = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Sample document keys: {list(documents.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks if not already done\n",
    "if not os.path.exists('./data/chunks.json'):\n",
    "    print(\"Creating text chunks...\")\n",
    "    chunker = TextChunker(chunk_size=512, overlap_size=50)\n",
    "    chunks = chunker.process_documents(documents, \"./data\")\n",
    "else:\n",
    "    print(\"Loading existing chunks...\")\n",
    "    with open('./data/chunks.json', 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Sample chunk: {chunks[0]['chunk_id']}\")\n",
    "print(f\"Sample text (first 200 chars): {chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Embedding Generation and FAISS Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and index if not already done\n",
    "if not os.path.exists('./data/faiss_index.bin'):\n",
    "    print(\"Creating embeddings and FAISS index...\")\n",
    "    indexer = EmbeddingIndexer(output_directory=\"./data\")\n",
    "    index, chunks = indexer.build_index_from_chunks(\"./data/chunks.json\")\n",
    "    print(f\"Index created with {index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"FAISS index already exists!\")\n",
    "    indexer = EmbeddingIndexer(output_directory=\"./data\")\n",
    "    index, chunk_metadata = indexer.load_index_and_metadata()\n",
    "    print(f\"Loaded index with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: RAG Retrieval Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever\n",
    "retriever = RAGRetriever(data_directory=\"./data\")\n",
    "print(\"RAG Retriever initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_display(query, k=3):\n",
    "    print(f\"\\n🔍 Searching for: '{query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = retriever.search(query, k=k)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n📄 Result {i}\")\n",
    "        print(f\"   Document: {result['document_id']}\")\n",
    "        print(f\"   Chunk: {result['chunk_id']}\")\n",
    "        print(f\"   Similarity Score: {result['similarity_score']:.4f}\")\n",
    "        print(f\"   Token Count: {result['token_count']}\")\n",
    "        print(\"\\n   Text:\")\n",
    "        text = result['text']\n",
    "        if len(text) > 300:\n",
    "            print(f\"   {text[:300]}...\")\n",
    "        else:\n",
    "            print(f\"   {text}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries for cs.CL (Computational Linguistics) papers\n",
    "example_queries = [\n",
    "    \"transformer architecture\",\n",
    "    \"attention mechanism\",\n",
    "    \"natural language processing\",\n",
    "    \"machine translation\",\n",
    "    \"language models\"\n",
    "]\n",
    "\n",
    "# Run searches for all example queries\n",
    "for query in example_queries:\n",
    "    search_and_display(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive search - you can modify this query\n",
    "custom_query = \"BERT embeddings\"\n",
    "search_and_display(custom_query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunks for analysis\n",
    "with open('./data/chunks.json', 'r', encoding='utf-8') as f:\n",
    "    chunks_data = json.load(f)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df = pd.DataFrame(chunks_data)\n",
    "\n",
    "print(\"📊 Dataset Statistics:\")\n",
    "print(f\"Total Documents: {df['document_id'].nunique()}\")\n",
    "print(f\"Total Chunks: {len(df)}\")\n",
    "print(f\"Average Chunks per Document: {len(df) / df['document_id'].nunique():.1f}\")\n",
    "print(f\"Average Token Count: {df['token_count'].mean():.1f}\")\n",
    "print(f\"Min Token Count: {df['token_count'].min()}\")\n",
    "print(f\"Max Token Count: {df['token_count'].max()}\")\n",
    "\n",
    "# Distribution of chunks per document\n",
    "chunks_per_doc = df.groupby('document_id').size()\n",
    "print(f\"\\nChunks per document distribution:\")\n",
    "print(chunks_per_doc.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a complete RAG pipeline for searching through arXiv cs.CL papers:\n",
    "\n",
    "✅ **PDF Processing**: Extracted text from 50 research papers  \n",
    "✅ **Text Chunking**: Created overlapping chunks of 250-512 tokens  \n",
    "✅ **Embedding Generation**: Used sentence-transformers to create dense vectors  \n",
    "✅ **FAISS Indexing**: Built efficient search index  \n",
    "✅ **Retrieval**: Implemented semantic search with similarity scoring  \n",
    "\n",
    "The system can now answer questions about computational linguistics research by finding the most relevant passages from the paper collection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}